title,abstract,authors,affiliations,citations,source,url,acknowledgments,data_descriptions,disclosure_review,rdc_mentions,dataset_mentions,project_id,project_pi,project_rdc,project_status,project_start_year,project_end_year,project_abstract
"The Impact of Geopolitical Conflicts on Trade, Growth, and Innovation","Geopolitical conflicts have increasingly been a driver of trade policy. We
study the potential effects of global and persistent geopolitical conflicts on
trade, technological innovation, and economic growth. In conventional trade
models the welfare costs of such conflicts are modest. We build a multi-sector
multi-region general equilibrium model with dynamic sector-specific knowledge
diffusion, which magnifies welfare losses of trade conflicts. Idea diffusion is
mediated by the input-output structure of production, such that both sector
cost shares and import trade shares characterize the source distribution of
ideas. Using this framework, we explore the potential impact of a ""decoupling
of the global economy,"" a hypothetical scenario under which technology systems
would diverge in the global economy. We divide the global economy into two
geopolitical blocs -- East and West -- based on foreign policy similarity and
model decoupling through an increase in iceberg trade costs (full decoupling)
or tariffs (tariff decoupling). Results yield three main insights. First, the
projected welfare losses for the global economy of a decoupling scenario can be
drastic, as large as 15% in some regions and are largest in the lower income
regions as they would benefit less from technology spillovers from richer
areas. Second, the described size and pattern of welfare effects are specific
to the model with diffusion of ideas. Without diffusion of ideas the size and
variation across regions of the welfare losses would be substantially smaller.
Third, a multi-sector framework exacerbates diffusion inefficiencies induced by
trade costs relative to a single-sector one.","['Carlos Góes', 'Eddy Bekkers']",[],0,arXiv,http://arxiv.org/abs/2203.12173v2,False,True,False,False,False,5,J Bradford Jensen,Boston,Completed,2001,2004.0,"The project will proceed in three phases. The first will enhance the export and import information on Economic Censuses and Surveys. We will develop and test linkages between transaction level Foreign Trade data and Economic Census and Survey data and compare the links developed by CES to those developed by the Census Bureau’s Foreign Trade Division (FTD). If improvements in linkages methods are identified, we will provide advice to FTD. The second phase will analyze transaction level detail to examine changes in foreign trade transactions, focusing initially on related party transactions, potential underreporting, and impact of FTD outreach efforts. This component of the project will focus on identifying reasons for such changes. The third phase of the project will develop empirical and analytical framework to investigate the impact of trade on the U.S. economy - focusing on how firms allocate economic activity between domestic and foreign production and the impact of this on the domestic economy (including workers and regional economies). This component makes use of the data developed in the previous phases to develop new estimates of the impact of foreign trade on U.S. industries. It will examine how imports and exports affect domestic production, employment, and productivity. It will also examine how firm responses to trade affect local labor market outcomes."
"Quantifying the status of economies in international crop trade
  networks: An correlation structure analysis of various node-ranking metrics","International food trade is a growing complement to gaps in domestic food
supply and demand, but it is vulnerable to disruptions due to some unforeseen
shocks. This paper assembles the international crop trade networks using maize,
rice, soybean, and wheat trade data sets from 1986 to 2020. We assess the
importance of economies using multidimensional node importance metrics. We
analyze the correlation structure of different node important metrics based on
the random matrix theory and incorporate 20 metrics into a single metric. We
find that some metrics have many similarities and dissimilarities, especially
for metrics based on the same trade flow directions. We also find that European
economies have a significant impact on the iCTNs. Additionally, economies with
poor crop production play a major role in import trade, whereas economies with
higher food production or smaller populations are crucial to export trade. Our
findings have practical implications for identifying key economies in the
international crop trade networks, preventing severe damage to the food trade
system caused by trade disruptions in some economies, maintaining the stability
of the food supply, and ensuring food security.","['Yin-Ting Zhang', 'Wei-Xing Zhou']",[],0,arXiv,http://arxiv.org/abs/2303.00669v2,False,True,False,False,False,5,J Bradford Jensen,Boston,Completed,2001,2004.0,"The project will proceed in three phases. The first will enhance the export and import information on Economic Censuses and Surveys. We will develop and test linkages between transaction level Foreign Trade data and Economic Census and Survey data and compare the links developed by CES to those developed by the Census Bureau’s Foreign Trade Division (FTD). If improvements in linkages methods are identified, we will provide advice to FTD. The second phase will analyze transaction level detail to examine changes in foreign trade transactions, focusing initially on related party transactions, potential underreporting, and impact of FTD outreach efforts. This component of the project will focus on identifying reasons for such changes. The third phase of the project will develop empirical and analytical framework to investigate the impact of trade on the U.S. economy - focusing on how firms allocate economic activity between domestic and foreign production and the impact of this on the domestic economy (including workers and regional economies). This component makes use of the data developed in the previous phases to develop new estimates of the impact of foreign trade on U.S. industries. It will examine how imports and exports affect domestic production, employment, and productivity. It will also examine how firm responses to trade affect local labor market outcomes."
"Barriers to Integration: Physical Boundaries and the Spatial Structure
  of Residential Segregation","Despite modest declines in residential segregation levels since the Civil
Rights Era, segregation remains a defining feature of the U.S. landscape. This
study highlights the importance of considering physical barriers--features of
the urban environment that disconnect locations--when measuring segregation. We
use population and geographic data for 20 U.S. Rustbelt cities from the 2010
decennial census and a novel approach for measuring and analyzing segregation
that incorporates the connectivity of roads and the excess distance imposed by
physical barriers, such as highways, railroad tracks, and dead-end streets. We
find that physical barriers divide urban space in ways that reinforce or
exacerbate segregation, but there is substantial variation in the extent to
which they increase segregation both within and across these cities and for
different ethnoracial groups. By uncovering a new source of variation in the
segregation experienced by city residents, the findings have implications for
understanding the mechanisms that contribute to the persistence of segregation
and the consequences of segregation.","['Elizabeth Roberto', 'Jackelyn Hwang']",[],0,arXiv,http://arxiv.org/abs/1509.02574v3,False,True,False,False,True,9,Judith K Hellerstein,Washington,Completed,2001,2005.0,"The proposed research will accomplish two goals.  The first goal is the completion of the construction of a unique, very large, representative data set on workers, employers, and employer characteristics, where the employer characteristics are obtained in part from information on the multiple workers working for each employer.  For each individual worker in the data, this data set will also include detailed information on both residential location and geographic location of the place of work.  This matched employee-employer data set will represent a significant improvement upon existing matched data sets for the U.S., and result in gains to the Census Bureau in terms of new data products and improvements in current data programs.  The second goal is to complete a number of projects exploiting these data to bring new evidence to bear on the alternative supply-side explanations of racial, ethnic, and language differences in labor market outcomes, and in so doing not only test these explanations but also provide an assessment of their importance relative to demand-side theories."
A Machine Learning Approach to Improving Occupational Income Scores,"Historical studies of labor markets frequently lack data on individual
income. The occupational income score (OCCSCORE) is often used as an
alternative measure of labor market outcomes. We consider the consequences of
using OCCSCORE when researchers are interested in earnings regressions. We
estimate race and gender earnings gaps in modern decennial Censuses as well as
the 1915 Iowa State Census. Using OCCSCORE biases results towards zero and can
result in estimated gaps of the wrong sign. We use a machine learning approach
to construct a new adjusted score based on industry, occupation, and
demographics. The new income score provides estimates closer to earnings
regressions. Lastly, we consider the consequences for estimates of
intergenerational mobility elasticities.","['Martin Saavedra', 'Tate Twinam']",[],0,arXiv,http://arxiv.org/abs/1704.08299v3,False,True,False,False,True,9,Judith K Hellerstein,Washington,Completed,2001,2005.0,"The proposed research will accomplish two goals.  The first goal is the completion of the construction of a unique, very large, representative data set on workers, employers, and employer characteristics, where the employer characteristics are obtained in part from information on the multiple workers working for each employer.  For each individual worker in the data, this data set will also include detailed information on both residential location and geographic location of the place of work.  This matched employee-employer data set will represent a significant improvement upon existing matched data sets for the U.S., and result in gains to the Census Bureau in terms of new data products and improvements in current data programs.  The second goal is to complete a number of projects exploiting these data to bring new evidence to bear on the alternative supply-side explanations of racial, ethnic, and language differences in labor market outcomes, and in so doing not only test these explanations but also provide an assessment of their importance relative to demand-side theories."
"Assessing Statistical Disclosure Risk for Differentially Private,
  Hierarchical Count Data, with Application to the 2020 U.S. Decennial Census","We propose Bayesian methods to assess the statistical disclosure risk of data
released under zero-concentrated differential privacy, focusing on settings
with a strong hierarchical structure and categorical variables with many
levels. Risk assessment is performed by hypothesizing Bayesian intruders with
various amounts of prior information and examining the distance between their
posteriors and priors. We discuss applications of these risk assessment methods
to differentially private data releases from the 2020 decennial census and
perform simulation studies using public individual-level data from the 1940
decennial census. Among these studies, we examine how the data holder's choice
of privacy parameter affects the disclosure risk and quantify the increase in
risk when a hypothetical intruder incorporates substantial amounts of
hierarchical information.","['Zeki Kazan', 'Jerome Reiter']",[],0,arXiv,http://arxiv.org/abs/2204.04253v2,False,True,False,False,True,9,Judith K Hellerstein,Washington,Completed,2001,2005.0,"The proposed research will accomplish two goals.  The first goal is the completion of the construction of a unique, very large, representative data set on workers, employers, and employer characteristics, where the employer characteristics are obtained in part from information on the multiple workers working for each employer.  For each individual worker in the data, this data set will also include detailed information on both residential location and geographic location of the place of work.  This matched employee-employer data set will represent a significant improvement upon existing matched data sets for the U.S., and result in gains to the Census Bureau in terms of new data products and improvements in current data programs.  The second goal is to complete a number of projects exploiting these data to bring new evidence to bear on the alternative supply-side explanations of racial, ethnic, and language differences in labor market outcomes, and in so doing not only test these explanations but also provide an assessment of their importance relative to demand-side theories."
Being at the core: firm product specialisation,"We propose a novel measure to investigate firms' product specialisation:
product coreness, that captures the centrality of exported products within the
firm's export basket. We study product coreness using firm-product level data
between 2018 and 2020 for Colombia, Ecuador, and Peru. Three main findings
emerge from our analysis. First, the composition of firms' export baskets
changes relatively little from one year to the other, and products far from the
firm's core competencies, with low coreness, are more likely to be dropped.
Second, higher coreness is associated with larger export flows at the firm
level. Third, such firm-level patterns also have implications at the aggregate
level: products that are, on average, exported with higher coreness have higher
export flows at the country level, which holds across all levels of product
complexity. Therefore, the paper shows that how closely a product fits within a
firm's capabilities is important for economic performance at both the firm and
country level. We explore these issues within an econometric framework, finding
robust evidence both across our three countries and for each country
separately.","['Filippo Bontadini', 'Mercedes Campi', 'Marco Dueñas']",[],0,arXiv,http://arxiv.org/abs/2302.02767v2,False,True,False,False,False,11,Pinar Celikkol Geylani,CMU,Completed,2000,2001.0,"Productivity advancement has been a major contributing factor to economic growth in the postwar U.S. economy and more accurate measurement of productivity and growth will assist future industry and government decision-making activities. The proposed research address some fundamental issues related to productivity and growth in the food-manufacturing sector by focusing on the stimuli to productivity growth. By focusing on the investment patterns of Food and Kindred Products Industry, this project will
• Examine the behavior of capital adjustment patterns of plants and firms
• Examine the dynamic factor demand decisions and resource allocations of the constituent units of the firm
• Examine long-argued capital measurement problems surrounding the time series data
• Investigate implications of the empirical evidence for the shape of the adjustment cost function
• Analyze productivity changes of plants and firms under these capital adjustment cost and investment patterns
• Identify and distinguish the gains from investment spikes as indicative of the lifting the plateau to a new one that can lead to longer periods of productivity growth from the productivity gains within an investment period. Thus this proposal intends to take the literature one step further by investigating how capital and its adjustments influence productivity.
Generally, plant-level studies analyzing productivity dynamics concentrate on the overall manufacturing plants in U.S., while studies analyzing productivity issues in the Food and Kindred Products industry primarily concentrate on the aggregate level. There are currently no studies that analyze the productivity dynamics at the most disaggregated plant level, considering all product subgroups of the food-manufacturing sector. With this study, we will be able to focus on investment patterns and lumpy capital adjustment costs separately analyzing all sub-industries of the food sector, which allows us to capture extensive heterogeneity within and across industries. The Food and Kindred Products industry is an excellent candidate for investigating lumpy investment patterns as the industry has become increasingly capital-intensive, and high-tech over the past few decades in the processing, packaging, and marketing of food products (Morrison, 1997). Therefore, the data required for this research is the U.S. Census’ Longitudinal Research Database (LRD) for the years 1963-1999 containing the annual establishment level production data for the manufacturing plants and firms specifically in Food and Kindred Products Industry. This non-publicly available Census data is crucial for understanding the productivity dynamics, the forces of productivity transition and the performance of the industry micro-and macro-level. The LRD has detailed information on products produced, employment and capital investment, labor, and material and energy at both the plant-level and firm level, as well as information on whether firms are single-plant or multi-plant firms. Additionally variables on the size of the establishments by employment and the age of the plants can be defined. For an accurate measurement of productivity growth, we need to consider economies of scale, productivity enhancing changes arising from factors such as experience, learning-by-doing, increased knowledge, new innovations, better techniques for producing output, measuring relative capital intensity of production technology, existence of quasi-fixity of inputs, and the adjustment cost of these factors. As capital input is a significant component of total cost, analyzing the behavior of quasi-fixed factors in the measurement of productivity is crucial, especially if the firms require massive amounts of capital in the form of plant and equipment. Therefore, confidential Census data is essential to investigate the effect of capital adjustment on productivity and to develop a method, which can control for plant and firm fixed effects in measuring the productivity at plant and firm level. This research will provide benefit to the Census Bureau by developing a new improved measure of economic growth associated with investment spikes. This methodology removes procyclical biases associated with the business cycle. Thus it has the potential to propose changes in the questionnaire design and collection methodology to improve the economic content of the information gathered by the Annual Survey of Manufacturers (ASM) and the Census of Manufacturers (CM) in the area of capital input series and total factor productivity at both the plant and firm level. By computing productivity at both levels, the researcher will be able to separately estimate both plant level productivity and firm level productivity in relation to aggregate level productivity. Previous studies have shown that aggregate growth measures may be significantly reduced when using the plant level data. Therefore the disaggregated measure of total factor productivity generated by this research will assist the Census Bureau in determining and evaluating whether the aggregation problem found in the literature is due to underlying economic forces or if it is possibly due to the questionnaire design or collection methodology."
Audit fees in auditor switching,"The auditor work is examining that a company's financial statements
faithfully reflect its financial situation. His wage, the audit fees, are not
fixed among all companies, but can be affected by the financial and structural
characteristics of the company, as well as the characteristics of the firm he
belongs to. Another factor that may affect his wage in an auditor switching,
which can be resulted from changes in the company that may influence the fees.
This paper examines the effect nature of the auditor switching on his wage, and
the factors of the company characteristics and the economy data which determine
the wage at switching. A product of the research are tools for predicting and
evaluating the auditor wage at switching. These tools are important for the
auditor himself, but also for the company manager to correctly determine the
wage due to the possibility that the quality of the audit work depends on its
fees. Two main results are obtained. First, the direction of the wage change in
the switching year depends on the economic stability of the economy. Second,
the switching effect on the direction and the change size in wage depends on
the change size in the company characteristics before and after switching - a
large change versus a stable one. We get that forecasting the change size in
wage for companies with a larger change is their characteristics is paralleled
to forecasting a wage increasing. And vice versa, forecasting the change size
in wage for companies with a stable change in their characteristics is
paralleled to forecasting a wage decreasing. But, whereas the former can be
achieved based on the company characteristics and macroeconomics factors, the
predictably of these characteristics and factors is negligible for the letter.",['Sarit Agami'],[],0,arXiv,http://arxiv.org/abs/2311.08250v1,False,True,False,False,False,18,Edith A Wiarda,Michigan,Completed,2006,2006.0,"Prior work using the LRD has demonstrated considerable plant-level heterogeneity in productivity (and, to a lesser extent, wages), and that much of this ""plant effect"" persists over time. This project examines those plants that experience large movement in their relative productivity, or in their relative wages, over the LRD time period. In particular, we wish to estimate the likelihood of sizeable (and consistent) movement in relative productivity or wages as a function of industry, plant size, firm size, sales growth, and geography (urban/rural, region). Besides estimating statistical models, we wish to construct a 5x5 tabulation of ""beginning-of-period quintile"" versus ""end-of-period quintile"" for productivity and for wages."
"A duopoly preemption game with two alternative stochastic investment
  choices","This paper studies a duopoly investment model with uncertainty. There are two
alternative irreversible investments. The first firm to invest gets a monopoly
benefit for a specified period of time. The second firm to invest gets
information based on what happens with the first investor, as well as cost
reduction benefits. We describe the payoff functions for both the leader and
follower firm. Then, we present a stochastic control game where the firms can
choose when to invest, and hence influence whether they become the leader or
the follower. In order to solve this problem, we combine techniques from
optimal stopping and game theory. For a specific choice of parametres, we show
that no pure symmetric subgame perfect Nash equilibrium exists. However, an
asymmetric equilibrium is characterized. In this equilibrium, two disjoint
intervals of market demand level give rise to preemptive investment behavior of
the firms, while the firms otherwise are more reluctant to be the first mover.","['Kristina Rognlien Dahl', 'Espen Stokkereit']",[],0,arXiv,http://arxiv.org/abs/1902.11009v1,False,True,False,False,False,22,Chad Shirley,UCLA,Completed,2003,2004.0,"This project will examine changes in plant-level inventory behavior over the past three decades.  In particular, this work will quantitatively estimate the relationship between changes in plant inventory levels and investments in the construction and maintenance of public roads and highways over the past three decades.  This project will also examine and control for other influences on inventory behavior, including trends in the use of information technology, changes in inventory management practices, and product proliferation, and their interaction with this transportation investment."
Online busy time scheduling with flexible jobs,"We present several competitive ratios for the online busy time scheduling
problem with flexible jobs. The busy time scheduling problem is a fundamental
scheduling problem motivated by energy efficiency with the goal of minimizing
the total time that machines with multiple processors are enabled. In the busy
time scheduling problem, an unbounded number of machines is given, where each
machine has $g$ processors. No more than $g$ jobs can be scheduled
simultaneously on each machine. A machine consumes energy whenever at least one
job is scheduled at any time on the machine. Scheduling a single job at some
time $t$ consumes the same amount of energy as scheduling $g$ jobs at time $t$.
In the online setting, jobs are revealed when they are released.
  We consider the cases where $g$ is unbounded and bounded. In this paper, we
revisit the bounds of the unbounded general setting from the literature and
tighten it significantly. We also consider agreeable jobs. For the bounded
setting, we show a tightened upper bound. Furthermore, we show the first
constant competitive ratio in the bounded setting that does not require
lookahead.","['Susanne Albers', 'G. Wessel van der Heijden']",[],0,arXiv,http://arxiv.org/abs/2405.08595v1,False,True,False,False,False,23,David C Ribar,Washington,Completed,2000,2001.0,"This investigation will examine the effects that expansions, contractions, openings, and closings of different-sized establishments in local labor markets have on low-skill employment and wages.  It will first use establishment-specific information from the Business Information Tracking System (BITS) to characterize job flows separately for small and large businesses within different industries within counties.  The investigation will next link its constructed local area job flow measures with individual-level data on earnings, employment, and other personal attributes for low-skill workers from the Sample Edited Detail File (SEDF) of the 1990 Decennial Census and the 1991-97 Annual Demographic files of the Current Population Survey (CPS).  It will regress the individual-level wage and employment outcomes against the local job flow measures, controlling for time- and area-specific fixed effects and observed personal characteristics such as age and race. The investigation will extend recent work by the Principal Investigator on the effects of local employment conditions on skill-specific outcomes by characterizing employment conditions within very detailed industries, distinguishing between different types of job flows, and distinguishing between different-sized establishments."
Emission impossible: Balancing Environmental Concerns and Inflation,"We provide a theoretical framework to examine how carbon pricing policies
influence inflation and to estimate the policy-driven impact on goods prices
from achieving net-zero emissions. Firms control emissions by adjusting
production, abating, or purchasing permits, and these strategies determine
emissions reductions that affect the consumer price index. We first examine an
emissions-regulated economy, solving the market equilibrium under any dynamic
allocation of allowances set by the regulator. Next, we analyze a regulator
balancing emission reduction and inflation targets, identifying the optimal
allocation when accounting for both environmental and inflationary concerns. By
adjusting penalties for deviations from these targets, we demonstrate how
regulatory priorities shape equilibrium outcomes. Under reasonable model
parameterisation, even when considerable emphasis is placed on maintaining
inflation at acceptable levels or grant lower priority to emissions reduction
targets, the costs associated with emission deviations still exceed any savings
from marginally lower inflation. Emission reduction goals should remain the
primary focus for policymakers.","['René Aïd', 'Maria Arduca', 'Sara Biagini', 'Luca Taschini']",[],0,arXiv,http://arxiv.org/abs/2501.16953v1,False,True,False,False,False,28,Jhih-shyang Shih,Washington,Completed,2002,2004.0,"The degree of heterogeneity among pollution sources in their marginal costs of pollution abatement may be the single most important factor affecting the relative cost of economic incentive-based versus other approaches to achieving environmental performance.  We will develop econometrically estimable, structural models of plant-level pollution abatement costs that explicitly allow for significant heterogeneity.  After estimating these models using Census data, they will first assess the degree and sources of cost heterogeneity, and then use simulations to measure the potential gains from using economic incentives relative to other regulatory approaches."
"Product Differentiation and Geographical Expansion of Exports Network at
  Industry level","Industries can enter one country first, and then enter its neighbors'
markets. Firms in the industry can expand trade network through the export
behavior of other firms in the industry. If a firm is dependent on a few
foreign markets, the political risks of the markets will hurt the firm. The
frequent trade disputes reflect the importance of the choice of export
destinations. Although the market diversification strategy was proposed before,
most firms still focus on a few markets, and the paper shows reasons.In this
paper, we assume the entry cost of firms is not all sunk cost, and show 2 ways
that product heterogeneity impacts extensive margin of exports theoretically
and empirically. Firstly, the increase in product heterogeneity promotes the
increase in market power and profit, and more firms are able to pay the entry
cost. If more firms enter the market, the information of the market will be
known by other firms in the industry. Firms can adjust their behavior according
to other firms, so the information changes entry cost and is not sunk cost
completely. The information makes firms more likely to entry the market, and
enter the surrounding markets of existing markets of other firms in the
industry. When firms choose new markets, they tend to enter the markets with
few competitors first.Meanwhile, product heterogeneity will directly affect the
firms' network expansion, and the reduction of product heterogeneity will
increase the value of peer information. This makes firms more likely to entry
the market, and firms in the industry concentrate on the markets.",['Xuejian Wang'],[],0,arXiv,http://arxiv.org/abs/2012.07008v1,False,True,False,False,False,36,Catherine Armington,Washington,Completed,2001,2002.0,"This project will examine service sector data, focusing on new firm entries. Evaluate the completeness and apparent accuracy (partially judged by consistency over time) of initial industry and geographic coding from the SSEL.  It will analyze survival patterns of single-employee new firms versus multi-employee new firms.  It will also analyze probable predecessors of apparent large new firms, especially those in the rapidly growing employee-leasing business. Extract data on credible new service firms and their older competitors, for each year from 1990 to 1997, for each of 394 Labor Market Areas, and aggregate into 6-8 subsectors, based on their primary market.  Distinguishing seasoned startups (surviving at least 3 years) from failed startups, estimate models (using publicly available socioeconomic data) to help explain regional differences in startup rates, survival rates, and employment growth rates."
"Can ethnic tolerance curb self-reinforcing school segregation? A
  theoretical Agent Based Model","Schelling and Sakoda prominently proposed computational models suggesting
that strong ethnic residential segregation can be the unintended outcome of a
self-reinforcing dynamic driven by choices of individuals with rather tolerant
ethnic preferences. There are only few attempts to apply this view to school
choice, another important arena in which ethnic segregation occurs. In the
current paper, we explore with an agent-based theoretical model similar to
those proposed for residential segregation, how ethnic tolerance among parents
can affect the level of school segregation. More specifically, we ask whether
and under which conditions school segregation could be reduced if more parents
hold tolerant ethnic preferences. We move beyond earlier models of school
segregation in three ways. First, we model individual school choices using a
random utility discrete choice approach. Second, we vary the pattern of ethnic
segregation in the residential context of school choices systematically,
comparing residential maps in which segregation is unrelated to parents' level
of tolerance to residential maps reflecting their ethnic preferences. Thirdly,
we introduce heterogeneity in tolerance levels among parents belonging to the
same group. Our simulation experiments suggest that ethnic school segregation
can be a very robust phenomenon, occurring even when about half of the
population prefers mixed to segregated schools. However, we also identify a
sweet spot in the parameter space in which a larger proportion of tolerant
parents makes the biggest difference. This is the case when the preference for
nearby schools weighs heavily in parents' utility function and the residential
map is only moderately segregated. Further experiments are presented that
unravel the underlying mechanisms.","['Lucas Sage', 'Andreas Flache']",[],0,arXiv,http://arxiv.org/abs/2006.13531v1,False,True,False,False,False,44,John Mark Ellis,UCLA,Completed,2001,2004.0,"We propose a three-year study of the residential choices of interracial partners to expand our understanding, empirically and theoretically, of multiraciality and interracial partnerships in American metropolitan areas. The basic questions we intend to address are as follows: Do interracial families live in segregated neighborhoods? Do they live in neighborhoods dominated by a particular race/ethnic group? Does this depend on the race of the male partner or the female partner? Do the social class positions of the partners affect the couple’s residential choices? These initial steps in the investigation of the residential geography of interracial households set the stage for us to address a set of derivative questions associated with the permanence of ethnic and racial boundaries. In particular, what are the implications of the geography of residential choice for the childbearing decisions and the racial/ethnic identity of children of interracial couples? To answer these questions we will make use of two data sets. The first contains detailed individual level information from a special version of the 1990 Census of Population and Housing. These data provide a one in six sample of individuals that permits us to study the residential location of interracial couples at the scale of the census tract. Simply stated, these data allow us analyze the residential location of interracial couples with previously unavailable geographical detail. The second data set is mortgage application information from the Home Mortgage Disclosure Act. These data record the race of single and joint applicants for home mortgages and the tract location of the property for which the loan is sought. We intend to merge these data for a number of years in the 1990s to analyze the residential preferences of interracial couples. The analysis will benefit the bureau in four ways: improvements in ethnic and racial imputation procedures; checking the validity of decennial data on interracial couples against an alternative federal data source; identifying tracts which should have high rates of multiracial reporting on Census 2000; and improving ethnic and racial population projections through better understanding of ethnic and racial identity formation of children of interracial couples."
Shifted Windows Transformers for Medical Image Quality Assessment,"To maintain a standard in a medical imaging study, images should have
necessary image quality for potential diagnostic use. Although CNN-based
approaches are used to assess the image quality, their performance can still be
improved in terms of accuracy. In this work, we approach this problem by using
Swin Transformer, which improves the poor-quality image classification
performance that causes the degradation in medical image quality. We test our
approach on Foreign Object Classification problem on Chest X-Rays (Object-CXR)
and Left Ventricular Outflow Tract Classification problem on Cardiac MRI with a
four-chamber view (LVOT). While we obtain a classification accuracy of 87.1%
and 95.48% on the Object-CXR and LVOT datasets, our experimental results
suggest that the use of Swin Transformer improves the Object-CXR classification
performance while obtaining a comparable performance for the LVOT dataset. To
the best of our knowledge, our study is the first vision transformer
application for medical image quality assessment.","['Caner Ozer', 'Arda Guler', 'Aysel Turkvatan Cansever', 'Deniz Alis', 'Ercan Karaarslan', 'Ilkay Oksuz']",[],0,arXiv,http://arxiv.org/abs/2208.06034v1,False,True,False,False,False,51,John P Sommers,Washington,Completed,2000,2001.0,"The project is meant to analyze output from the Medical Expenditure Panel Survey-Insurance Component (MEPS-IC) in  order to improve data collected, develop and test new editing and imputation techniques, improve sample design and  improve the number and quality of estimates produced.  The work is to performed by statisticians from the Agency for Healthcare Research and Quality in order to apply its subject matter and technical expertise to improvement of the results from the MEPS-IC."
"Measuring the Time-Varying Market Efficiency in the Prewar and Wartime
  Japanese Stock Market, 1924-1943","This study explores the time-varying structure of market efficiency in the
prewar and wartime Japanese stock market using a new market
capitalization-weighted stock price index, the equity performance index. We
examine whether the adaptive market hypothesis (AMH) is supported in that era.
First, we find that the degree of market efficiency in the prewar and wartime
Japanese stock market varies over time and with major historical events. This
implies that the AMH is supported in this market. Second, we find that the
variation in market efficiency observed in this study is significantly
different from that in previous studies because of whether the price index is
capitalization weighted. Finally, as government intervention in the market
intensified throughout the 1930s, market efficiency declined as the war risk
premium rose, especially from the time when the Pacific War became inevitable.","['Kenichi Hirayama', 'Akihiko Noda']",[],0,arXiv,http://arxiv.org/abs/1911.04059v6,False,True,False,False,False,54,Antoinette Schoar,Boston,Completed,2002,2002.0,"The aim of the proposed research project is to study the workings of internal capital markets. These are of primary importance in the capital allocation process in most developed economies, where a large fraction of investment decisions are made within big corporations. On average 60-80% of any new investment in the U.S. was financed through internal capital markets. Evidently, huge amounts of funds are not allocated via market prices in external capital markets, but through allocation mechanisms within the corporate hierarchy play a major role. However, we have only very limited understanding about what determines capital allocations within companies. One area where this problem manifests itself most prominently is corporate diversification. The two most prominent ideas are that internal capital markets either lead to 'winner-picking' of segments with good investment opportunities or inefficient cross-subsidization of under-performing segments. Using plant level data on investment, we plan to analyze the workings of internal capital markets in diversified firms. The structure of the LRD data provides a unique possibility to identify investment opportunities at the individual plant level. It allows us to form several different measures of investment opportunities to study how plant and segment level capital expenditures respond to changes in these measures. Because of the unique structure of the LRD, we will be able to differentiate between investments that are made at the individual plant level and investments that involve the decision to acquire or divest a whole plant or segment. The latter are normally decided at higher levels in the firm's hierarchy. The possibility to break down investment projects this way will allow us back out whether there exists a difference in the efficiency of projects that are decided at different levels in the organizational structure. Since the LRD itself does not provide information about which plants belong to a segment, we want to match COMPUSTAT Segment data to the LRD using a crosswalk developed by a researcher. Using plant level information from the LRD will be crucial to the proposed tests in this project. First, information form the LRD will allow us to analyze more precisely the extent and direction of capital flows across segments or plants. Since publicly available information from COMPUSTAT segment information files are prone to distortions due to managerial discretion and accounting requirements, the LRD gives a much more accurate picture of the different industries and lines of business a firm operates in. This will enable us to construct much more adequate measures of the investment opportunity set at the industry level than it is possible with publicly available data. Second, a main advantage lies with the fact that the LRD allows us to distinguish investments that are made at the individual plant level from those that are decisions to buy new plants or segments. This differentiation is possible due to the unique structure of the LRD, which surveys firms at the plant level."
"Balance, growth and diversity of financial markets","A financial market comprising of a certain number of distinct companies is
considered, and the following statement is proved: either a specific agent will
surely beat the whole market unconditionally in the long run, or (and this ""or""
is not exclusive) all the capital of the market will accumulate in one company.
Thus, absence of any ""free unbounded lunches relative to the total capital""
opportunities lead to the most dramatic failure of diversity in the market: one
company takes over all other until the end of time. In order to prove this, we
introduce the notion of perfectly balanced markets, which is an equilibrium
state in which the relative capitalization of each company is a martingale
under the physical probability. Then, the weaker notion of balanced markets is
discussed where the martingale property of the relative capitalizations holds
only approximately, we show how these concepts relate to growth-optimality and
efficiency of the market, as well as how we can infer a shadow interest rate
that is implied in the economy in the absence of a bank.",['Constantinos Kardaras'],[],0,arXiv,http://arxiv.org/abs/0803.1858v1,False,True,False,False,False,54,Antoinette Schoar,Boston,Completed,2002,2002.0,"The aim of the proposed research project is to study the workings of internal capital markets. These are of primary importance in the capital allocation process in most developed economies, where a large fraction of investment decisions are made within big corporations. On average 60-80% of any new investment in the U.S. was financed through internal capital markets. Evidently, huge amounts of funds are not allocated via market prices in external capital markets, but through allocation mechanisms within the corporate hierarchy play a major role. However, we have only very limited understanding about what determines capital allocations within companies. One area where this problem manifests itself most prominently is corporate diversification. The two most prominent ideas are that internal capital markets either lead to 'winner-picking' of segments with good investment opportunities or inefficient cross-subsidization of under-performing segments. Using plant level data on investment, we plan to analyze the workings of internal capital markets in diversified firms. The structure of the LRD data provides a unique possibility to identify investment opportunities at the individual plant level. It allows us to form several different measures of investment opportunities to study how plant and segment level capital expenditures respond to changes in these measures. Because of the unique structure of the LRD, we will be able to differentiate between investments that are made at the individual plant level and investments that involve the decision to acquire or divest a whole plant or segment. The latter are normally decided at higher levels in the firm's hierarchy. The possibility to break down investment projects this way will allow us back out whether there exists a difference in the efficiency of projects that are decided at different levels in the organizational structure. Since the LRD itself does not provide information about which plants belong to a segment, we want to match COMPUSTAT Segment data to the LRD using a crosswalk developed by a researcher. Using plant level information from the LRD will be crucial to the proposed tests in this project. First, information form the LRD will allow us to analyze more precisely the extent and direction of capital flows across segments or plants. Since publicly available information from COMPUSTAT segment information files are prone to distortions due to managerial discretion and accounting requirements, the LRD gives a much more accurate picture of the different industries and lines of business a firm operates in. This will enable us to construct much more adequate measures of the investment opportunity set at the industry level than it is possible with publicly available data. Second, a main advantage lies with the fact that the LRD allows us to distinguish investments that are made at the individual plant level from those that are decisions to buy new plants or segments. This differentiation is possible due to the unique structure of the LRD, which surveys firms at the plant level."
"Evaluating the impact of items and cooperation in inventory models with
  exemptable ordering costs","In this paper we introduce and analyse, from a game theoretical perspective,
several multi-agent or multi-item continuous review inventory models in which
the buyers are exempted from ordering costs if the price of their orders is
greater than or equal to a certain amount. For all models we obtain the optimal
ordering policy. We first analyse a simple model with one firm and one item.
Then, we study a model with one firm and several items, for which we design a
procedure based on cooperative game theory to evaluate the impact of each item
on the total cost. Then, we deal with a model with several firms and one item
for each firm, for which we characterise a rule to allocate the total cost
among the firms in a coalitionally stable way. Finally, we discuss a model with
several firms and several items, for which we characterise a rule to allocate
the total cost among the firms in a coalitionally stable way and to evaluate
the impact of each item on the cost that would be payable to each firm when
using the allocation rule. All the concepts and results of this article are
illustrated using data from a case study.","['M. Gloria Fiestras-Janeiro', 'Ignacio García-Jurado', 'Ana Meca', 'Manuel A. Mosquera']",[],0,arXiv,http://arxiv.org/abs/2402.06545v1,False,True,False,False,False,56,Linda T Bui,Boston,Completed,2001,2004.0,"The Toxics Release Inventory (TRI) is part of the Community Right to Know legislation that was passed by Congress in 1986.  The TRI requires manufacturing plants to publicly report their toxic releases to the community.   Since the TRI started, reported releases have declined by more than 40%.  One difficulty in evaluating the effectiveness of TRI regulation is determining what, if anything, firms have done in response to this ""informal"" regulation.  This difficulty is compounded by the fact that (1) there are no records of toxic releases that pre-date the TRI which can be used to provide information on firm behavior prior to the regulation and (2) TRI data are self-reported by firms. We propose to focus on plant level behavior to determine how plants have responded to the TRI regulation.  In particular, we will determine whether the reductions in TRI releases are related to reductions induced by the formal regulation of other pollutants, such as the criteria air pollutants, by looking at the relationship between TRI reductions and PACE expenditures.  I also plan to investigate whether changes in plant behavior induced by TRI regulation had productivity consequences.   If TRI reductions are related to more formal regulation of other pollutants then PACE expenditures over-estimate the true costs of those regulations because they do not take into account the positive externality associated with reductions in toxic releases.  Anecdotal evidence suggests that this maybe the case.  This would also suggest that TRI regulation is being given too much credit for reductions in toxic emissions.  Furthermore, if TRI regulations caused firms to change their production processes or inputs in a fashion that changed their productivity, the costs of this regulation may differ significantly from their ""gross"" costs.  Previous research on petroleum refineries and the cost of compliance in the South Coast Air Basin suggest that there may be important productivity gains associated with environmental regulation.  Both the true costs of the TRI and the ability to use PACE data to estimate those costs cannot be determined until these questions are answered."
"Atlas-X Equity Financing: Unlocking New Methods to Securely Obfuscate
  Axe Inventory Data Based on Differential Privacy","Banks publish daily a list of available securities/assets (axe list) to
selected clients to help them effectively locate Long (buy) or Short (sell)
trades at reduced financing rates. This reduces costs for the bank, as the list
aggregates the bank's internal firm inventory per asset for all clients of long
as well as short trades. However, this is somewhat problematic: (1) the bank's
inventory is revealed; (2) trades of clients who contribute to the aggregated
list, particularly those deemed large, are revealed to other clients. Clients
conducting sizable trades with the bank and possessing a portion of the
aggregated asset exceeding $50\%$ are considered to be concentrated clients.
This could potentially reveal a trading concentrated client's activity to their
competitors, thus providing an unfair advantage over the market.
  Atlas-X Axe Obfuscation, powered by new differential private methods, enables
a bank to obfuscate its published axe list on a daily basis while under
continual observation, thus maintaining an acceptable inventory Profit and Loss
(P&L) cost pertaining to the noisy obfuscated axe list while reducing the
clients' trading activity leakage. Our main differential private innovation is
a differential private aggregator for streams (time series data) of both
positive and negative integers under continual observation.
  For the last two years, Atlas-X system has been live in production across
three major regions-USA, Europe, and Asia-at J.P. Morgan, a major financial
institution, facilitating significant profitability. To our knowledge, it is
the first differential privacy solution to be deployed in the financial sector.
We also report benchmarks of our algorithm based on (anonymous) real and
synthetic data to showcase the quality of our obfuscation and its success in
production.","['Antigoni Polychroniadou', 'Gabriele Cipriani', 'Richard Hua', 'Tucker Balch']",[],0,arXiv,http://arxiv.org/abs/2404.06686v1,False,True,False,False,False,56,Linda T Bui,Boston,Completed,2001,2004.0,"The Toxics Release Inventory (TRI) is part of the Community Right to Know legislation that was passed by Congress in 1986.  The TRI requires manufacturing plants to publicly report their toxic releases to the community.   Since the TRI started, reported releases have declined by more than 40%.  One difficulty in evaluating the effectiveness of TRI regulation is determining what, if anything, firms have done in response to this ""informal"" regulation.  This difficulty is compounded by the fact that (1) there are no records of toxic releases that pre-date the TRI which can be used to provide information on firm behavior prior to the regulation and (2) TRI data are self-reported by firms. We propose to focus on plant level behavior to determine how plants have responded to the TRI regulation.  In particular, we will determine whether the reductions in TRI releases are related to reductions induced by the formal regulation of other pollutants, such as the criteria air pollutants, by looking at the relationship between TRI reductions and PACE expenditures.  I also plan to investigate whether changes in plant behavior induced by TRI regulation had productivity consequences.   If TRI reductions are related to more formal regulation of other pollutants then PACE expenditures over-estimate the true costs of those regulations because they do not take into account the positive externality associated with reductions in toxic releases.  Anecdotal evidence suggests that this maybe the case.  This would also suggest that TRI regulation is being given too much credit for reductions in toxic emissions.  Furthermore, if TRI regulations caused firms to change their production processes or inputs in a fashion that changed their productivity, the costs of this regulation may differ significantly from their ""gross"" costs.  Previous research on petroleum refineries and the cost of compliance in the South Coast Air Basin suggest that there may be important productivity gains associated with environmental regulation.  Both the true costs of the TRI and the ability to use PACE data to estimate those costs cannot be determined until these questions are answered."
"Observation of bosonic coalescence and fermionic anti-coalescence with
  indistinguishable photons","The symmetrization postulate asserts that the state of particular species of
particles can only be of one permutation symmetry type: symmetric for bosons
and antisymmetric for fermions. We report some experimental results showing
that pairs of photons indistinguishable by all degrees of freedom can exhibit
not only a bosonic behavior, as expected for photons, but also a surprisingly
sharp fermionic behavior under specific conditions.","['Guillaume Adenier', 'Joakim Bergli', 'Andreas P. Thörn', 'Arnt Inge Vistnes']",[],0,arXiv,http://arxiv.org/abs/1309.1084v1,False,True,False,False,False,66,Jessica P Vistnes,Washington,Completed,2001,2002.0,"The Medical Expenditure Panel Survey (MEPS) - Insurance Component (IC) conducted by the United States Census Bureau is a key economic survey that provides important information on employer sponsored health insurance for the nation. Published results from this survey are used by the Bureau of Economic Analyses as a key input to health care costs in Gross Domestic Product. Results are also used by numerous other government agencies, including, the General Accounting Office and Treasury to support Congressional requests and assess the status and costs of employer health insurance. States also use the data as key input into their economic analyses. As a key user and sponsor of this survey, the Agency for Healthcare Quality and Research (AHRQ) wishes to increase the quality and utility of this Census data. Because of the principal investigators’ unique knowledge of employer-sponsored health insurance and their econometric and statistical expertise, they have already contributed extensively to the technical aspects of the survey and to improvements in the quality of the IC data and its uses. This project should contribute to this effort to improve the survey. Among the goals of this research are the following:
• Produce estimates related to the supply and demand of employer-sponsored health insurance.
• Develop new and improved methodologies for producing such population estimates.
• Develop an understanding of the quality of data collected, through analysis of response rates, item response rates and data collection results, in order to produce changes in questionnaire structure and collection methodology that will improve collected data.
• Identify shortcomings of the questionnaire to obtain the information necessary to produce reliable population estimates related to employer-sponsored health insurance."
Register-based Census in Thailand: a Case Study in Chachoengsao Province,"The use of registers has been increasingly popular in the field of population
census because of its advantages over the traditional census. While the
traditional census requires a large amount of fieldwork and data collection,
the registered-based census can rely on pre-existing administrative data. As a
result, the register-based census can save both time and budget. Thailand
explored a use of the register-based census in 2020. In this paper, the authors
layout the methodology in an aspect of data preparation and integration as well
as analyze data quality of the register-based census compared with the
traditional census in Chachoengsao province, Thailand. In addition, we compared
conceptual frameworks that are commonly used for a register-based census in
several countries and the number of databases (a recent single database VS
multiple databases) used to construct the register-based census. We found that
using a conceptual framework that counts the number of populations based on the
main census variables on a single recent database is better than using a
framework that counts population who appears on many registers in term of
overcoverage and data distribution regarding to sex. This provides the evidence
of using one recent and complete database is sufficient for conducting the
register-based census. The authors end up with recommendations for conducing
the register-based census.","['Nuttirudee Charoenruk', 'Narongrid Asavaroungpipop', 'Pannee Pattanapradit', 'Kittiya Ku-kiattikun', 'Chainarong Amornbunchornvej']",[],0,arXiv,http://arxiv.org/abs/2304.12543v1,False,True,False,False,False,69,Thomas H Pollak,Washington,Completed,2001,2002.0,"The purposes of the project are, first and foremost, to compare the Unified Database of Arts Organizations with the Economic Census database in order to assess the strengths and weaknesses of the two databases for research on arts organizations and establishments, and, second, to study changes in the distribution and finances of the arts sector. The comparison of the two databases will focus on three questions:  the extent to which the two databases capture the full population of arts organizations, the quality of the NAICS and Unified Database classifications, and the quality of the financial data. "
"The Role of Direct Capital Cash Transfers Towards Poverty and Extreme
  Poverty Alleviation -- An Omega Risk Process","Trapping refers to the event when a household falls into the area of poverty.
Households that live or fall into the area of poverty are said to be in a
poverty trap, where a poverty trap is a state of poverty from which it is
difficult to escape without external help. Similarly, extreme poverty is
considered as the most severe type of poverty, in which households experience
severe deprivation of basic human needs. In this article, we consider an Omega
risk process with deterministic growth and a multiplicative jump (collapse)
structure to model the capital of a household. It is assumed that, when a
household's capital level is above a certain capital barrier level that
determines a household's eligibility for a capital cash transfer programme, its
capital grows exponentially. As soon as its capital falls below the capital
barrier level, the capital dynamics incorporate external support in the form of
direct transfers (capital cash transfers) provided by donors or governments.
Otherwise, once trapped, the capital grows only due to the capital cash
transfers. Under this model, we first derive closed-form expressions for the
trapping probability and then do the same for the probability of extreme
poverty, which only depends on the current value of the capital given by some
extreme poverty rate function. Numerical examples illustrate the role of
capital cash transfers on poverty and extreme poverty dynamics.","['José Miguel Flores-Contró', 'Séverine Arnold']",[],0,arXiv,http://arxiv.org/abs/2401.06141v3,False,True,False,False,False,71,Enrico Moretti,UCLA,Completed,2002,2002.0,"Previous work has shown that workers' productivity is systematically correlated with average education in the metropolitan area where they live.  This project is to investigate a possible explanation for this correlation by looking at how firm behavior is affected by human capital in their local labor market.  In particular, I plan to match the LRD with the SMT & the public version of the Census of Population & Housing to test whether productivity & investment in new technologies of firms in a given metro area are correlated with average education there, after controlling for firms' own human capital"
"Multiclass Language Identification using Deep Learning on Spectral
  Images of Audio Signals","The first step in any voice recognition software is to determine what
language a speaker is using, and ideally this process would be automated. The
technique described in this paper, language identification for audio
spectrograms (LIFAS), uses spectrograms generated from audio signals as inputs
to a convolutional neural network (CNN) to be used for language identification.
LIFAS requires minimal pre-processing on the audio signals as the spectrograms
are generated during each batch as they are input to the network during
training.
  LIFAS utilizes deep learning tools that are shown to be successful on image
processing tasks and applies it to audio signal classification. LIFAS performs
binary language classification with an accuracy of 97\%, and multi-class
classification with six languages at an accuracy of 89\% on 3.75 second audio
clips.","['Shauna Revay', 'Matthew Teschke']",[],0,arXiv,http://arxiv.org/abs/1905.04348v1,False,True,False,False,False,79,Shauna M Saunders,Triangle,Completed,2001,2002.0,"The aim of this research is to examine some of the key questions surrounding the effects of cultural policy.  Over the past decade, government funding for cultural institutions has undergone dramatic changes, most notably in the reductions of National Endowment for the Arts (NEA) funding and in the shift away from federal funding to state and local funding. The consequences of these changes on cultural provision remain largely unknown. Turning to the demand side, cultural policy makers often argue that cultural projects have effects that extend beyond the audiences of the projects, and that significant spillovers to the local economy, in the form of lower crime rates, increased civic engagement, and a better citizenry generally, result. Again, most of these claims, though testable, have remained unexamined. In addition, this research on cultural policy ties into two distinct lines of ongoing research in economics, although the explicit consideration of cultural policy is novel. First, the project addresses the effects of different funding sources on the provision of public goods. Second, the project fits into the growing body of research on social interactions."
"Dilepton production in high energy heavy ion collisions with 3+1D
  relativistic viscous hydrodynamics","We present a first calculation of the dilepton yield and elliptic flow done
with 3+1D viscous hydrodynamical simulations of relativistic heavy ion
collisions at the top RHIC energy. A comparison with recent experimental data
from the STAR collaboration is made.","['G. Vujanovic', 'C. Young', 'B. Schenke', 'S. Jeon', 'R. Rapp', 'C. Gale']",[],0,arXiv,http://arxiv.org/abs/1211.0022v1,False,True,False,False,False,81,Gale A Boyd,Washington,Completed,2001,2001.0,"Industrial energy demand typically involves base load consumption. As a result the industrial component of energy demand has important implications in evaluating energy infrastructure, i.e. oil, gas, electric power. There is growing concern that the energy infrastructure could be subject to both natural (e.g. storms or equipment failure) and manmade (e.g. terrorist) disruptions. To address the economic and human implications of such a disruption, planning agencies are taking a closer look at the energy infrastructure and its customers to assess its robustness and ability to continue vital functions as well as identify potential weaknesses. This project uses the LRD and MECS databases to estimate a distribution-company-level model of industrial energy demand for natural gas and electricity via a plant-level energy demand equation. An element unique to this study is the use of “establishment location” in a geographic information system (GIS) to create new, supplemental data on the relationship between the plant and the energy distribution system. These supplemental data are then used to improve the forecasting abilities of the econometric model. The benefits to the Census Bureau include 1) the creation of the GIS layers that can be used to access various data sources such as the LRD, MECS, and SSEL, in an intuitive visual mode which highlights spatial relationships, 2) the link across the LRD and MECS to create plant-level energy prices, and 3) the forecasting equations that can be used to impute non-response to energy questions in the ASM and MECS."
Intermediate mass dileptons in relativistic nuclear collisions,"We consider the production of lepton pairs with intermediate invariant
masses, in relativistic nuclear collisions. Thermal sources are briefly
discussed, as well as a newer production mechanism which involves jet-plasma
interactions. Estimates for high p_T lepton pairs are provided.","['Charles Gale', 'Simon Turbide']",[],0,arXiv,http://arxiv.org/abs/hep-ph/0610055v1,False,True,False,False,False,81,Gale A Boyd,Washington,Completed,2001,2001.0,"Industrial energy demand typically involves base load consumption. As a result the industrial component of energy demand has important implications in evaluating energy infrastructure, i.e. oil, gas, electric power. There is growing concern that the energy infrastructure could be subject to both natural (e.g. storms or equipment failure) and manmade (e.g. terrorist) disruptions. To address the economic and human implications of such a disruption, planning agencies are taking a closer look at the energy infrastructure and its customers to assess its robustness and ability to continue vital functions as well as identify potential weaknesses. This project uses the LRD and MECS databases to estimate a distribution-company-level model of industrial energy demand for natural gas and electricity via a plant-level energy demand equation. An element unique to this study is the use of “establishment location” in a geographic information system (GIS) to create new, supplemental data on the relationship between the plant and the energy distribution system. These supplemental data are then used to improve the forecasting abilities of the econometric model. The benefits to the Census Bureau include 1) the creation of the GIS layers that can be used to access various data sources such as the LRD, MECS, and SSEL, in an intuitive visual mode which highlights spatial relationships, 2) the link across the LRD and MECS to create plant-level energy prices, and 3) the forecasting equations that can be used to impute non-response to energy questions in the ASM and MECS."
"Hierarchical Additive Modeling of Nonlinear Association with Spatial
  Correlations-An Application to Relate Alcohol Outlet Density and Neighborhood
  Assault Rates","Previous studies have suggested a link between alcohol outlets and assaultive
violence. In this paper, we explore the effects of alcohol availability on
assault crimes at the census tract level over time. The statistical analysis is
challenged by several features of the data: (1) the effects of possible
covariates (for example, the alcohol outlet density of each census tract) on
the assaultive crime rates may be complex; (2) the covariates may be highly
correlated with each other; (3) there are a lot of missing inputs in the data;
and (4) spatial correlations exist in the outcome assaultive crime rates. We
propose a hierarchical additive model, where the nonlinear correlations and the
complex interaction effects are modeled using the multiple additive regression
trees (MART) and the spatial variances in the assaultive rates that cannot be
explained by the specified covariates are smoothed trough the Conditional
Autoregressive (CAR) model. We develop a two-stage algorithm that connect the
non-parametric trees with CAR to look for important variables covariates
associated with the assaultive crime rates, while taking account of the spatial
correlations among adjacent census tracts. The proposed methods are applied to
the Los Angeles assaultive data (1990-1999) and compared with traditional
method.","['Qingzhao Yu', 'Bin Li', 'Richard Scribner', 'Deborah Cohen']",[],0,arXiv,http://arxiv.org/abs/0802.0450v1,False,True,False,False,False,82,Jacqueline Cohen,CMU,Completed,2001,2002.0,"Our knowledge of the prevalence and nature of non-lethal violence is highly dependent on data collected by agencies that serve victims. While such data sources contribute to our understanding of violence, the victim contacts that are detected by these systems understate and possibly misrepresent total prevalence and incidence rates. The proposed research pursues two main objectives. First, we will use National Crime Victimization Survey (NCVS) data to estimate the probabilities that a victim will contact the police, seek medical care, or both, conditional on individual and situational factors. Second, we will explore whether distinctive contextual features of local areas contribute to further variation in the likelihood that victims seek services. To achieve the first objective, the proposed research will rely on public-use NCVS data of assault incidents to explore the influence of various incident features on the probability of agency contacts with police, healthcare providers, or both systems. To achieve the second objective, we will estimate contextual effects by linking area-identified NCVS data with criminal justice policy and healthcare data for 48 of the largest 50 US cities. The analysis will seek to identify system features that are associated with an increased likelihood of victim/agency contacts."
"Distributional Semantics Approach to Detect Intent in Twitter
  Conversations on Sexual Assaults","The recent surge in women reporting sexual assault and harassment (e.g.,
#metoo campaign) has highlighted a longstanding societal crisis. This injustice
is partly due to a culture of discrediting women who report such crimes and
also, rape myths (e.g., 'women lie about rape'). Social web can facilitate the
further proliferation of deceptive beliefs and culture of rape myths through
intentional messaging by malicious actors. This multidisciplinary study
investigates Twitter posts related to sexual assaults and rape myths for
characterizing the types of malicious intent, which leads to the beliefs on
discrediting women and rape myths. Specifically, we first propose a novel
malicious intent typology for social media using the guidance of social
construction theory from policy literature that includes Accusational,
Validational, or Sensational intent categories. We then present and evaluate a
malicious intent classification model for a Twitter post using semantic
features of the intent senses learned with the help of convolutional neural
networks. Lastly, we analyze a Twitter dataset of four months using the intent
classification model to study narrative contexts in which malicious intents are
expressed and discuss their implications for gender violence policy design.","['Rahul Pandey', 'Hemant Purohit', 'Bonnie Stabile', 'Aubrey Grant']",[],0,arXiv,http://arxiv.org/abs/1810.01012v1,False,True,False,False,False,82,Jacqueline Cohen,CMU,Completed,2001,2002.0,"Our knowledge of the prevalence and nature of non-lethal violence is highly dependent on data collected by agencies that serve victims. While such data sources contribute to our understanding of violence, the victim contacts that are detected by these systems understate and possibly misrepresent total prevalence and incidence rates. The proposed research pursues two main objectives. First, we will use National Crime Victimization Survey (NCVS) data to estimate the probabilities that a victim will contact the police, seek medical care, or both, conditional on individual and situational factors. Second, we will explore whether distinctive contextual features of local areas contribute to further variation in the likelihood that victims seek services. To achieve the first objective, the proposed research will rely on public-use NCVS data of assault incidents to explore the influence of various incident features on the probability of agency contacts with police, healthcare providers, or both systems. To achieve the second objective, we will estimate contextual effects by linking area-identified NCVS data with criminal justice policy and healthcare data for 48 of the largest 50 US cities. The analysis will seek to identify system features that are associated with an increased likelihood of victim/agency contacts."
Resilient Urban Housing Markets: Shocks vs. Fundamentals,"In the face of a pandemic, urban protests, and an affordability crisis, is
the desirability of dense urban settings at a turning point? Assessing cities'
long term trends remains challenging. The first part of this chapter describes
the short-run dynamics of the housing market in 2020. Evidence from prices and
price-to-rent ratios suggests expectations of resilience. Zip-level evidence
suggests a short-run trend towards suburbanization, and some impacts of urban
protests on house prices. The second part of the chapter analyzes the long-run
dynamics of urban growth between 1970 and 2010. It analyzes what, in such urban
growth, is explained by short-run shocks as opposed to fundamentals such as
education, industrial specialization, industrial diversification, urban
segregation, and housing supply elasticity. This chapter's original results as
well as a large established body of literature suggest that fundamentals are
the key drivers of growth. The chapter illustrates this finding with two case
studies: the New York City housing market after September 11, 2001; and the San
Francisco Bay Area in the aftermath of the 1989 Loma Prieta earthquake. Both
areas rebounded strongly after these shocks, suggesting the resilience of the
urban metropolis.",['Amine Ouazad'],[],0,arXiv,http://arxiv.org/abs/2010.00413v2,False,True,False,False,False,83,Stephen L Ross,Boston,Completed,2002,2006.0,"In this project, we propose to estimate an empirical model that describes the equilibrium location and employment decisions made by households and firms within a single metropolitan area. In our model, each household makes its residential location decision based on a wide variety of factors including characteristics of housing and schools, neighborhood sociodemographic characteristics, housing prices, other neighborhood amenities, and employment access. The employment of each household member is in turn influenced by his/her access to suitable jobs, neighborhood characteristics, and individual/household characteristics. Firms make decisions based on access to different types of workers, other firms, and customers, and in our most general specification a firm’s location directly influences the characteristics of its workforce. These decisions combine to create a complex spatial equilibrium, which determines land use, prices, and wages throughout the metropolitan area. We use the estimates of this model to explore complex interactions in urban labor and housing markets, focusing on two well-known questions in which the simultaneity of the household residential location and employment decisions as well as the simultaneity of household and firm location decisions have been quite evident. These questions relate to whether access to employment influences labor market outcomes, and whether the quality of a neighborhood affects labor market outcomes. We begin this proposal by describing these important research questions and the difficulties researchers have had controlling for the endogeneity problems introduced by the simultaneity of these decisions. We then describe our methodology and explain how the estimates of our model can be used to shed new light on these questions."
Matching with Transfers under Distributional Constraints,"We study two-sided many-to-one matching markets with transferable utilities,
e.g., labor and rental housing markets, in which money can exchange hands
between agents, subject to distributional constraints on the set of feasible
allocations. In such markets, we establish the efficiency of equilibrium
arrangements, specified by an assignment and transfers between agents on the
two sides of the market, and study the conditions on the distributional
constraints and agent preferences under which equilibria exist and can be
computed efficiently. To this end, we first consider the setting when the
number of institutions (e.g., firms in a labor market) is one and show that
equilibrium arrangements exist irrespective of the nature of the constraint
structure or the agents' preferences. However, equilibrium arrangements may not
exist in markets with multiple institutions even when agents on each side have
linear (or additively separable) preferences over agents on the other side.
Thus, for markets with linear preferences, we study sufficient conditions on
the constraint structure that guarantee the existence of equilibria using
linear programming duality. Our linear programming approach not only
generalizes that of Shapley and Shubik (1971) in the one-to-one matching
setting to the many-to-one matching setting under distributional constraints
but also provides a method to compute market equilibria efficiently.","['Devansh Jalota', 'Michael Ostrovsky', 'Marco Pavone']",[],0,arXiv,http://arxiv.org/abs/2202.05232v2,False,True,False,False,False,83,Stephen L Ross,Boston,Completed,2002,2006.0,"In this project, we propose to estimate an empirical model that describes the equilibrium location and employment decisions made by households and firms within a single metropolitan area. In our model, each household makes its residential location decision based on a wide variety of factors including characteristics of housing and schools, neighborhood sociodemographic characteristics, housing prices, other neighborhood amenities, and employment access. The employment of each household member is in turn influenced by his/her access to suitable jobs, neighborhood characteristics, and individual/household characteristics. Firms make decisions based on access to different types of workers, other firms, and customers, and in our most general specification a firm’s location directly influences the characteristics of its workforce. These decisions combine to create a complex spatial equilibrium, which determines land use, prices, and wages throughout the metropolitan area. We use the estimates of this model to explore complex interactions in urban labor and housing markets, focusing on two well-known questions in which the simultaneity of the household residential location and employment decisions as well as the simultaneity of household and firm location decisions have been quite evident. These questions relate to whether access to employment influences labor market outcomes, and whether the quality of a neighborhood affects labor market outcomes. We begin this proposal by describing these important research questions and the difficulties researchers have had controlling for the endogeneity problems introduced by the simultaneity of these decisions. We then describe our methodology and explain how the estimates of our model can be used to shed new light on these questions."
Essays on Responsible and Sustainable Finance,"The dissertation consists of three essays on responsible and sustainable
finance. I show that local communities should be seen as stakeholders to
decisions made by corporations. In the first essay, I examine whether the
imposition of fiduciary duty on municipal advisors affects bond yields and
advising fees. Using a difference-in-differences analysis, I show that bond
yields reduce by 9\% after the imposition of the SEC Municipal Advisor Rule. In
the second essay, we analyze the impact of USD 40 billion of corporate
subsidies given by U.S. local governments on their borrowing costs. We find
that winning counties experience a 15 bps increase in bond yield spread as
compared to the losing counties. In the third essay, we provide new evidence
that the bankruptcy filing of a locally-headquartered and publicly-listed
manufacturing firm imposes externalities on the local governments. Compared to
matched counties with similar economic trends, municipal bond yields for
affected counties increase by 10 bps within a year of the firm filing for
bankruptcy. The final essay examines whether managers walk the talk on the
environmental and social discussion. We train a deep-learning model on various
corporate sustainability frameworks to construct a comprehensive Environmental
and Social (E and S) dictionary. Using this dictionary, we find that the
discussion of environmental topics in the earnings conference calls of U.S.
public firms is associated with higher pollution abatement and more future
green patents.",['Baridhi Malakar'],[],0,arXiv,http://arxiv.org/abs/2406.12995v1,False,True,False,False,False,91,Wayne Gray,Boston,Completed,2001,2005.0,"This proposal examines the impact of corporate restructuring on management of environmental and workplace risks and the determinants of compliance with environmental regulations.  Three industries are involved - paper, oil, and steel.  Both projects require good data on pollution abatement costs, productivity, and plant-firm ownership links.  We'll be linking in outside data on ownership, compliance, enforcement, and environmental performance, checking for consistency with Census data and over time.  The results will include estimates of econometric models for academic publication, and reports to Census on data quality issues.  "
"Kicking You When You're Already Down: The Multipronged Impact of
  Austerity on Crime","The UK Welfare Reform Act 2012 imposed a series of deep welfare cuts, which
disproportionately affected ex-ante poorer areas. In this paper, we provide the
first evidence of the impact of these austerity measures on two different but
complementary elements of crime -- the crime rate and the less-studied
concentration of crime -- over the period 2011-2015 in England and Wales, and
document four new facts. First, areas more exposed to the welfare reforms
experience increased levels of crime, an effect driven by a rise in violent
crime. Second, both violent and property crime become more concentrated within
an area due to the welfare reforms. Third, it is ex-ante more deprived
neighborhoods that bear the brunt of the crime increases over this period.
Fourth, we find no evidence that the welfare reforms increased recidivism,
suggesting that the changes in crime we find are likely driven by new
criminals. Combining these results, we document unambiguous evidence of a
negative spillover of the welfare reforms at the heart of the UK government's
austerity program on social welfare, which reinforced the direct
inequality-worsening effect of this program. Guided by a hedonic house price
model, we calculate the welfare effects implied by the cuts in order to provide
a financial quantification of the impact of the reform. We document an implied
welfare loss of the policy -- borne by the public -- that far exceeds the
savings made to government coffers.","['Corrado Giulietti', 'Brendon McConnell']",[],0,arXiv,http://arxiv.org/abs/2012.08133v4,False,True,False,False,False,92,Henry E Brady,Berkeley,Completed,2002,2005.0,"This proposal is part of a larger study of the impact of welfare reform on California’s immigrant families. We propose to use the Survey of Income and Program Participation (SIPP) and Current Population Survey (CPS) linked to two major statewide administrative data sets in California, the Medi-Cal Eligibility Data System (MEDS) of the California Department of Health Services and Unemployment Insurance/Disability Insurance (hereafter referred to as EDD data) of the California Employment Development Department. The linkage of these data sets will produce longitudinal files for the California sub-sample of the CPS and SIPP with detailed information on immigration status and household characteristics combined with administrative data on public assistance program participation and earnings over time. The proposed research will benefit the Census Bureau in a number of distinct ways. First, it will provide the Census Bureau with a basis to evaluate the accuracy of important self-reported measures in the Survey of Income and Program Participation (SIPP) and the Current Population Survey (CPS), including Medicaid coverage, receipt of cash aid, Food Stamps, and Supplemental Security Income (SSI). This validation will be valuable for the future use of these surveys because SIPP and CPS tend to differ significantly in their estimates of participation in these important programs. Second, comparing the SIPP and CPS data on program participation with the MEDS records will allow us to investigate differences between self-reports and administrative records for the two same individuals due to characteristics of the surveys, such as reference time periods and question wording, which are linked to the inaccuracies of survey respondents’ self reports. Third, the match of EDD records with CPS and SIPP records will provide us with estimates of the direction and size of errors in reported earnings in the surveys. Fourth, this match will enable us to examine more closely the differences in poverty measures and rates using the CPS and SIPP by comparing the two surveys to administrative wage records and public assistance use. Fifth, using the administrative data we will evaluate the Survey of Population Dynamics (SPD) for its ability to track the use of welfare programs. Finally, the proposed match offers important insights on the residence and movements of immigrants and will contribute to improving the Demographic Analysis population projections."
"ARA-O-RAN: End-to-End Programmable O-RAN Living Lab for Agriculture and
  Rural Communities","As wireless networks evolve towards open architectures like O-RAN, testing,
and integration platforms are crucial to address challenges like
interoperability. This paper describes ARA-O-RAN, a novel O-RAN testbed
established through the NSF Platforms for Advanced Wireless Research (PAWR) ARA
platform. ARA provides an at-scale rural wireless living lab focused on
technologies for digital agriculture and rural communities. As an O-RAN
Alliance certified Open Testing and Integration Centre (OTIC), ARA launched
ARA-O-RAN -- the first public O-RAN testbed tailored to rural and agriculture
use cases, together with the end-to-end, whole-stack programmability. ARA-O-RAN
uniquely combines support for outdoor testing across a university campus,
surrounding farmlands, and rural communities with a 50-node indoor sandbox. The
testbed facilitates vital R\&D to implement open architectures that can meet
rural connectivity needs. The paper outlines ARA-O-RAN's hardware system
design, software architecture, and enabled research experiments. It also
discusses plans aligned with national spectrum policy and rural spectrum
innovation. ARA-O-RAN exemplifies the value of purpose-built wireless testbeds
in accelerating impactful wireless research.","['Tianyi Zhang', 'Joshua Ofori Boateng', 'Taimoor UI Islam', 'Arsalan Ahmad', 'Hongwei Zhang', 'Daji Qiao']",[],0,arXiv,http://arxiv.org/abs/2407.10982v1,False,True,False,False,False,98,Steven Buccola,Berkeley,Completed,2001,2003.0,The purpose of this study is to examine the relationship between basic biological research and agricultural biotechnology innovations in U.S. firms and universities.  The study will assist the Census Bureau in understanding the principal factors affecting research investment and technological change in U.S. biotechnology.
"Trends in Open Access Academic Outputs of State Agricultural
  Universities in India: Patterns from OpenAlex","Purpose: The study examines the Open Access (OA) landscape of Indian state
agricultural universities, focusing on OA growth, leading institutions,
prolific authors, preferred sources, funding, APC usage, and trending topics.
It aims to identify research gaps, guide future research, and support
policymakers in developing effective OA policies Design/methodology/approach
The experiment utilized the OpenAlex database to collect global open access
(OA) publications from Indian state agricultural universities over the past ten
years (2014-2023). Using the Research Organization Registry ID, 97,536
publications were extracted. Data analysis was performed with OpenRefine, and
ArcGIS 10.8 and Microsoft Excel were used for visualization. Findings: The
global OA research output from state agricultural universities amounted to
65,889 publications across five OA categories: Green OA (7.35%), Diamond OA
(6.74%), Gold OA (57.27%), Hybrid OA (9.24%), and Bronze OA (19.41%). Notably,
78.34% of articles were published in 864 low-impact domestic journals. Tamil
Nadu Agricultural University produced the most publications in Gold, Diamond,
Hybrid, and Bronze OA categories, while Punjab Agricultural University excelled
in Green OA and received the highest funding, incurring the most article
processing charges (APCs). Collaborative research focusing on agricultural
policies, rice water management, soil fertility, and crop productivity had a
greater impact. Originality/value The experiment is the first effort to
evaluate the OA global academic research outputs of Indian state agriculture
universities. The findings offer institutions, state governments, and funding
agencies the opportunity to prioritise open-access publishing to promote
sustainable agricultural research. Research limitations/implications The study
is limited to the publications data indexed in the OpenAlex database.","['Abhijit Roy', 'Akhandanand Shukla', 'Aditya Tripathi']",[],0,arXiv,http://arxiv.org/abs/2503.18506v1,False,True,False,False,False,98,Steven Buccola,Berkeley,Completed,2001,2003.0,The purpose of this study is to examine the relationship between basic biological research and agricultural biotechnology innovations in U.S. firms and universities.  The study will assist the Census Bureau in understanding the principal factors affecting research investment and technological change in U.S. biotechnology.
"A new type of Multiverse, Gödel theorems and the nonstandard logic of
  classical, quantum mechanics and quantum gravity","The problem is posed of establishing a possible relationship between a new
type of Multi-verse representation, G\""odel undecidability theorems and the
logic of classical, quantum mechanics and quantum gravity. For this purpose
example cases of multi-verses are first discussed in the context of
non-relativistic classical, quantum mechanics and quantum gravity. As a result,
it is confirmed that thanks to G\""odel theorems non-relativistic classical and
quantum mechanics, as well as quantum gravity theory are incomplete. As a
consequence, they necessarily admit undecidable logical propositions and
therefore obey a three-way boolean logical, i.e., a propositional logic with
the three different logical truth values true, false and undecidable.","['Massimo Tessarotto', 'Claudio Asci', 'Alessandro Soranzo', 'Marco Tessarotto', 'Gino Tironi']",[],0,arXiv,http://arxiv.org/abs/2501.04045v1,False,True,False,False,False,108,Robert K Triest,Boston,Completed,1998,1998.0,"Our proposed project is an extension of ongoing research on job creation and destruction in U.S. manufacturing summarized in Davis, Haltiwanger, and Schuh (1996).  A central finding of this research is the prominent role of job destruction in business cycle fluctuations.  Specifically, job destruction increases sharply and regularly in recessions, but job creation typically declines much less and by varying amounts across recessions.  Together, these features imply that job reallocation rises significantly during recessions. 
	Although the Davis, Haltiwanger and Schuh (DHS) results provide a fascinating look at what happens to job destruction -- i.e., how large it is, how much it varies, how long it lasts, in which kind of plants it occurs, etc. -- very little is known yet about why job destruction happens.  Thus, the goal of this research proposal is to discover what causes job destruction and, in doing so, delineate the causes from the consequences of  job destruction.  Initially, this research will follow the tradition of DHS, which is a largely descriptive empirical analysis of the data.  Later, we expect to introduce and estimate formal econometric models."
"On the stochastic and asymptotic improvement of First-Come First-Served
  and Nudge scheduling","Recently it was shown that, contrary to expectations, the
First-Come-First-Served (FCFS) scheduling algorithm can be stochastically
improved upon by a scheduling algorithm called {\it Nudge} for light-tailed job
size distributions. Nudge partitions jobs into 4 types based on their size, say
small, medium, large and huge jobs. Nudge operates identical to FCFS, except
that whenever a {\it small} job arrives that finds a {\it large} job waiting at
the back of the queue, Nudge swaps the small job with the large one unless the
large job was already involved in an earlier swap.
  In this paper, we show that FCFS can be stochastically improved upon under
far weaker conditions. We consider a system with $2$ job types and limited
swapping between type-$1$ and type-$2$ jobs, but where a type-$1$ job is not
necessarily smaller than a type-$2$ job. More specifically, we introduce and
study the Nudge-$K$ scheduling algorithm which allows type-$1$ jobs to be
swapped with up to $K$ type-$2$ jobs waiting at the back of the queue, while
type-$2$ jobs can be involved in at most one swap. We present an explicit
expression for the response time distribution under Nudge-$K$ when both job
types follow a phase-type distribution. Regarding the asymptotic tail
improvement ratio (ATIR) , we derive a simple expression for the ATIR, as well
as for the $K$ that maximizes the ATIR. We show that the ATIR is positive and
the optimal $K$ tends to infinity in heavy traffic as long as the type-$2$ jobs
are on average longer than the type-$1$ jobs.",['Benny Van Houdt'],[],0,arXiv,http://arxiv.org/abs/2206.10428v1,False,True,False,False,False,108,Robert K Triest,Boston,Completed,1998,1998.0,"Our proposed project is an extension of ongoing research on job creation and destruction in U.S. manufacturing summarized in Davis, Haltiwanger, and Schuh (1996).  A central finding of this research is the prominent role of job destruction in business cycle fluctuations.  Specifically, job destruction increases sharply and regularly in recessions, but job creation typically declines much less and by varying amounts across recessions.  Together, these features imply that job reallocation rises significantly during recessions. 
	Although the Davis, Haltiwanger and Schuh (DHS) results provide a fascinating look at what happens to job destruction -- i.e., how large it is, how much it varies, how long it lasts, in which kind of plants it occurs, etc. -- very little is known yet about why job destruction happens.  Thus, the goal of this research proposal is to discover what causes job destruction and, in doing so, delineate the causes from the consequences of  job destruction.  Initially, this research will follow the tradition of DHS, which is a largely descriptive empirical analysis of the data.  Later, we expect to introduce and estimate formal econometric models."
"The role of industry, occupation, and location specific knowledge in the
  survival of new firms","How do regions acquire the knowledge they need to diversify their economic
activities? How does the migration of workers among firms and industries
contribute to the diffusion of that knowledge? Here we measure the industry,
occupation, and location-specific knowledge carried by workers from one
establishment to the next using a dataset summarizing the individual work
history for an entire country. We study pioneer firms--firms operating in an
industry that was not present in a region--because the success of pioneers is
the basic unit of regional economic diversification. We find that the growth
and survival of pioneers increase significantly when their first hires are
workers with experience in a related industry, and with work experience in the
same location, but not with past experience in a related occupation. We compare
these results with new firms that are not pioneers and find that
industry-specific knowledge is significantly more important for pioneer than
non-pioneer firms. To address endogeneity we use Bartik instruments, which
leverage national fluctuations in the demand for an activity as shocks for
local labor supply. The instrumental variable estimates support the finding
that industry-related knowledge is a predictor of the survival and growth of
pioneer firms. These findings expand our understanding of the micro-mechanisms
underlying regional economic diversification events.","['C. Jara-Figueroa', 'Bogang Jun', 'Edward Glaeser', 'Cesar Hidalgo']",[],0,arXiv,http://arxiv.org/abs/1808.01237v1,False,True,False,False,False,116,Brian Headd,Washington,Completed,1999,1999.0,This research uses data from the 1992 CBO and focuses on the survivability of new firms born in 1992 and whether or not they were successful.  An economic model will attempt to clarify factors contributing to the survival and/or success by major industry and owner type.
"The Human Capital Accumulation at Research Infrastructures: Reexamining
  Wage Returns to Training, Models, Interpretation, and Magnitude","We design a research strategy to measure wage returns of research training
targeting early career researchers (ECRs) at research infrastructures (RIs).
Grounded in established economic models of education and training, our strategy
improves upon existing studies on the same topic in labour market relevance,
scope, and economic modelling. We draw on a survey of ECRs at the European
Organisation for Nuclear Research (CERN) and find that CERN research training
increases ECRs wages by 7 per cent on average, ranging from 2 to 10 per cent.
Wage gains materialise early in their careers, typically within the first
decade of employment. Wage returns are driven by hard skills, such as software
development, data analysis, and problem solving capacity, as well as sought
after soft skills, such as communication, leadership, and networking. Our
findings suggest that ECRs wage returns primarily reflect productivity
improvements rather than the signalling effect of CERN affiliation. Our
research applies to other RIs. We conclude by discussing methodological
considerations, policy implications, and avenues for future research.","['Erica Delugas', 'Francesco Giffoni', 'Emanuela Sirtori', 'Johannes Gutleber']",[],0,arXiv,http://arxiv.org/abs/2502.07419v1,False,True,False,False,False,118,Lisa M Lynch,Boston,Completed,1999,2001.0,"The first part of the project will examine the role of worker characteristics (education, training, gender, race, turnover), establishment characteristics (capital stock, materials, unionization, age of the establishment, computer usage, age of the capital stock) and innovations in workplace practices (profit-sharing, team work, employee involvement in decision making, TQM, reengineering, and job rotation) on labor productivity.  The second part of the analysis examines the relationship between workplace innovation and labor costs and profits.  We will analyze whether the relative wages of workers who work in businesses characterized by high performance workplace (HPW) practices reflect or not their  marginal products.  Finally we study how employer characteristics and past history affect the probability of adopting HPW practices and how this may affect analyses of the productivity effects of these practices."
"The wage transition in developed countries and its implications for
  China","The expression ""wage transition"" refers to the fact that over the past two or
three decades in all developed economies wage increases have levelled off.
There has been a widening divergence and decoupling between wages on the one
hand and GDP per capita on the other hand. Yet, in China wages and GDP per
capita climbed in sync (at least up to now). In the first part of the paper we
present comparative statistical evidence which measures the extent of the wage
transition effect. In a second part we consider the reasons of this phenomenon,
in particular we explain how the transfers of labor from low productivity
sectors (such as agriculture) to high productivity sectors (such as
manufacturing) are the driver of productivity growth, particularly through
their synergetic effects. Although rural flight represents only one of these
effects, it is certainly the most visible because of the geographical
relocation that it implies; it is also the most well-defined statistically.
Moreover, it will be seen that it is a good indicator of the overall
productivity and attractivity of the non-agricultural sector. Because this
model accounts fairly well for the observed evolution in industrialized
countries, we use it to predict the rate of Chinese economic growth in the
coming decades. Our forecast for the average annual growth of real wages ranges
from 4% to 6% depending on how well China will control the development of its
healthcare industry.","['Belal Baaquie', 'Bertrand M. Roehner', 'Qinghai Wang']",[],0,arXiv,http://arxiv.org/abs/1605.01949v1,False,True,False,False,False,118,Lisa M Lynch,Boston,Completed,1999,2001.0,"The first part of the project will examine the role of worker characteristics (education, training, gender, race, turnover), establishment characteristics (capital stock, materials, unionization, age of the establishment, computer usage, age of the capital stock) and innovations in workplace practices (profit-sharing, team work, employee involvement in decision making, TQM, reengineering, and job rotation) on labor productivity.  The second part of the analysis examines the relationship between workplace innovation and labor costs and profits.  We will analyze whether the relative wages of workers who work in businesses characterized by high performance workplace (HPW) practices reflect or not their  marginal products.  Finally we study how employer characteristics and past history affect the probability of adopting HPW practices and how this may affect analyses of the productivity effects of these practices."
"The miniJPAS survey quasar selection III: Classification with artificial
  neural networks and hybridisation","This paper is part of large effort within the J-PAS collaboration that aims
to classify point-like sources in miniJPAS, which were observed in 60 optical
bands over $\sim$ 1 deg$^2$ in the AEGIS field. We developed two algorithms
based on artificial neural networks (ANN) to classify objects into four
categories: stars, galaxies, quasars at low redshift ($z < 2.1)$, and quasars
at high redshift ($z \geq 2.1$). As inputs, we used miniJPAS fluxes for one of
the classifiers (ANN$_1$) and colours for the other (ANN$_2$). The ANNs were
trained and tested using mock data in the first place. We studied the effect of
augmenting the training set by creating hybrid objects, which combines fluxes
from stars, galaxies, and quasars. Nevertheless, the augmentation processing
did not improve the score of the ANN. We also evaluated the performance of the
classifiers in a small subset of the SDSS DR12Q superset observed by miniJPAS.
In the mock test set, the f1-score for quasars at high redshift with the
ANN$_1$ (ANN$_2$) are $0.99$ ($0.99$), $0.93$ ($0.92$), and $0.63$ ($0.57$) for
$17 < r \leq 20$, $20 < r \leq 22.5$, and $22.5 < r \leq 23.6$, respectively,
where $r$ is the J-PAS rSDSS band. In the case of low-redshift quasars,
galaxies, and stars, we reached $0.97$ ($0.97$), $0.82$ ($0.79$), and $0.61$
($0.58$); $0.94$ ($0.94$), $0.90$ ($0.89$), and $0.81$ ($0.80$); and $1.0$
($1.0$), $0.96$ ($0.94$), and $0.70$ ($0.52$) in the same r bins. In the SDSS
DR12Q superset miniJPAS sample, the weighted f1-score reaches 0.87 (0.88) for
objects that are mostly within $20 < r \leq 22.5$. Finally, we estimate the
number of point-like sources that are quasars, galaxies, and stars in miniJPAS.","['G. Martínez-Solaeche', 'Carolina Queiroz', 'R. M. González Delgado', 'Natália V. N. Rodrigues', 'R. García-Benito', 'Ignasi Pérez-Ràfols', 'L. Raul Abramo', 'Luis Díaz-García', 'Matthew M. Pieri', 'Jonás Chaves-Montero', 'A. Hernán-Caballero', 'J. E. Rodríguez-Martín', 'Silvia Bonoli', 'Sean S. Morrison', 'Isabel Márquez', 'J. M. Vílchez', 'C. López-Sanjuan', 'A. J. Cenarro', 'R. A. Dupke', 'A. Martín-Franch', 'J. Varel', 'H. Vázquez Ramió', 'D. Cristóbal-Hornillos', 'M. Moles', 'J. Alcaniz', 'N. Benitez', 'J. A. Fernández-Ontiveros', 'A. Ederoclite', 'V. Marra', 'C. Mendes de Oliveira', 'K. Taylor']",[],0,arXiv,http://arxiv.org/abs/2303.12684v1,False,True,False,False,False,123,Ann C Schatzer,UCLA,Completed,1999,1999.0,
"Identification and Estimation of Demand Models with Endogenous Product
  Entry and Exit","This paper deals with the endogeneity of firms' entry and exit decisions in
demand estimation. Product entry decisions lack a single crossing property in
terms of demand unobservables, which causes the inconsistency of conventional
methods dealing with selection. We present a novel and straightforward two-step
approach to estimate demand while addressing endogenous product entry. In the
first step, our method estimates a finite mixture model of product entry
accommodating latent market types. In the second step, it estimates demand
controlling for the propensity scores of all latent market types. We apply this
approach to data from the airline industry.","['Victor Aguirregabiria', 'Alessandro Iaria', 'Senay Sokullu']",[],0,arXiv,http://arxiv.org/abs/2308.14196v1,False,True,False,False,False,125,Ann C Schatzer,Washington,Completed,2000,2000.0,"This research will empirically analyze how dynamic considerations affect competition among firms in concentrated markets.  When firms compete over time, they have more potential profit-maximizing strategies at their disposal.  For example, in markets where sunk entry costs are high, firms may overinvest in initial capacity to deter subsequent entry.  If additional firms fail to enter the market as a result, the incumbent can maintain its market power over time.  Similarly, where there is uncertainty about the profitability of entry opportunities, potential entrants may learn about markets by looking at the prices existing firms charge.  An incumbent might charge a lower than optimal price in early periods to signal to potential entrants that profits in this market would be low.  Such strategies involve firms bearing costs initially (through high set-up costs or sub-optimal profits), but enjoying excess profits through market power in several subsequent years."
The Structure and Incentives of a COVID related Emergency Wage Subsidy,"During recent crisis, wage subsidies played a major role in sheltering firms
and households from economic shocks. During COVID-19, most workers were
affected and many liberal welfare states introduced new temporary wage
subsidies to protected workers' earnings and employment (OECD, 2021). New wage
subsidies marked a departure from the structure of traditional income support
payments and required reform. This paper uses simulated datasets to assess the
structure and incentives of the Irish COVID-19 wage subsidy scheme (CWS) under
five designs. We use a nowcasting approach to update 2017 microdata, producing
a near real time picture of the labour market at the peak of the crisis. Using
microsimulation modelling, we assess the impact of different designs on income
replacement, work incentives and income inequality. Our findings suggest that
pro rata designs support middle earners more and flat rate designs support low
earners more. We find evidence for strong work disincentives under all designs,
though flat rate designs perform better. Disincentives are primarily driven by
generous unemployment payments and work related costs. The impact of design on
income inequality depends on the generosity of payments. Earnings related pro
rata designs were associated to higher market earnings inequality. The
difference in inequality levels falls once benefits, taxes and work related
costs are considered. In our discussion, we turn to transaction costs, the
rationale for reform and reintegration of CWS. We find some support for the
claim that design changes were motivated by political considerations. We
suggest that establishing permanent wage subsidies based on sectorial turnover
rules could offer enhanced protection to middle-and high-earners and reduce
uncertainty, the need for reform, and the risk of politically motivated
designs.","['Jules Linden', ""Cathal O'Donoghue"", 'Denisa M. Sologon']",[],0,arXiv,http://arxiv.org/abs/2108.04198v1,False,True,False,False,False,126,Ann C Schatzer,Washington,Completed,1998,1999.0,"This study, which will be a joint project of BEA's International Investment and Regional Economic Analysis Divisions, will compare the geographic distribution of foreign- and U.S.-owned establishments and attempt to explain the reasons for any differences that are observed.  Location patterns of both manufacturing and nonmanufacturing establishments will be examined at the sub-state level by using county-level data available from the Economic Censuses and SSEL to create totals for foreign- and U.S.-owned establishments for BEA ""Economic Areas"" and ""Component Economic Areas."""
On estimating Armington elasticities for Japan's meat imports,"By fully accounting for the distinct tariff regimes levied on imported meat,
we estimate substitution elasticities of Japan's two-stage import aggregation
functions for beef, chicken and pork. While the regression analysis crucially
depends on the price that consumers face, the post-tariff price of imported
meat depends not only on ad valorem duties but also on tariff rate quotas and
gate price system regimes. The effective tariff rate is consequently evaluated
by utilizing monthly transaction data. To address potential endogeneity
problems, we apply exchange rates that we believe to be independent of the
demand shocks for imported meat. The panel nature of the data allows us to
retrieve the first-stage aggregates via time dummy variables, free of demand
shocks, to be used as part of the explanatory variable and as an instrument in
the second-stage regression.","['Satoshi Nakano', 'Kazuhiko Nishimura']",[],0,arXiv,http://arxiv.org/abs/2210.05358v2,False,True,False,False,False,127,Catherine Armington,Washington,Completed,1999,1999.0,"Although the United States has a relatively high level of entrepreneurial activity in comparison to other major industrialized countries (see Reynolds, Hay, and Camp, 1999), there is substantial geographic and sector diversity in entrepreneurial activity within the U.S. This proposal specifies a method of measuring this geographical diversity in entrepreneurial activity, and identifying important regional characteristics associated with differences in various types of entrepreneurial activity.  This analysis, and the regional data constructed for it, will serve as a starting point for a more extensive research project focused on the diversity in business entry rates."
"Identification of Regulatory Requirements Relevant to Business
  Processes: A Comparative Study on Generative AI, Embedding-based Ranking,
  Crowd and Expert-driven Methods","Organizations face the challenge of ensuring compliance with an increasing
amount of requirements from various regulatory documents. Which requirements
are relevant depends on aspects such as the geographic location of the
organization, its domain, size, and business processes. Considering these
contextual factors, as a first step, relevant documents (e.g., laws,
regulations, directives, policies) are identified, followed by a more detailed
analysis of which parts of the identified documents are relevant for which step
of a given business process. Nowadays the identification of regulatory
requirements relevant to business processes is mostly done manually by domain
and legal experts, posing a tremendous effort on them, especially for a large
number of regulatory documents which might frequently change. Hence, this work
examines how legal and domain experts can be assisted in the assessment of
relevant requirements. For this, we compare an embedding-based NLP ranking
method, a generative AI method using GPT-4, and a crowdsourced method with the
purely manual method of creating relevancy labels by experts. The proposed
methods are evaluated based on two case studies: an Australian insurance case
created with domain experts and a global banking use case, adapted from SAP
Signavio's workflow example of an international guideline. A gold standard is
created for both BPMN2.0 processes and matched to real-world textual
requirements from multiple regulatory documents. The evaluation and discussion
provide insights into strengths and weaknesses of each method regarding
applicability, automation, transparency, and reproducibility and provide
guidelines on which method combinations will maximize benefits for given
characteristics such as process usage, impact, and dynamics of an application
scenario.","['Catherine Sai', 'Shazia Sadiq', 'Lei Han', 'Gianluca Demartini', 'Stefanie Rinderle-Ma']",[],0,arXiv,http://arxiv.org/abs/2401.02986v1,False,True,False,False,False,127,Catherine Armington,Washington,Completed,1999,1999.0,"Although the United States has a relatively high level of entrepreneurial activity in comparison to other major industrialized countries (see Reynolds, Hay, and Camp, 1999), there is substantial geographic and sector diversity in entrepreneurial activity within the U.S. This proposal specifies a method of measuring this geographical diversity in entrepreneurial activity, and identifying important regional characteristics associated with differences in various types of entrepreneurial activity.  This analysis, and the regional data constructed for it, will serve as a starting point for a more extensive research project focused on the diversity in business entry rates."
"An Algorithm to Extract Rules from Artificial Neural Networks for
  Medical Diagnosis Problems","Artificial neural networks (ANNs) have been successfully applied to solve a
variety of classification and function approximation problems. Although ANNs
can generally predict better than decision trees for pattern classification
problems, ANNs are often regarded as black boxes since their predictions cannot
be explained clearly like those of decision trees. This paper presents a new
algorithm, called rule extraction from ANNs (REANN), to extract rules from
trained ANNs for medical diagnosis problems. A standard three-layer feedforward
ANN with four-phase training is the basis of the proposed algorithm. In the
first phase, the number of hidden nodes in ANNs is determined automatically by
a constructive algorithm. In the second phase, irrelevant connections and input
nodes are removed from trained ANNs without sacrificing the predictive accuracy
of ANNs. The continuous activation values of the hidden nodes are discretized
by using an efficient heuristic clustering algorithm in the third phase.
Finally, rules are extracted from compact ANNs by examining the discretized
activation values of the hidden nodes. Extensive experimental studies on three
benchmark classification problems, i.e. breast cancer, diabetes and lenses,
demonstrate that REANN can generate high quality rules from ANNs, which are
comparable with other methods in terms of number of rules, average number of
conditions for a rule, and predictive accuracy.","['S. M. Kamruzzaman', 'Md. Monirul Islam']",[],0,arXiv,http://arxiv.org/abs/1009.4566v1,False,True,False,False,False,128,Ann C Schatzer,Boston,Completed,1999,2000.0,"This project will identify the effect of information technology on the location of economic activity. Advances in information technology have the potential to affect firms   location in many ways: with new technologies, a firm might find that it no longer needs to be in cities, or it might no longer need to be near rural clients; a firm might decide to disperse its operations over several distant sites, or it might decide to consolidate in one location. The goal of this research is to determine what kind of places   the central cities of large metropolitan areas, suburbs of large metropolitan areas, smaller metropolitan areas, small towns, or rural areas, for instance   are benefiting from firms   adoption of information technology. 
By linking county-level data on changes in firm location with industry-level data on information technology usage, this research will distinguish which of the popular notions about technology and cities is accurate. The methodology for this study consists of local industry growth regressions. The dependent variable is the employment change over time in local industry (that is, an industry-county cell). The independent variable of interest is the interaction between information technology usage and location characteristics, like metropolitan area size and county density. The coefficients on these interaction terms will identify whether information technology favors employment shifts to big cities, smaller cities, suburbs, or rural areas. 
The success of this research depends on the quality of the firm location data. The most appropriate firm location data for this study is the Longitudinal Enterprise and Establishment Microdata (LEEM) file. The LEEM file has the advantages of tracking individual establishments over time; of identifying the firm that an establishment belongs to; of providing exact employment counts instead of estimates; and of covering all sectors including services, which is the sector most likely to be affected by advances in information technology. "
Optimal Ordering Policies for Multi-Echelon Supply Networks,"In this paper, we formulate an optimal ordering policy as a stochastic
control problem where each firm decides the amount of input goods to order from
their upstream suppliers based on the current inventory level of its output
good. For this purpose, we provide a closed-form solution for the optimal
request of the raw materials for given a fixed production policy. We implement
the proposed policy on a 15-firm acyclic network based on a real product supply
chain. We first simulate ideal demand situations, and then we implement
demand-side shocks (i.e., demand levels outside of those considered in the
policy formulation) and supply-side shocks (i.e., halts in production for some
suppliers) to evaluate the robustness of the proposed policies.","['Jose I. Caiza', 'Ian Walter', 'Jitesh H. Panchal', 'Junjie Qin', 'Philip E. Pare']",[],0,arXiv,http://arxiv.org/abs/2209.04789v1,False,True,False,False,False,129,Chad Shirley,UCLA,Completed,1999,1999.0,This project assesses the potential benefits of a range of transportation policies by analyzing how transportation systems  affect firms' inventory and logistics behavior
Impact of carbon market on production emissions,"The aim of this paper is to address the effect of the carbon emission
allowance market on the production policy of a large polluter production firm.
We investigate this effect in two cases; when the large polluter cannot affect
the risk premium of the allowance market, and when it can change the risk
premium by its production. In this simple model, we ignore any possible
investment of the firm in pollution reducing technologies. We formulate the
problem of optimal production by a stochastic optimization problem. Then, we
show that, as expected, the market reduces the optimal production policy in the
first case if the firm is not given a generous initial cheap allowance package.
However, when the large producer activities can change the market risk premium,
the cut on the production and consequently pollution cannot be guaranteed. In
fact, there are cases in this model when the optimal production is {\it always}
larger than expected, and an increase in production, and thus pollution, can
increase the profit of the firm. We conclude that some of the parameters of the
market which contribute to this effect can be wisely controlled by the
regulators in order to diminish this manipulative behavior of the firm.","['Arash Fahim', 'Nizar Touzi']",[],0,arXiv,http://arxiv.org/abs/2312.03665v1,False,True,False,False,False,129,Chad Shirley,UCLA,Completed,1999,1999.0,This project assesses the potential benefits of a range of transportation policies by analyzing how transportation systems  affect firms' inventory and logistics behavior
Talent Hoarding in Organizations,"Most organizations rely on managers to identify talented workers. However,
managers who are evaluated on team performance have an incentive to hoard
workers. This study provides the first empirical evidence of talent hoarding
using personnel records and survey evidence from a large manufacturing firm.
Talent hoarding is reported by three-fourths of managers, is detectable in
managerial decisions, and occurs more frequently when hoarding incentives are
stronger. Using quasi-random variation in exposure to talent hoarding, I
demonstrate that hoarding deters workers from applying to new positions,
inhibiting worker career progression and altering the allocation of talent in
the firm.",['Ingrid Haegele'],[],0,arXiv,http://arxiv.org/abs/2206.15098v2,False,True,False,False,False,137,Ann C Schatzer,Washington,Completed,1999,2002.0,
"Talents from Abroad. Foreign Managers and Productivity in the United
  Kingdom","In this paper, we test the contribution of foreign management on firms'
competitiveness. We use a novel dataset on the careers of 165,084 managers
employed by 13,106 companies in the United Kingdom in the period 2009-2017. We
find that domestic manufacturing firms become, on average, between 7% and 12%
more productive after hiring the first foreign managers, whereas foreign-owned
firms register no significant improvement. In particular, we test that previous
industry-specific experience is the primary driver of productivity gains in
domestic firms (15.6%), in a way that allows the latter to catch up with
foreign-owned firms. Managers from the European Union are highly valuable, as
they represent about half of the recruits in our data. Our identification
strategy combines matching techniques, difference-in-difference, and
pre-recruitment trends to challenge reverse causality. Results are robust to
placebo tests and to different estimators of Total Factor Productivity.
Eventually, we argue that upcoming limits to the mobility of foreign talents
after the Brexit event can hamper the allocation of productive managerial
resources.","['Dimitrios Exadaktylos', 'Massimo Riccaboni', 'Armando Rungi']",[],0,arXiv,http://arxiv.org/abs/2007.04055v1,False,True,False,False,False,137,Ann C Schatzer,Washington,Completed,1999,2002.0,
"Developing Talent from a Supply-Demand Perspective: An Optimization
  Model for Managers","While executives emphasize that human resources (HR) are a firm's biggest
asset, the level of research attention devoted to planning talent pipelines for
complex global organizational environments does not reflect this emphasis.
Numerous challenges exist in establishing human resource management strategies
aligned with strategic operations planning and growth strategies. We generalize
the problem of managing talent from a supply-demand standpoint through a
resource acquisition lens, to an industrial business case where an organization
recruits for multiple roles given a limited pool of potential candidates
acquired through a limited number of recruiting channels. In this context, we
develop an innovative analytical model in a stochastic environment to assist
managers with talent planning in their organizations. We apply supply chain
concepts to the problem, whereby individuals with specific competencies are
treated as unique products. We first develop a multi-period mixed integer
nonlinear programming model and then exploit chance-constrained programming to
a linearized instance of the model to handle stochastic parameters, which
follow any arbitrary distribution functions. Next, we use an empirical study to
validate the model with a large global manufacturing company, and demonstrate
how the proposed model can effectively manage talents in a practical context. A
stochastic analysis on the implemented case study reveals that a reasonable
improvement is derived from incorporating randomness into the problem.","['Hadi Moheb-Alizadeh', 'Robert B. Handfield']",[],0,arXiv,http://arxiv.org/abs/1708.01983v1,False,True,False,False,False,137,Ann C Schatzer,Washington,Completed,1999,2002.0,
"The Two-Component Camassa-Holm Equations CH(2,1) and CH(2,2):
  First-Order Integrating Factors and Conservation Laws","Recently, Holm and Ivanov, proposed and studied a class of multi-component
generalisations of the Camassa-Holm equations [D D Holm and R I Ivanov,
Multi-component generalizations of the CH equation: geometrical aspects,
peakons and numerical examples, J. Phys A: Math. Theor. 43, 492001 (20pp),
2010]. We consider two of those systems, denoted by Holm and Ivanov by CH(2,1)
and CH(2,2), and report a class of integrating factors and its corresponding
conservation laws for these two systems. In particular, we obtain the complete
sent of first-order integrating factors for the systems in Cauchy-Kovalevskaya
form and evaluate the corresponding sets of conservation laws for CH(2,1) and
CH(2,2).","['Marianna Euler', 'Norbert Euler', 'Thomas Wolf']",[],0,arXiv,http://arxiv.org/abs/1204.3738v2,False,True,False,False,False,139,Thomas J Holmes,Washington,Completed,1999,2000.0,"Recently, there has been great interest in theories that place the geographic distribution of demand front and center in accounting for the geographic distribution of economic activity.   One implication of this theory that has received attention in the empirical literature is the home-market effect; i.e., that an increase in demand should be followed by an increase in production that exceeds the increase in demand (a slope greater than one).   However, the existing literature has not taken into account problems that arise when product differentiation is important and data is aggregated across differentiated products.  In this case, the relationship between production and local demand should be convex and the slope will be greater than one only when evaluated at high levels of demand.
The proposed project will use data from the Census of  Wholesale trade  to examine the relationship between wholesale activity and local demand as proxied by population.   The project will examine the extent to which the relationship is convex and whether the slope is relatively steep when evaluated at high levels of demand, as predicted by the theory.  The paper will also examine whether the convexity in the relationship for aggregate-level industries emerges from the aggregation of narrowly-defined industries, as predicted by the theory."
Ramsey numbers of cubes versus cliques,"The cube graph Q_n is the skeleton of the n-dimensional cube. It is an
n-regular graph on 2^n vertices. The Ramsey number r(Q_n, K_s) is the minimum N
such that every graph of order N contains the cube graph Q_n or an independent
set of order s. Burr and Erdos in 1983 asked whether the simple lower bound
r(Q_n, K_s) >= (s-1)(2^n - 1)+1 is tight for s fixed and n sufficiently large.
We make progress on this problem, obtaining the first upper bound which is
within a constant factor of the lower bound.","['David Conlon', 'Jacob Fox', 'Choongbum Lee', 'Benny Sudakov']",[],0,arXiv,http://arxiv.org/abs/1208.1732v2,False,True,False,False,False,146,David S Lee,Berkeley,Completed,2002,2003.0,"The purpose of this project is two-fold: 1) To exploit information on voting patterns from NLRB representation elections to generate quasi-experimental estimates of the impact of certification and unionization on business establishment survival and employment using the Longitudinal Research Database (LRD), and 2) to assess the reliability of both the LRD employment variable as well as its record linkage over time using an alternative and independent measure obtained from a commercially-available establishment-level database on survival and employment levels."
"A Simple Combinatorial Criterion for Projective Toric Manifolds with
  Dual Defect","We show that any smooth lattice polytope P with codegree greater or equal
than (dim(P)+3)/2 (or equivalently, with degree smaller than dim(P)/2), defines
a dual defective projective toric manifold. This implies that P is Q-normal (in
the terminology of a recent paper by Di Rocco, Piene and the first author) and
answers partially an adjunction-theoretic conjecture by Beltrametti and
Sommese. Also, it follows that smooth lattice polytopes with this property are
precisely strict Cayley polytopes, which completes the answer of a question of
Batyrev and the second author in the nonsingular case.","['Alicia Dickenstein', 'Benjamin Nill']",[],0,arXiv,http://arxiv.org/abs/1001.2792v1,False,True,False,False,False,152,Alicia P Cackley,Washington,Completed,2001,2002.0,"This project examines how benefits received by minorities and non-minorities differ under the current Social Security system, and how benefits might differ under a reformed Social Security system based on individual retirement accounts.  To accomplish our work, we will analyze measures of Social Security's moneysworth separately for minorities and non-minorities.  More specifically, we will calculate rates of return to Social Security, benefits received net of taxes paid into the system, and benefit/tax ratios, including both primary and auxiliary benefits in our analysis.  This information will provide a more thorough understanding of the Social Security's ""moneysworth"" for both minorities and non-minorities and will allow us to assess the impact on both groups of some proposed changes to the Social Security system.  "
"Asymmetric networks, clientelism and their impacts: households' access
  to workfare employment in rural India","In this paper we explore two intertwined issues. First, using primary data we
examine the impact of asymmetric networks, built on rich relational information
on several spheres of living, on access to workfare employment in rural India.
We find that unidirectional relations, as opposed to reciprocal relations, and
the concentration of such unidirectional relations increase access to workfare
jobs. Further in-depth exploration provides evidence that patron-client
relations are responsible for this differential access to such employment for
rural households. Complementary to our empirical exercises, we construct and
analyse a game-theoretical model supporting our findings.","['Anindya Bhattacharya', 'Anirban Kar', 'Alita Nandi']",[],0,arXiv,http://arxiv.org/abs/2304.04236v1,False,True,False,False,False,155,Robin M Leichenko,Washington,Completed,2002,2003.0,"The goal of this project is to improve the utility of Census Bureau data by providing new measures of the impacts of U.S. international manufacturing trade on employment, wages, and income inequality in rural (nonmetro) counties of the United States. The project involves two components: 1) regression analysis of the employment impacts of international trade on rural counties and 2) regression analysis of the impacts of international trade on wages and income inequality within and across rural counties. The project will utilize 4-digit industrial shipment and foreign export shipment data from the Longitudinal Research Database (LRD). The LRD data will be aggregated to the county-level and will be linked with 4-digit international import and export data and exchange rate data to create a unique, county-level, international export and import exchange rate dataset. This new dataset will be used to estimate via regression modeling the county-level employment, income and wage effects of changing patterns of international trade between 1972 and 1997. The employment and wage models will also utilize LRD data on average, production, and nonproduction worker manufacturing employment wages. Other data for the project will come from publicly available sources including the Regional Economic Information Systems (REIS) data series and the Census of Population. Improved understanding of the quality of census data will be accomplished through analysis of the 1997 export estimates. These estimates, which have not yet been evaluated in comparison to prior census years, will be compared to export measures in the 1987 and 1992 Economic Censuses. Enhancement of the data collected in the Economic Census will be accomplished through development of a new dataset that relates international trade patterns to rural manufacturing."
"Dynamical geography and transition paths of Sargassum in the tropical
  Atlantic","By analyzing a time-homogeneous Markov chain constructed using trajectories
of undrogued drifting buoys from the NOAA's Global Drifter Program, we find
that probability density can distribute in a manner that resembles very closely
the recently observed recurrent belt of high Sargassum density in the tropical
Atlantic between 5--10$^{\circ}$N, coined the \emph{Great Atlantic Sargassum
Belt} (\emph{GASB}). A spectral analysis of the associated transition matrix
further unveils a forward attracting almost-invariant set in the northwestern
Gulf of Mexico with a corresponding basin of attraction disconnected from the
Sargasso Sea, but including the nutrient-rich regions around the Amazon and
Orinoco rivers mouths and also the upwelling system off the northern coast of
west Africa. This represents a data-based inference of potential remote sources
of Sargassum recurrently invading the Intra-Americas Seas (IAS). By further
applying Transition Path Theory (TPT) on the data-derived Markov chain model,
two potential pathways for Sargassum into the IAS from the upwelling system off
the coast of Africa are revealed. One TPT-inferred pathway takes place along
the GASB. The second pathway is more southern and slower, first going through
the Gulf of Guinea, then across the tropical Atlantic toward the mouth of the
Amazon River, and finally along the northeastern South American margin. The
existence of such a southern TPT-inferred pathway may have consequences for
bloom stimulation by nutrients from river runoff.","['F. J. Beron-Vera', 'M. J. Olascoaga', 'N. Putman', 'J. Trinanes', 'G. J. Goni', 'R. Lumpkin']",[],0,arXiv,http://arxiv.org/abs/2209.06290v1,False,True,False,False,False,158,John J Beggs,Washington,Completed,2002,2004.0,"The research we propose in this project centers on the definition of oil and gas involved areas along the coast of Gulf of Mexico. Previous research has necessarily relied on publicly available data sources such as household census PUMS (public-use microdata samples) files, (STF files) summary tape files, economic censuses, and County Business Patterns data. For public data of this sort, geographic specificity is a serious concern.  In the case of Gulf of Mexico oil and gas involvement, delineations have been based on counties (parishes).  Coastal counties and parishes can be quite large in land area.  As such, county-based involvement schema can be less than precise.  The internal files available to us at the Census Bureau are the microdata (household-level and establishment-level) which underlie the public-use data that are widely published.  The internal microdata code residential geography to the block level (population censuses) and the tract level (economic censuses). Place of work geography is coded to the tract level as well.  As such, we are in position to specify with geographic precision those land areas along the coast of the Gulf of Mexico that exhibit significant involvement in the oil and gas industries.  In doing so, we will conduct analysis of significance to the Minerals Management Service.  We would like to conduct the initial phase of the project at CES headquarters.  We would like to have the option of moving the project to Carnegie Mellon University or Chicago (should an RDC open there) for the later phases of the project."
"Preventing Eviction-Caused Homelessness through ML-Informed Distribution
  of Rental Assistance","Rental assistance programs provide individuals with financial assistance to
prevent housing instabilities caused by evictions and avert homelessness. Since
these programs operate under resource constraints, they must decide who to
prioritize. Typically, funding is distributed by a reactive or first-come-first
serve allocation process that does not systematically consider risk of future
homelessness. We partnered with Allegheny County, PA to explore a proactive
allocation approach that prioritizes individuals facing eviction based on their
risk of future homelessness. Our ML system that uses state and county
administrative data to accurately identify individuals in need of support
outperforms simpler prioritization approaches by at least 20% while being fair
and equitable across race and gender. Furthermore, our approach would identify
28% of individuals who are overlooked by the current process and end up
homeless. Beyond improvements to the rental assistance program in Allegheny
County, this study can inform the development of evidence-based decision
support tools in similar contexts, including lessons about data needs, model
design, evaluation, and field validation.","['Catalina Vajiac', 'Arun Frey', 'Joachim Baumann', 'Abigail Smith', 'Kasun Amarasinghe', 'Alice Lai', 'Kit Rodolfa', 'Rayid Ghani']",[],0,arXiv,http://arxiv.org/abs/2403.12599v1,False,True,False,False,False,160,Dirk W Early,Berkeley,Completed,2001,2001.0,"This study would use the National Survey of Homeless Assistance Providers and Clients (NSHAPC) to study the causes of homelessness in the U.S. and analyze policies designed to reduce the problem.  Researchers are still at odds over the main causes of homelessness and, consequently, the best solutions to the problem.  The NSHAPC data will allow for a comprehensive examination of the causes of homelessness across the U.S.  Furthermore, the findings can be used to estimate the role cash and in-kind assistance programs play in reducing the number of homeless in this country and to simulate the effects of welfare reform."
"Are the Spatial Concentrations of Core-City and Suburban Poverty
  Converging in the Rust Belt?","Decades of deindustrialization have led to economic decline and population
loss throughout the U.S. Midwest, with the highest national poverty rates found
in Detroit, Cleveland, and Buffalo. This poverty is often confined to core
cities themselves, however, as many of their surrounding suburbs continue to
prosper. Poverty can therefore be highly concentrated at the MSA level, but
more evenly distributed within the borders of the city proper. One result of
this disparity is that if suburbanites consider poverty to be confined to the
central city, they might be less willing to devote resources to alleviate it.
But due to recent increases in suburban poverty, particularly since the 2008
recession, such urban-suburban gaps might be shrinking. Using Census
tract-level data, this study quantifies poverty concentrations for four ""Rust
Belt"" MSAs, comparing core-city and suburban concentrations in 2000, 2010, and
2015. There is evidence of a large gap between core cities and outlying areas,
which is closing in the three highest-poverty cities, but not in Milwaukee. A
set of four comparison cities show a smaller, more stable city-suburban divide
in the U.S. ""Sunbelt,"" while Chicago resembles a ""Rust Belt"" metro.",['Scott W. Hegerty'],[],0,arXiv,http://arxiv.org/abs/2105.07824v1,False,True,False,False,False,161,Robert T Greenbaum,CMU,Completed,2002,2002.0,"The proposed research seeks to study spatial patterns of intrametropolitan job creation and destruction in large urban areas.  The analysis will examine the role of establishment births, deaths, expansions, and contractions in both cities and suburbs.  Comparisons will be made between growing and shrinking areas, and additional analysis will examine the reallocation of employment across different sectors of the economy in the various types of places.  The research will also investigate geographic differences in job quality and will attempt to uncover correlations between the spatial distributions of household characteristics and the details of subsequent employment dynamics."
Small cities face greater impact from automation,"The city has proven to be the most successful form of human agglomeration and
provides wide employment opportunities for its dwellers. As advances in
robotics and artificial intelligence revive concerns about the impact of
automation on jobs, a question looms: How will automation affect employment in
cities? Here, we provide a comparative picture of the impact of automation
across U.S. urban areas. Small cities will undertake greater adjustments, such
as worker displacement and job content substitutions. We demonstrate that large
cities exhibit increased occupational and skill specialization due to increased
abundance of managerial and technical professions. These occupations are not
easily automatable, and, thus, reduce the potential impact of automation in
large cities. Our results pass several robustness checks including potential
errors in the estimation of occupational automation and sub-sampling of
occupations. Our study provides the first empirical law connecting two societal
forces: urban agglomeration and automation's impact on employment.","['Morgan R. Frank', 'Lijun Sun', 'Manuel Cebrian', 'Hyejin Youn', 'Iyad Rahwan']",[],0,arXiv,http://arxiv.org/abs/1705.05875v2,False,True,False,False,False,161,Robert T Greenbaum,CMU,Completed,2002,2002.0,"The proposed research seeks to study spatial patterns of intrametropolitan job creation and destruction in large urban areas.  The analysis will examine the role of establishment births, deaths, expansions, and contractions in both cities and suburbs.  Comparisons will be made between growing and shrinking areas, and additional analysis will examine the reallocation of employment across different sectors of the economy in the various types of places.  The research will also investigate geographic differences in job quality and will attempt to uncover correlations between the spatial distributions of household characteristics and the details of subsequent employment dynamics."
"An Empirical Study on LLM-based Classification of Requirements-related
  Provisions in Food-safety Regulations","As Industry 4.0 transforms the food industry, the role of software in
achieving compliance with food-safety regulations is becoming increasingly
critical. Food-safety regulations, like those in many legal domains, have
largely been articulated in a technology-independent manner to ensure their
longevity and broad applicability. However, this approach leaves a gap between
the regulations and the modern systems and software increasingly used to
implement them. In this article, we pursue two main goals. First, we conduct a
Grounded Theory study of food-safety regulations and develop a conceptual
characterization of food-safety concepts that closely relate to systems and
software requirements. Second, we examine the effectiveness of two families of
large language models (LLMs) -- BERT and GPT -- in automatically classifying
legal provisions based on requirements-related food-safety concepts. Our
results show that: (a) when fine-tuned, the accuracy differences between the
best-performing models in the BERT and GPT families are relatively small.
Nevertheless, the most powerful model in our experiments, GPT-4o, still
achieves the highest accuracy, with an average Precision of 89% and an average
Recall of 87%; (b) few-shot learning with GPT-4o increases Recall to 97% but
decreases Precision to 65%, suggesting a trade-off between fine-tuning and
few-shot learning; (c) despite our training examples being drawn exclusively
from Canadian regulations, LLM-based classification performs consistently well
on test provisions from the US, indicating a degree of generalizability across
regulatory jurisdictions; and (d) for our classification task, LLMs
significantly outperform simpler baselines constructed using long short-term
memory (LSTM) networks and automatic keyword extraction.","['Shabnam Hassani', 'Mehrdad Sabetzadeh', 'Daniel Amyot']",[],0,arXiv,http://arxiv.org/abs/2501.14683v1,False,True,False,False,False,162,Michael E Ollinger,Washington,Completed,2004,2007.0,"This project examines the impact of HACCP regulation on the meat and poultry industry and the adoption and use of food safety technologies and methods.  After first matching outside data sets with Census data, this project will examine the 1) characteristics of food safety innovation adopters,2) impact of food safety innovations and plant technology on pathogen-reduction, 3) marginal benefits and costs of HACCP practices, 4) plant exits under HACCP regulation, 5) impact of HACCP plans and food safety innovations on plant costs.  "
"U.S. Long-Term Earnings Outcomes by Sex, Race, Ethnicity, and Place of
  Birth","This paper is part of the Global Income Dynamics Project cross-country
comparison of earnings inequality, volatility, and mobility. Using data from
the U.S. Census Bureau's Longitudinal Employer-Household Dynamics (LEHD)
infrastructure files we produce a uniform set of earnings statistics for the
U.S. From 1998 to 2019, we find U.S. earnings inequality has increased and
volatility has decreased. The combination of increased inequality and reduced
volatility suggest earnings growth differs substantially across different
demographic groups. We explore this further by estimating 12-year average
earnings for a single cohort of age 25-54 eligible workers. Differences in
labor supply (hours paid and quarters worked) are found to explain almost 90%
of the variation in worker earnings, although even after controlling for labor
supply substantial earnings differences across demographic groups remain
unexplained. Using a quantile regression approach, we estimate counterfactual
earnings distributions for each demographic group. We find that at the bottom
of the earnings distribution differences in characteristics such as hours paid,
geographic division, industry, and education explain almost all the earnings
gap, however above the median the contribution of the differences in the
returns to characteristics becomes the dominant component.","['Kevin L. McKinney', 'John M. Abowd', 'Hubert P. Janicki']",[],0,arXiv,http://arxiv.org/abs/2112.05822v1,True,True,False,False,False,163,Mary E Farrell,Washington,Completed,2005,2006.0,"We will address the following research questions on the relationship between the long-term earnings patterns of the working poor who are eligible for the food stamp program (FSP) and their participation in the program: 
• How do the historical earnings patterns of the 1996 cohort of participants and eligible nonparticipants among the working poor differ, and what are the explanations for any differences? 
• To what extent are historical and future earnings patterns predictive of participation in the FSP for the 1996 cohort, given individual characteristics and state welfare policies? 
• To what extent do historical earnings of the 1996 cohort predict future earnings, and how is that related to FSP participation? 
• How do the earnings patterns of the 1996 cohort compare to an earlier cohort from 1992? 
A major concern among policy makers is that a significant number of eligible households, especially the working poor, do not participate in the program. One study found that only 46 percent of working FSP eligible households participated in the program in 1994, compared to an aggregate rate of 69 percent for all FSP eligible households. Some argue that these low participation rates might be an indication that the FSP is not fulfilling its primary purpose of providing food assistance to all who need it. Another explanation is that these households are eligible for a short period of time and anticipate an increase in their earnings. 
We would like to use restricted research files of the Survey of Income and Program Participation (SIPP) matched to the Social Security Administration’s Summary Earnings Records (SER) to identify long-term earnings patterns of the working poor. To date, there is very limited information on the historical earnings patterns of these groups, primarily because of data limitations. The matched SIPP/SER data address this limitation by providing complete earnings histories for nationally representative samples, including large samples of the working poor. Hence, our analysis will provide the first comprehensive analysis of long-term earnings patterns of the working poor. In addition, this study will provide the USDA with important information regarding the reliability of the participation estimates it obtains from the SIPP. The accuracy of the number of eligible persons is based, in part, on the accuracy of the earnings estimates. This is an important concern, as the share of food stamp recipients who are working has been growing in recent years. It might be, for instance, that many working poor households that appear eligible for the FSP based on SIPP data, but say they do not participate, are really ineligible because their earnings are higher than what they report. Our study will examine the accuracy of the earnings data in the SIPP core files, as well as the validity and usefulness of the employment information in the SIPP’s employment history topical module. 
We are interested in the entire history of earnings because for policy reasons it is important to understand how longer-term earnings patterns for adults in working poor families are related to participation in the FSP. The SIPP can support limited analysis of this issue through use of self-reported income over the panel period and some very limited information that is captured in an employment history module. We would like to use the matched data to assess whether better information about past or expected future earnings would improve our understanding of food stamp participation. In summary, this study will provide a better understanding of (1) who under-reports or overreports earnings and employment on the SIPP; (2) how the underreporting or overreporting affects findings on the working poor population and the take-up rates of the FSP; and (3) whether individuals reporting employment on the employment history topical module are able to recall past jobs."
"Personalized Food Image Classification: Benchmark Datasets and New
  Baseline","Food image classification is a fundamental step of image-based dietary
assessment, enabling automated nutrient analysis from food images. Many current
methods employ deep neural networks to train on generic food image datasets
that do not reflect the dynamism of real-life food consumption patterns, in
which food images appear sequentially over time, reflecting the progression of
what an individual consumes. Personalized food classification aims to address
this problem by training a deep neural network using food images that reflect
the consumption pattern of each individual. However, this problem is
under-explored and there is a lack of benchmark datasets with individualized
food consumption patterns due to the difficulty in data collection. In this
work, we first introduce two benchmark personalized datasets including the
Food101-Personal, which is created based on surveys of daily dietary patterns
from participants in the real world, and the VFNPersonal, which is developed
based on a dietary study. In addition, we propose a new framework for
personalized food image classification by leveraging self-supervised learning
and temporal image feature information. Our method is evaluated on both
benchmark datasets and shows improved performance compared to existing works.
The dataset has been made available at:
https://skynet.ecn.purdue.edu/~pan161/dataset_personal.html","['Xinyue Pan', 'Jiangpeng He', 'Fengqing Zhu']",[],0,arXiv,http://arxiv.org/abs/2309.08744v1,False,True,False,False,False,163,Mary E Farrell,Washington,Completed,2005,2006.0,"We will address the following research questions on the relationship between the long-term earnings patterns of the working poor who are eligible for the food stamp program (FSP) and their participation in the program: 
• How do the historical earnings patterns of the 1996 cohort of participants and eligible nonparticipants among the working poor differ, and what are the explanations for any differences? 
• To what extent are historical and future earnings patterns predictive of participation in the FSP for the 1996 cohort, given individual characteristics and state welfare policies? 
• To what extent do historical earnings of the 1996 cohort predict future earnings, and how is that related to FSP participation? 
• How do the earnings patterns of the 1996 cohort compare to an earlier cohort from 1992? 
A major concern among policy makers is that a significant number of eligible households, especially the working poor, do not participate in the program. One study found that only 46 percent of working FSP eligible households participated in the program in 1994, compared to an aggregate rate of 69 percent for all FSP eligible households. Some argue that these low participation rates might be an indication that the FSP is not fulfilling its primary purpose of providing food assistance to all who need it. Another explanation is that these households are eligible for a short period of time and anticipate an increase in their earnings. 
We would like to use restricted research files of the Survey of Income and Program Participation (SIPP) matched to the Social Security Administration’s Summary Earnings Records (SER) to identify long-term earnings patterns of the working poor. To date, there is very limited information on the historical earnings patterns of these groups, primarily because of data limitations. The matched SIPP/SER data address this limitation by providing complete earnings histories for nationally representative samples, including large samples of the working poor. Hence, our analysis will provide the first comprehensive analysis of long-term earnings patterns of the working poor. In addition, this study will provide the USDA with important information regarding the reliability of the participation estimates it obtains from the SIPP. The accuracy of the number of eligible persons is based, in part, on the accuracy of the earnings estimates. This is an important concern, as the share of food stamp recipients who are working has been growing in recent years. It might be, for instance, that many working poor households that appear eligible for the FSP based on SIPP data, but say they do not participate, are really ineligible because their earnings are higher than what they report. Our study will examine the accuracy of the earnings data in the SIPP core files, as well as the validity and usefulness of the employment information in the SIPP’s employment history topical module. 
We are interested in the entire history of earnings because for policy reasons it is important to understand how longer-term earnings patterns for adults in working poor families are related to participation in the FSP. The SIPP can support limited analysis of this issue through use of self-reported income over the panel period and some very limited information that is captured in an employment history module. We would like to use the matched data to assess whether better information about past or expected future earnings would improve our understanding of food stamp participation. In summary, this study will provide a better understanding of (1) who under-reports or overreports earnings and employment on the SIPP; (2) how the underreporting or overreporting affects findings on the working poor population and the take-up rates of the FSP; and (3) whether individuals reporting employment on the employment history topical module are able to recall past jobs."
"Female teachers effect on male pupils' voting behavior and preference
  formation","This study examines the influence of learning in a female teacher homeroom
class in elementary school on pupils' voting behavior later in life, using
independently collected individual-level data. Further, we evaluate its effect
on preference for women's participation in the workplace in adulthood. Our
study found that having a female teacher in the first year of school makes
individuals more likely to vote for female candidates, and to prefer policy for
female labor participation in adulthood. However, the effect is only observed
among males, and not female pupils. These findings offer new evidence for the
female socialization hypothesis.",['Eiji Yamamura'],[],0,arXiv,http://arxiv.org/abs/2101.08487v1,False,True,False,False,False,170,Marigee Bacolod,UCLA,Completed,2002,2004.0,"As skilled women have responded to the rise in labor market opportunities and entered the professions, declining proportions of these women are in teaching. Employing data from the Integrated Public Use Microdata Series and the National Longitudinal Surveys of Young Men, Young Women, and Youth-79, I seek to estimate the effect of local labor market changes in teacher and in professional earnings opportunities on employment in teaching and in the quality of those who teach, as measured by performance on standardized tests."
"Three Approaches to the Automation of Laser System Alignment and Their
  Resource Implications: A Case Study","The alignment of optical systems is a critical step in their manufacture.
Alignment normally requires considerable knowledge and expertise of skilled
operators. The automation of such processes has several potential advantages,
but requires additional resource and upfront costs. Through a case study of a
simple two mirror system we identify and examine three different automation
approaches. They are: artificial neural networks; practice-led, which mimics
manual alignment practices; and design-led, modelling from first principles. We
find that these approaches make use of three different types of knowledge 1)
basic system knowledge (of controls, measurements and goals); 2) behavioural
skills and expertise, and 3) fundamental system design knowledge. We
demonstrate that the different automation approaches vary significantly in
human resources, and measurement sampling budgets. This will have implications
for practitioners and management considering the automation of such tasks.","['David A. Robb', 'Donald Risbridger', 'Ben Mills', 'Ildar Rakhmatulin', 'Xianwen Kong', 'Mustafa Erden', 'M. J. Daniel Esser', 'Richard M. Carter', 'Mike J. Chantler']",[],0,arXiv,http://arxiv.org/abs/2409.11090v1,False,True,False,False,False,171,William H Carter,Washington,Completed,2002,2005.0,"The project will use the National Employer Surveys (NES) to examine the use, causes and effects of various innovative human resource and organizational design practice (such as employee involvement plans, organizational learning practices, nonstandard employment relations and personnel practices). It will make use of the NES' detailed questions on these aspects in combination with NES and merged Census data on such outcome as establishment productivity and cost performance, and employee turnover and wages. Many analyzes will be longitudinal. Census bureau benefits include identification of new topics, estimation of otherwise unexamined population characteristics, and independent checks on the reliability of SSEL and Economic Survey data."
"Structuring the Chaos: Enabling Small Business Cyber-Security Risks &
  Assets Modelling with a UML Class Model","Small businesses are increasingly adopting IT, and consequently becoming more
vulnerable to cyber-incidents. Whilst small businesses are aware of the
cyber-security risks, many struggle with implementing mitigations. Some of
these can be traced to fundamental differences in the characteristics of small
business versus large enterprises where modern cyber-security solutions are
widely deployed.
  Small business specific cyber-security tools are needed. Currently available
cyber-security tools and standards assume technical expertise and time
resources often not practical for small businesses. Cyber-security competes
with other roles that small business owners take on, e.g. cleaning, sales etc.
A small business model, salient and implementable at-scale, with simplified
non-specialist terminologies and presentation is needed to encourage sustained
participation of all stakeholders, not just technical ones.
  We propose a new UML class (Small IT Data (SITD)) model to support the often
chaotic information-gathering phase of a small business' first foray into
cyber-security. The SITD model is designed in the UML format to help small
business implement technical solutions. The SITD model structure stays relevant
by using generic classes and structures that evolve with technology and
environmental changes. The SITD model keeps security decisions proportionate to
the business by highlighting relationships between business strategy tasks and
IT infrastructure.
  We construct a set of design principles to address small business
cyber-security needs. Model components are designed in response to these needs.
The uses of the SITD model are then demonstrated and design principles
validated by examining a case study of a real small business operational and IT
information. The SITD model's ability to illustrate breach information is also
demonstrated using the NotPetya incident.","['Tracy Tam', 'Asha Rao', 'Joanne Hall']",[],0,arXiv,http://arxiv.org/abs/2403.14872v1,False,True,False,False,False,188,Robert W Fairlie,Washington,Completed,2002,2003.0,"The high failure rates, low profits, and small employment sizes of black- and Hispanic-owned businesses, relative to their white and Asian counterparts, are of concern to many policy makers.  To date, we do not fully understand why black- and Hispanic-owned firms lag behind white and Asian-owned firms.  In the proposed research project, I will use data from the Characteristics of Business Owners (CBO) to explore the role that intergenerational links in self-employment play in contributing to racial differences in small business outcomes, such as failures, sales, profits, and employment size.  A careful examination of how family business experience differs by race and ethnicity may uncover some answers.  The inability of blacks and Hispanics to acquire business experience may be at the root of their limited success in business ownership.  Furthermore, family-owned businesses have historically provided a route out of poverty and into long-term, sometimes intergenerational, self-sufficiency for many families.  A better understanding of this process may shed light on why some families are not successful in their small businesses."
Tripartite Vector Representations for Better Job Recommendation,"Job recommendation is a crucial part of the online job recruitment business.
To match the right person with the right job, a good representation of job
postings is required. Such representations should ideally recommend jobs with
fitting titles, aligned skill set, and reasonable commute. To address these
aspects, we utilize three information graphs ( job-job, skill-skill, job-skill)
from historical job data to learn a joint representation for both job titles
and skills in a shared latent space. This allows us to gain a representation of
job postings/ resume using both elements, which subsequently can be combined
with location. In this paper, we first present how the presentation of each
component is obtained, and then we discuss how these different representations
are combined together into one single space to acquire the final
representation. The results of comparing the proposed methodology against
different base-line methods show significant improvement in terms of relevancy.","['Mengshu Liu', 'Jingya Wang', 'Kareem Abdelfatah', 'Mohammed Korayem']",[],0,arXiv,http://arxiv.org/abs/1907.12379v1,False,True,False,False,False,192,John R Logan,UCLA,Completed,2000,2003.0,"This project studies the residential and labor force positions of ethnic and racial groups in 1990.  This was a period of intense immigration and also secondary migration to new parts of the country.  The purpose is to assess the degree to which these groups experienced a process of assimilation into the mainstream or, alternatively, created or were confronted by enduring group boundaries.   "
"Estimating the Likelihood of Arrest from Police Records in Presence of
  Unreported Crimes","Many important policy decisions concerning policing hinge on our
understanding of how likely various criminal offenses are to result in arrests.
Since many crimes are never reported to law enforcement, estimates based on
police records alone must be adjusted to account for the likelihood that each
crime would have been reported to the police. In this paper, we present a
methodological framework for estimating the likelihood of arrest from police
data that incorporates estimates of crime reporting rates computed from a
victimization survey. We propose a parametric regression-based two-step
estimator that (i) estimates the likelihood of crime reporting using logistic
regression with survey weights; and then (ii) applies a second regression step
to model the likelihood of arrest. Our empirical analysis focuses on racial
disparities in arrests for violent crimes (sex offenses, robbery, aggravated
and simple assaults) from 2006--2015 police records from the National Incident
Based Reporting System (NIBRS), with estimates of crime reporting obtained
using 2003--2020 data from the National Crime Victimization Survey (NCVS). We
find that, after adjusting for unreported crimes, the likelihood of arrest
computed from police records decreases significantly. We also find that, while
incidents with white offenders on average result in arrests more often than
those with black offenders, the disparities tend to be small after accounting
for crime characteristics and unreported crimes.","['Riccardo Fogliato', 'Arun Kumar Kuchibhotla', 'Zachary Lipton', 'Daniel Nagin', 'Alice Xiang', 'Alexandra Chouldechova']",[],0,arXiv,http://arxiv.org/abs/2310.07935v1,False,False,False,False,True,194,Eric P Baumer,Chicago,Completed,2001,2003.0,"Although it has been well documented that a substantial portion of all crime experienced by citizens in the U.S. is not reported to the police, very few studies have systematically examined whether residents of certain types of communities are more, or less, likely to report crime victimizations to the police. This issue has not been addressed extensively largely because the data needed to do so—data on victims of crime and on the communities in which they reside—traditionally have not been available to researchers. If approved by the Census Bureau, the proposed research will use data from the 1995-2001 Area-Identified National Crime Victimization Survey (NCVS), linked with data from the Sample Survey of Law Enforcement Agencies (SSLEA), the Uniform Crime Reporting (UCR) program, and census data on tracts and places to investigate the effects of several characteristics of communities on the likelihood of police notification by crime victims. The community characteristics considered will include neighborhood features such as socioeconomic disadvantage, minority concentration, immigrant concentration, and residential instability, and place-level indicators such as the degree of local police involvement in community policing activities and the racial composition of the local police agency. The proposed research contributes significantly to the literature on victim crime reporting, and the analyses have important implications for macro-level research and theory tests which often assume little or no systematic variation in crime reporting across communities. In addition, the research will benefit the Census Bureau by adding contextual data to the NCVS, evaluating and improving the usefulness of the NCVS, using methodologies (e.g., survey regression techniques, multilevel modeling) that will enhance understanding of these data, and highlighting the value of the NCVS for cutting-edge theoretical and relevant research."
"Bayesian models to adjust for response bias in survey data for
  estimating rape and domestic violence rates from the NCVS","It is difficult to accurately estimate the rates of rape and domestic
violence due to the sensitive nature of these crimes. There is evidence that
bias in estimating the crime rates from survey data may arise because some
women respondents are ""gagged"" in reporting some types of crimes by the use of
a telephone rather than a personal interview, and by the presence of a spouse
during the interview. On the other hand, as data on these crimes are collected
every year, it would be more efficient in data analysis if we could identify
and make use of information from previous data. In this paper we propose a
model to adjust the estimates of the rates of rape and domestic violence to
account for the response bias due to the ""gag"" factors. To estimate parameters
in the model, we identify the information that is not sensitive to time and
incorporate this into prior distributions. The strength of Bayesian estimators
is their ability to combine information from long observational records in a
sensible way. Within a Bayesian framework, we develop an
Expectation-Maximization-Bayesian (EMB) algorithm for computation in analyzing
contingency table and we apply the jackknife to estimate the accuracy of the
estimates. Our approach is illustrated using the yearly crime data from the
National Crime Victimization Survey. The illustration shows that compared with
the classical method, our model leads to more efficient estimation but does not
require more complicated computation.","['Qingzhao Yu', 'Elizabeth A. Stasny', 'Bin Li']",[],0,arXiv,http://arxiv.org/abs/0801.3442v2,False,False,False,False,True,194,Eric P Baumer,Chicago,Completed,2001,2003.0,"Although it has been well documented that a substantial portion of all crime experienced by citizens in the U.S. is not reported to the police, very few studies have systematically examined whether residents of certain types of communities are more, or less, likely to report crime victimizations to the police. This issue has not been addressed extensively largely because the data needed to do so—data on victims of crime and on the communities in which they reside—traditionally have not been available to researchers. If approved by the Census Bureau, the proposed research will use data from the 1995-2001 Area-Identified National Crime Victimization Survey (NCVS), linked with data from the Sample Survey of Law Enforcement Agencies (SSLEA), the Uniform Crime Reporting (UCR) program, and census data on tracts and places to investigate the effects of several characteristics of communities on the likelihood of police notification by crime victims. The community characteristics considered will include neighborhood features such as socioeconomic disadvantage, minority concentration, immigrant concentration, and residential instability, and place-level indicators such as the degree of local police involvement in community policing activities and the racial composition of the local police agency. The proposed research contributes significantly to the literature on victim crime reporting, and the analyses have important implications for macro-level research and theory tests which often assume little or no systematic variation in crime reporting across communities. In addition, the research will benefit the Census Bureau by adding contextual data to the NCVS, evaluating and improving the usefulness of the NCVS, using methodologies (e.g., survey regression techniques, multilevel modeling) that will enhance understanding of these data, and highlighting the value of the NCVS for cutting-edge theoretical and relevant research."
"""It's Just Part of Me:"" Understanding Avatar Diversity and
  Self-presentation of People with Disabilities in Social Virtual Reality","In social Virtual Reality (VR), users are embodied in avatars and interact
with other users in a face-to-face manner using avatars as the medium. With the
advent of social VR, people with disabilities (PWD) have shown an increasing
presence on this new social media. With their unique disability identity, it is
not clear how PWD perceive their avatars and whether and how they prefer to
disclose their disability when presenting themselves in social VR. We fill this
gap by exploring PWD's avatar perception and disability disclosure preferences
in social VR. Our study involved two steps. We first conducted a systematic
review of fifteen popular social VR applications to evaluate their avatar
diversity and accessibility support. We then conducted an in-depth interview
study with 19 participants who had different disabilities to understand their
avatar experiences. Our research revealed a number of disability disclosure
preferences and strategies adopted by PWD (e.g., reflect selective
disabilities, present a capable self). We also identified several challenges
faced by PWD during their avatar customization process. We discuss the design
implications to promote avatar accessibility and diversity for future social VR
platforms.","['Kexin Zhang', 'Elmira Deldari', 'Zhicong Lu', 'Yaxing Yao', 'Yuhang Zhao']",[],0,arXiv,http://arxiv.org/abs/2208.11170v1,False,True,False,False,False,196,Susan E Chen,Chicago,Completed,2007,2007.0,"This study will evaluate the impact of the Social Security Disability Insurance program (SSDI) on the labor force behavior of men. I will use two main approaches. First, I will follow Bound (1989) using a more current dataset, with more complete information on rejected SSDI applicants. This technique will produce a more precise upper bound on the elasticity of SSDI participation. Second, in addition to providing an upper bound, I will adopt a quasi-experimental approach to provide a point estimate of the impact for an important sub-sample of applicants: those whose eligibility is based in part on vocational factors. More specifically, with data on rejected as well as accepted SSDI applicants, I will be able to exploit the idiosyncrasies of the Social Security Administration’s Disability Determination Process. I will be able to show that the SSDI participation rule fits within a quasi-experimental design framework that has recently gained significant attention in the economics literature: the Regression Discontinuity Design. The data that will be used are the 1990-1996 panels of SIPP exact matched to the Social Security Disability Determination 831 file. Creation of this new dataset will enhance the reach of the SIPP data in three ways. First, by matching the SIPP to Social Security data, I will obtain a more accurate measure of SSDI participation than available in the SIPP. Second, I will be to study the accuracy of and relationships between self-reported SSDI benefit receipt, health measures and disability status in the SIPP. And third, I will be able to compare the level of mis-reporting across all five panels of the SIPP."
On the joint volatility dynamics in dairy markets,"The present study investigates the price (co)volatility of four dairy
commodities -- skim milk powder, whole milk powder, butter and cheddar cheese
-- in three major dairy markets. It uses a multivariate factor stochastic
volatility model for estimating the time-varying covariance and correlation
matrices by imposing a low-dimensional latent dynamic factor structure. The
empirical results support four factors representing the European Union and
Oceania dairy sectors as well as the milk powder markets. Factor volatilities
and marginal posterior volatilities of each dairy commodity increase after the
2006/07 global (food) crisis, which also coincides with the free trade
agreements enacted from 2007 onwards and EU and US liberalization policy
changes. The model-implied correlation matrices show increasing dependence
during the second half of 2006, throughout the first half of 2007, as well as
during 2008 and 2014, which can be attributed to various regional agricultural
dairy policies. Furthermore, in-sample value at risk measures (VaRs and CoVaRs)
are provided for each dairy commodity under consideration.","['Anthony N. Rezitis', 'Gregor Kastner']",[],0,arXiv,http://arxiv.org/abs/2104.12707v1,False,True,False,False,False,215,Brian W Gould,Chicago,Completed,2002,2004.0,"The primary objective of this project  is to undertake an analysis of the U.S. dairy processing sector.  We intend to examine the extent to which their exists significant economies of scale, factor substitution and the role of technological change in the evolution of this industry over the 1962-1997 period.  To our knowledge, there has not been any firm-level analyses of this industry.  Subject to data availability we will examine the structure of cheese manufacture, butter production, fluid milk bottlers and manufacturers of dried milk products. The proposed econometric analysis will be based on a neoclassical cost function model of production."
Mobility and Transit Segregation in Urban Spaces,"Segregation is a highly nuanced concept that researchers have worked to
define and measure over the past several decades. Conventional approaches tend
to estimate segregation based on residential patterns in a static manner. In
this work, we analyse socioeconomic inequalities, assessing segregation in
various dimensions of the urban experience. Moreover, we consider the pivotal
role that transport plays in democratising access to opportunities. Using
transport networks, amenity visitations, and census data, we develop a
framework to approximate segregation, within the United States, for various
dimensions of urban life. We find that neighbourhoods that are segregated in
the residential domain, tend to exhibit similar levels of segregation in
amenity visitation patterns and transit usage, albeit to a lesser extent. We
identify inequalities embedded into transit service, which impose constraints
on residents from segregated areas, limiting the neighbourhoods that they can
access within an hour to areas that are similarly disadvantaged. By exploring
socioeconomic segregation from a transit perspective, we underscore the
importance of conceptualising segregation as a dynamic measure, while also
highlighting how transport systems can contribute to a cycle of disadvantage.","['Nandini Iyer', 'Ronaldo Menezes', 'Hugo Barbosa']",[],0,arXiv,http://arxiv.org/abs/2304.07086v1,False,True,False,False,False,221,Jacob Vigdor,Triangle,Completed,2002,2005.0,"The purpose of this research is to document the extent of immigrant segregation in U.S. metropolitan areas, explain the formation and dissipation of immigrant ghettos, and examine the implications of residential concentration for immigrants’ socioeconomic and developmental well-being. The project will make use of public-use Census data to compute segregation indices and statistically analyze residential location choices and individual outcomes in 1910. The project will use non-public use Census microdata to perform statistical analysis in 1990, as well as 1980 and 2000 as the data become available. This project will benefit the Census Bureau in two ways. First, the segregation data derived from this effort will complement the Bureau’s own data on residential segregation, which to this point has focused on broad racial categories rather than individual immigrant groups. Second, learning about the dynamic evolution of immigrant enclaves will assist the Bureau in planning for future Census enumerations."
"Trade, Trees, and Lives","This paper shows a cascading mechanism through which international
trade-induced deforestation results in a decline of health outcomes in cities
distant from where trade activities occur. We examine Brazil, which has ramped
up agricultural export over the last two decades to meet rising global demand.
Using a shift-share research design, we first show that export shocks cause
substantial local agricultural expansion and a virtual one-for-one decline in
forest cover. We then construct a dynamic area-of-effect model that predicts
where atmospheric changes should be felt - due to loss of forests that would
otherwise serve to filter out and absorb air pollutants as they travel -
downwind of the deforestation areas. Leveraging quasi-random variation in these
atmospheric connections, we establish a causal link between deforestation
upstream and subsequent rises in air pollution and premature deaths downstream,
with the mortality effects predominantly driven by cardiovascular and
respiratory causes. Our estimates reveal a large telecoupled health externality
of trade deforestation: over 700,000 premature deaths in Brazil over the past
two decades. This equates to $0.18 loss in statistical life value per $1
agricultural exports over the study period.","['Xinming Du', 'Lei Li', 'Eric Zou']",[],0,arXiv,http://arxiv.org/abs/2411.13516v1,False,True,False,False,False,244,Charles I Mead,Washington,Completed,2003,2004.0,"This project examines whether the coverage of exports in the Annual Survey of Manufacturers (ASM) systematically changes over the period of 1993-98. It does so by performing three tasks. First, it creates annual estimates of the total value of exports from the data collected in the ASM and compares these estimates over time to related statistics in the U.S. International Trade in Goods and Services (FT990) series. Second, it performs the same type of comparisons for industries in which it is believed that the quality of related FT990 estimates has declined over the period of the study. Third, it uses the ASM data to examine whether there are systematic differences in export-growth rates across different types of manufacturing establishments. Particular attention is paid to an assessment of whether the quality of the ASM data is such that it can reasonably be used to shed light on the quality of the estimates provided in the FT990 series."
"A method for investigating relative timing information on phylogenetic
  trees","In this paper we present a new way to understand the timing of branching
events in phylogenetic trees. Our method explicitly considers the relative
timing of diversification events between sister clades; as such it is
complimentary to existing methods using lineages-through-time plots which
consider diversification in aggregate. The method looks for evidence of
diversification happening in lineage-specific ``bursts'', or the opposite,
where diversification between two clades happens in an unusually regular
fashion. In order to be able to distinguish interesting events from
stochasticity, we propose two classes of neutral models on trees with timing
information and develop a statistical framework for testing these models. Our
models substantially generalize both the coalescent with ancestral population
size variation and the global-rate speciation-extinction models. We end the
paper with several example applications: first, we show that the evolution of
the Hepatitis C virus appears to proceed in a lineage-specific bursting
fashion. Second, we analyze a large tree of ants, demonstrating that a period
of elevated diversification rates does not appear to occurred in a bursting
manner.","['Daniel Ford', 'Tanja Gernhard', 'Frederick Matsen']",[],0,arXiv,http://arxiv.org/abs/0803.1510v1,False,True,False,False,False,246,Belen Villalonga,Boston,Completed,2002,2003.0,"This project will accomplish two research objectives. First, it will revise a paper produced as part of the earlier project “Corporate Diversification and Quasi-Diversification: Causes and Consequences” (LA 99-05). The paper examines whether the finding of a diversification discount in U.S. stock markets is only an artifact of the use of Compustat segment data. Using a common methodological approach on a sample of firms which exhibit a diversification discount according to segment data, it finds that, when BITS data are used, diversified firms actually trade at a significant average premium. The second research objective of this project is to revise and conclude the author’s earlier efforts to match establishment-level data from the Census Bureau’s Business Information Tracking Series (BITS) to firm-level data from Standard and Poor’s Compustat. The resulting database provides information on the financial characteristics of public U.S. firms and a more objective and detailed breakdown of their activities by industry than that offered by segment-level data. The merged database is therefore an extremely rich source of information that can be used to investigate a variety of topics. The author will be making available to the Center for Economic Studies the matching file that will enable future researchers at the Center to recreate the merged dataset, together with a document that will describe in detail the process followed to create that matching file."
"Encouraging Emotion Regulation in Social Media Conversations through
  Self-Reflection","Anonymity in social media platforms keeps users hidden behind a keyboard.
This absolves users of responsibility, allowing them to engage in online rage,
hate speech, and other text-based toxicity that harms online well-being. Recent
research in the field of Digital Emotion Regulation (DER) has revealed that
indulgence in online toxicity can be a result of ineffective emotional
regulation (ER). This, we believe, can be reduced by educating users about the
consequences of their actions. Prior DER research has primarily focused on
exploring digital emotion regulation practises, identifying emotion regulation
using multimodal sensors, and encouraging users to act responsibly in online
conversations. While these studies provide valuable insights into how users
consciously utilise digital media for emotion regulation, they do not capture
the contextual dynamics of emotion regulation online. Through interaction
design, this work provides an intervention for the delivery of ER support. It
introduces a novel technique for identifying the need for emotional regulation
in online conversations and delivering information to users in a way that
integrates didactic learning into their daily life. By fostering
self-reflection in periods of intensified emotional expression, we present a
graph-based framework for on-the-spot emotion regulation support in online
conversations. Our findings suggest that using this model in a conversation can
help identify its influential threads/nodes to locate where toxicity is
concentrated and help reduce it by up to 12\%. This is the first study in the
field of DER that focuses on learning transfer by inducing self-reflection and
implicit emotion regulation.","['Akriti Verma', 'Shama Islam', 'Valeh Moghaddam', 'Adnan Anwar']",[],0,arXiv,http://arxiv.org/abs/2303.00884v1,False,True,False,False,False,247,Lori S Bennear,Triangle,Completed,2004,2007.0,"There is increasing interest in the United States and other countries in the use of information disclosure programs as potential substitutes for, or complements to, conventional command-and control or market-based environmental policy instruments. Much of this interest can be attributed to the apparent success of the Toxics Release Inventory (TRI) program, which requires large manufacturing facilities to report publicly their annual releases of certain chemicals. Since the inception of the TRI program in 1986, reported releases of over 300 regulated chemicals have fallen by more than 45 percent. However, since release data is only available for facilities that are required to report to TRI, one cannot determine what reductions in releases would have occurred in the absence of TRI and, therefore, one cannot infer how much of this 45 percent reduction is the direct result of the TRI reporting requirements. The primary purpose of the proposed research is to better isolate the causal effect of TRI on facility-level outcomes. To accomplish this, it is necessary to recognize that the production of toxic releases at a facility is directly linked to production of output at the facility. While we cannot observe releases for facilities that are not required to report to TRI, we can observe output levels for both facilities that are required to report and facilities that are not required to report. This framework can be used to develop a natural experiment for testing the effects of TRI on production technologies and productivity. We will estimate production functions separately for facilities that are required to report to TRI and facilities that are not required to report to TRI and empirically test for differences in the production technologies employed and the productivity of the two groups of facilities. This will help determine whether public disclosure of environmental performance information changes the way facilities do business. In addition, this project will provide a better understanding of the ways in which public disclosure affects facility decision-making. There are several pathways through which reporting of environmental information might lead to pollution reduction, including: green consumerism, green investing, community pressure, the threat of future regulation, and organizational limitations of the firm. We will construct indictors for each of the pathways, estimate the effect of these pathways on changes in the production functions used by reporting and non-reporting facilities, and compare these effects with predictions from a theoretical model of facility-level response to information disclosure requirements. A critical element in the analysis is the construction of a facility-level dataset that contains information on environmental variables, economic variables, and variables that proxy for the four different pathways. We will create this dataset by linking publicly available data from the Toxics Release Inventory, trade organizations, environmental organizations, and other sources with confidential facility-level economic data from the Longitudinal Research Database (LRD) and the Pollution Abatement Costs and Expenditures (PACE) Survey for the years 1982 to 1999. In addition to providing a clearer understanding of the causal relationship between TRI and facility-level economic outcomes, the creation of this unique dataset will provide opportunities to evaluate and enhance Census data. The Census collects detailed data on the products produced, and the materials used in production, at the 7-digit SIC level (8-digit NAICS level beginning in 1997). However, many facilities do not report data at this level of detail. Instead they report data on products and materials at a higher level of aggregation. Because several of the chemicals that are reported under the TRI are also 7-digit SIC categories (or 8-digit NAICS categories) on the materials and product trailers, we can evaluate whether facilities are better able to report detailed data to Census on their usage of these specific chemicals if they are required to track that data to comply with the TRI. We will also evaluate potential improvements in data reporting that could be obtained if the connections between TRI reported chemicals and Census product and materials categories are enhanced."
"A Bayesian hierarchical small-area population model accounting for data
  source specific methodologies from American Community Survey, Population
  Estimates Program, and Decennial Census data","Small area estimates of population are necessary for many epidemiological
studies, yet their quality and accuracy are often not assessed. In the United
States, small area estimates of population counts are published by the United
States Census Bureau (USCB) in the form of the Decennial census counts,
Intercensal population projections (PEP), and American Community Survey (ACS)
estimates. Although there are significant relationships between these data
sources, there are important contrasts in data collection and processing
methodologies, such that each set of estimates may be subject to different
sources and magnitudes of error. Additionally, these data sources do not report
identical small area population counts due to post-survey adjustments specific
to each data source. Resulting small area disease/mortality rates may differ
depending on which data source is used for population counts (denominator
data). To accurately capture annual small area population counts, and
associated uncertainties, we present a Bayesian population model (B-Pop), which
fuses information from all three USCB sources, accounting for data source
specific methodologies and associated errors. The main features of our
framework are: 1) a single model integrating multiple data sources, 2)
accounting for data source specific data generating mechanisms, and
specifically accounting for data source specific errors, and 3) prediction of
estimates for years without USCB reported data. We focus our study on the 159
counties of Georgia, and produce estimates for years 2005-2021.","['Emily N Peterson', 'Rachel C Nethery', 'Tullia Padellini', 'Jarvis T Chen', 'Brent A Coull', 'Frederic B Piel', 'Jon Wakefield', 'Marta Blangiardo', 'Lance A Waller']",[],0,arXiv,http://arxiv.org/abs/2112.09813v1,True,True,False,False,True,248,Emily Parkany,Washington,Completed,2002,2003.0,"It is hypothesized that continuous measurement of transportation variables such as work trip mode and  journey-to-work time is subject to seasonal effects.  The data examined here is in the county including Springfield, Massachusetts.  This metropolitan area is the second largest in New England and includes the region's second largest transit system.  This research will look at seasonal differences in transportation-related data, consider their significance, and consider the impact on using the new data for transportation planning.  Project results will include statistical tests of the seasonality  hypotheses, monthly tabulations of transportation-related data, and advantages and disadvantages to collecting data continuously for transportation planning applications."
"Multidimensional well-being of US households at a fine spatial scale
  using fused household surveys: fusionACS","Social science often relies on surveys of households and individuals. Dozens
of such surveys are regularly administered by the U.S. government. However,
they field independent, unconnected samples with specialized questions,
limiting research questions to those that can be answered by a single survey.
The fusionACS project seeks to integrate data from multiple U.S. household
surveys by statistically ""fusing"" variables from ""donor"" surveys onto American
Community Survey (ACS) microdata. This results in an integrated microdataset of
household attributes and well-being dimensions that can be analyzed to address
research questions in ways that are not currently possible. The presented data
comprise the fusion onto the ACS of select donor variables from the Residential
Energy Consumption Survey (RECS) of 2015, the National Household Transportation
Survey (NHTS) of 2017, the American Housing Survey (AHS) of 2019, and the
Consumer Expenditure Survey - Interview (CEI) for the years 2015-2019. The
underlying statistical techniques are included in an open-source $R$ package,
fusionModel, that provides generic tools for the creation, analysis, and
validation of fused microdata.","['Kevin Ummel', 'Miguel Poblete-Cazenave', 'Karthik Akkiraju', 'Nick Graetz', 'Hero Ashman', 'Cora Kingdon', 'Steven Herrera Tenorio', 'Aaryaman ""Sunny"" Singhal', 'Daniel Aldana Cohen', 'Narasimha D. Rao']",[],0,arXiv,http://arxiv.org/abs/2309.11512v1,False,True,False,False,True,248,Emily Parkany,Washington,Completed,2002,2003.0,"It is hypothesized that continuous measurement of transportation variables such as work trip mode and  journey-to-work time is subject to seasonal effects.  The data examined here is in the county including Springfield, Massachusetts.  This metropolitan area is the second largest in New England and includes the region's second largest transit system.  This research will look at seasonal differences in transportation-related data, consider their significance, and consider the impact on using the new data for transportation planning.  Project results will include statistical tests of the seasonality  hypotheses, monthly tabulations of transportation-related data, and advantages and disadvantages to collecting data continuously for transportation planning applications."
"The effect of remote work on urban transportation emissions: evidence
  from 141 cities","The overall impact of working from home (WFH) on transportation emissions
remains a complex issue, with significant implications for policymaking. This
study matches socioeconomic information from American Community Survey (ACS) to
the global carbon emissions dataset for selected Metropolitan Statistical Areas
(MSAs) in the US. We analyze the impact of WFH on transportation emissions
before and during the COVID-19 pandemic. Employing cross-sectional multiple
regression models and Blinder-Oaxaca decomposition, we examine how WFH,
commuting mode, and car ownership influence transportation emissions across 141
MSAs in the United States. We find that the prevalence of WFH in 2021 is
associated with lower transportation emissions, whereas WFH in 2019 did not
significantly impact transportation emissions. After controlling for public
transportation usage and car ownership, we find that a 1% increase in WFH
corresponds to a 0.17 kilogram or 1.8% reduction of daily average
transportation emissions per capita. The Blinder-Oaxaca decomposition shows
that WFH is the main driver in reducing transportation emissions per capita
during the pandemic. Our results show that the reductive influence of public
transportation on transportation emissions has declined, while the impact of
car ownership on increasing transportation emissions has risen. Collectively,
these results indicate a multifaceted impact of WFH on transportation
emissions. This study underscores the need for a nuanced, data-driven approach
in crafting WFH policies to mitigate transportation emissions effectively.","['Sophia Shen', 'Xinyi Wang', 'Nicholas Caros', 'Jinhua Zhao']",[],0,arXiv,http://arxiv.org/abs/2503.00422v1,False,False,False,False,True,248,Emily Parkany,Washington,Completed,2002,2003.0,"It is hypothesized that continuous measurement of transportation variables such as work trip mode and  journey-to-work time is subject to seasonal effects.  The data examined here is in the county including Springfield, Massachusetts.  This metropolitan area is the second largest in New England and includes the region's second largest transit system.  This research will look at seasonal differences in transportation-related data, consider their significance, and consider the impact on using the new data for transportation planning.  Project results will include statistical tests of the seasonality  hypotheses, monthly tabulations of transportation-related data, and advantages and disadvantages to collecting data continuously for transportation planning applications."
Short Selling with Margin Risk and Recall Risk,"Short sales are regarded as negative purchases in textbook asset pricing
theory. In reality, however, the symmetry between purchases and short sales is
broken by a variety of costs and risks peculiar to the latter. We formulate an
optimal stopping model in which the decision to cover a short position is
affected by two short sale-specific frictions---margin risk and recall risk.
Margin risk refers to the fact that short sales are collateralised
transactions, which means that short sellers may be forced to close out their
positions involuntarily if they cannot fund margin calls. Recall risk refers to
a peculiarity of the stock lending market, which permits lenders to recall
borrowed stock at any time, once again triggering involuntary close-outs. We
examine the effect of these frictions on the optimal close-out strategy and
quantify the loss of value resulting from each. Our results show that realistic
short selling constraints have a dramatic impact on the optimal behaviour of a
short seller, and are responsible for a substantial loss of value relative to
the first-best situation without them. This has implications for many familiar
no-arbitrage identities, which are predicated on the assumption of unfettered
short selling.","['Kristoffer Glover', 'Hardy Hulley']",[],0,arXiv,http://arxiv.org/abs/1903.11804v1,False,True,False,False,False,249,Christian A Hilber,Washington,Completed,2002,2002.0,"The project investigates 1) whether mortgage lenders take into account neighborhood risk when they decide whether to grant or deny a credit, and 2) whether neighborhood risk thereby affects homeownership decisions.  These hypotheses can be tested using a direct measure for neighborhood specific risk from Hilber (2001) and using census-track specific mortgage lending decision data (HMDA-data) and data from the AHS that discloses the census-track of a housing unit.  We can also test whether mortgage lending denials or portfolio diversification considerations are the main reason for why neighborhood risk affects the homeownership decision."
"Improving the accuracy of freight mode choice models: A case study using
  the 2017 CFS PUF data set and ensemble learning techniques","The US Census Bureau has collected two rounds of experimental data from the
Commodity Flow Survey, providing shipment-level characteristics of nationwide
commodity movements, published in 2012 (i.e., Public Use Microdata) and in 2017
(i.e., Public Use File). With this information, data-driven methods have become
increasingly valuable for understanding detailed patterns in freight logistics.
In this study, we used the 2017 Commodity Flow Survey Public Use File data set
to explore building a high-performance freight mode choice model, considering
three main improvements: (1) constructing local models for each separate
commodity/industry category; (2) extracting useful geographical features,
particularly the derived distance of each freight mode between
origin/destination zones; and (3) applying additional ensemble learning methods
such as stacking or voting to combine results from local and unified models for
improved performance. The proposed method achieved over 92% accuracy without
incorporating external information, an over 19% increase compared to directly
fitting Random Forests models over 10,000 samples. Furthermore, SHAP (Shapely
Additive Explanations) values were computed to explain the outputs and major
patterns obtained from the proposed model. The model framework could enhance
the performance and interpretability of existing freight mode choice models.","['Diyi Liu', 'Hyeonsup Lim', 'Majbah Uddin', 'Yuandong Liu', 'Lee D. Han', 'Ho-ling Hwang', 'Shih-Miao Chin']",[],0,arXiv,http://arxiv.org/abs/2402.00654v2,True,True,False,False,True,253,Michael J Margreta,Washington,Completed,2002,2003.0,
"Modeling Freight Mode Choice Using Machine Learning Classifiers: A
  Comparative Study Using the Commodity Flow Survey (CFS) Data","This study explores the usefulness of machine learning classifiers for
modeling freight mode choice. We investigate eight commonly used machine
learning classifiers, namely Naive Bayes, Support Vector Machine, Artificial
Neural Network, K-Nearest Neighbors, Classification and Regression Tree, Random
Forest, Boosting and Bagging, along with the classical Multinomial Logit model.
US 2012 Commodity Flow Survey data are used as the primary data source; we
augment it with spatial attributes from secondary data sources. The performance
of the classifiers is compared based on prediction accuracy results. The
current research also examines the role of sample size and training-testing
data split ratios on the predictive ability of the various approaches. In
addition, the importance of variables is estimated to determine how the
variables influence freight mode choice. The results show that the tree-based
ensemble classifiers perform the best. Specifically, Random Forest produces the
most accurate predictions, closely followed by Boosting and Bagging. With
regard to variable importance, shipment characteristics, such as shipment
distance, industry classification of the shipper and shipment size, are the
most significant factors for freight mode choice decisions.","['Majbah Uddin', 'Sabreena Anowar', 'Naveen Eluru']",[],0,arXiv,http://arxiv.org/abs/2402.00659v1,False,False,False,False,True,253,Michael J Margreta,Washington,Completed,2002,2003.0,
Price Impact of Health Insurance,"This paper examines the equilibrium effects of insurance contracts on
healthcare markets using a mechanism design framework. A population of
risk-averse agents with preferences as in Yaari (1987) face the risk of
developing an illness of unknown severity, which can be treated in a
competitive hospital services market at the prevailing market price. After
privately observing their health risk, but before learning their sickness
level, agents have the option to purchase insurance from a monopolistic
provider. Insurance contracts specify premiums, out-of-pocket costs (OPCs), and
hospital service coverage, thus determining demand and price in the downstream
hospital market through a market-clearing condition. Our first main result
shows that optimal insurance contracts take a simple form: agents can choose
between full hospital coverage with a high OPC or restricted coverage with a
low OPC. This highlights a novel form of under-insurance (rationing or
restricted access to healthcare services) emerging purely due to the insurer's
attempt to control his price impact. Our second key result illustrates the
nuanced effect of price impact on the amount of insurance provided. Higher
healthcare prices increase insurer payouts but also worsen agents' outside
options, making them more willing to pay for insurance ex ante. The net effect
of these forces determines whether insurance provision exceeds or falls short
of a price-taking benchmark.","['Andrea Di Giovan Paolo', 'Jose Higueras']",[],0,arXiv,http://arxiv.org/abs/2503.01780v1,False,True,False,False,False,255,Susan Marquis,Washington,Completed,2003,2004.0,"Our study will investigate the role the individual health insurance market in California plays in covering the population. It will examine the objective factors, motivations, and attitudes that influence consumers’ decisions to participate in the individual market; how changes in price, benefits design, and public policies would affect that role and the number of uninsured; how purchasers of coverage decide among the plans available to them; and patterns of entry and exit from the individual market for health insurance. We propose to use pooled cross-section and time-series data from two Census surveys—the March Current Population Survey (years 1996 through 2001) and the 1996 SIPP panel—to study decisions to purchase insurance by the potential market in California.
The specific questions we will answer using the CPS and SIPP include:
• What share of the potential market purchases individual coverage in California? How does this vary among important population subgroups?
• What factors influence the decision to purchase individual insurance? Specifically, what is the role of variables that might be affected by factors such as price, nature and number of products available?
• Does the availability of safety net providers affect decisions, especially of the near poor? Is there health selection in participation?
• Does greater diversity in the nature of products available increase market segmentation and alter the health risks participating in the market?
• What affects intra-family decisions about coverage for individual family members?
The study would benefit the Census Bureau in several ways. First, we will increase the utility of the Census data by using it to address important health care issues that we expect will help inform the decision making progress. In addition, to providing estimates of important behavioral relationships in the population, the analysis will yield important descriptive information about those who purchase in the individual insurance market. Second, we will be estimating models using multiple household surveys. Similarities or differences in modeling results will yield information about the reliability and validity of measures from the various surveys. In addition, we have administrative data from insurers in California that we can use to help assess the validity of survey measures. (For example, we can compare average duration of coverage measured in the SIPP panel with duration from administrative records. Any restricted administrative data from participating insurers will be analyzed outside of the Census data center.) Finally, our project statistician is developing ways of combining nonlinear regressions analyses from multiple household surveys that could be used by other researchers."
"Modelling income, wealth, and expenditure data by use of Econophysics","In the present paper, we identify several distributions from Physics and
study their applicability to phenomena such as distribution of income, wealth,
and expenditure. Firstly, we apply logistic distribution to these data and we
find that it fits very well the annual data for the entire income interval
including for upper income segment of population. Secondly, we apply
Fermi-Dirac distribution to these data. We seek to explain possible
correlations and analogies between economic systems and statistical
thermodynamics systems. We try to explain their behavior and properties when we
correlate physical variables with macroeconomic aggregates and indicators. Then
we draw some analogies between parameters of the Fermi-Dirac distribution and
macroeconomic variables. Thirdly, as complex systems are modeled using
polynomial distributions, we apply polynomials to the annual sets of data and
we find that it fits very well also the entire income interval. Fourthly, we
develop a new methodology to approach dynamically the income, wealth, and
expenditure distribution similarly with dynamical complex systems. This
methodology was applied to different time intervals consisting of consecutive
years up to 35 years. Finally, we develop a mathematical model based on a
Hamiltonian that maximizes utility function applied to Ramsey model using
Fermi-Dirac and polynomial utility functions. We find some theoretical
connections with time preference theory. We apply these distributions to a
large pool of data from countries with different levels of development, using
different methods for calculation of income, wealth, and expenditure.",['Elvis Oltean'],[],0,arXiv,http://arxiv.org/abs/1603.08383v1,False,True,False,False,False,258,Daniel J Wilson,Berkeley,Completed,2003,2003.0,"The 1998 Annual Capital Expenditure Survey (ACES) is the first micro-level data set to contain information of investment spending broken down into the actual types of capital goods in which firms invest. Part of this project, therefore, will simply be to document the investment behavior of firms in terms of their allocation of capital expenditures across types. In particular, how similar are firms’ investment distributions in total, within-industry, and between-industry. Such an analysis will help researchers determine how representative industry-level capital flows data are of firm-level behavior. I also propose to compare industry-level estimates of the investment distribution from the BEA to those implied by the ACES survey data. Both the documentation of investment patterns and the comparison of the implied industry-level investment distributions to those estimated by the BEA will help determine the value of collecting this detailed investment by type data in ACES and therefore will benefit the Census Bureau. The main purpose of this project, however, will be to measure the individual productivity contributions of various types of capital goods. A large body of empirical research has recently emerged that has found that IT investment, in particular, has a positive contribution to TFP above and beyond the contribution of total capital. The natural question, then, is: are there other capital types that also have “excess” productivity contributions? Matching ACES data to public financial data from Compustat, we can measure output, labor, materials, and total capital and thus construct a conventional measure of total factor productivity (TFP). We can then test for significant relationships between investment shares (i.e. investment in a particular capital types as a share of total investment) and TFP via regression analysis. The coefficients on the investment shares will identify the sign and the magnitude of the excess contributions of each capital type to productivity. This will be of great interest to researchers and decision-makers and therefore will help demonstrate the value of the ACES program."
"INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance
  in Insurance","Large Vision-Language Models (LVLMs) have demonstrated outstanding
performance in various general multimodal applications such as image
recognition and visual reasoning, and have also shown promising potential in
specialized domains. However, the application potential of LVLMs in the
insurance domain-characterized by rich application scenarios and abundant
multimodal data-has not been effectively explored. There is no systematic
review of multimodal tasks in the insurance domain, nor a benchmark
specifically designed to evaluate the capabilities of LVLMs in insurance. This
gap hinders the development of LVLMs within the insurance domain. In this
paper, we systematically review and distill multimodal tasks for four
representative types of insurance: auto insurance, property insurance, health
insurance, and agricultural insurance. We propose INS-MMBench, the first
comprehensive LVLMs benchmark tailored for the insurance domain. INS-MMBench
comprises a total of 2.2K thoroughly designed multiple-choice questions,
covering 12 meta-tasks and 22 fundamental tasks. Furthermore, we evaluate
multiple representative LVLMs, including closed-source models such as GPT-4o
and open-source models like BLIP-2. This evaluation not only validates the
effectiveness of our benchmark but also provides an in-depth performance
analysis of current LVLMs on various multimodal tasks in the insurance domain.
We hope that INS-MMBench will facilitate the further application of LVLMs in
the insurance domain and inspire interdisciplinary development. Our dataset and
evaluation code are available at https://github.com/FDU-INS/INS-MMBench.","['Chenwei Lin', 'Hanjia Lyu', 'Xian Xu', 'Jiebo Luo']",[],0,arXiv,http://arxiv.org/abs/2406.09105v1,False,True,False,False,False,263,Jessica P Vistnes,Washington,Completed,2002,2005.0,"Among the goals of this research using the Medical Expenditure Panel Survey (MEPS) - Insurance Component (IC) data are the following:
• Produce estimates related to the supply and demand of employer-sponsored health insurance.
• Develop new and improved methodologies for producing such population estimates.
• Develop an understanding of the quality of data collected, through analysis of response rates, item response rates and data collection results, in order to produce changes in questionnaire structure and collection methodology that will improve collected data.
• Identify shortcomings of the questionnaire to obtain the information necessary to produce reliable population estimates related to employer-sponsored health insurance."
A return-diversification approach to portfolio selection,"In this paper, we propose a general bi-objective model for portfolio
selection, aiming to maximize both a diversification measure and the portfolio
expected return. Within this general framework, we focus on maximizing a
diversification measure recently proposed by Choueifaty and Coignard for the
case of volatility as a risk measure. We first show that the maximum
diversification approach is actually equivalent to the Risk Parity approach
using volatility under the assumption of equicorrelated assets. Then, we extend
the maximum diversification approach formulated for general risk measures.
Finally, we provide explicit formulations of our bi-objective model for
different risk measures, such as volatility, Mean Absolute Deviation,
Conditional Value-at-Risk, and Expectiles, and we present extensive
out-of-sample performance results for the portfolios obtained with our model.
The empirical analysis, based on five real-world data sets, shows that the
return-diversification approach provides portfolios that tend to outperform the
strategies based only on a diversification method or on the classical
risk-return approach.","['Francesco Cesarone', 'Rosella Giacometti', 'Manuel Luis Martino', 'Fabio Tardella']",[],0,arXiv,http://arxiv.org/abs/2312.09707v1,False,True,False,False,False,265,Namsuk Kim,Washington,Completed,2003,2005.0,"During business cycles, firms adjust various margins. Research using plant-level data unmasks the plant-level investment path. However, many decisions are made at the firm level. This project explores one of the margins that a firm can adjust during the business cycle; to wit, product diversification. I set up a suggestive model, showing diversification, investment and debt policy. My project will analyze why, when and how the firm diversifies, which is not done in the existing literature. Diversification decisions would be one of the possible reasons why the firm's investment is lumpy and traditional investment and inventory models cannot explain investment. My project is consistent with other research that is beginning to analyze various adjustment margins of the firm. Along with the simulation results from my model, I will find new stylized facts of the annual pattern of firm's diversification at firm and plant level. I have three approaches to do so, dealing with certainty company cases, single units, and multi units. The first two approaches require the ASM and CM. The last one needs LBD data as well. My project will compare three datasets and evaluate their analytical abilities. Three datasets can be combined to overcome the defects of each of the datasets."
"Community-based Behavioral Understanding of Crisis Activity Concerns
  using Social Media Data: A Study on the 2023 Canadian Wildfires in New York
  City","New York City (NYC) topped the global chart for the worst air pollution in
June 2023, owing to the wildfire smoke drifting in from Canada. This
unprecedented situation caused significant travel disruptions and shifts in
traditional activity patterns of NYC residents. This study utilized large-scale
social media data to study different crisis activity concerns (i.e.,
evacuation, staying indoors, shopping, and recreational activities among
others) in the emergence of the 2023 Canadian wildfire smoke in NYC. In this
regard, one week (June 02 through June 09, 2023) geotagged Twitter data from
NYC were retrieved and used in the analysis. The tweets were processed using
advanced text classification techniques and later integrated with national
databases such as Social Security Administration data, Census, and American
Community Survey. Finally, a model has been developed to make community
inferences of different activity concerns in a major wildfire. The findings
suggest, during wildfires, females are less likely to engage in discussions
about evacuation, trips for medical, social, or recreational purposes, and
commuting for work, likely influenced by workplaces maintaining operations
despite poor air quality. There were also racial disparities in these
discussions, with Asians being more likely than Hispanics to discuss evacuation
and work commute, and African Americans being less likely to discuss social and
recreational activities. Additionally, individuals from low-income
neighborhoods and non-higher education students expressed fewer concerns about
evacuation. This study provides valuable insights for policymakers, emergency
planners, and public health officials, aiding them in formulating targeted
communication strategies and equitable emergency response plans.","['Khondhaker Al Momin', 'Md Sami Hasnine', 'Arif Mohaimin Sadri']",[],0,arXiv,http://arxiv.org/abs/2402.01683v1,False,True,False,False,False,267,Debbie A Niemeier,Berkeley,Completed,2003,2004.0,"With the impending replacement of the decennial long form with the  American Community Survey  (ACS), there are potential concerns related to  using the new ACS data specifically for travel demand modeling and forecasting.  Although ACS differs from the decennial long form in several significant ways,  one of the most prominent is that five years of surveying will used to be used  to obtain an equivalent CTPP sample for reporting on small geographic units  such as Traffic Analysis Zones (TAZs) or census tracts. This project specifically  examines the transportation implications of workplace geo-coding at geographic  units of evaluation, such as the TAZ. In addition, recommendations for improving  and expediting workplace geo-coding and maintaining reference files will be  developed."
"Workplace Breastfeeding Legislation and Female Labor Force Participation
  in the United States","This paper studies the effects of legislation mandating the provision of
workplace breastfeeding amenities on the labor force participation of women in
the United States. Using both the American Community Survey and the Panel Study
of Income Dynamics, in a staggered difference-in-differences framework, I find
evidence that workplace breastfeeding legislation significantly increases the
likelihood of female labor force participation (FLFP) across both datasets and
multiple specifications, by at least 1.5 percentage points. The timing and
magnitude of the post-law increases in FLFP differ across the two datasets. I
bolster the analyses using the CDC's Infant Feeding Practices Survey and the
Childhood and Adoption Supplement to the PSID, which further suggest an
influence of the laws on breastfeeding women. Heterogeneity analysis indicates
the presence of substantial treatment effect heterogeneity across subgroups,
but the findings are specific to the separate datasets. Across both datasets,
the legislation appears to be more effective in states where average pre-law
FLFP was comparatively low. I also find evidence of a negative spillover
effect, whereby women without children and women with older children may have
reduced their LFP in response to the legislation.",['Julia Hatamyar'],[],0,arXiv,http://arxiv.org/abs/2209.05916v2,False,False,False,False,True,267,Debbie A Niemeier,Berkeley,Completed,2003,2004.0,"With the impending replacement of the decennial long form with the  American Community Survey  (ACS), there are potential concerns related to  using the new ACS data specifically for travel demand modeling and forecasting.  Although ACS differs from the decennial long form in several significant ways,  one of the most prominent is that five years of surveying will used to be used  to obtain an equivalent CTPP sample for reporting on small geographic units  such as Traffic Analysis Zones (TAZs) or census tracts. This project specifically  examines the transportation implications of workplace geo-coding at geographic  units of evaluation, such as the TAZ. In addition, recommendations for improving  and expediting workplace geo-coding and maintaining reference files will be  developed."
IndoFashion : Apparel Classification for Indian Ethnic Clothes,"Cloth categorization is an important research problem that is used by
e-commerce websites for displaying correct products to the end-users. Indian
clothes have a large number of clothing categories both for men and women. The
traditional Indian clothes like ""Saree"" and ""Dhoti"" are worn very differently
from western clothes like t-shirts and jeans. Moreover, the style and patterns
of ethnic clothes have a very different distribution from western outfits. Thus
the models trained on standard cloth datasets fail miserably on ethnic outfits.
To address these challenges, we introduce the first large-scale ethnic dataset
of over 106k images with 15 different categories for fine-grained
classification of Indian ethnic clothes. We gathered a diverse dataset from a
large number of Indian e-commerce websites. We then evaluate several baselines
for the cloth classification task on our dataset. In the end, we obtain 88.43%
classification accuracy. We hope that our dataset would foster research in the
development of several algorithms such as cloth classification, landmark
detection, especially for ethnic clothes.","['Pranjal Singh Rajput', 'Shivangi Aneja']",[],0,arXiv,http://arxiv.org/abs/2104.02830v1,False,True,False,False,False,270,Patrick J Conway,Triangle,Completed,2003,2005.0,"The US manufacturing sector as a whole has faced increasing international competition in the last generation, but competition was especially intense in textiles and apparel production. It is crucial, both for our understanding of economic forces as well as for decision-making, that the determinants of this evolution be disentangled. I propose to investigate the causes of this decline through empirical examination of production and trade in subsectors of the US textile and apparel industries. In the textile sector the production of broadwoven cotton cloth has stagnated over the last 20 years while the production sub-categories of broadwoven cotton has expanded. I will document the differences in historical performance, and then use plant-level data from the US Census of Manufactures to identify the relevant determinants of this divergence. The methodology used will be regression-based, with semi-parametric techniques used when appropriate. The result will be a decomposition of the observed evolution in employment, wages, productivity and investment in these industries into that attributable to import competition and that attributable to specific other causes. Access to the Longitudinal Research Database (LRD), will provide consistent links between the plant-level data of the LRD and the data on international trade and finance found in public databases that will be useful in welfare analyses of trade and globalization. Specifically, links to data on binding quotas in these industries will be created. The project also will provide documentation of potentially useful additions to the Census questionnaire to address the question of unemployment due to trade-related dislocation, as specified in the Trade Adjustment Act and other legislation of the US Congress. Finally, the project will provide an example of the application of a profit-maximization based estimating structure upon the data for the manufacturing plants of the LRD."
"Evaluation of the Impact of Low-Emission Zone: Madrid Central as a Case
  Study","Air-quality in urban areas and its relation with public health is one of the
most critical concerns for governments. For this reason, they implement actions
aimed at reducing the concentration of the most critical atmospheric
pollutants, among others the implementation of Low-Emission Zones. Madrid
Central is a major initiative of Madrid city council for reducing motor traffic
and the associated air pollution at the city center. It is created by enlarging
a previous and smaller area of restricted motor traffic. This initiative starts
at the end of 2018, but the first fully-operational period corresponds to
2019Q2 (second quarter of 2019). The activation of Low-Emission Zones has
social and economic impacts, besides environmental ones. For this reason, their
impact must be assessed to ensure the achievement of the commitments in the
reduction of atmospheric pollutants levels. In this work, two metrics for
evaluating the impact of Madrid Central in the reduction of nitrogen dioxide
concentration are proposed. Mean daily concentrations of NO2 from 2010 to 2019
corresponding to the Plaza del Carmen monitoring station are employed for the
evaluation of the impact of Madrid Central. Two other monitoring stations
around Madrid Central are analysed for ascertaining the rise of detrimental
effects due to an increment of the motor traffic around the traffic restricted
area. In consequence, two methodologies pollutant-independent for evaluating
the impact and the evolution of LEZs, as well as a statement about the impact
of the first year of Madrid Central are presented.",['Miguel Cárdenas-Montes'],[],0,arXiv,http://arxiv.org/abs/2012.13782v2,False,True,False,False,False,272,Michael Greenstone,Boston,Completed,2004,2007.0,"Over the last three decades, the federal government has attempted to balance the dual and often conflicting goals of promoting economic activity and environmental quality. The tension between these goals arises because regulations that reduce environmental degradation are likely to hamper economic progress. This project will examine one of the most important examples of such a tension – the federal government’s regulation of air pollution through the Clean Air Act. It will be the first comprehensive attempt to empirically estimate the direct costs and benefits of this regulatory program at the plant level. The proposed project will exploit pollution categories established by the EPA. As directed by the 1970 Clean Air Act Amendments, the EPA established separate national ambient air quality standards (NAAQS) – a minimum level of air quality that all counties are required to meet – for six criteria pollutants: carbon monoxide, lead, nitrogen dioxide, ozone, sulfur dioxide, and particulate matter. As part of this legislation every county annually receives separate nonattainment or attainment designations for each of the six pollutants. The nonattainment designation is reserved for counties whose air contains concentrations of the relevant pollutant that exceed the federal standard. Emitters of the regulated pollutant in nonattainment counties are subject to greater regulatory oversight than emitters in attainment counties. Non-polluters are free from regulation in both categories of counties. This division of counties sets up an interesting quasi-experiment for measuring some of the direct effects of regulation. In principle, it is possible to identify these effects by comparing changes in outcomes in nonattainment and attainment counties. An essential element of this project is the creation of a unique data file that contains plant-level information on pollution emissions and economic activity, as well as detailed measures of regulation. I have already created this file by linking the Toxics Release Inventory (TRI) and the Annual Survey of Manufactures (and the Census of Manufactures) for the years 1987 through 1997. The former contains plant-specific releases of more than 600 toxics into the environment, including whether the release was into the air, water, or ground. The latter provides information on plant-level employment, investment, shipments, and other characteristics (e.g., industry, age, and location). This plant-level file will be merged with one containing the annual, pollutant-specific, attainment/nonattainment designations. The application of this quasi-experiment to the above data file will provide estimates for a class of questions that cannot  e posed with traditional public use files. I will estimate the effect of attainment/nonattainment status on pollution emissions, total employment, capital stock, and output. These results will be useful for many purposes. For instance, it will be possible to calculate the regulation-induced trade-off between environmental quality and employment; this trade-off has not been estimated previously but is central to any debate about environmental regulations. Additionally, I will fit plant-level production functions where pollution emissions are treated as an input. The nonattainment variables will serve as instruments for pollution emissions in these equations. The estimation of these equations will produce measures of the increased costs associated with mandated reductions in pollution. It will also be possible to gain insight into how firms adjust their production processes to comply with regulation. In summary, this project’s results will provide an improved understanding of the costs and benefits of air pollution regulations and of how plants alter their production patterns to optimally comply with them. The predominant purpose of this project is to benefit the Census Bureau’s program and it will do so in at least three ways. First, in addition to linking the TRI to the LRD, this project will produce a cross-walk between the LRD PPNs and EPA establishment IDs. This will link the LRD to all plant-level EPA data sets. Thus, the project should be of great use for researchers interested in examining other aspects of pollution releases by manufacturing plants. Second, the project will produce estimates of the economic costs of environmental regulations and these estimates will serve as an important complement to cost estimates from the Census Bureau’s Survey of Pollution Abatement Costs and Expenditures (PACE). Third, this project’s results may be helpful in identifying methods to refine the PACE’s questionnaire in order to elicit more valuable information."
"Effects of the COVID-19 lockdown on urban light emissions: ground and
  satellite comparison","'Lockdown' periods in response to COVID-19 have provided a unique opportunity
to study the impacts of economic activity on environmental pollution (e.g.
NO$_2$, aerosols, noise, light). The effects on NO$_2$ and aerosols have been
very noticeable and readily demonstrated, but that on light pollution has
proven challenging to determine. The main reason for this difficulty is that
the primary source of nighttime satellite imagery of the earth is the
SNPP-VIIRS/DNB instrument, which acquires data late at night after most human
nocturnal activity has already occurred and much associated lighting has been
turned off. Here, to analyze the effect of lockdown on urban light emissions,
we use ground and satellite data for Granada, Spain, during the COVID-19
induced confinement of the city's population from March 14 until May 31, 2020.
We find a clear decrease in light pollution due both to a decrease in light
emissions from the city and to a decrease in anthropogenic aerosol content in
the atmosphere which resulted in less light being scattered. A clear
correlation between the abundance of PM10 particles and sky brightness is
observed, such that the more polluted the atmosphere the brighter the urban
night sky. An empirical expression is determined that relates PM10 particle
abundance and sky brightness at three different wavelength bands.","['Máximo Bustamante-Calabria', 'Alejandro Sánchez de Miguel', 'Susana Martín-Ruiz', 'Jose-Luis Ortiz', 'J. M. Vílchez', 'Alicia Pelegrina', 'Antonio García', 'Jaime Zamorano', 'Jonathan Bennie', 'Kevin J. Gaston']",[],0,arXiv,http://arxiv.org/abs/2011.09252v1,False,True,False,False,False,272,Michael Greenstone,Boston,Completed,2004,2007.0,"Over the last three decades, the federal government has attempted to balance the dual and often conflicting goals of promoting economic activity and environmental quality. The tension between these goals arises because regulations that reduce environmental degradation are likely to hamper economic progress. This project will examine one of the most important examples of such a tension – the federal government’s regulation of air pollution through the Clean Air Act. It will be the first comprehensive attempt to empirically estimate the direct costs and benefits of this regulatory program at the plant level. The proposed project will exploit pollution categories established by the EPA. As directed by the 1970 Clean Air Act Amendments, the EPA established separate national ambient air quality standards (NAAQS) – a minimum level of air quality that all counties are required to meet – for six criteria pollutants: carbon monoxide, lead, nitrogen dioxide, ozone, sulfur dioxide, and particulate matter. As part of this legislation every county annually receives separate nonattainment or attainment designations for each of the six pollutants. The nonattainment designation is reserved for counties whose air contains concentrations of the relevant pollutant that exceed the federal standard. Emitters of the regulated pollutant in nonattainment counties are subject to greater regulatory oversight than emitters in attainment counties. Non-polluters are free from regulation in both categories of counties. This division of counties sets up an interesting quasi-experiment for measuring some of the direct effects of regulation. In principle, it is possible to identify these effects by comparing changes in outcomes in nonattainment and attainment counties. An essential element of this project is the creation of a unique data file that contains plant-level information on pollution emissions and economic activity, as well as detailed measures of regulation. I have already created this file by linking the Toxics Release Inventory (TRI) and the Annual Survey of Manufactures (and the Census of Manufactures) for the years 1987 through 1997. The former contains plant-specific releases of more than 600 toxics into the environment, including whether the release was into the air, water, or ground. The latter provides information on plant-level employment, investment, shipments, and other characteristics (e.g., industry, age, and location). This plant-level file will be merged with one containing the annual, pollutant-specific, attainment/nonattainment designations. The application of this quasi-experiment to the above data file will provide estimates for a class of questions that cannot  e posed with traditional public use files. I will estimate the effect of attainment/nonattainment status on pollution emissions, total employment, capital stock, and output. These results will be useful for many purposes. For instance, it will be possible to calculate the regulation-induced trade-off between environmental quality and employment; this trade-off has not been estimated previously but is central to any debate about environmental regulations. Additionally, I will fit plant-level production functions where pollution emissions are treated as an input. The nonattainment variables will serve as instruments for pollution emissions in these equations. The estimation of these equations will produce measures of the increased costs associated with mandated reductions in pollution. It will also be possible to gain insight into how firms adjust their production processes to comply with regulation. In summary, this project’s results will provide an improved understanding of the costs and benefits of air pollution regulations and of how plants alter their production patterns to optimally comply with them. The predominant purpose of this project is to benefit the Census Bureau’s program and it will do so in at least three ways. First, in addition to linking the TRI to the LRD, this project will produce a cross-walk between the LRD PPNs and EPA establishment IDs. This will link the LRD to all plant-level EPA data sets. Thus, the project should be of great use for researchers interested in examining other aspects of pollution releases by manufacturing plants. Second, the project will produce estimates of the economic costs of environmental regulations and these estimates will serve as an important complement to cost estimates from the Census Bureau’s Survey of Pollution Abatement Costs and Expenditures (PACE). Third, this project’s results may be helpful in identifying methods to refine the PACE’s questionnaire in order to elicit more valuable information."
"Integrated Reference Cavity for Dual-mode Optical Thermometry and
  Frequency Stabilization","Optical frequency stabilization is a critical component for precision
scientific systems including quantum sensing, precision metrology, and atomic
timekeeping. Ultra-high quality factor photonic integrated optical resonators
are a prime candidate for reducing their size, weight and cost as well as
moving these systems on chip. However, integrated resonators suffer from
temperature-dependent resonance drift due to the large thermal response as well
as sensitivity to external environmental perturbations. Suppression of the
cavity resonance drift can be achieved using precision interrogation of the
cavity temperature through the dual-mode optical thermometry. This approach
enables measurement of the cavity temperature change by detecting the resonance
difference shift between two polarization or optical frequency modes. Yet this
approach has to date only been demonstrated in bulk-optic whispering gallery
mode and fiber resonators. In this paper, we implement dual-mode optical
thermometry using dual polarization modes in a silicon nitride waveguide
resonator for the first time, to the best of our knowledge. The temperature
responsivity and sensitivity of the dual-mode TE/TM resonance difference is
180.7$\pm$2.5 MHz/K and 82.56 $\mu$K, respectively, in a silicon nitride
resonator with a 179.9E6 intrinsic TM mode Q factor and a 26.6E6 intrinsic TE
mode Q factor. Frequency stabilization is demonstrated by locking a laser to
the TM mode cavity resonance and applying the dual-mode resonance difference to
a feedforward laser frequency drift correction circuit with a drift rate
improvement to 0.31 kHz/s over the uncompensated 10.03 kHz/s drift rate. Allan
deviation measurements with dual-mode feedforward-correction engaged shows that
a fractional frequency instability of 9.6E-11 over 77 s can be achieved.","['Qiancheng Zhao', 'Mark W. Harrington', 'Andrei Isichenko', 'Ryan O. Behunin', 'Scott B. Papp', 'Peter T. Rakich', 'Chad W. Hoyt', 'Chad Fertig', 'Daniel J. Blumenthal']",[],0,arXiv,http://arxiv.org/abs/2105.12119v1,False,True,False,False,False,277,Chad W Syverson,Chicago,Completed,2003,2005.0,"The proposed project seeks to exploit firm identifiers in establishment lists (the Standard Statistical Establishment Lists [SSELs] and the Census of Manufactures [CM], and possibly the Longitudinal Business Database [LBD]) to obtain measures of vertical integration within the manufacturing sector, and then apply these measures to empirical testing. We wish to measure the extent to which vertically integrated firms comprise industries and, in some cases, markets within industries. We will identify manufacturing establishments in vertically integrated firms by identifying plants whose owning firm also owns establishments in vertically linked industries. A comprehensive vertical integration measure would account for establishments in non-manufacturing sectors (e.g., when a manufacturer owns its retail outlets). If data limitations preclude this, it is still possible to obtain (more limited) “within-manufacturing” measures of vertical integration using only Longitudinal Research Database / CM data. We will conduct both industry-level and plant-level empirical analyses with these vertical integration measures. The proposed project will expectedly produce a number of benefits to the Census Bureau. We will construct and document an industry-level panel data set of vertical integration measures that can be used for a number of purposes. These include, but are not limited to, economic research into the interactions between integration and/or vertical mergers on input and output markets, comparison to more simply derived industry vertical integration measures (such as the ratio of value added to total output) to gauge the more simple measures’ accuracy, and analysis of legal policies that affect or are affected by the extent of vertical integration. These indices are novel and would be computed at an aggregate (industry) level; as such, they may be able to be made available to other researchers. Additionally, by investigating specifically the quality of the firm identifiers in the SSELs (which are of course of crucial interest to the success of this project), we will augment the considerable establishment-linking efforts that have been made to create the Longitudinal Business Database. Given the availability of establishment lists outside of the manufacturing sector, we will also be able to characterize the extent to which intersectoral ownership has changed over time. This may be of particular interest to the Census Bureau as its economic programs shift focus to non-manufacturing sectors. Finally, we will be able to provide estimates of a number of elasticities in the population. These include the responsiveness of vertical integration to technology and market structure (at both the industry and establishment levels), and the correlations between changes in vertical integration status and productivity. These estimates can be made exploiting both between-establishment differences and intertemporal variations within establishments."
Scientific Talent Leaks Out of Funding Gaps,"We study how delays in NIH grant funding affect the career outcomes of
research personnel. Using comprehensive earnings and tax records linked to
university transaction data along with a difference-in-differences design, we
find that a funding interruption of more than 30 days has a substantial effect
on job placements for personnel who work in labs with a single NIH R01 research
grant, including a 3 percentage point (40%) increase in the probability of not
working in the US. Incorporating information from the full 2020 Decennial
Census and data on publications, we find that about half of those induced into
nonemployment appear to permanently leave the US and are 90% less likely to
publish in a given year, with even larger impacts for trainees (postdocs and
graduate students). Among personnel who continue to work in the US, we find
that interrupted personnel earn 20% less than their continuously-funded peers,
with the largest declines concentrated among trainees and other non-faculty
personnel (such as staff and undergraduates). Overall, funding delays account
for about 5% of US nonemployment in our data, indicating that they have a
meaningful effect on the scientific labor force at the national level.","['Wei Yang Tham', 'Joseph Staudt', 'Elisabeth Ruth Perlman', 'Stephanie D. Cheng']",[],0,arXiv,http://arxiv.org/abs/2402.07235v1,False,True,False,False,False,282,Rick Boden,Washington,Completed,2002,2003.0,"There has been a relative groundswell of empirical studies of business survival/turnover over the past decade.  However, most of these studies have one notable limitation--i.e., they are typically restricted to employer businesses.  The Center for Economic Studies (CES) has acquired microdata on tens of millions of nonemployer ""firms.""  I propose longitudinally linking as many of these records as possible over the years for which the nonemployer microdata are available.  I also plan to match these records to the Census Bureau's business register, as well as one of the Bureau's longitudinal business microdata files.  Upon completion of these matches, I intend to revisit empirical modeling of business dissolution (using hazard functions), to analyze the inter-temporal transition of business entities into and out of employer status, and to analyze patterns in receipts change (for surviving entities) over time.  Some of the other questions that I intend to address in this project include the following.  How many new employer firms were previously nonemployer firms?  What are the general patterns of transition of business entities between employer and nonemployer status?  How many firms with receipts below Census' cutoff (to be included in published nonemployer statistics) actually continue and grow to exceed that receipts threshold in subsequent years?  How do macro- and local economic conditions influence business formation and dissolution? "
The Census-Stub Graph Invariant Descriptor,"An invariant descriptor captures meaningful structural features of networks,
useful where traditional visualizations, like node-link views, face challenges
like the hairball phenomenon (inscrutable overlap of points and lines).
Designing invariant descriptors involves balancing abstraction and information
retention, as richer data summaries demand more storage and computational
resources. Building on prior work, chiefly the BMatrix -- a matrix descriptor
visualized as the invariant network portrait heatmap -- we introduce
BFS-Census, a new algorithm computing our Census data structures: Census-Node,
Census-Edge, and Census-Stub. Our experiments show Census-Stub, which focuses
on stubs (half-edges), has orders of magnitude greater discerning power
(ability to tell non-isomorphic graphs apart) than any other descriptor in this
study, without a difficult trade-off: the substantial increase in resolution
does not come at a commensurate cost in storage space or computation power. We
also present new visualizations -- our Hop-Census polylines and Census-Census
trajectories -- and evaluate them using real-world graphs, including a
sensitivity analysis that shows graph topology change maps to visual Census
change.","['Matt I. B. Oddo', 'Stephen Kobourov', 'Tamara Munzner']",[],0,arXiv,http://arxiv.org/abs/2412.04582v1,False,True,False,False,False,282,Rick Boden,Washington,Completed,2002,2003.0,"There has been a relative groundswell of empirical studies of business survival/turnover over the past decade.  However, most of these studies have one notable limitation--i.e., they are typically restricted to employer businesses.  The Center for Economic Studies (CES) has acquired microdata on tens of millions of nonemployer ""firms.""  I propose longitudinally linking as many of these records as possible over the years for which the nonemployer microdata are available.  I also plan to match these records to the Census Bureau's business register, as well as one of the Bureau's longitudinal business microdata files.  Upon completion of these matches, I intend to revisit empirical modeling of business dissolution (using hazard functions), to analyze the inter-temporal transition of business entities into and out of employer status, and to analyze patterns in receipts change (for surviving entities) over time.  Some of the other questions that I intend to address in this project include the following.  How many new employer firms were previously nonemployer firms?  What are the general patterns of transition of business entities between employer and nonemployer status?  How many firms with receipts below Census' cutoff (to be included in published nonemployer statistics) actually continue and grow to exceed that receipts threshold in subsequent years?  How do macro- and local economic conditions influence business formation and dissolution? "
"The 2020 United States Decennial Census Is More Private Than You (Might)
  Think","The U.S. Decennial Census serves as the foundation for many high-profile
policy decision-making processes, including federal funding allocation and
redistricting. In 2020, the Census Bureau adopted differential privacy to
protect the confidentiality of individual responses through a disclosure
avoidance system that injects noise into census data tabulations. The Bureau
subsequently posed an open question: Could stronger privacy guarantees be
obtained for the 2020 U.S. Census compared to their published guarantees, or
equivalently, had the privacy budgets been fully utilized?
  In this paper, we address this question affirmatively by demonstrating that
the 2020 U.S. Census provides significantly stronger privacy protections than
its nominal guarantees suggest at each of the eight geographical levels, from
the national level down to the block level. This finding is enabled by our
precise tracking of privacy losses using $f$-differential privacy, applied to
the composition of private queries across these geographical levels. Our
analysis reveals that the Census Bureau introduced unnecessarily high levels of
noise to meet the specified privacy guarantees for the 2020 Census.
Consequently, we show that noise variances could be reduced by $15.08\%$ to
$24.82\%$ while maintaining nearly the same level of privacy protection for
each geographical level, thereby improving the accuracy of privatized census
statistics. We empirically demonstrate that reducing noise injection into
census statistics mitigates distortion caused by privacy constraints in
downstream applications of private census data, illustrated through a study
examining the relationship between earnings and education.","['Buxin Su', 'Weijie J. Su', 'Chendi Wang']",[],0,arXiv,http://arxiv.org/abs/2410.09296v2,True,True,False,False,True,282,Rick Boden,Washington,Completed,2002,2003.0,"There has been a relative groundswell of empirical studies of business survival/turnover over the past decade.  However, most of these studies have one notable limitation--i.e., they are typically restricted to employer businesses.  The Center for Economic Studies (CES) has acquired microdata on tens of millions of nonemployer ""firms.""  I propose longitudinally linking as many of these records as possible over the years for which the nonemployer microdata are available.  I also plan to match these records to the Census Bureau's business register, as well as one of the Bureau's longitudinal business microdata files.  Upon completion of these matches, I intend to revisit empirical modeling of business dissolution (using hazard functions), to analyze the inter-temporal transition of business entities into and out of employer status, and to analyze patterns in receipts change (for surviving entities) over time.  Some of the other questions that I intend to address in this project include the following.  How many new employer firms were previously nonemployer firms?  What are the general patterns of transition of business entities between employer and nonemployer status?  How many firms with receipts below Census' cutoff (to be included in published nonemployer statistics) actually continue and grow to exceed that receipts threshold in subsequent years?  How do macro- and local economic conditions influence business formation and dissolution? "
"Differential Privacy in the 2020 Decennial Census and the Implications
  for Available Data Products","In early 2021, the US Census Bureau will begin releasing statistical tables
based on the decennial census conducted in 2020. Because of significant changes
in the data landscape, the Census Bureau is changing its approach to disclosure
avoidance. The confidentiality of individuals represented ""anonymously"" in
these statistical tables will be protected by a ""formal privacy"" technique that
allows the Bureau to mathematically assess the risk of revealing information
about individuals in the released statistical tables. The Bureau's approach is
an implementation of ""differential privacy,"" and it gives a rigorously
demonstrated guaranteed level of privacy protection that traditional methods of
disclosure avoidance do not. Given the importance of the Census Bureau's
statistical tables to democracy, resource allocation, justice, and research,
confusion about what differential privacy is and how it might alter or
eliminate data products has rippled through the community of its data users,
namely: demographers, statisticians, and census advocates.
  The purpose of this primer is to provide context to the Census Bureau's
decision to use a technique based on differential privacy and to help data
users and other census advocates who are struggling to understand what this
mathematical tool is, why it matters, and how it will affect the Bureau's data
products.",['danah boyd'],[],0,arXiv,http://arxiv.org/abs/1907.03639v1,True,True,True,False,True,282,Rick Boden,Washington,Completed,2002,2003.0,"There has been a relative groundswell of empirical studies of business survival/turnover over the past decade.  However, most of these studies have one notable limitation--i.e., they are typically restricted to employer businesses.  The Center for Economic Studies (CES) has acquired microdata on tens of millions of nonemployer ""firms.""  I propose longitudinally linking as many of these records as possible over the years for which the nonemployer microdata are available.  I also plan to match these records to the Census Bureau's business register, as well as one of the Bureau's longitudinal business microdata files.  Upon completion of these matches, I intend to revisit empirical modeling of business dissolution (using hazard functions), to analyze the inter-temporal transition of business entities into and out of employer status, and to analyze patterns in receipts change (for surviving entities) over time.  Some of the other questions that I intend to address in this project include the following.  How many new employer firms were previously nonemployer firms?  What are the general patterns of transition of business entities between employer and nonemployer status?  How many firms with receipts below Census' cutoff (to be included in published nonemployer statistics) actually continue and grow to exceed that receipts threshold in subsequent years?  How do macro- and local economic conditions influence business formation and dissolution? "
"Idle Vehicle Relocation Strategy through Deep Learning for Shared
  Autonomous Electric Vehicle System Optimization","In optimization of a shared autonomous electric vehicle (SAEV) system, idle
vehicle relocation strategies are important to reduce operation costs and
customers' wait time. However, for an on-demand service, continuous
optimization for idle vehicle relocation is computationally expensive, and
thus, not effective. This study proposes a deep learning-based algorithm that
can instantly predict the optimal solution to idle vehicle relocation problems
under various traffic conditions. The proposed relocation process comprises
three steps. First, a deep learning-based passenger demand prediction model
using taxi big data is built. Second, idle vehicle relocation problems are
solved based on predicted demands, and optimal solution data are collected.
Finally, a deep learning model using the optimal solution data is built to
estimate the optimal strategy without solving relocation. In addition, the
proposed idle vehicle relocation model is validated by applying it to optimize
the SAEV system. We present an optimal service system including the design of
SAEV vehicles and charging stations. Further, we demonstrate that the proposed
strategy can drastically reduce operation costs and wait times for on-demand
services.","['Seongsin Kim', 'Ungki Lee', 'Ikjin Lee', 'Namwoo Kang']",[],0,arXiv,http://arxiv.org/abs/2010.09847v1,False,True,False,False,False,289,Yoonsoo Lee,Michigan,Completed,2006,2008.0,"A firm can grow over a sustained period of time by renewing itself through recurrent responses to various internal and external challenges. In the short run, a firm expands and contracts its activities and the number of workers it employs. Some radical changes in the environment, however, may lead a firm to shut down a plant and start over in a new location. Because of this, competition among state and local governments to lure businesses has attracted considerable interest from economists, as well as legislators and decision-makers, regarding issues influencing relocation of a firm’s manufacturing activities. While this process of relocation can cause dramatic shifts in activity and employment at the regional levels, as well as at the firm levels, very little is known about the actual patterns of relocation in the U.S. economy. Only a few previous studies have looked at how manufacturing firms geographically locate their production, and most of these have focused on either small manufacturing samples or small geographic regions. This project expands on this previous work by summarizing the patterns of plant relocation and the post-move performance of relocated plants using the full population of manufacturing establishments in the United States over the period 1963-1999 using non-publicly available plant and firm level data from the U.S. Census Longitudinal Research Database (LRD). Focusing on an individual firm’s decision to relocate, this project analyzes information on the relocation of a firm’s manufacturing activities in the following three subprojects. First, this project assesses the relative importance of relocation across industries and regions by constructing industry level measures of entry, relocation, and exit. The study then examines whether relocation produces different patterns in plant openings and closings compared to de novo entry and permanent exit. Second, this project studies the characteristics of relocated plants along with their decision to relocate. Econometric model estimation will characterize how individual firms’ geographic shifts of production processes are influenced by taxes, unionization, factor prices, ownership, and other geographic and plant specific characteristics. Third, this project investigates the impact of geographic shifts on a firm’s post move production by comparing the growth rates of output and productivity for newly relocated plants to those of existing plants in the original location. The inverse growth-age relation suggested by Jovanovic’s (1982) firm- learning model is tested for relocating plants to examine whether the inverse growth-age relation observed among young firms also holds for relocating plants that start over in a new space. This project provides a number of benefits to the Census Bureau. These benefits include producing new statistics on the geographic movement of manufacturing activities at the firm level thereby suggesting a new way to expand the utility of the LRD in describing the geographic patterns of economic activities in the United States. Results of this research may also demonstrate the need for new measures of relocation to be incorporated in future surveys. Additionally by establishing links to the original plant of relocated plant, this research examines the consistency of geographic identifiers by potentially identifying previously undocumented coding problems and improves the understanding of regional linkages in the LRD. A better understanding of the dynamic geographic distribution of firm activity will help characterize the patterns of firm ownership that could be valuable for designing inquires on the Company organization survey that is an important Title 13 component of the Standard Statistical Establishment List."
"Position: Solve Layerwise Linear Models First to Understand Neural
  Dynamical Phenomena (Neural Collapse, Emergence, Lazy/Rich Regime, and
  Grokking)","In physics, complex systems are often simplified into minimal, solvable
models that retain only the core principles. In machine learning, layerwise
linear models (e.g., linear neural networks) act as simplified representations
of neural network dynamics. These models follow the dynamical feedback
principle, which describes how layers mutually govern and amplify each other's
evolution. This principle extends beyond the simplified models, successfully
explaining a wide range of dynamical phenomena in deep neural networks,
including neural collapse, emergence, lazy and rich regimes, and grokking. In
this position paper, we call for the use of layerwise linear models retaining
the core principles of neural dynamical phenomena to accelerate the science of
deep learning.","['Yoonsoo Nam', 'Seok Hyeong Lee', 'Clementine Domine', 'Yea Chan Park', 'Charles London', 'Wonyl Choi', 'Niclas Goring', 'Seungjai Lee']",[],0,arXiv,http://arxiv.org/abs/2502.21009v1,False,True,False,False,False,289,Yoonsoo Lee,Michigan,Completed,2006,2008.0,"A firm can grow over a sustained period of time by renewing itself through recurrent responses to various internal and external challenges. In the short run, a firm expands and contracts its activities and the number of workers it employs. Some radical changes in the environment, however, may lead a firm to shut down a plant and start over in a new location. Because of this, competition among state and local governments to lure businesses has attracted considerable interest from economists, as well as legislators and decision-makers, regarding issues influencing relocation of a firm’s manufacturing activities. While this process of relocation can cause dramatic shifts in activity and employment at the regional levels, as well as at the firm levels, very little is known about the actual patterns of relocation in the U.S. economy. Only a few previous studies have looked at how manufacturing firms geographically locate their production, and most of these have focused on either small manufacturing samples or small geographic regions. This project expands on this previous work by summarizing the patterns of plant relocation and the post-move performance of relocated plants using the full population of manufacturing establishments in the United States over the period 1963-1999 using non-publicly available plant and firm level data from the U.S. Census Longitudinal Research Database (LRD). Focusing on an individual firm’s decision to relocate, this project analyzes information on the relocation of a firm’s manufacturing activities in the following three subprojects. First, this project assesses the relative importance of relocation across industries and regions by constructing industry level measures of entry, relocation, and exit. The study then examines whether relocation produces different patterns in plant openings and closings compared to de novo entry and permanent exit. Second, this project studies the characteristics of relocated plants along with their decision to relocate. Econometric model estimation will characterize how individual firms’ geographic shifts of production processes are influenced by taxes, unionization, factor prices, ownership, and other geographic and plant specific characteristics. Third, this project investigates the impact of geographic shifts on a firm’s post move production by comparing the growth rates of output and productivity for newly relocated plants to those of existing plants in the original location. The inverse growth-age relation suggested by Jovanovic’s (1982) firm- learning model is tested for relocating plants to examine whether the inverse growth-age relation observed among young firms also holds for relocating plants that start over in a new space. This project provides a number of benefits to the Census Bureau. These benefits include producing new statistics on the geographic movement of manufacturing activities at the firm level thereby suggesting a new way to expand the utility of the LRD in describing the geographic patterns of economic activities in the United States. Results of this research may also demonstrate the need for new measures of relocation to be incorporated in future surveys. Additionally by establishing links to the original plant of relocated plant, this research examines the consistency of geographic identifiers by potentially identifying previously undocumented coding problems and improves the understanding of regional linkages in the LRD. A better understanding of the dynamic geographic distribution of firm activity will help characterize the patterns of firm ownership that could be valuable for designing inquires on the Company organization survey that is an important Title 13 component of the Standard Statistical Establishment List."
"Interpolating Population Distributions using Public-use Data: An
  Application to Income Segregation using American Community Survey Data","Income segregation measures the extent to which households choose to live
near other households with similar incomes. Sociologists theorize that income
segregation can exacerbate the impacts of income inequality, and have developed
indices to measure it at the metro area level, including the information theory
index introduced in \citet{reardon2011income}, and the divergence index
presented in \citet{roberto2015divergence}. To study their differences, we
construct both indices using recent American Community Survey (ACS) estimates
of features of the income distribution. Since the elimination of the decennial
census long form, methods of computing these estimates must be updated to use
ACS estimates and account for survey error. We propose a model-based method to
interpolate estimates of features of the income distribution that accounts for
this error. This method improves on previous approaches by allowing for the use
of more types of estimates, and by providing uncertainty quantification. We
apply this method to estimate U.S. census tract-level income distributions
using ACS tabulations, and in turn use these to construct both income
segregation indices. We find major differences between the two indices in the
relative ranking of metro areas, as well as differences in how both indices
correlate with the Gini index.","['Matthew Simpson', 'Scott H. Holan', 'Christopher K. Wikle', 'Jonathan R. Bradley']",[],0,arXiv,http://arxiv.org/abs/1802.02626v3,False,True,False,False,True,297,Dennis Epple,CMU,Completed,2002,2005.0,"There is substantial theoretical and empirical research investigating the role of neighborhood effects in influencing socio-economic outcomes for minorities.  This research output points to both the importance of differentiating among ethnic groups and incorporating neighborhood effects into locational equilibrium models.  Urban economists have also investigated sorting by race when households have preferences about the racial composition of the area in which they live. Less work has been devoted to analyzing the interaction of preferences for neighborhood demographic composition and local public goods in determining the sorting of population by race across communities. We would like to know whether observed differences in racial sorting are due to differences in tastes for housing, tastes for public goods and endowments, or whether they can be attributed to tastes for racial homogeneity.  We propose to develop and estimate a locational equilibrium model to analyze sorting of households by demographic characteristics and income within a system of communities, paying special attention to neighborhood and local spillover effects. Thus understanding the formation of communities and neighborhoods, the sorting of households that differ by income, the demographic characteristics and tastes among a set of communities, and the interaction of households within communities are the objective of our proposed research.  Although issues of individual sorting into communities and the provision of local public services have been studied for the last 50 years, previous empirical studies have been hampered by important data limitations.  In particular, representative publicly available micro data are available only at a relatively high degree of aggregation thus limiting research attempts to understand the underlying household choice process involved.  This research proposes the use of the Decennial Census Long Form (CENSAS) data, which provides a detailed picture of each household's characteristics including place of residence and place(s) of work. The precise geographic information allows us to accurately match the important economic variables associated with each possible choice that a household could make. The level of geographic and demographic detail of the confidential data is essential to allowing us to properly analyze community characteristics that directly effect how households from different backgrounds sort across communities.  Our primary interest is in using the CENSAS Data for 1990 for a number of US metropolitan areas such as Boston, Chicago, Cleveland, Detroit, and Pittsburgh (as the decennial data is representative of the U.S. population, the wider applicability of our results is assured). We are also interested in comparing measures of the stratification and segregation of households by income and race across communities and between public and private schools in the 2000 data to those in the 1990 data.  Consequently, we would like to analyze the 2000 CENSAS Data as well when it becomes available.  We should emphasize, however, that our research project is feasible using only the 1990 data.   "
"Addressing Census data problems in race imputation via fully Bayesian
  Improved Surname Geocoding and name supplements","Prediction of individual's race and ethnicity plays an important role in
social science and public health research. Examples include studies of racial
disparity in health and voting. Recently, Bayesian Improved Surname Geocoding
(BISG), which uses Bayes' rule to combine information from Census surname files
with the geocoding of an individual's residence, has emerged as a leading
methodology for this prediction task. Unfortunately, BISG suffers from two
Census data problems that contribute to unsatisfactory predictive performance
for minorities. First, the decennial Census often contains zero counts for
minority racial groups in the Census blocks where some members of those groups
reside. Second, because the Census surname files only include frequent names,
many surnames -- especially those of minorities -- are missing from the list.
To address the zero counts problem, we introduce a fully Bayesian Improved
Surname Geocoding (fBISG) methodology that accounts for potential measurement
error in Census counts by extending the naive Bayesian inference of the BISG
methodology to full posterior inference. To address the missing surname
problem, we supplement the Census surname data with additional data on last,
first, and middle names taken from the voter files of six Southern states where
self-reported race is available. Our empirical validation shows that the fBISG
methodology and name supplements significantly improve the accuracy of race
imputation across all racial groups, and especially for Asians. The proposed
methodology, together with additional name data, is available via the
open-source software WRU.","['Kosuke Imai', 'Santiago Olivella', 'Evan T. R. Rosenman']",[],0,arXiv,http://arxiv.org/abs/2205.06129v3,False,True,False,False,True,297,Dennis Epple,CMU,Completed,2002,2005.0,"There is substantial theoretical and empirical research investigating the role of neighborhood effects in influencing socio-economic outcomes for minorities.  This research output points to both the importance of differentiating among ethnic groups and incorporating neighborhood effects into locational equilibrium models.  Urban economists have also investigated sorting by race when households have preferences about the racial composition of the area in which they live. Less work has been devoted to analyzing the interaction of preferences for neighborhood demographic composition and local public goods in determining the sorting of population by race across communities. We would like to know whether observed differences in racial sorting are due to differences in tastes for housing, tastes for public goods and endowments, or whether they can be attributed to tastes for racial homogeneity.  We propose to develop and estimate a locational equilibrium model to analyze sorting of households by demographic characteristics and income within a system of communities, paying special attention to neighborhood and local spillover effects. Thus understanding the formation of communities and neighborhoods, the sorting of households that differ by income, the demographic characteristics and tastes among a set of communities, and the interaction of households within communities are the objective of our proposed research.  Although issues of individual sorting into communities and the provision of local public services have been studied for the last 50 years, previous empirical studies have been hampered by important data limitations.  In particular, representative publicly available micro data are available only at a relatively high degree of aggregation thus limiting research attempts to understand the underlying household choice process involved.  This research proposes the use of the Decennial Census Long Form (CENSAS) data, which provides a detailed picture of each household's characteristics including place of residence and place(s) of work. The precise geographic information allows us to accurately match the important economic variables associated with each possible choice that a household could make. The level of geographic and demographic detail of the confidential data is essential to allowing us to properly analyze community characteristics that directly effect how households from different backgrounds sort across communities.  Our primary interest is in using the CENSAS Data for 1990 for a number of US metropolitan areas such as Boston, Chicago, Cleveland, Detroit, and Pittsburgh (as the decennial data is representative of the U.S. population, the wider applicability of our results is assured). We are also interested in comparing measures of the stratification and segregation of households by income and race across communities and between public and private schools in the 2000 data to those in the 1990 data.  Consequently, we would like to analyze the 2000 CENSAS Data as well when it becomes available.  We should emphasize, however, that our research project is feasible using only the 1990 data.   "
"Are high school degrees and university diplomas equally heritable in the
  US? A new measure of relative intergenerational mobility","This paper proposes a new measure of relative intergenerational mobility
along the educational trait as a proxy of inequality of opportunity. The new
measure is more suitable for controlling for the variations in the trait
distributions of individuals and their parents than the commonly used
intergenerational persistence coefficient. This point is illustrated by our
empirical analysis of US census data from the period between 1960 and 2015: we
show that controlling for the variations in the trait distributions adequately
is vital in assessing the part of intergenerational mobility which is not
caused by the educational expansion. Failing to do so can potentially reverse
the relative priority of various policies aiming at reducing the ""heritability""
of high school degrees and tertiary education diplomas.","['Anna Naszodi', 'Liliana Cuccu']",[],0,arXiv,http://arxiv.org/abs/2303.08445v1,False,True,False,False,False,299,Bhashkar Mazumder,Chicago,Completed,2007,2011.0,"This research project proposes three major areas of study in order to better understand the intergenerational transmission of inequality. First, building on Mazumder’s previous work, the 1984 and 1990 Surveys of Income and Program Participation (SIPP) matched to the Social Security Administration’s Summary Earnings Records (SER) and Detailed Earnings Records (DER) will be used to measure the intergenerational elasticity in earnings between fathers and their children. Second, a highly structured model of earnings dynamics will be estimated using pooled data from the 1984, 1990, 1991, 1992, 1993, and 1996 SIPPs matched to both the SER and DER. Among other things, this will provide a definitive view of the degree to which the rise in inequality during the 1980s and 1990s reflected changes in the distribution of permanent income. Third, a rich array of measures of family background and neighborhood characteristics will be used to better understand the underlying process by which income is transmitted from parents to children. This analysis will make use of the internal SIPP and Survey of Program Dynamics (SPD) files that contain the detailed geographic identifiers. There are four benefits to the U.S. Census Bureau that will be derived from this study: an analysis of the reliability of using short-term averages of SIPP earnings as a proxy for permanent earnings, a study of the quality of earnings data for an attrited sample such as the SPD, an analysis of the quality of self-employment income data in the SIPP, and an analysis of the biases from using a sample derived from a match based on social security numbers."
"Algorithmic Inheritance: Surname Bias in AI Decisions Reinforces
  Intergenerational Inequality","Surnames often convey implicit markers of social status, wealth, and lineage,
shaping perceptions in ways that can perpetuate systemic biases and
intergenerational inequality. This study is the first of its kind to
investigate whether and how surnames influence AI-driven decision-making,
focusing on their effects across key areas such as hiring recommendations,
leadership appointments, and loan approvals. Using 72,000 evaluations of 600
surnames from the United States and Thailand, two countries with distinct
sociohistorical contexts and surname conventions, we classify names into four
categories: Rich, Legacy, Normal, and phonetically similar Variant groups. Our
findings show that elite surnames consistently increase AI-generated
perceptions of power, intelligence, and wealth, which in turn influence
AI-driven decisions in high-stakes contexts. Mediation analysis reveals
perceived intelligence as a key mechanism through which surname biases
influence AI decision-making process. While providing objective qualifications
alongside surnames mitigates most of these biases, it does not eliminate them
entirely, especially in contexts where candidate credentials are low. These
findings highlight the need for fairness-aware algorithms and robust policy
measures to prevent AI systems from reinforcing systemic inequalities tied to
surnames, an often-overlooked bias compared to more salient characteristics
such as race and gender. Our work calls for a critical reassessment of
algorithmic accountability and its broader societal impact, particularly in
systems designed to uphold meritocratic principles while counteracting the
perpetuation of intergenerational privilege.","['Pat Pataranutaporn', 'Nattavudh Powdthavee', 'Pattie Maes']",[],0,arXiv,http://arxiv.org/abs/2501.19407v2,False,True,False,False,False,299,Bhashkar Mazumder,Chicago,Completed,2007,2011.0,"This research project proposes three major areas of study in order to better understand the intergenerational transmission of inequality. First, building on Mazumder’s previous work, the 1984 and 1990 Surveys of Income and Program Participation (SIPP) matched to the Social Security Administration’s Summary Earnings Records (SER) and Detailed Earnings Records (DER) will be used to measure the intergenerational elasticity in earnings between fathers and their children. Second, a highly structured model of earnings dynamics will be estimated using pooled data from the 1984, 1990, 1991, 1992, 1993, and 1996 SIPPs matched to both the SER and DER. Among other things, this will provide a definitive view of the degree to which the rise in inequality during the 1980s and 1990s reflected changes in the distribution of permanent income. Third, a rich array of measures of family background and neighborhood characteristics will be used to better understand the underlying process by which income is transmitted from parents to children. This analysis will make use of the internal SIPP and Survey of Program Dynamics (SPD) files that contain the detailed geographic identifiers. There are four benefits to the U.S. Census Bureau that will be derived from this study: an analysis of the reliability of using short-term averages of SIPP earnings as a proxy for permanent earnings, a study of the quality of earnings data for an attrited sample such as the SPD, an analysis of the quality of self-employment income data in the SIPP, and an analysis of the biases from using a sample derived from a match based on social security numbers."
"An outlier-resistant indicator of anomalies among inter-laboratory
  comparison data with associated uncertainty","A new robust pairwise statistic, the pairwise median scaled difference (MSD),
is proposed for the detection of anomalous location/uncertainty pairs in
heteroscedastic interlaboratory study data with associated uncertainties. The
distribution for the IID case is presented and approximate critical values for
routine use are provided. The determination of observation-specific quantiles
and p-values for heteroscedastic data, using parametric bootstrapping, is
demonstrated by example. It is shown that the statistic has good power for
detecting anomalies compared to a previous pairwise statistic, and offers much
greater resistance to multiple outlying values.",['Stephen L. R. Ellison'],[],0,arXiv,http://arxiv.org/abs/1809.01094v1,False,True,False,False,False,305,Glenn Ellison,Boston,Completed,2002,2004.0,"This project proposal has two research goals. First, using the Longitudinal Research Database (LRD), we plan to revise our working paper NBER #6270 and CES-WP-98-3 we produced under a prior BRDC project. Second, we intend to investigate the impact of the change in industrial classification systems from the Standard Industrial Classification (SIC) to the North American Industrial Classification System (NAICS) from a spatial perspective. The NAICS represents a move forward in industrial classification from the SIC, but it is still difficult to appraise the geographic implications of this change in classification, both from the perspective of the longitudinal transition across years of Census of Manufacturers before and after 1997, as well as for what we know about the geographic distribution of economic activity. Does the geographic concentration of NAICS classified industries differ from that of comparable SIC classified industries, or are they very similar? By comparing locational measures for different industry classifications, we can begin to tell whether different classifications create different biases that Census planners and researchers need to be aware of. Finally, by looking at changes in industry classification for plants over time, we can check whether the NAICS system has led to more or less consistency in classification over time."
"The Resurgence of Trumponomics: Implications for the Future of ESG
  Investments in a Changing Political Landscape","Public policy shapes the economic landscape, influencing everything from
corporate behavior to individual investment decisions. For Environmental,
Social, and Governance (ESG) investors, these policy shifts can create
opportunities and challenges as they navigate an ever-changing regulatory
environment. The contrast between the Trump and Biden administrations offers a
striking example of how differing political agendas can affect ESG investments.
Trump's first term was marked by deregulation and policies favoring fossil
fuels, which created an uncertain environment for sustainable investments. When
Biden assumed office, his focus on climate action and clean energy
reinvigorated the ESG sector, offering a more stable and supportive landscape
for green investments. However, with Trump's return to power in his second
term, these policies are being reversed again, leading to further volatility.
This paper explores how such dramatic shifts in public policy influence
economic strategies and directly impact ESG investors' decisions, forcing them
to constantly reassess their portfolios in response to changing political
climates.",['Innocentus Alhamis'],[],0,arXiv,http://arxiv.org/abs/2502.02627v1,False,True,False,False,False,306,William R Kerr,Boston,Completed,2005,2007.0,"This study will characterize how changes in political environments from 1963–2000 influenced the employment and investment decisions, including technology adoption, of manufacturing establishments. Special attention will be given to the impact of elections themselves, including expectations of the candidates’ ideologies. In the face of uncertain elections, are plants more reserved in their hiring or investing behavior? Do the victories of candidates with very strong ideologies lead to discrete adjustments in anticipation of future conditions? Three econometric specifications will be considered: standard cross-sectional regressions; a state border discontinuity analysis; and a longitudinal analysis using a balanced 1973–1988 panel of Annual Survey of Manufacturing plants. Use of detailed plant data housed at the Center for Economic Studies is essential for isolating the impact of local politics on establishment behavior, employing a border effects analysis that requires county identification, and characterizing the different reactions of local, single-plant firms versus establishments part of large, regionally-diverse enterprises. 
This project is a response to the July 2001 Research Opportunities at the Census Bureau publication that requests proposals studying how higher-order moments (i.e., skewness, kurtosis) of the cross-section distribution of investment and employment variables should be made publicly available. To identify the influences of regional political environments and elections, I will need to calculate these higher-order moments for individual states and perhaps smaller county/MSA divisions. In a technical memo, I will be able to characterize the best candidate metrics for release from the perspectives of the Census Bureau and potential researchers, the limits to disaggregating geographically these moments (either due to confidentiality concerns or data quality issues), and if and how these higher-order moments should be calculated in non-Census of Manufacturers years. Additional benefits are also identified in the proposal."
From the Top Down: Does Corruption Affect Performance?,"Corruption, fraud, and unethical activities have emerged as significant
obstacles to global economic, political, and social progress. Although many
empirical studies have focused on country-level corruption metrics, this study
is the first to utilize a substantial international dataset to assess the
effects of illicit and unethical managerial practices on firm performance.
Employing cross-sectional data, this research examines the influence of
corruption on corporate outcomes. Our definition of corruption evaluates the
degree to which managers engage in mismanagement, misconduct, or corrupt
activities. The repercussions for corporate governance, especially concerning
the process of appointing managers, are both crucial and strategic.","['Maurizio La Rocca', 'Tiziana La Rocca', 'Francesco Fasano', 'Javier Sanchez-Vidal']",[],0,arXiv,http://arxiv.org/abs/2310.20028v1,False,True,False,False,False,308,Antoinette Schoar,Boston,Completed,2004,2007.0,"The purpose of this proposal is threefold. First, we intend to extend prior work matching COMPUSTAT data to the Longitudinal Research Database (LRD) to perform a data cross check and comparative analysis for consistency. This work builds off a currently active project by the principal investigators that will perform the basic matching work and will write a technical memorandum summarizing findings. This proposal will use that work in an extension to identify potential sources and explanations for the cases where data do not coincide. These discrepancies will be systematically categorized. Second, we will investigate the relationship between corporate governance and plant performance. Twenty years ago the term “corporate governance"" did not exist in the English language. In the last two decades, however, corporate governance issues have gained a prominent role in the academic literature. In spite of the importance of the topic and the magnitude of the interests at stake, there is very little empirical evidence on what the effects of corporate governance are and what constitutes “good” and “bad” governance. For this purpose we will build on the LRD-COMPUSTAT match and use ownership concentration and governance variables from CD-Spectrum to investigate the relationship between firm performance and governance."
CSRCZ: A Dataset About Corporate Social Responsibility in Czech Republic,"As stakeholders' pressure on corporates for disclosing their corporate social
responsibility operations grows, it is crucial to understand how efficient
corporate disclosure systems are in bridging the gap between corporate social
responsibility reports and their actual practice. Meanwhile, research on
corporate social responsibility is still not aligned with the recent
data-driven strategies, and little public data are available. This paper aims
to describe CSRCZ, a newly created dataset based on disclosure reports from the
websites of 1000 companies that operate in Czech Republic. Each company was
analyzed based on three main parameters: company size, company industry, and
company initiatives. We describe the content of the dataset as well as its
potential use for future research. We believe that CSRCZ has implications for
further research, since it is the first publicly available dataset of its kind.","['Xhesilda Vogli', 'Erion Çano']",[],0,arXiv,http://arxiv.org/abs/2301.03404v1,False,True,False,False,False,317,Ekaterina E Emm,Triangle,Completed,2003,2005.0,"In this project, we contribute to the ongoing research on the rationales for corporate diversification. Using the Longitudinal Research Database (LRD) as our main source of data, we examine whether combining several lines of business under one corporate umbrella leads to increased productive performance. Studying the direct effect of diversification on productive efficiency allows us to discern between two major theories of corporate diversification: the agency cost hypothesis and the synergy hypothesis. Further, the project contributes to the literature by investigating whether efficiency differences between diversified and focused firms lead to the “diversification discount”. To measure productive efficiency, we employ a non-parametric approach, the Weak Axiom of Profit Maximization (WAPM), using establishment-level and firm-level data. This method has several advantages over other conventional measures of productive efficiency. To the best of our knowledge, this project is the first application of the WAPM to a sample of non-financial institutions. This project will provide benefits to the Census Bureau’s data programs through understanding and improving the quality of the data. The project will merge the Annual Survey of Manufactures (ASM/LRD), Census of Manufactures (CM/LRD), Company Auxiliary Offices (CAO), National Employer Survey (NES), Standard Statistical Establishment List (SSEL), and the time-linked version of the SSEL called the Longitudinal Business Database with the public-use databases COMPUSTAT, Center for Research in Security Prices (CRSP) and Security Data Corporation (SDC). Once the databases are merged, comparisons of the data items collected by the Census Bureau to the data contained in the public-use databases will be performed. "
"Comment: The Essential Role of Policy Evaluation for the 2020 Census
  Disclosure Avoidance System","In ""Differential Perspectives: Epistemic Disconnects Surrounding the US
Census Bureau's Use of Differential Privacy,"" boyd and Sarathy argue that
empirical evaluations of the Census Disclosure Avoidance System (DAS),
including our published analysis, failed to recognize how the benchmark data
against which the 2020 DAS was evaluated is never a ground truth of population
counts. In this commentary, we explain why policy evaluation, which was the
main goal of our analysis, is still meaningful without access to a perfect
ground truth. We also point out that our evaluation leveraged features specific
to the decennial Census and redistricting data, such as block-level population
invariance under swapping and voter file racial identification, better
approximating a comparison with the ground truth. Lastly, we show that accurate
statistical predictions of individual race based on the Bayesian Improved
Surname Geocoding, while not a violation of differential privacy, substantially
increases the disclosure risk of private information the Census Bureau sought
to protect. We conclude by arguing that policy makers must confront a key
trade-off between data utility and privacy protection, and an epistemic
disconnect alone is insufficient to explain disagreements between policy
choices.","['Christopher T. Kenny', 'Shiro Kuriwaki', 'Cory McCartan', 'Evan T. R. Rosenman', 'Tyler Simko', 'Kosuke Imai']",[],0,arXiv,http://arxiv.org/abs/2210.08383v1,True,True,True,False,True,335,Fernando V Ferreira,Berkeley,Completed,2003,2006.0,"The proposed project uses the 1990 Decennial Census Long Form Data for California to investigate the consequences of barriers to sorting, such as availability of houses and transaction costs, in the housing market equilibrium and its effects on the valuation of school quality. It builds on research by Bayer, McMillan, and Rueben. The methodology is based on the estimation of random coefficients multinomial logits, where I examine how different choice sets affect household sorting, after accounting for transaction costs, and modeling household mobility explicitly (mobility as a probability of moving to a different house unit). The inclusion of this extra modeling will permit the estimation of unbiased probabilities of households choosing a certain house/neighborhood. Also, the new models make it possible to identify the characteristics of individuals most affected by barriers to sorting. Only with such information can one understand how individual preferences determine location decisions and how important school quality is in determining housing choices within a heterogeneous population. Hypothesis testing will be performed on the average marginal willingness to pay for school quality estimates, as well as on the heterogeneity in individual preferences estimates. The research is feasible given the richness of the Decennial Census Long Form Data. These data provide the geographic location of each household’s place of residence at the Census block level, allowing the choice sets to be defined precisely in terms of house attributes/neighborhood characteristics. Also, with the Decennial Census Long Form Data, I can accurately assign a public school to each house unit, providing a school quality measure that will be extensively used in the estimations. This research yields benefits to the Census Bureau that fall into four areas: (1) compiling a series of external datasets to be linked to the 2000 Decennial Census Long Form Data; (2) analyzing measurement errors in the self-reported ‘travel time to work’ variable; (3) providing comparisons between allocation methods for the ‘travel time to work’ variable; (4) providing an algorithm for linking Census data with data from other sources, with an application to public school data. I will also attempt to create weights for school attendance areas and estimates relating school-age children and households with children in school."
"The Impact of De-Identification on Single-Year-of-Age Counts in the U.S.
  Census","In 2020, the U.S. Census Bureau transitioned from data swapping to
differential privacy (DP) in its approach to de-identifying decennial census
data. This decision has faced considerable criticism from data users,
particularly due to concerns about the accuracy of DP. We compare the relative
impacts of swapping and DP on census data, focusing on the use case of school
planning, where single-year-of-age population counts (i.e., the number of
four-year-olds in the district) are used to estimate the number of incoming
students and make resulting decisions surrounding faculty, classrooms, and
funding requests. We examine these impacts for school districts of varying
population sizes and age distributions.
  Our findings support the use of DP over swapping for single-year-of-age
counts; in particular, concerning behaviors associated with DP (namely, poor
behavior for smaller districts) occur with swapping mechanisms as well. For the
school planning use cases we investigate, DP provides comparable, if not
improved, accuracy over swapping, while offering other benefits such as
improved transparency.","['Sarah Radway', 'Miranda Christ']",[],0,arXiv,http://arxiv.org/abs/2308.12876v1,True,True,False,False,True,335,Fernando V Ferreira,Berkeley,Completed,2003,2006.0,"The proposed project uses the 1990 Decennial Census Long Form Data for California to investigate the consequences of barriers to sorting, such as availability of houses and transaction costs, in the housing market equilibrium and its effects on the valuation of school quality. It builds on research by Bayer, McMillan, and Rueben. The methodology is based on the estimation of random coefficients multinomial logits, where I examine how different choice sets affect household sorting, after accounting for transaction costs, and modeling household mobility explicitly (mobility as a probability of moving to a different house unit). The inclusion of this extra modeling will permit the estimation of unbiased probabilities of households choosing a certain house/neighborhood. Also, the new models make it possible to identify the characteristics of individuals most affected by barriers to sorting. Only with such information can one understand how individual preferences determine location decisions and how important school quality is in determining housing choices within a heterogeneous population. Hypothesis testing will be performed on the average marginal willingness to pay for school quality estimates, as well as on the heterogeneity in individual preferences estimates. The research is feasible given the richness of the Decennial Census Long Form Data. These data provide the geographic location of each household’s place of residence at the Census block level, allowing the choice sets to be defined precisely in terms of house attributes/neighborhood characteristics. Also, with the Decennial Census Long Form Data, I can accurately assign a public school to each house unit, providing a school quality measure that will be extensively used in the estimations. This research yields benefits to the Census Bureau that fall into four areas: (1) compiling a series of external datasets to be linked to the 2000 Decennial Census Long Form Data; (2) analyzing measurement errors in the self-reported ‘travel time to work’ variable; (3) providing comparisons between allocation methods for the ‘travel time to work’ variable; (4) providing an algorithm for linking Census data with data from other sources, with an application to public school data. I will also attempt to create weights for school attendance areas and estimates relating school-age children and households with children in school."
A Tale of Two Mechanisms: Incentivizing Investments in Security Games,"In a system of interdependent users, the security of an entity is affected
not only by that user's investment in security measures, but also by the
positive externality of the security decisions of (some of) the other users.
The provision of security in such system is therefore modeled as a public good
provision problem, and is referred to as a security game. In this paper, we
compare two well-known incentive mechanisms in this context for incentivizing
optimal security investments among users, namely the Pivotal and the
Externality mechanisms. The taxes in a Pivotal mechanism are designed to ensure
users' voluntary participation, while those in an Externality mechanism are
devised to maintain a balanced budget. We first show the more general result
that, due to the non-excludable nature of security, no mechanism can
incentivize the socially optimal investment profile, while at the same time
ensuring voluntary participation and maintaining a balanced budget for all
instances of security games. To further illustrate, we apply the Pivotal and
Externality mechanisms to the special case of weighted total effort
interdependence models, and identify some of the effects of varying
interdependency between users on the budget deficit in the Pivotal mechanism,
as well as on the participation incentives in the Externality mechanism.","['Parinaz Naghizadeh', 'Mingyan Liu']",[],0,arXiv,http://arxiv.org/abs/1503.07377v1,False,True,False,False,False,347,Jhih-shyang Shih,Washington,Completed,2005,2008.0," 
Collected under Title 13, Chapter 5, of the U.S. Code, the Census of Manufactures, Annual Survey of Manufactures, and Manufacturing Energy Consumption survey all contain important information about plant-level activity in the United States and associated material and energy use. Over the past decade, voluntary environmental programs have played an increasingly important role in environmental and energy management. Yet existing programs have been subject to only limited empirical study. An important question is whether participation in these programs is important enough to warrant inclusion in future surveys, analogous to current questions about energy management. 
This project will increase the knowledge base of the U.S. Census Bureau and other researchers and analysts by merging existing data with additional information on emissions and voluntary program participation. First, this project will allow us to examine the impact of voluntary program participation and whether it warrants inclusion in future surveys. Second, the project improves our understanding of plant characteristics and activities while checking the quality of existing data. Third, the merged datasets will allow us to calculate population estimates of emissions and other measures of plant activity with and without the voluntary programs. 
All three results promise important benefits for the Census Bureau in its effort to improve the quality and usefulness of both existing Title 13, Chapter 5 data, as well as future survey instruments. Understanding how program participation interacts with other inputs and outputs can indicate whether participation indicators would be useful in future data collection. Comparisons with newly merged datasets allow for verification of some data elements. Even where direct comparisons are not possible, we can observe anomalies in indirect comparisons (for example, energy use and emissions) that signal a quality issue. As we compute population estimates of plant emissions and activity with and without voluntary programs, we will make use of state-of-the-art sample selection techniques. Such techniques, which are analogous to missing data techniques, could prove useful in other areas of work with Census Bureau data where population estimates are necessary despite significant problems with missing data. Finally, we expect this work to generate suggestions for improved survey design in the future. 
The last result will provide some of the first quantified estimates of voluntary program consequences involving careful attention to sample selection issues. Drawing on experience with EPA’s 33/50 and Climate Wise programs, and DOE’s 1605(b) program, the proposed research will identify program consequences based on competing sample selection approaches that jointly model voluntary program participation and emission outcome. The assumptions inherent in these competing models can alter or reverse estimated population effects. Comparing estimates across models and programs, we expect to draw conclusions that will be more robust and therefore more valuable for future decision-making. "
"Effects of the Affordable Care Act Dependent Coverage Mandate on Health
  Insurance Coverage for Individuals in Same-Sex Couples","A large body of research documents that the 2010 dependent coverage mandate
of the Affordable Care Act was responsible for significantly increasing health
insurance coverage among young adults. No prior research has examined whether
sexual minority young adults also benefitted from the dependent coverage
mandate, despite previous studies showing lower health insurance coverage among
sexual minorities and the fact that their higher likelihood of strained
relationships with their parents might predict a lower ability to use parental
coverage. Our estimates from the American Community Surveys using
difference-in-differences and event study models show that men in same-sex
couples age 21-25 were significantly more likely to have any health insurance
after 2010 compared to the associated change for slightly older 27 to
31-year-old men in same-sex couples. This increase is concentrated among
employer-sponsored insurance, and it is robust to permutations of time periods
and age groups. Effects for women in same-sex couples and men in different-sex
couples are smaller than the associated effects for men in same-sex couples.
These findings confirm the broad effects of expanded dependent coverage and
suggest that eliminating the federal dependent mandate could reduce health
insurance coverage among young adult sexual minorities in same-sex couples.","['Christopher S. Carpenter', 'Gilbert Gonzales', 'Tara McKay', 'Dario Sansone']",[],0,arXiv,http://arxiv.org/abs/2004.02296v1,False,False,False,False,True,348,Anthony T Lo sasso,Chicago,Completed,2004,2006.0,"Little is known about the role of the local health care safety net in affecting employers’ decisions to offer health insurance to their workers and the characteristics of the health insurance offered. The availability of safety net health care services may induce some firms not to offer health insurance to workers. Examples of firms that might be most likely to consider public sources of health care a potential substitute for privately-offered insurance include small firms with potentially high health insurance costs, firms hiring predominantly low-wage, low-skill workers, or firms in economically depressed areas. We propose to use Medical Expenditure Panel Survey Insurance Component (MEPS-IC) data for the years 1996-2000 combined with detailed measures of local health care facilities to examine the link between local safety net characteristics and employer-provided health insurance. By surveying approximately 25,000 firms annually regarding the characteristics of their health insurance, the MEPS-IC provides an ideal means of examining how local health care safety net characteristics affect the willingness of firms to offer health insurance. The detailed measures, the consistency of measurement over time, and the sample size will allow us to explore not only how the health care safety net affects whether employers offer health insurance but also the effects on the attributes of offered health insurance. We have identified three primary benefits to Census Bureau from our research. First, related to understanding the quality of the data, our research will involve different means of measuring the extent and characteristics of health insurance provision by employers for different subgroups of employers. Second, related to potential improvements in the quality of the data, our project involves a new approach to measuring the substitution of privately provided health care for publicly provided health care. Third, our project will enhance the MEPS-IC data by adding information from other data sources on local safety-net measures."
Price and capacity competition in balancing markets with energy storage,"Energy storage can absorb variability from the rising number of wind and
solar power producers. Storage is different from the conventional generators
that have traditionally balanced supply and demand on fast time scales due to
its hard energy capacity constraints, dynamic coupling, and low marginal costs.
These differences are leading system operators to propose new mechanisms for
enabling storage to participate in reserve and real-time energy markets. The
persistence of market power and gaming in electricity markets suggests that
these changes will expose new vulnerabilities.
  We develop a new model of strategic behavior among storages in energy
balancing markets. Our model is a two-stage game that generalizes a classic
model of capacity followed by Bertrand-Edgeworth price competition by
explicitly modeling storage dynamics and uncertainty in the pricing stage. By
applying the model to balancing markets with storage, we are able to compare
capacity and energy-based pricing schemes, and to analyze the dynamic effects
of the market horizon and energy losses due to leakage. Our first key finding
is that capacity pricing leads to higher prices and higher capacity
commitments, and that energy pricing leads to lower, randomized prices and
lower capacity commitments. Second, we find that a longer market horizon and
higher physical efficiencies lead to lower prices by inducing the storage to
compete to have their states of charge cycled more frequently.","['Josh A. Taylor', 'Johanna L. Mathieu', 'Duncan S. Callaway', 'Kameshwar Poolla']",[],0,arXiv,http://arxiv.org/abs/1503.06851v2,False,True,False,False,False,355,Joshua Linn,Chicago,Completed,2006,2010.0,"This project will use factor price and quantity data from the Longitudinal Research Database (LRD) and the Manufacturing Energy Consumption Survey (MECS) to perform a detailed analysis of energy data and plant productivity.  The project will be beneficial to the U.S. Census Bureau in several ways. I will check data quality in these datasets by comparisons with publicly available data from the U.S. Department of Energy. I will investigate the extent to which greater detail in energy price aggregates and indices could be published, both in terms of confidentiality risks and whether there is sufficient variation to make this valuable. I will also perform an investigation of data quality and the measurement of technological change to deter-mine whether observed changes in energy efficiency are caused by a response to energy prices, as opposed to misreporting. This study will improve the Census Bureau’s knowledge base regarding manufacturers’ energy input values and the response of manufacturing production and technology to energy prices and use.  Estimates of cost functions for each industry will be used to calculate changes in energy productivity over time. This approach allows measurement of the effect of price-induced technical change on energy productivity by comparing the changes according to the industry’s initial energy intensity. The analysis will com-pare the energy efficiency of entering and existing plants, which will show how much technology adoption by existing plants reduces the total demand for energy."
"The Growing US-Mexico Natural Gas Trade and Its Regional Economic
  Impacts in Mexico","With the recent administration change in Mexico, the fluctuations in national
energy policy have generated widespread concerns among investors and the
public. The debate centers around Mexico's energy dependence on the US and how
Mexico's energy development should move forward. The goal of this study is
two-fold. We first review the history and background of the recent energy
reforms in Mexico. The focus of the study is on quantifying the state-level
regional economic impact of the growing US-Mexico natural gas trade in Mexico.
We examine both the quantity effect (impact of import volume) and the price
effect (impact of natural gas price changes). Our empirical analysis adopts a
fixed-effects regression model and the instrumental variables (IV) estimation
approach to address spatial heterogeneities and the potential endogeneity
associated with natural gas import. The quantity effect analysis suggests a
statistically significant positive employment impact of imports in non-mining
sectors. The impact in the mining sector, however, is insignificant. The
state-level average (non-mining) employment impact is 127 jobs per million MCFs
of natural gas imported from the US. The price effect analysis suggests a
statistically significant positive employment impact of price increases in the
mining sector. A one-percentage increase in natural gas price (1.82 Pesos/GJ,
in 2015 Peso) leads to an average state-level mining employment increase of 140
(or 2.38%). We also explored the implications of our findings for Mexico's
energy policy, trade policy, and energy security.","['Haoying Wang', 'Rafael Garduno Rivera']",[],0,arXiv,http://arxiv.org/abs/2208.06928v1,False,True,False,False,False,355,Joshua Linn,Chicago,Completed,2006,2010.0,"This project will use factor price and quantity data from the Longitudinal Research Database (LRD) and the Manufacturing Energy Consumption Survey (MECS) to perform a detailed analysis of energy data and plant productivity.  The project will be beneficial to the U.S. Census Bureau in several ways. I will check data quality in these datasets by comparisons with publicly available data from the U.S. Department of Energy. I will investigate the extent to which greater detail in energy price aggregates and indices could be published, both in terms of confidentiality risks and whether there is sufficient variation to make this valuable. I will also perform an investigation of data quality and the measurement of technological change to deter-mine whether observed changes in energy efficiency are caused by a response to energy prices, as opposed to misreporting. This study will improve the Census Bureau’s knowledge base regarding manufacturers’ energy input values and the response of manufacturing production and technology to energy prices and use.  Estimates of cost functions for each industry will be used to calculate changes in energy productivity over time. This approach allows measurement of the effect of price-induced technical change on energy productivity by comparing the changes according to the industry’s initial energy intensity. The analysis will com-pare the energy efficiency of entering and existing plants, which will show how much technology adoption by existing plants reduces the total demand for energy."
"Big Pharma, little science? A bibliometric perspective on big pharma's
  R&D decline","There is a widespread perception that pharmaceutical R&D is facing a
productivity crisis characterised by stagnation in the numbers of new drug
approvals in the face of increasing R&D costs. This study explores
pharmaceutical R&D dynamics by examining the publication activities of all R&D
laboratories of the major European and US pharmaceutical firms during the
period 1995-2009. The empirical findings present an industry in
transformation.In the first place, we observe a decline of the total number of
publications by large firms. Second, we show a relative increase of their
external collaborations suggesting a tendency to outsource, and a
diversification of the disciplinary base, in particular towards computation,
health services and more clinical approaches. Also evident is a more pronounced
decline in publications by both R&D laboratories located in Europe and by firms
with European headquarters. Finally, while publications by Big Pharma in
emerging economies sharply increase, they remain extremely low compared with
those in developed countries. In summary, the trend in this transformation is
one of a gradual decrease in internal research efforts and increasing reliance
on external research. These empirical insights support the view that large
pharmaceutical firms are increasingly becoming networks integrators rather than
the prime locus of drug discovery.","['Ismael Rafols', 'Michael M. Hopkins', 'Jarno Hoekman', 'Josh Siepel', ""Alice O'Hare"", 'Antonio Perianes-Rodriguez', 'Paul Nightingale']",[],0,arXiv,http://arxiv.org/abs/1306.0947v1,False,True,False,False,False,361,Yukako Ono,Chicago,Completed,2006,2011.0,"The purpose of this proposal is to examine the quality of U.S. Census Bureau data on auxiliary offices and their firms and to perform an analysis of the firm determinants of nonresponse in Auxiliary Establishment (AE) datasets. We will investigate the determinants of a firm’s decision to establish a separate headquarters facility, where to locate it, and the determinants of out-sourcing behavior by headquarters.   We will use AE data, which provide information about the administrative establishments of firms. Since few researchers have used these data, assessing the quality of the data is an important part of the project.  First, we will link the AE data to Compustat® and to the Business Register in order to recover the main headquarters of the firm. Second, the characteristics of firms, as well as of local input service suppliers, will be constructed.  Finally, the purchased service expenditures information in the AE and other datasets will be used to investigate outsourcing behavior.   The project will benefit Census Bureau programs by exploring identification of the function performed by central administrative office (CAO) auxiliary establishments in the AE survey, both from the perspective of differentiating headquarters offices from other administrative units, as well as through an exploration of nonresponse to the function related questions on the survey. The project will explain which firms tend to have central administrative offices, where they locate them, and how their outsourcing behavior can be characterized."
The Network of Inter-Regional Direct Investment Stocks across Europe,"We propose a methodological framework to study the dynamics of inter-regional
investment flow in Europe from a Complex Networks perspective, an approach with
recent proven success in many fields including economics. In this work we study
the network of investment stocks in Europe at two different levels: first, we
compute the inward-outward investment stocks at the level of firms, based on
ownership shares and number of employees; then we estimate the inward-outward
investment stock at the level of regions in Europe, by aggregating the
ownership network of firms, based on their headquarter location. Despite the
intuitive value of this approach for EU policy making in economic development,
to our knowledge there are no similar works in the literature yet. In this
paper we focus on statistical distributions and scaling laws of activity,
investment stock and connectivity degree both at the level of firms and at the
level of regions. In particular we find that investment stock of firms is power
law distributed with an exponent very close to the one found for firm activity.
On the other hand investment stock and activity of regions turn out to be
log-normal distributed. At both levels we find scaling laws relating investment
to activity and connectivity. In particular, we find that investment stock
scales with connectivity in a similar way as has been previously found for
stock market data, calling for further investigations on a possible general
scaling law holding true in economical networks.","['Stefano Battiston', 'João F. Rodrigues', 'Hamza Zeytinoglu']",[],0,arXiv,http://arxiv.org/abs/physics/0508206v1,False,True,False,False,False,361,Yukako Ono,Chicago,Completed,2006,2011.0,"The purpose of this proposal is to examine the quality of U.S. Census Bureau data on auxiliary offices and their firms and to perform an analysis of the firm determinants of nonresponse in Auxiliary Establishment (AE) datasets. We will investigate the determinants of a firm’s decision to establish a separate headquarters facility, where to locate it, and the determinants of out-sourcing behavior by headquarters.   We will use AE data, which provide information about the administrative establishments of firms. Since few researchers have used these data, assessing the quality of the data is an important part of the project.  First, we will link the AE data to Compustat® and to the Business Register in order to recover the main headquarters of the firm. Second, the characteristics of firms, as well as of local input service suppliers, will be constructed.  Finally, the purchased service expenditures information in the AE and other datasets will be used to investigate outsourcing behavior.   The project will benefit Census Bureau programs by exploring identification of the function performed by central administrative office (CAO) auxiliary establishments in the AE survey, both from the perspective of differentiating headquarters offices from other administrative units, as well as through an exploration of nonresponse to the function related questions on the survey. The project will explain which firms tend to have central administrative offices, where they locate them, and how their outsourcing behavior can be characterized."
"A Statistical Field Perspective on Capital Allocation and Accumulation:
  Individual dynamics","We have shown, in a series of articles, that a classical description of a
large number of economic agents can be replaced by a statistical fields
formalism. To better understand the accumulation and allocation of capital
among different sectors, the present paper applies this statistical fields
description to a large number of heterogeneous agents divided into two groups.
The first group is composed of a large number of firms in different sectors
that collectively own the entire physical capital. The second group, investors,
holds the entire financial capital and allocates it between firms across
sectors according to investment preferences, expected returns, and stock prices
variations on financial markets. In return, firms pay dividends to their
investors. Financial capital is thus a function of dividends and stock
valuations, whereas physical capital is a function of the total capital
allocated by the financial sector. Whereas our previous work focused on the
background fields that describe potential long-term equilibria, here we compute
the transition functions of individual agents and study their probabilistic
dynamics in the background field, as a function of their initial state. We show
that capital accumulation depends on various factors. The probability
associated with each firm's trajectories is the result of several contradictory
effects: the firm tends to shift towards sectors with the greatest long-term
return, but must take into account the impact of its shift on its
attractiveness for investors throughout its trajectory. Since this trajectory
depends largely on the average capital of transition sectors, a firm's
attractiveness during its relocation depends on the relative level of capital
in those sectors. Thus, an under-capitalized firm reaching a high-capital
sector will experience a loss of attractiveness, and subsequently, in
investors. Moreover, the firm must also consider the effects of competition in
the intermediate sectors. An under-capitalized firm will tend to be ousted out
towards sectors with lower average capital, while an over-capitalized firm will
tend to shift towards higher averagecapital sectors. For investors, capital
allocation depends on their short and long-term returns. These returns are not
independent: in the short-term, returns are composed of both the firm's
dividends and the increase in its stock prices. In the long-term, returns are
based on the firm's growth expectations, but also, indirectly, on expectations
of higher stock prices. Investors' capital allocation directly depends on the
volatility of stock prices and {\ldots}rms'dividends. Investors will tend to
reallocate their capital to maximize their short and long-term returns. The
higher their level of capital, the stronger the reallocation will be.","['Pierre Gosselin', 'Aïleen Lotz']",[],0,arXiv,http://arxiv.org/abs/2401.06142v1,False,True,False,False,False,369,Andrea L Eisfeldt,Chicago,Completed,2003,2008.0,"This project will study capital reallocation and the liquidity of real assets. Using COMPUSTAT data, we have shown that the amount of capital reallocation is procyclical. In contrast the benefits to capital reallocation appear countercyclical. We find that a counter cyclical process for capital illiquidity can reconcile these series and discuss possible sources of this illiquidity. Our project will build on and extend our previous work. We propose to study the cyclical properties of capital reallocation and capital illiquidity, understand the nature of capital illiquidity, understand the role of used capital markets in firm dynamics, and construct stylized facts describing firm heterogeneity including the cyclical properties of dispersion in productivity, capacity utilization and investment opportunities across firms. This research project on measuring capital reallocation has as its predominant purpose (I) to further the understanding and improve the quality of the data, (ii) to improve the methodology for measurement, and (iii) to prepare estimates of characteristics of the population. By measuring capital at the establishment or firm level and the reallocation of capital (i.e., sale or purchases of capital) across establishments or firms and by comparing the findings from the non-public data to our findings using COMPUSTAT data, the project will further the understanding of the quality of the data and improve the quality of the data. In addition, by documenting what can be learned from the existing data from the ASM/CM and ACES about the reallocation of capital, the project should lead to new or improved methodology to collect data to measure capital and its redeployment in different use. The benefits should compare to the productive interaction between the measurement of labor reallocation by Davis, Haltiwanger and coauthors and the data collection by the Bureau of the Census. The project will result in estimates of population and characteristics of population such as an aggregate capital reallocation series and documentation of its cyclical properties. Furthermore, the project will prepare estimates of dispersion measures of productivity across firms as well as the dynamics of heterogeneity across firms, an important feature of the microdata which has not been sufficiently studied."
The Future of Work and Capital: Analyzing AGI in a CES Production Model,"The integration of Artificial General Intelligence (AGI) into economic
production represents a transformative shift with profound implications for
labor markets, income distribution, and technological growth. This study
extends the Constant Elasticity of Substitution (CES) production function to
incorporate AGI-driven labor and capital alongside traditional inputs,
providing a comprehensive framework for analyzing AGI's economic impact.
  Four key models emerge from this framework. First, we examine the
substitution and complementarity between AGI labor and human labor, identifying
conditions under which AGI augments or displaces human workers. Second, we
analyze how AGI capital accumulation influences wage structures and income
distribution, highlighting potential disruptions to labor-based earnings.
Third, we explore long-run equilibrium dynamics, demonstrating how an economy
dominated by AGI capital may lead to the collapse of human wages and
necessitate redistributive mechanisms. Finally, we assess the impact of AGI on
total factor productivity, showing that technological growth depends on whether
AGI serves as a complement to or a substitute for human labor.
  Our findings underscore the urgent need for policy interventions to ensure
economic stability and equitable wealth distribution in an AGI-driven economy.
Without appropriate regulatory measures, rising inequality and weakened
aggregate demand could lead to economic stagnation despite technological
advancements. Moreover this research suggests a renegoation of the Social
Contract.",['Pascal Stiefenhofer'],[],0,arXiv,http://arxiv.org/abs/2502.07044v1,False,True,False,False,False,378,Mine Z Senses,Washington,Completed,2004,2007.0,"This study intends to examine the effects of increased economic integration on labor demand elasticities of skilled and unskilled labor, focusing on the effects of international trade, specifically outsourcing, as well as increased capital flows. Rodrik (1997) suggests that increased possibility of substituting domestic labor with its foreign counterparts through outsourcing and foreign direct investment should make labor demand more elastic. Greater product markets competition is also likely to flatten the labor demand curve. More intense competition in the final goods market is observed due to decline in trade protection and entry of less developed nations into production in manufacturing sector as a result of increased transmission of technology worldwide. The validity of the Rodrik hypothesis will be tested for the US manufacturing sector using plant-level data from the Census Bureau’s Annual Survey of Manufactures and the Census of Manufacturers for 1972 to 2001. This project will increase the Census Bureau’s knowledge base through the estimation of labor demand elasticities and the impact of economic integration upon these estimated elasticities over the 1972-2001 period for establishments in the Annual Survey of Manufactures and the Census of Manufacturers. A second benefit of this project will be understanding and/or improving the quality of data produced, by attempting to identify possible directions and magnitudes of the biases in the estimated coefficients resulting from the inclusion of imputed variables. I also intend to compile a summary of information regarding imputations throughout the Annual Survey of Manufactures and the Census of Manufacturers in order to assist further research using this dataset. Final benefit of this study to the Census Bureau will be enhancing the data collected, by improving imputations for non-response. I will identify possible problems with the imputation techniques used regarding the usage of prior survey information on payroll and employment to predict currently unavailable data, in conjunction with current year BLS aggregates."
"The Coevolution of Banks and Corporate Securities Markets: The Financing
  of Belgium's Industrial Take-Off in the 1830s","Recent developments in the literature on financial architecture suggest that
banks and markets not only coexist, but also coevolve in ways that are
non-neutral from the viewpoint of optimality. This article aims to analyse the
concrete mechanisms of this coevolution by focusing on a very relevant case
study: Belgium (the first Continental country to industrialize) at the time of
the very first emergence of a modern financial system (the 1830s). The article
shows that intermediaries played a crucial role in developing secondary
securities markets (as banks acted as securitizers), but market conditions also
had a strong feedback on banks' balance sheets and activities (as banks also
acted as market-makers for the securities they had issued). The findings
suggest that not only structural, but also cyclical factors can be important
determinants of changes in financial architecture.",['Stefano Ugolini'],[],0,arXiv,http://arxiv.org/abs/1906.11023v1,False,True,False,False,False,381,Nicola Cetorelli,Baruch,Completed,2006,2010.0,"This project explores the effect of banking market structure on the market structure and growth of nonfinancial industries. It asks whether concentration in the banking market promotes the formation of industries constituted by a few, large firms, or, rather, whether it facilitates the continuous entry of new firms, thus maintaining unconcentrated market structures across industries. Theoretical arguments could be made to support either hypothetical scenario. Further, it looks at the impact of banking market structure on employment growth, new firm entry, and establishment exit rates. Empirical evidence will be derived merging the panel information contained in the U.S. Census Bureau’s Longitudinal Business Database (LBD) with that on the banking industry contained in the publicly available Commercial Bank Report on Condition and Income of the Federal Reserve System. This project will evaluate the quality of the Finance, Insurance, and Real Estate census, and LBD data in two ways: 1) by assessing missing items, imputations, and inequalities; and 2) by comparing it to the Commercial Bank and Holding Company Database compiled by the Federal Reserve."
Covert Cycle Stealing in a Single FIFO Server,"Consider a setting where Willie generates a Poisson stream of jobs and routes
them to a single server that follows the first-in first-out discipline. Suppose
there is an adversary Alice, who desires to receive service without being
detected. We ask the question: what is the number of jobs that she can receive
covertly, i.e. without being detected by Willie? In the case where both Willie
and Alice jobs have exponential service times with respective rates $\mu_1$ and
$\mu_2$, we demonstrate a phase-transition when Alice adopts the strategy of
inserting a single job probabilistically when the server idles : over $n$ busy
periods, she can achieve a covert throughput, measured by the expected number
of jobs covertly inserted, of $\mathcal{O}(\sqrt{n})$ when $\mu_1 < 2\mu_2$,
$\mathcal{O}(\sqrt{n/\log n})$ when $\mu_1 = 2\mu_2$, and
$\mathcal{O}(n^{\mu_2/\mu_1})$ when $\mu_1 > 2\mu_2$. When both Willie and
Alice jobs have general service times we establish an upper bound for the
number of jobs Alice can execute covertly. This bound is related to the Fisher
information. More general insertion policies are also discussed.","['Bo Jiang', 'Philippe Nain', 'Don Towsley']",[],0,arXiv,http://arxiv.org/abs/2003.05135v3,False,True,False,False,False,382,John A Figura,Washington,Completed,2004,2006.0,"The project I am proposing will contribute in four ways to the understanding and improvement of Census data: first, it will develop filtering and flagging programs, which will identify unusual and potentially spurious observations for plants in the Annual Survey of Manufacturers (ASM). Second, it will also estimate or impute data for missing observations or observations deemed likely to be spurious. Third, because the project estimates population moments from relatively long panels of plants, it will further the understanding of the representativeness and usefulness of estimates constructed from longitudinal data. Finally, the project will further the understanding of the dynamic behavior of plants, in particular their demand for labor. In this regard, the project proposes to answer two sets of questions related to the behavior of plant level employment: (1) do plants time their restructuring activities to correspond with fluctuations in the business cycle and if so, which types of plants are more likely to do so, and (2) are there asymmetric movements in job flows at the plant level and do these plant-level asymmetric movements, if they exist, translate into asymmetric job flow movements at the aggregate level. Answering these questions requires the construction of panels of plants from the ASM, with panels grouped by relevant plant characteristics, such as size and industry. To prevent spurious data from contaminating results, I will filter out or flag unusual patterns in plants’ time series of characteristics as well as impute data that is either missing or appears spurious. To appropriately interpret my results, I will compare characteristics of plants from my longitudinal samples to the overall ASM, evaluating the representativeness, and hence the usefulness, of estimates constructed from longitudinal data. The data sets I will need are the ASM from 1972 to the most recent available and the Census of Manufacturers (CM) from 1967 to the most recent available. Considerable processing of this data has already been performed in the construction of data files for the Gross Flows project. Thus, I would also like to have access to many of the files constructed from the ASM and the CM in the Gross Flows project."
A Moment of Perfect Clarity I: The Parallel Census Technique,"We discuss the history and uses of the parallel census technique---an elegant
tool in the study of certain computational objects having polynomially bounded
census functions. A sequel will discuss advances (including Cai, Naik, and
Sivakumar [CNS95] and Glasser [Gla00]), some related to the parallel census
technique and some due to other approaches, in the complexity-class collapses
that follow if NP has sparse hard sets under reductions weaker than (full)
truth-table reductions.","['Christian Glasser', 'Lane A. Hemaspaandra']",[],0,arXiv,http://arxiv.org/abs/cs/0007025v1,False,True,False,False,False,390,Norman H Nie,Boston,Completed,2003,2003.0,"This project explains whether and how the Census 2000 mobilization efforts influenced public cooperation, particularly among minorities. Preliminary analysis using Knowledge Networks data finds mailback cooperation was negatively impacted by the privacy debate but positively effected by mobilization efforts.  We propose a similar analysis with the NORC?s Census 2000 Integrated Partnership and Marketing Program. NORC data includes a validated measure of Census behavior that is essential for our analysis, and not available in the Knowledge Network data. The findings from this study should help lead to improved data collection and a better understanding of the dynamics of Census 2000. "
2MASS Extended Sources in the Zone of Avoidance,"The Two Micron All Sky Survey (2MASS) is now underway and will provide a
complete census of galaxies as faint as 13.5 mag (3 mJy) at 2.2 microns for
most of the sky, and ~12.1 mag (10 mJy) for regions veiled by the Milky Way.
This census has already discovered nearby galaxies previously hidden behind our
Galaxy, and will allow delineation of large-scale structures in the
distribution of galaxies across the whole sky. Here we report the detection and
discovery of new extended sources from this survey for fields incorporating the
Galactic plane at longitudes between 40 and 70 deg. The area-normalized
detection rate is ~1-2 galaxies per deg^2 brighter than 12.1 mag (10 mJy),
roughly constant with Galactic latitude throughout the ""Zone of Avoidance"", of
which 85-95% are newly discovered sources. In conjunction with the deep HI
surveys, 2MASS will greatly increase the current census of galaxies hidden
behind the Milky Way.","['T. H. Jarrett', 'T. Chester', 'R. Cutri', 'S. Schneider', 'J. Rosenberg', 'J. P. Huchra', 'J. Mader']",[],0,arXiv,http://arxiv.org/abs/astro-ph/0005017v1,False,True,False,False,False,390,Norman H Nie,Boston,Completed,2003,2003.0,"This project explains whether and how the Census 2000 mobilization efforts influenced public cooperation, particularly among minorities. Preliminary analysis using Knowledge Networks data finds mailback cooperation was negatively impacted by the privacy debate but positively effected by mobilization efforts.  We propose a similar analysis with the NORC?s Census 2000 Integrated Partnership and Marketing Program. NORC data includes a validated measure of Census behavior that is essential for our analysis, and not available in the Knowledge Network data. The findings from this study should help lead to improved data collection and a better understanding of the dynamics of Census 2000. "
"Regional economic integration via detection of circular flow in
  international value-added network","Global value chains (GVCs) are formed through value-added trade, and some
regions promote economic integration by concluding regional trade agreements to
promote these chains. However, there is no way to quantitatively assess the
scope and extent of economic integration involving various sectors in multiple
countries. In this study, we used the World Input--Output Database to create a
cross-border sector-wise trade in value-added network (international
value-added network (IVAN)) covering the period of 2000--2014 and evaluated
them using network science methods. By applying Infomap to the IVAN, we
confirmed for the first time the existence of two regional communities: Europe
and the Pacific Rim. Helmholtz--Hodge decomposition was used to decompose the
value flows within the region into potential and circular flows, and the annual
evolution of the potential and circular relationships between countries and
sectors was clarified. The circular flow component of the decomposition was
used to define an economic integration index, and findings confirmed that the
degree of economic integration in Europe declined sharply after the economic
crisis in 2009 to a level lower than that in the Pacific Rim. The European
economic integration index recovered in 2011 but again fell below that of the
Pacific Rim in 2013. Moreover, sectoral analysis showed that the economic
integration index captured the effect of Russian mineral resources, free
movement of labor in Europe, and international division of labor in the Pacific
Rim, especially in GVCs for the manufacture of motor vehicles and high-tech
products.","['Sotaro Sada', 'Yuichi Ikeda']",[],0,arXiv,http://arxiv.org/abs/2103.08179v1,False,True,False,False,False,392,Nigel Melville,Michigan,Completed,2006,2010.0,"Manufacturers are increasingly using the Internet to electronically integrate their business processes internally and across their network of trading partners. However, due to a lack of available data sources, information about the adoption and economic impact of Internet business practices is limited. The proposed study seeks to address this shortcoming by analyzing a dataset containing 39 measures of computer net-work use within U.S. manufacturing plants collected within the Computer Network Use Supplement (CNUS) to the Annual Survey of Manufacturers (ASM).  Additional datasets employed provide data on plant characteristics and efficiency for a period of multiple years prior to and beyond the CNUS year (1999), including the Census of Manufactures and the Survey of Plant Capacity Utilization. The sample of establishments responding to the CNUS accounts for roughly 50 percent of all manufacturing employment and salaries and 95 per-cent of online manufacturing cost of materials, with substantial variation in response rate by geography and plant size. Given that there may be systematic differences between respondents and nonrespondents, three approaches will be used to examine nonresponse bias. First, known differences in the samples (ASM characteristics such as value of shipments and employees) will be examined. For example, if CNUS respondents tend to be from plants that are on average 10 percent larger, I can estimate the nonresponse bias by estimating the impact of a 10 percent change in plant size on CNUS variables. Second, I will compare measures from an independent survey with those of the CNUS survey. Third, I will compare the response rate to an ASM question on the value of e-shipments as a percentage of all shipments with measures collected within the CNUS, as well as the profile of responding plants to the ASM question versus the CNUS question. Regarding parameter estimates, innovation variables will be computed using the 39 dichotomous items available from the CNUS."
"Weathering the Storm: How Foreign Aid and Institutions Affect
  Entrepreneurship Following Natural Disasters","This study examines how foreign aid and institutions affect entrepreneurship
activity following natural disasters. We use insights from the
entrepreneurship, development, and institutions literature to develop a model
of entrepreneurship activity in the aftermath of natural disasters. First, we
hypothesize the effect of natural disasters on entrepreneurship activity
depends on the amount of foreign aid received. Second, we hypothesize that
natural disasters and foreign aid either encourages or discourages
entrepreneurship activity depending on two important institutional conditions:
the quality of government and economic freedom. The findings from our panel of
85 countries from 2006 to 2016 indicate that natural disasters are negatively
associated with entrepreneurship activity, but both foreign aid and economic
freedom attenuate this effect. In addition, we observe that foreign aid is
positively associated with entrepreneurship activity but only in countries with
high quality government. Hence, we conclude that the effect of natural
disasters on entrepreneurship depends crucially on the quality of government,
economic freedom, and foreign aid. Our findings provide new insights into how
natural disasters and foreign aid affect entrepreneurship and highlight the
important role of the institutional context.","['Christopher Boudreaux', 'Anand Jha', 'Monica Escaleras']",[],0,arXiv,http://arxiv.org/abs/2104.12008v1,False,True,False,False,False,396,Zulema Valdez,Michigan,Completed,2004,2006.0,"What is the effect of ethnicity on entrepreneurial activity? The neo-classical perspective suggests that human capital, defined as education and work experience, explains entrepreneurial outcomes regardless of ethnicity. The ethnic entrepreneurship approach argues that ethnicity provides social capital, such as reciprocal obligations, which facilitates enterprise. Drawing on economic sociology, this study combines the human capital and ethnic entrepreneurship approaches by introducing a framework derived from Polanyi (1944). Following Polanyi, this research argues that societies are constituted by three forms of market integration: market-exchange, reciprocity, and redistribution. Under capitalism, the market-exchange relationship is the dominant form of economic integration. Secondary relationships of reciprocity and redistribution, however, exist alongside the primary exchange relationship and provide compensatory support in the face of market uncertainty or disadvantage. Using the 1990 and 2000 US Census and the 1992 Characteristics of Business Owners, this study proposes to examine the effects of primary and secondary relationships of market integration on entrepreneurial participation and economic success. This research provides evidence that reciprocal relationships, which generate social capital, contribute to an explanation of entrepreneurial participation. Yet, findings reveal that market-exchange relationships, constituted in part by human capital or class background, better explain entrepreneurial economic success, regardless of ethnicity. The analysis of the 1990 and 2000 Census and the 1992 Characteristics of Business Owners Survey will benefit the US Census Bureau in two ways. First, this proposed analysis will construct, verify, and improve the sampling frame for future Census and CBO (SBO) data collection. Second, this proposed analysis would provide estimates of a variety of understudied ethnic and racial group populations and characteristics of these populations."
"Rock, Rap, or Reggaeton?: Assessing Mexican Immigrants' Cultural
  Assimilation Using Facebook Data","The degree to which Mexican immigrants in the U.S. are assimilating
culturally has been widely debated. To examine this question, we focus on
musical taste, a key symbolic resource that signals the social positions of
individuals. We adapt an assimilation metric from earlier work to analyze
self-reported musical interests among immigrants in Facebook. We use the
relative levels of interest in musical genres, where a similarity to the host
population in musical preferences is treated as evidence of cultural
assimilation. Contrary to skeptics of Mexican assimilation, we find significant
cultural convergence even among first-generation immigrants, which
problematizes their use as assimilative ""benchmarks"" in the literature.
Further, 2nd generation Mexican Americans show high cultural convergence
vis-\`a-vis both Anglos and African-Americans, with the exception of those who
speak Spanish. Rather than conforming to a single assimilation path, our
findings reveal how Mexican immigrants defy simple unilinear theoretical
expectations and illuminate their uniquely heterogeneous character.","['Ian Stewart', 'René Flores', 'Tim Riffe', 'Ingmar Weber', 'Emilio Zagheni']",[],0,arXiv,http://arxiv.org/abs/1902.09453v1,False,True,False,False,False,401,Anna Paulson,Chicago,Completed,2006,2011.0,"While most discussions of immigrant assimilation focus on labor and housing markets, immigrant participation in financial markets is a critical and largely unstudied dimension of economic assimilation.  The degree to which immigrants assimilate into the financial mainstream has profound implications for the U.S. economy. This project will provide new evidence on the extent of immigrant participation in financial markets and the key determinants of financial assimilation. The aspects of financial assimilation that will be studied include use of checking and savings accounts, participation in the stock market, and investment in risky vs. “safe” assets. The analysis will be based on data from the Survey of Income Program and Participation (SIPP). These data will be supplemented with characteristics of the countries of origin and with data on the destination communities. The predominant purpose of this project is to benefit the U.S. Census Bureau’s program, and it will do so in at least three ways. First, this study will com-pare SIPP data on immigrant year of arrival and country of origin to comparable data from the former Immigration and Naturalization Service. Second, this study will analyze the factors that account for the higher attrition rate of immigrants relative to the native born in the SIPP panel. Finally, the project will produce population estimates of the pace of immigrant financial assimilation."
"Intergenerational transmission of culture among immigrants: Gender gap
  in education among first and second generations","This paper illustrates the intergenerational transmission of the gender gap
in education among first and second-generation immigrants. Using the Current
Population Survey (1994-2018), we find that the difference in female-male
education persists from the home country to the new environment. A one standard
deviation increase of the ancestral country female-male difference in schooling
is associated with 17.2% and 2.5% of a standard deviation increase in the
gender gap among first and second generations, respectively. Since gender
perspective in education uncovers a new channel for cultural transmission among
families, we interpret the findings as evidence of cultural persistence among
first generations and partial cultural assimilation of second generations.
Moreover, Disaggregation into country-groups reveals different paths for this
transmission: descendants of immigrants of lower-income countries show fewer
attachments to the gender opinions of their home country. Average local
education of natives can facilitate the acculturation process. Immigrants
residing in states with higher education reveal a lower tendency to follow
their home country attitudes regarding the gender gap.","['Hamid NoghaniBehambari', 'Nahid Tavassoli', 'Farzaneh Noghani']",[],0,arXiv,http://arxiv.org/abs/2101.05364v1,False,True,False,False,False,401,Anna Paulson,Chicago,Completed,2006,2011.0,"While most discussions of immigrant assimilation focus on labor and housing markets, immigrant participation in financial markets is a critical and largely unstudied dimension of economic assimilation.  The degree to which immigrants assimilate into the financial mainstream has profound implications for the U.S. economy. This project will provide new evidence on the extent of immigrant participation in financial markets and the key determinants of financial assimilation. The aspects of financial assimilation that will be studied include use of checking and savings accounts, participation in the stock market, and investment in risky vs. “safe” assets. The analysis will be based on data from the Survey of Income Program and Participation (SIPP). These data will be supplemented with characteristics of the countries of origin and with data on the destination communities. The predominant purpose of this project is to benefit the U.S. Census Bureau’s program, and it will do so in at least three ways. First, this study will com-pare SIPP data on immigrant year of arrival and country of origin to comparable data from the former Immigration and Naturalization Service. Second, this study will analyze the factors that account for the higher attrition rate of immigrants relative to the native born in the SIPP panel. Finally, the project will produce population estimates of the pace of immigrant financial assimilation."
"Machine Learning, Deep Learning, and Hedonic Methods for Real Estate
  Price Prediction","In recent years several complaints about racial discrimination in appraising
home values have been accumulating. For several decades, to estimate the sale
price of the residential properties, appraisers have been walking through the
properties, observing the property, collecting data, and making use of the
hedonic pricing models. However, this method bears some costs and by nature is
subjective and biased. To minimize human involvement and the biases in the real
estate appraisals and boost the accuracy of the real estate market price
prediction models, in this research we design data-efficient learning machines
capable of learning and extracting the relation or patterns between the inputs
(features for the house) and output (value of the houses). We compare the
performance of some machine learning and deep learning algorithms, specifically
artificial neural networks, random forest, and k nearest neighbor approaches to
that of hedonic method on house price prediction in the city of Boulder,
Colorado. Even though this study has been done over the houses in the city of
Boulder it can be generalized to the housing market in any cities. The results
indicate non-linear association between the dwelling features and dwelling
prices. In light of these findings, this study demonstrates that random forest
and artificial neural networks algorithms can be better alternatives over the
hedonic regression analysis for prediction of the house prices in the city of
Boulder, Colorado.",['Mahdieh Yazdani'],[],0,arXiv,http://arxiv.org/abs/2110.07151v1,False,True,False,False,False,406,Bhashkar Mazumder,Chicago,Completed,2007,2012.0,"The proposed research program will develop and implement methods to estimate hedonic price supply and demand models applied to two important classes of empirical economic issues where hedonic models are applicable, real estate and labor markets. Since these models include attributes that are highly location specific, this project will also develop and implement methods to link the micro-observations of Census Bureau datasets to micro-observations of other Census Bureau datasets and to external datasets. This linkage will be based on locations of the micro-observations, i.e., their physical geospatial proximity with each other, and will be performed using Geographic Information Systems (GIS). These two activities, development of hedonic prices and geospatial linking, form the principal benefits to the Census Bureau. In addition, through the application of these models to specific topics of interest, we will also generate bene-fits to specific Census Bureau surveys, such as the American Housing Survey and other surveys used through the course of the project. In order to apply hedonic models to study location issues, datasets containing highly detailed geographic information and robust methods for establishing the geospatial relationships are required. The project will use GIS modeling tools to create the necessary statistical measures of collocation that will enable us to examine specific topics using the hedonic approach. In addition, part of the proposed research will develop new methodological approaches that address some of the theoretical and empirical shortcomings with the classical hedonic model. The topics that will be studied in this project include: an analysis of residential real estate markets; an analysis of school quality, education, location, and neighborhood effects; commercial real estate markets and community economic development; and a hedonic analysis of labor markets.  This research program is focused not only on developing the data sources and tools needed to apply the hedonic approach to these questions but also on testing our progress with a series of interrelated topical studies that focus on some of these aspects. While the longer term goal is a more integrated assessment of the hedonic values across all of these factors, we begin with examining several more “manageable” sized research topics in this general area by incrementally developing the data and tools needed to measure the aforementioned community factors and estimate the hedonic prices associated with them. In doing so we also expect to address some important social science research questions with better data and better methodology."
"Designing User-Centered Simulations of Leadership Situations for Cave
  Automatic Virtual Environments: Development and Usability Study","Given that experience is a pivotal dimension of learning processes in the
field of leadership, the ongoing and unresolved issue is how such experiential
moments could be provided when developing leadership skills and competencies.
Role-plays and business simulations are widely used in this context as they are
said to teach relevant social leadership skills, like those required by
everyday communication to followers, by decision-making on compensation,
evaluating performance, dealing with conflicts, or terminating contracts.
However, the effectiveness of simulations can highly vary depending on the
counterpart's ability to act in the given scenarios. In our project, we deal
with how immersive media could create experiential learning based on
simulations for leadership development. In recent years different variations of
extended reality got significant technological improvements. Head-mounted
displays are an easy and cost-efficient way to present high-resolution virtual
environments. For groups of people that are part of an immersive experience,
cave automatic virtual environments offer an excellent trade-off between actual
exchange with other humans and interaction with virtual content simultaneously.
The work presented is based on developing a user-centered simulation of
leadership situations for cave automatic virtual environments and includes the
results of a first usability study. In the future, the presented results can
help to support the development and evaluation of simulated situations for cave
automatic virtual environments with an emphasis on leadership-related
scenarios.","['Francesco Vona', 'Miladin Ćeranić', 'Irma Rybnikova', 'Jan-Niklas Voigt-Antons']",[],0,arXiv,http://arxiv.org/abs/2403.10312v1,False,True,False,False,False,420,Kevin Caves,UCLA,Completed,2004,2005.0,"Information technology (IT) plays a key role in theoretical explanations of economic trends, which are the subject of some of the more important economic policy debates of our time. We propose to study the productive role of IT by matching data on computer use in the workplace from the Current Population Survey (CPS) with plant-level production data from the Longitudinal Research Database (LRD). We plan to employ a new econometric technique for estimating production functions, which allows us to test whether IT affects plants’ total factor productivity (TFP), in addition to allowing for tests of whether IT causes information to flow more efficiently through firms. An important objective of this study will be to prepare estimates of production function parameters and TFP for the manufacturing industries using a new technique, which is designed to produce more accurate estimates than older methods. As we explain in the Project Description and in the Predominant Purpose Statement, the accurate measurement of TFP is directly relevant to the Census Bureau’s stated mission. To study the role of IT in production, we plan to use information on IT usage from both the CPS and the LRD, two sources that are actually complementary to each other. These two surveys provide, among other things, conceptually distinct measures of IT adoption. The CES datasets required for estimation consist of the datasets that comprise the LRD: the Annual Survey of Manufactures and the Census of Manufactures. Because our techniques employ nonparametric econometrics, it will be vital to have as many observations as possible to ensure efficient production function parameter estimates. Therefore, we would request access to all years for which data is available for the LRD. (This would include the years 1963, 1967 and 1972-2001). Because we will often be making comparisons at the industry level to study the role of IT in the production process, we need to obtain data from as many industries as possible in order to make accurate inferences. Therefore, we would request access to data on all manufacturing industries. All additional data required for the project will be publicly available and would be supplied by the researchers. This includes data on the prices of inputs and outputs, which will be necessary to transform the nominal data from the LRD into real variables. For most years (1963-1996), data from the NBER-CES manufacturing productivity database will suffice. For subsequent years, it may be necessary to obtain the relevant price series directly from the Bureau of Labor Statistics and the Bureau of Economic Analysis. Note that our methodology does not require us to observe input prices at the plant level. The other dataset we will employ is the public use version of the Current Population Survey (CPS). Questions on computer use are included in the following years of the CPS: 1984, 1989, 1993, 1994, 1997, 1998, 2000, and 2001. Moreover, in several of the years listed above, the CPS asked respondents about the specific types of computer applications they used at work. We plan to make use of this data, in addition to CPS data on the industry classification and the relevant statistical weight of the respondents. We will match the CPS data with the LRD data using 3-digit industry codes."
The structure of graphs with a vital linkage of order 2,"A linkage of order k of a graph G is a subgraph with k components, each of
which is a path. A linkage is vital if it spans all vertices, and no other
linkage connects the same pairs of end vertices. We give a characterization of
the graphs with a vital linkage of order 2: they are certain minors of a family
of highly structured graphs.","['Dillon Mayhew', 'Geoff Whittle', 'Stefan H. M. van Zwam']",[],0,arXiv,http://arxiv.org/abs/1006.5485v2,False,True,False,False,False,422,Allan G Collard-Wexler,Chicago,Completed,2004,2007.0,"This project will examine supplier-customer relationships (also known as vertical linkages) and the impact of these relationships on investment decisions. The Census Bureau has explicitly stated a need for “specific recommendations regarding how to better capture and describe supply chain activities in the 2002 Economic Census and in our current economic statistics” (Mesenbourg 2001). This project will develop a methodology to capture and describe these supply chain activities using internal Census economic microdata (Census of Manufacturing, Annual Survey of Manufacturing, Census of Construction, Longitudinal Business Database, Survey of Plant Capacity Utilization) combined with Input-Output Tables of the American Economy compiled by the Bureau of Economic Analysis, and will provide this methodology to Census. In addition, knowledge gained from these analyses will be used to address several data quality issues in these data sets (i.e., studying patterns of non-response, developing new imputation methods, conducting consistency checks). The project will then use these relationship measures to examine firms’ decisions to adopt electronic commerce infrastructure based on their suppliers’ and customers’ actions as well as to examine capital investment decisions by ready-mix concrete plants based on downstream local construction activity. Hence, this project will not only benefit the Census Bureau by producing a methodology to capture and describe these supply chains but will also provide knowledge useful for addressing a multitude of data quality issues. Vertical linkages between firms play an enormous role in the functioning of the economy. A large fraction of output from plants across the country is not consumed by individuals but is utilized in other firms’ production processes. These linkages are key to explaining investment behavior -- if a firm’s customers are growing rapidly, it is apt to expand its operations to meet future demand. The two applications chosen for this project, enumerated above, have a guiding methodological principle: the use of information about the relationships between firms, either sectors that trade with each other in the case of e-commerce or concrete plants that sell neighboring construction projects, to understand why firms make capital investments. Since both applications of these projects study firm-level decisions, non-public Census data are required. This project will primarily use data from the Longitudinal Research Database (i.e., the longitudinally linked conjunction of the Census of Manufacturing and the ASM) but will also use data from the Census of Construction, the Longitudinal Business Database, and the Survey of Plant Capacity Utilization."
Job Posting-Enriched Knowledge Graph for Skills-based Matching,"The labor market is constantly evolving. Occupations are changing, being
added, or disappearing to fit the needs of today's market. In recent years the
pace of this change has accelerated, due to factors such as globalization,
digitization, and the shift to working from home. Different factors are
relevant when selecting employment, e.g., cultural fit, compensation, provided
degree of freedom. To successfully fulfill an occupation the gap between
required (by the job) and possessed (by the job seeker) skills needs to be as
small as possible. Decreasing this skill-gap improves the fit between a job
candidate and occupation. In this paper we propose a custom-built Skills &
Occupation Knowledge Graph (KG) that fits the above described dynamic nature of
the labor market, by leveraging existing skills and occupation taxonomies
enriched with external job posting data. We leverage this KG and explore
several applications for skills-based matching of jobs to job seekers. First,
we study link prediction as a means to quantify relevance of skills to
occupations, which can help in prioritizing learning and development of
employees. Next, we study node similarity methods and shortest path algorithms
for career pathfinding. Finally, we leverage a term weighting method for
identifying which skills are most ""distinctive"" for different (types of)
occupations.","['Maurits de Groot', 'Jelle Schutte', 'David Graus']",[],0,arXiv,http://arxiv.org/abs/2109.02554v1,False,True,False,False,False,429,Andrew K Hildreth,Berkeley,Completed,2003,2008.0,"The project will link individual records for California residents in the Displaced Worker Supplements (DWS) (conducted in February 1994, February 1996, February 1998, and February 2000), with the March Current Population Survey files for 1991-2000 and the California Base Wage records 1991-2000. The proposed research seeks to address a number of issues in understanding how and why an individual loses their job using matched employee data. From the outset, the DWS was designed to elicit responses on the displacement of workers. Displacement was defined as being laid-off (without recall), a plant closing, or the employer going out of business. This is separate from a workers wish to quit or leave a job for their own reasons. As well as providing evidence on the accuracy of the DWS in measuring the cost of job displacement, there will be substantial scientific contributions and benefits to the Census Bureau from the work. We will conduct an assessment of the accuracy and shortcomings of the DWS in measuring the displacement of workers, of compiling displacement statistics, and measuring the cost of job loss. In particular, the scientific and bureau benefits are the following. First, the work will assess the importance of missing information on workers in the DWS. In particular, the work history and the measurement of wages can both be learned from the UI Base Wage files and their importance assessed. By including these two items into the DWS file, the analysis will be able to assess directly the importance of the missing information on job history, and the problems of reporting a retrospective wage for the last job for the displaced workers. Both of these items will impact how the wage change from displacement is estimated from the DWS. Second, assess the representative quality of the measurement of displacement against other sources of information. Matching the DWS to the UI Base Wage files, a more complete investigation is possible on the measurement of displacement from the ‘plant closed down’ response in the DWS. The UI Base Wage files can determine when a plant closed down through a change in the number of workers at a particular employer. This is will directly assess how researchers view the representative quality of the DWS in its estimate of displacement figures. As part of the Benefit to the Bureau, the research will provide technical memorandum describing the data base development and the differences between displacement statistics, pre and post displacement wages, and the reason for job loss. The technical memorandum will also address the implications for the Census Bureau’s data collection program. In particular, the usefulness of questions on displacement, the accuracy of recalling past wages, and the importance of missing information on the job loss between main employment spells."
"Convergence-aware Clustered Federated Graph Learning Framework for
  Collaborative Inter-company Labor Market Forecasting","Labor market forecasting on talent demand and supply is essential for
business management and economic development. With accurate and timely
forecasts, employers can adapt their recruitment strategies to align with the
evolving labor market, and employees can have proactive career path planning
according to future demand and supply. However, previous studies ignore the
interconnection between demand-supply sequences among different companies and
positions for predicting variations. Moreover, companies are reluctant to share
their private human resource data for global labor market analysis due to
concerns over jeopardizing competitive advantage, security threats, and
potential ethical or legal violations. To this end, in this paper, we formulate
the Federated Labor Market Forecasting (FedLMF) problem and propose a
Meta-personalized Convergence-aware Clustered Federated Learning (MPCAC-FL)
framework to provide accurate and timely collaborative talent demand and supply
prediction in a privacy-preserving way. First, we design a graph-based
sequential model to capture the inherent correlation between demand and supply
sequences and company-position pairs. Second, we adopt meta-learning techniques
to learn effective initial model parameters that can be shared across
companies, allowing personalized models to be optimized for forecasting
company-specific demand and supply, even when companies have heterogeneous
data. Third, we devise a Convergence-aware Clustering algorithm to dynamically
divide companies into groups according to model similarity and apply federated
aggregation in each group. The heterogeneity can be alleviated for more stable
convergence and better performance. Extensive experiments demonstrate that
MPCAC-FL outperforms compared baselines on three real-world datasets and
achieves over 97% of the state-of-the-art model, i.e., DH-GEM, without exposing
private company data.","['Zhuoning Guo', 'Hao Liu', 'Le Zhang', 'Qi Zhang', 'Hengshu Zhu', 'Hui Xiong']",[],0,arXiv,http://arxiv.org/abs/2409.19545v1,False,True,False,False,False,429,Andrew K Hildreth,Berkeley,Completed,2003,2008.0,"The project will link individual records for California residents in the Displaced Worker Supplements (DWS) (conducted in February 1994, February 1996, February 1998, and February 2000), with the March Current Population Survey files for 1991-2000 and the California Base Wage records 1991-2000. The proposed research seeks to address a number of issues in understanding how and why an individual loses their job using matched employee data. From the outset, the DWS was designed to elicit responses on the displacement of workers. Displacement was defined as being laid-off (without recall), a plant closing, or the employer going out of business. This is separate from a workers wish to quit or leave a job for their own reasons. As well as providing evidence on the accuracy of the DWS in measuring the cost of job displacement, there will be substantial scientific contributions and benefits to the Census Bureau from the work. We will conduct an assessment of the accuracy and shortcomings of the DWS in measuring the displacement of workers, of compiling displacement statistics, and measuring the cost of job loss. In particular, the scientific and bureau benefits are the following. First, the work will assess the importance of missing information on workers in the DWS. In particular, the work history and the measurement of wages can both be learned from the UI Base Wage files and their importance assessed. By including these two items into the DWS file, the analysis will be able to assess directly the importance of the missing information on job history, and the problems of reporting a retrospective wage for the last job for the displaced workers. Both of these items will impact how the wage change from displacement is estimated from the DWS. Second, assess the representative quality of the measurement of displacement against other sources of information. Matching the DWS to the UI Base Wage files, a more complete investigation is possible on the measurement of displacement from the ‘plant closed down’ response in the DWS. The UI Base Wage files can determine when a plant closed down through a change in the number of workers at a particular employer. This is will directly assess how researchers view the representative quality of the DWS in its estimate of displacement figures. As part of the Benefit to the Bureau, the research will provide technical memorandum describing the data base development and the differences between displacement statistics, pre and post displacement wages, and the reason for job loss. The technical memorandum will also address the implications for the Census Bureau’s data collection program. In particular, the usefulness of questions on displacement, the accuracy of recalling past wages, and the importance of missing information on the job loss between main employment spells."
"Implications of Environmental Uncertainty for Business-IT Alignment: A
  Comparative Study of SMEs and Large Organizations","This paper presents a comprehensive study of the influence of environmental
uncertainty on business-IT alignment. The existing literature postulates
environmental uncertainty as a key challenge to achieving business-IT
alignment. Hence, the first objective of this study is to identify the extent
of the impact of environmental uncertainty on business-IT alignment, and to
determine its relative impact in the light of the other antecedents.
Furthermore, small and medium sized enterprises (SMEs) differ fundamentally
from large firms in many ways. Thus this paper also aims to investigate the
variation between SMEs and large firms with regard to the antecedents for
strategic alignment. Based on data collected from 212 firms, a conceptual model
is tested against the research objectives. The findings provide important
contributions to both research and practice by demonstrating the relative
impact of environmental uncertainty, and showing how the antecedents of
alignment vary between SMEs and large firms.","['Amitha Padukkage', 'Val Hooper', 'Janet Toland']",[],0,arXiv,http://arxiv.org/abs/1606.00744v1,False,True,False,False,False,434,Patrick J Kehoe,Chicago,Completed,2005,2010.0," 
This proposal will examine the relative behavior of small and large firms over the business cycle. A widely held view is that monetary policy fluctuations play a central role in the business cycle and that these fluctuations affect small firms disproportionately. We plan to ask whether the data support this view. Furthermore, some researchers have argued that markups of prices over costs fluctuate systematically with the business cycle and that these fluctuations are tied to the size of firms. We plan to document the relationship between the cyclical properties of markups and the size of firms. 
This project will use the Longitudinal Research Database, the Longitudinal Business Database, the Quarterly Financial Reports, and the Enterprise Summary Report (ES9100) to obtain establishment- and firm-level information about sales, employment, value added, inventories, capital expenditures, the cost of materials, and ownership. These data will be compiled into a panel dataset of establishments and of the firms to which these establishments belong. For larger firms, we plan to link these data to data from Compustat® on the financial conditions of the firms, as well as to monetary policy indicators and other business cycle indicators. These will be examined for fluctuations over time and with respect to the business cycle. 
The predominant purpose of this proposal is to inform the U.S. Census Bureau about differences in behavior of small and large firms in varying economic climates. Hence, the project will prepare estimates of the population characteristics regarding the differential sensitivity of small and large firms to business cycles. These analyses will not only further the understanding of the quality of Census Bureau data for small vs. large firms, but could also lead to improvements in the methodology for collecting, measuring, or tabulating data in Title 13, Chapter 5 surveys and censuses."
Why not now? Intended timing in entrepreneurial intentions,"Purpose: Understanding the formation of entrepreneurial intentions is
critical, given that it is the first step in the entrepreneurial process.
Although entrepreneurial intention has been extensively studied, little
attention has been paid on the intended timing of future entrepreneurial
projects. This paper analyses entrepreneurial intentions among final-year
university students after graduation in terms of the timeframe to start a
business. Potentially rapid entrepreneurs and entrepreneurs-in-waiting were
compared using the Theory of Planned Behaviour (TPB). Methodology: A
variance-based structural equation modelling approach was used for the sample
of 851 final-year university students with entrepreneurial intentions who
participated in GUESSS project. Findings: The results obtained contribute to
the understanding of how entrepreneurial intentions are formed, particularly,
how intended timing plays a moderating role in the relationships of the
variables of the theoretical model of TPB. This study provides empirical
evidence that significant differences exist between potential rapid
entrepreneurs and entrepreneurs-in-waiting. Practical implications: The
findings of this study have practical implications for entrepreneurship
education, and they can help policy makers develop more effective policies and
programs to promote entrepreneurship. Originality: Intention-based models have
traditionally examined the intent -- but not the timing -- of new venture
creation. However, the time elapsed between the formation of the
entrepreneurial intent and the identification of a business opportunity can
vary considerably. Therefore, analysing the moderating role of intended timing
could be relevant to entrepreneurial intention research.","['Antonio Rafael Ramos-Rodriguez', 'Jose Aurelio Medina-Garrido', 'Jose Ruiz-Navarro']",[],0,arXiv,http://arxiv.org/abs/2401.13682v1,False,True,False,False,False,447,Austan Goolsbee,Chicago,Completed,2004,2008.0,"The purpose of this proposal is to improve the accuracy and quality of the ongoing Survey of Business Owners by helping distinguish true entrepreneurial ventures from individuals with small amounts of supplemental income and by addressing some of the difficulties arising from follow-up surveys of small firms that have gone out of business. It will do this by studying the economic determinants of the survival and operation of legitimate entrepreneurial ventures. The project will use information from the 1992 Characteristics of Business Owners (CBO) survey on the health and operation of businesses in 1992 and 1994 further matched to the establishment lists (SSEL) and, where possible, the Longitudinal Business Database (LBD) and the future release of the Survey of Business Owners (SBO) to examine the survival and operation of entrepreneurs. It will look at two basic areas. The first is determining what economic factors influence the probability of survival for entrepreneurial firms. This will focus particularly on local credit conditions and local economic growth as well as on the influence of direct government support to entrepreneurs at the state and local level. The determinants of small firm survival are directly tied to the difficulties of follow-up activity required by the Census Bureau. The second is examining the impact of taxes (and some regulations) on the operation and offerings of the entrepreneurial firms. This will focus specifically on the subjects of how marginal income tax rates affect the hours that entrepreneurs choose to work on their businesses, how much income they report and the likelihood of hiring employees, as well as the role that they play on the likelihood that small businesses offer health insurance, pension plans, or medical leave to their workers, and to the probability that they operate the business out of the owner's home. Each of these subjects is correlated with a business being a true entrepreneurial venture (people with small amounts of supplemental income are unlikely to work long hours on the job, hire employees, offer health, pension or family leave, or move their businesses out of the home). The CBO, with its detailed micro level observations and ability to follow individuals across time even beyond the time frame of the survey by matching to the SSEL and the LBD, gives a unique opportunity to answer such questions. While the CBO is a sample of people with Schedule C income (as well as some partnerships), it avoids the typical problems of using tax return data by providing extensive information about the characteristics of the business owner, their background, and the like. This makes it the ideal basis for such a project."
"Roles of Retailers in the Peer-to-Peer Electricity Market: A Single
  Retailer Perspective","Despite extensive research in the past five years and several successfully
completed and on-going pilot projects, regulators are still reluctant to
implement peer-to-peer trading at a large-scale in today's electricity market.
The reason could partly be attributed to the perceived disadvantage of current
market participants like retailers due to their exclusion from market
participation - a fundamental property of decentralised peer-to-peer trading.
As a consequence, recently, there has been growing pressure from energy service
providers in favour of retailers' participation in peer-to-peer trading.
However, the role of retailers in the peer-to-peer market is yet to be
established as no existing study has challenged this fundamental circumspection
of decentralized trading. In this context, this perspective takes the first
step to discuss the feasibility of retailers' involvement in the peer-to-peer
market. In doing so, we identify key characteristics of retail-based and
peer-to-peer electricity markets and discuss our viewpoint on how to
incorporate a single retailer in a peer-to-peer market without compromising the
fundamental decision-making characteristics of both markets. Finally, we give
an example of a hypothetical business model to demonstrate how a retailer can
be a part of a peer-to-peer market with a promise of collective benefits for
the participants.","['Wayes Tushar', 'Chau Yuen', 'Tapan Saha', 'Deb Chattopadhyay', 'Sohrab Nizami', 'Sarmad Hanif', 'Jan E Alam', 'H. Vincent Poor']",[],0,arXiv,http://arxiv.org/abs/2110.09303v1,False,True,False,False,False,461,Shan N He,Boston,Completed,2006,2013.0,"While an initial public offering is probably the most heralded mechanism of going public, the most common and successful mechanism of going public is, however, through an acquisition of the private firm by an existing public company. Since going public allows the firm to access external financing through the equity market for the first time in its life, going public may have important implications for the firm’s product market performance as well. In this research project, we will analyze (for the first time in the literature) how the product market performance of a firm affects the timing of its going public decision. This analysis will inform the U.S. Census Bureau regarding the behavior of organizational change activity and its determinants, where the timing of changes in ownership informs business register processing activity.  We also analyze the consequences of a firm going public on various aspects of its subsequent product market performance. We propose to identify the sources of this poor performance by studying how a firm’s productivity, sales, market share, labor costs and employment levels, material costs, rental and administrative expenses, and capital expenditures change subsequent to going public. This analysis will provide important information on the way in which firms report the value of these measures as collected by Census Bureau programs."
"Monopoly Market with Externality: an Analysis with Statistical Physics
  and Agent Based Computational Economics","We explore the effects of social influence in a simple market model in which
a large number of agents face a binary choice: 'to buy/not to buy' a single
unit of a product at a price posted by a single seller (the monopoly case). We
consider the case of 'positive externalities': an agent is more willing to buy
if the other agents with whom he/she interacts make the same decision.
  We compare two special cases known in the economics literature as the
Thurstone and the McFadden approaches. We show that they correspond to modeling
the heterogenity in individual decision rules with, respectively, annealed and
quenched disorder. More precisely the first case leads to a standard Ising
model at finite temperature in a uniform external field, and the second case to
a random field Ising model (RFIM) at zero temperature.
  Considering the optimisation of profit by the seller within the McFadden/RFIM
model in the mean field limit, we exhibit a new first order phase transition:
if the social influence is strong enough, there is a regime where, if the mean
willingness to pay increases, or if the production costs decrease, the optimal
solution for the seller jumps from one with a high price and a small number of
buyers, to another one with a low price and a large number of buyers.","['Jean-Pierre Nadal', 'Denis Phan', 'Mirta B. Gordon', 'Jean Vannimenus']",[],0,arXiv,http://arxiv.org/abs/cond-mat/0311096v1,False,True,False,False,False,461,Shan N He,Boston,Completed,2006,2013.0,"While an initial public offering is probably the most heralded mechanism of going public, the most common and successful mechanism of going public is, however, through an acquisition of the private firm by an existing public company. Since going public allows the firm to access external financing through the equity market for the first time in its life, going public may have important implications for the firm’s product market performance as well. In this research project, we will analyze (for the first time in the literature) how the product market performance of a firm affects the timing of its going public decision. This analysis will inform the U.S. Census Bureau regarding the behavior of organizational change activity and its determinants, where the timing of changes in ownership informs business register processing activity.  We also analyze the consequences of a firm going public on various aspects of its subsequent product market performance. We propose to identify the sources of this poor performance by studying how a firm’s productivity, sales, market share, labor costs and employment levels, material costs, rental and administrative expenses, and capital expenditures change subsequent to going public. This analysis will provide important information on the way in which firms report the value of these measures as collected by Census Bureau programs."
"Defining urban and rural regions by multifractal spectrums of
  urbanization","The spatial pattern of urban-rural regional system is associated with the
dynamic process of urbanization. How to characterize the urban-rural terrain
using quantitative measurement is a difficult problem remaining to be solved.
This paper is devoted to defining urban and rural regions using ideas from
fractals. A basic postulate is that human geographical systems are of
self-similar patterns associated with recursive processes. Then multifractal
geometry can be employed to describe or define the urban and rural terrain with
the level of urbanization. A space-filling index of urban-rural region based on
the generalized correlation dimension is presented to reflect the degree of
geo-spatial utilization in terms of urbanization. The census data of America
and China are adopted to show how to make empirical analyses of urban-rural
multifractals. This work is not so much a positive analysis as a normative
study, but it proposes a new way of investigating urban and rural regional
systems using fractal theory.",['Yanguang Chen'],[],0,arXiv,http://arxiv.org/abs/1504.04224v1,False,True,False,False,False,463,E. Lance Howe,UCLA,Completed,2005,2010.0,"The purpose of the proposed research is to increase the utility of U.S. Census Bureau data especially as it relates to understanding mobility of Arctic indigenous peoples, with a particular focus on Inupiat and other Inuit people. Our research proposes using decennial census long form data (1990, 2000) at the UCLA Census Bureau Research Data Center. In addition, as a part of our analysis, we will use data from the Survey of Living Conditions in the Arctic (2003), the North Slope Borough Census (1988–2003), the Government of Nunavut (1999, 2001, 2004), and Statistics Canada Aboriginal Peoples Survey (1991, 2001) along with the public U.S. Decennial Census PUMS and Census Summary Files 1–4 (1980–2000). 
In the Proposed Benefits section we outline how our research will provide the following direct benefits to the Census Bureau: 
(1) understanding and improving the quality of data produced through a Title 13 census; (2) enhancing the data collected in a Title 13 census; (3) identifying shortcomings of current data, collection programs, and/or documenting new data collection needs; (4) preparing estimates of population and characteristics of population as authorized under Title 13. Briefly, our research proposes the following: 
First, we compare household migration and social characteristic variables from other surveys with Census Bureau data. Active temporary migration, in combination with high rates of nonresponse, have contributed to suspect Census Bureau place-level data for certain variables in rural Alaska. Because of the uniqueness of questions in other survey instruments (such as temporary migration), we can test for differences in data quality, identify possible under-counts or overcounts, and suggest methods for improved remote rural enumeration. Second, we will improve the quality of data by estimating the effect of age, sex, and race imputation among large rural Alaska households (age and sex information for large Alaska households was lost due to a data capture error). Third, we use fitted values from private instruments to estimate the effect of nonresponse imputation in Census Bureau data. Fourth, we link private data sources with Census Bureau data to create a dataset, stored with the CRDC, which includes all Inupiat households living in the United States and additional Arctic place-level characteristics. Finally, we provide estimates of migration patterns within and between the Arctic regions of Alaska and the Canadian North. 
The proposed research on migration specifically addresses migration of Arctic indigenous people between rural communities, larger regional centers, and urban areas. We have three primary research objectives: (1) improve the utility of census data in order to more precisely document the economic and social characteristics of Arctic indigenous peoples; (2) to refine and improve methods for analyzing migration decisions of individuals participating in mixed subsistence and cash economies in Arctic regions; 
(3) apply these improved methods to understand the particular migration behavior of the indigenous population in Arctic Alaska and Canada. 
We propose to address a number of questions about the causes and consequences of migration raised in previous studies. First, what are the roles of subsistence opportunities and community quality of life amenities in migration decisions? Second, how persistent and widespread are differences in migration patterns (such as gender differences)? Third, what can be said about the role of national policies regarding transfer income, education, and investment in community infrastructure on migration? Finally, what are the long-term consequences of migration decisions: is mobility on balance improving living conditions in Arctic communities, especially the poorest places, or is it draining leadership to larger settlements and exacerbating inequalities? Our approach views migration into and out of Arctic communities as a potential indicator of relative well-being for residents and takes into account subsistence opportunities and quality of life factors, as well as income earning opportunities. Our three levels of analysis include: i.) documenting patterns and stylized facts, ii.) testing community and regional differences, and iii.) applying a household production model to estimate well being by place. The model directly integrates subsistence opportunities into the migration decision and the estimated equations predict how changes in communities affect well-being directly and indirectly through their effects on migration. Comparing the Inupiat regions in Alaska to the Nunavut Territory of Canada in all three levels of analysis, we develop a demographic profile of migrants and migration rates over time and test hypotheses on the effect of changes in well-being on household migration decisions."
The Hawking-Unruh Temperature and Damping in a Linear Focusing Channel,"The Hawking-Unruh effective temperature, hbar a* / 2 pi c k, due to quantum
fluctuations in the radiation of an accelerated charged-particle beam can be
used to show that transverse oscillations of the beam in a practical linear
focusing channel damp to the quantum-mechanical limit. A comparison is made
between this behavior and that of beams in a wiggler.",['Kirk T. McDonald'],[],0,arXiv,http://arxiv.org/abs/physics/0003061v1,False,True,False,False,False,467,T Kirk K White,Triangle,Completed,2004,2008.0,"This research will seek to apply the insights of the literature on idiosyncratic shocks to individual labor productivity to the dynamics of plant-level total factor productivity.  Using the methodology of Olley and Pakes (1995) and Levinsohn and Petrin (1999), this research will estimate plant-level productivity over time and across many manufacturing industries.  The research will then go on to characterize the time series properties of plant-level idiosyncratic shocks to productivity, taking into account industry-level and economy-wide shocks."
"The Weizsacker-Williams Approximation to Trident Production in
  Electron-Photon Collisions","The appears to exist no detailed calculation of the multiphoton trident
process e + n omega_0 -> e' + e+e-, which can occur during the interaction of
an electron beam with an intense laser beam. We present a calculation in the
Weizsacker-Williams approximation that is in good agreement with QED
calculations for the weak-field case.","['C. Bula', 'K. T. McDonald']",[],0,arXiv,http://arxiv.org/abs/hep-ph/0004117v2,False,True,False,False,False,467,T Kirk K White,Triangle,Completed,2004,2008.0,"This research will seek to apply the insights of the literature on idiosyncratic shocks to individual labor productivity to the dynamics of plant-level total factor productivity.  Using the methodology of Olley and Pakes (1995) and Levinsohn and Petrin (1999), this research will estimate plant-level productivity over time and across many manufacturing industries.  The research will then go on to characterize the time series properties of plant-level idiosyncratic shocks to productivity, taking into account industry-level and economy-wide shocks."
"Analysis of Social Aspects of Migrant Labourers Living With HIV/AIDS
  Using Fuzzy Theory and Neutrosophic Cognitive Maps: With Special Reference to
  Rural Tamil Nadu in India","This book has seven chapters. The first chapter is introductory in nature and
it speaks about the migrant labourers. In chapter two we use Fuzzy Cognitive
Maps to analyze the socio-economic problems of HIV/AIDS infected migrant
labourers in rural areas of Tamil Nadu. In chapter three we analyze the role
played by the government helping these migrant labourers with HIV/AIDS and
factors of migration and their vulnerability in catching HIV/AIDS. For the
first time Neutrosophic Cognitive Maps are used in the study of migrant
labourers who have become HIV/AIDS victims. This study is done in Chapter IV.
In chapter V we use Neutrosophic Relational Maps and we define some new
neutrosophic tools like Combined Disjoint Block FRM, Combined Overlap NRM and
linked NRM. We adopt these new techniques in the study and analysis of this
problem. Chapter VI gives a very brief sketch of the life history of these 60
HIV/AIDS infected migrant labourers so that people from different social and
cultural backgrounds follow our analysis. The last chapter gives suggestions
and conclusions based on our study.","['W. B. Vasantha Kandasamy', 'Florentin Smarandache']",[],0,arXiv,http://arxiv.org/abs/math/0406304v1,False,True,False,False,False,482,Maritsa V Poros,Baruch,Completed,2008,2009.0,"This research is designed to evaluate and improve the quality of existing nativity questions on Census Bureau surveys. External data provide new information on the characteristics and patterns of migration that the Census Bureau’s migration data are not able to capture at present. The research design involves an analysis of qualitative data on the implications for producing intercensal demographic estimates of the population. The data consist of approximately 300 unstructured interviews with adult immigrants (aged 18 or over) who were born in any of 12 sending countries and who have lived in the United States for at least 3 months. The sending countries represent top source countries of recent migration (1995–2000) and/or very diverse types of migration ﬂows and experiences. The primary purpose of the project was to collect detailed data on what migrant ﬂows look like and to examine the demographic and other characteristics and experiences associated with diﬀerent types of ﬂows. Data include the socioeconomic background of migrants and their education, migration, work, and health histories. These data will be used to address the importance of social networks for international migration, occupational attainment, and residence (including internal migration and changes in household composition). These analyses will identify the limitations of and gaps in existing data that are currently used for intercensal demo-graphic estimates by providing the first systematic evaluation of migration questions since they were introduced. Second, they will provide a basis for proposing revisions to survey content, which can improve those estimates, and, in general, improve the quality of census survey data on the foreign born. Third, the results will address issues regarding the economic, political, and social impact of migrants on American society."
Snapshot Models of Undocumented Immigration,"The Mexican Migration Project (MMP) is a study that includes samples of
undocumented Mexican immigrants to the United States after their return to
Mexico. Of particular interest are the departure and return dates of a sampled
migrant's most recent sojourn in the United States, and the total number of
such journeys undertaken by that migrant household, for these data enable the
construction of data-driven undocumented immigration models. However, such data
are subject to an extreme physical bias, for to be included in such a sample, a
migrant must have returned to Mexico by the time of the survey, excluding those
undocumented immigrants still in the US. In our analysis, we account for this
bias by jointly modeling trip timing and duration to produce the likelihood of
observing the data in such a ""snapshot"" sample. Our analysis characterizes
undocumented migration flows including single visit migrants, repeat visitors,
and ""retirement"" from circular migration. Starting with 1987, we apply our
models to 30 annual random snapshot surveys of returned undocumented Mexican
migrants accounting for undocumented Mexican migration from 1980-2016. Contrary
to published estimates based on these same data, our results imply migrants
remain in the US much longer than previously estimated based on analysis that
ignored the physical snapshot bias. Scaling to population quantities, we
produce lower bounds on the total number of undocumented immigrants that are
much larger than conventional estimates based on US-based census-linked
surveys, and broadly consistent with the estimates reported by Fazel-Zarandi,
Feinstein and Kaplan (2018).","['Scott Rodilitz', 'Edward H. Kaplan']",[],0,arXiv,http://arxiv.org/abs/2002.06498v1,False,True,False,False,False,482,Maritsa V Poros,Baruch,Completed,2008,2009.0,"This research is designed to evaluate and improve the quality of existing nativity questions on Census Bureau surveys. External data provide new information on the characteristics and patterns of migration that the Census Bureau’s migration data are not able to capture at present. The research design involves an analysis of qualitative data on the implications for producing intercensal demographic estimates of the population. The data consist of approximately 300 unstructured interviews with adult immigrants (aged 18 or over) who were born in any of 12 sending countries and who have lived in the United States for at least 3 months. The sending countries represent top source countries of recent migration (1995–2000) and/or very diverse types of migration ﬂows and experiences. The primary purpose of the project was to collect detailed data on what migrant ﬂows look like and to examine the demographic and other characteristics and experiences associated with diﬀerent types of ﬂows. Data include the socioeconomic background of migrants and their education, migration, work, and health histories. These data will be used to address the importance of social networks for international migration, occupational attainment, and residence (including internal migration and changes in household composition). These analyses will identify the limitations of and gaps in existing data that are currently used for intercensal demo-graphic estimates by providing the first systematic evaluation of migration questions since they were introduced. Second, they will provide a basis for proposing revisions to survey content, which can improve those estimates, and, in general, improve the quality of census survey data on the foreign born. Third, the results will address issues regarding the economic, political, and social impact of migrants on American society."
"Partner support during pregnancy mediates social inequalities in
  maternal postpartum depression for non-migrant and first generation migrant
  women","Background An advantaged socioeconomic position (SEP) and satisfying social
support during pregnancy (SSP) have been found to be protective factors of
maternal postpartum depression (PDD). An advantaged SEP is also associated with
satisfying SSP, making SSP a potential mediator of social inequalities in PPD.
SEP, SSP and PPD are associated with migrant status. The aim of this study was
to quantify the mediating role of SSP in social inequalities in PPD regarding
mother's migrant status. Methods A sub-sample of 15,000 mothers from the French
nationally-representative ELFE cohort study was used for the present analyses.
SEP was constructed as a latent variable measured with educational attainment,
occupational grade, employment, financial difficulties and household income.
SSP was characterized as perceived support from partner (good relation,
satisfying support and paternal leave) and actual support from midwives
(psychosocial risk factors assessment and antenatal education). Mediation
analyses with multiple mediators, stratified by migrant status were conducted.
Results Study population included 76% of non-migrant women, 12% of second and
12% of first generation migrant. SEP was positively associated with support
from partner, regardless of migrant status. Satisfying partner support was
associated with a 8 (non-migrant women) to 11% (first generation migrant women)
reduction in PPD score. Limitations History of depression was not
reported.Conclusions Partner support could reduce social inequalities in PPD.
This work supports the need of interventions, longitudinal and qualitative
studies including fathers and adapted to women at risk of PPD to better
understand the role of SSP in social inequalities in PPD. Keywords social
support, postpartum depression, epidemiology, social inequalities, pregnancy,
mediation analysis","['Aurelie Nakamura', 'Fabienne El-Khoury', 'Anne-Laure Sutter-Dallay', 'Jeanna-Eve Franck', 'Xavier Thierry', 'Maria Melchior', 'Judith van der Waerden']",[],0,arXiv,http://arxiv.org/abs/2004.11244v1,False,True,False,False,False,482,Maritsa V Poros,Baruch,Completed,2008,2009.0,"This research is designed to evaluate and improve the quality of existing nativity questions on Census Bureau surveys. External data provide new information on the characteristics and patterns of migration that the Census Bureau’s migration data are not able to capture at present. The research design involves an analysis of qualitative data on the implications for producing intercensal demographic estimates of the population. The data consist of approximately 300 unstructured interviews with adult immigrants (aged 18 or over) who were born in any of 12 sending countries and who have lived in the United States for at least 3 months. The sending countries represent top source countries of recent migration (1995–2000) and/or very diverse types of migration ﬂows and experiences. The primary purpose of the project was to collect detailed data on what migrant ﬂows look like and to examine the demographic and other characteristics and experiences associated with diﬀerent types of ﬂows. Data include the socioeconomic background of migrants and their education, migration, work, and health histories. These data will be used to address the importance of social networks for international migration, occupational attainment, and residence (including internal migration and changes in household composition). These analyses will identify the limitations of and gaps in existing data that are currently used for intercensal demo-graphic estimates by providing the first systematic evaluation of migration questions since they were introduced. Second, they will provide a basis for proposing revisions to survey content, which can improve those estimates, and, in general, improve the quality of census survey data on the foreign born. Third, the results will address issues regarding the economic, political, and social impact of migrants on American society."
"Education Policy and Intergenerational Educational Persistence: Evidence
  from rural Benin","This paper employs a nonlinear difference-in-differences approach to
empirically examine the Maximally Maintained Inequality (MMI) hypothesis in
rural Benin. The findings of this study confirm the MMI hypothesis. In
particular, it is observed that when 76% of educated parents choose to educate
their daughters in the absence of educational programs, in contrast to only 37%
among non-educated parents, the average impact of tuition fee subsidy on
enrollment probability in primary schools stands at 3.8\% for non-educated
households and 0.27% for educated households. Conversely, in cases where only
27% of educated parents decide to educate their daughters without education
programs, the average effect of tuition fee waivers on enrollment probability
in primary schools increases to 19.64\% for non-educated households and 24\%
for educated households. From the analysis of household education decisions
influenced by a preference for education and budget constraints, three key
conclusions emerge to explain the mechanism behind the MMI. Firstly, when the
income advantage of educated households compared to non-educated households is
significantly high, irrespective of the level of their preference advantage,
reducing the financial cost of education induces a greater shift in education
decisions among non-educated households. Secondly, in situations where educated
households do not possess an income advantage relative to non-educated
households, the reduction in education-related financial costs leads to a more
pronounced change in education decisions among educated households. Lastly, for
the low-income advantage of educated households, as the income advantage of
educated households increases, non-educated households respond more to
education policy than educated parents, if the preference advantage of educated
households is relatively smaller.",['Christelle Zozoungbo'],[],0,arXiv,http://arxiv.org/abs/2401.17391v1,False,True,False,False,False,487,Stacey H Chen,Boston,Completed,2006,2011.0,"We propose to construct instrumental variables estimates of the effects of education and veteran status on average earnings, wage inequality, and a number of noneconomic outcomes. The empirical strategy relies on instrumental variables constructed from data on date and place of birth, derived from the 1990 and 2000 census long forms, as well as college proximity and college costs, derived from the National Longitudinal Survey Original Cohort geocode. Our project benefits the U.S. Census Bureau by using social security data to improve the imputation of a pre-1990 variable on highest grade completed from post-1990 categorical schooling variables and by establishing a procedure for matching the 1990 to 2000 censuses."
Will the last be the first? School closures and educational outcomes,"Governments have implemented school closures and online learning as one of
the main tools to reduce the spread of Covid-19. Despite the potential benefits
in terms of reduction of cases, the educational costs of these policies may be
dramatic. This work identifies the educational costs, expressed as decrease in
test scores, for the whole universe of Italian students attending the 5th, 8th
and 13th grade of the school cycle during the 2021/22 school year. The analysis
relies on a difference-in-difference model in relative time, where the control
group is the closest generation before the Covid-19 pandemic. The results
suggest a national average loss between 1.6-4.1% and 0.5-2.4% in Mathematics
and Italian test scores, respectively. After collecting the precise number of
days of school closures for the universe of students in Sicily, we estimate
that 30 additional days of closure decrease the test score by 1%. However, the
impact is much larger for students from high schools (1.8%) compared to
students from low and middle schools (0.5%). This is likely explained by the
lower relevance of parental inputs and higher reliance on peers inputs, within
the educational production function, for higher grades. Findings are also
heterogeneous across class size and parental job conditions, pointing towards
potential growing inequalities driven by the lack of in front teaching.","['Michele Battisti', 'Giuseppe Maggio']",[],0,arXiv,http://arxiv.org/abs/2208.11606v1,False,True,False,False,False,487,Stacey H Chen,Boston,Completed,2006,2011.0,"We propose to construct instrumental variables estimates of the effects of education and veteran status on average earnings, wage inequality, and a number of noneconomic outcomes. The empirical strategy relies on instrumental variables constructed from data on date and place of birth, derived from the 1990 and 2000 census long forms, as well as college proximity and college costs, derived from the National Longitudinal Survey Original Cohort geocode. Our project benefits the U.S. Census Bureau by using social security data to improve the imputation of a pre-1990 variable on highest grade completed from post-1990 categorical schooling variables and by establishing a procedure for matching the 1990 to 2000 censuses."
"Stochastic modeling of particle structures in spray fluidized bed
  agglomeration using methods from machine learning","Agglomeration is an industrially relevant process for the production of bulk
materials in which the product properties depend on the morphology of the
agglomerates, e.g., on the distribution of size and shape descriptors. Thus,
accurate characterization and control of agglomerate morphologies is essential
to ensure high and consistent product quality. This paper presents a pipeline
for image-based inline agglomerate characterization and prediction of their
time-dependent multivariate morphology distributions within a spray fluidized
bed process with transparent glass beads as primary particles. The framework
classifies observed objects in image data into three distinct morphological
classes--primary particles, chain-like agglomerates and raspberry-like
agglomerates--using various size and shape descriptors. To this end, a fast and
robust random forest classifier is trained. Additionally, the fraction of
primary particles belonging to each of these classes, either as individual
primary particles or as part of a larger structure in the form of chain-like or
raspberry-like agglomerates, is described using parametric regression
functions. Finally, the temporal evolution of bivariate size and shape
descriptor distributions of these classes is modeled using low-parametric
regression functions and Archimedean copulas. This approach improves the
understanding of agglomerate formation and allows the prediction of process
kinetics, facilitating precise control over class fractions and morphology
distributions.","['Lukas Fuchs', 'Sabrina Weber', 'Jialin Men', 'Niklas Eiermann', 'Orkun Furat', 'Andreas Bück', 'Volker Schmidt']",[],0,arXiv,http://arxiv.org/abs/2503.18882v1,False,True,False,False,False,490,Edward Feser,Triangle,Completed,2005,2008.0,"This study will produce estimates of manufacturing productivity of businesses in U.S. regions that differ according to their industrial structure and agglomeration characteristics. Specifically, it will compare the production efficiency and realized agglomeration economies of business establishments in regions dominated by a few large firms with establishments in regions with a broad mix of firms and sectors. The substantive results are expected to yield insights into the forces driving regional economic growth and adjustment. The project will aid the U.S. Census Bureau’s mission by producing new estimates of productivity that account for the role of regional corporate structure, by developing and documenting procedures for linking Census Bureau data to Dun and Bradstreet MarketPlace data, and by evaluating the consistency of Census Bureau data with Dun and Bradstreet data. The project will link the MarketPlace data to the Standard Statistical Establishment List (SSEL) and to the Longitudinal Research Database (LRD). In the process of linking Census Bureau data to MarketPlace data, the project will create a documented crosswalk file that can be used in the future to link establishments in the MarketPlace data to those in Census data. The project will use the linked data to compare data items (i.e., establishment name, establishment address) in the MarketPlace data to those in the SSEL. The project will also compare employment, sales, and ownership structure in the LRD to those items in the MarketPlace data to check the quality of data collected in the Annual Surveys and Censuses of Manufactures. "
"Production externalities and dispersion process in a multi-region
  economy","We consider an economic geography model with two inter-regional proximity
structures: one governing goods trade and the other governing production
externalities across regions. We investigate how the introduction of the latter
affects the timing of endogenous agglomeration and the spatial distribution of
workers across regions. As transportation costs decline, the economy undergoes
a progressive dispersion process. Mono-centric agglomeration emerges when
inter-regional trade and/or production externalities incur high transportation
costs, while uniform dispersion occurs when these costs become negligibly small
(i.e., when distance dies). In multi-regional geography, the network structure
of production externalities can determine the geographical distribution of
workers as economic integration increases. If production externalities are
governed solely by geographical distance, a mono-centric spatial distribution
emerges in the form of suburbanization. However, if geographically distant
pairs of regions are connected through tight production linkages, multi-centric
spatial distribution can be sustainable.","['Minoru Osawa', 'José M. Gaspar']",[],0,arXiv,http://arxiv.org/abs/2001.05095v2,False,True,False,False,False,490,Edward Feser,Triangle,Completed,2005,2008.0,"This study will produce estimates of manufacturing productivity of businesses in U.S. regions that differ according to their industrial structure and agglomeration characteristics. Specifically, it will compare the production efficiency and realized agglomeration economies of business establishments in regions dominated by a few large firms with establishments in regions with a broad mix of firms and sectors. The substantive results are expected to yield insights into the forces driving regional economic growth and adjustment. The project will aid the U.S. Census Bureau’s mission by producing new estimates of productivity that account for the role of regional corporate structure, by developing and documenting procedures for linking Census Bureau data to Dun and Bradstreet MarketPlace data, and by evaluating the consistency of Census Bureau data with Dun and Bradstreet data. The project will link the MarketPlace data to the Standard Statistical Establishment List (SSEL) and to the Longitudinal Research Database (LRD). In the process of linking Census Bureau data to MarketPlace data, the project will create a documented crosswalk file that can be used in the future to link establishments in the MarketPlace data to those in Census data. The project will use the linked data to compare data items (i.e., establishment name, establishment address) in the MarketPlace data to those in the SSEL. The project will also compare employment, sales, and ownership structure in the LRD to those items in the MarketPlace data to check the quality of data collected in the Annual Surveys and Censuses of Manufactures. "
"Warehouse Problem with Bounds, Fixed Costs and Complementarity
  Constraints","This paper studies an open question in the warehouse problem where a merchant
trading a commodity tries to find an optimal inventory-trading policy to decide
on purchase and sale quantities during a fixed time horizon in order to
maximize their total pay-off, making use of fluctuations in sale and cost
prices. We provide the first known polynomial-time algorithms for the case when
there are fixed costs for purchases and sales, optional complementarity
constraints that prohibit purchasing and selling during the same time period,
and bounds on purchase and sales quantities. We do so by providing an exact
characterization of the extreme points of the feasible region and using this to
construct a suitable network where a min-cost flow computation provides an
optimal solution. We are also able to provide polynomial extended linear
formulations for the original feasible regions. Our methods build on the work
by Wolsey and Yaman (Discrete Optimization 2018). We also consider the problem
without fixed costs and provide a fully polynomial time approximation scheme in
a setting with time-dependent bounds.","['Ishan Bansal', 'Oktay Günlük']",[],0,arXiv,http://arxiv.org/abs/2302.12136v1,False,True,False,False,False,491,Richard Chard,Washington,Completed,2006,2007.0,"This project will develop an improved method for using U.S. Census Bureau data to measure the economic impacts of the interregional flow of goods and services and by providing to the Census Bureau advice that will improve the methodologies used to collect information on the interregional flow of goods and services.  An additional purpose of this project is to analyze the shipment of manufactured goods among states and sub-state regions within the United States using Commodity Flows Survey (CFS) data. The analysis will model the patterns of trade in manufactured goods among states and BEA economic areas using widely accepted regional location theory.  Through this research, we plan to show how Census Bureau data could be better used to assess the economic impact of shocks by employing improved methods for using Census Bureau data and by suggesting changes to the collection methodologies used for the CFS. This will significantly benefit the Census Bureau through improved utility of its CFS.  The improved method employed for measuring the impact of economic shocks to localities relies on the estimation of regression-based Regional Purchase Coefficients (RPCs), based on Census Bureau microdata. We will calculate these RPCs using linked CFS, Annual Survey of Manufactures, and Census of Manufactures data. Ultimately, the RPCs will be used at BEA in two ways. First, an analysis of RPCs over time will shed light on how trade in intermediates has changed. Second, the RPCs will be used to estimate equations, which relate RPCs to characteristics of state economies."
Characteristics of Real Futures Trading Networks,"Futures trading is the core of futures business, and it is considered as one
of the typical complex systems. To investigate the complexity of futures
trading, we employ the analytical method of complex networks. First, we use
real trading records from the Shanghai Futures Exchange to construct futures
trading networks, in which nodes are trading participants, and two nodes have a
common edge if the two corresponding investors appear simultaneously in at
least one trading record as a purchaser and a seller respectively. Then, we
conduct a comprehensive statistical analysis on the constructed futures trading
networks. Empirical results show that the futures trading networks exhibit
features such as scale-free behavior with interesting odd-even-degree
divergence in low-degree regions, small-world effect, hierarchical
organization, power-law betweenness distribution, disassortative mixing, and
shrinkage of both the average path length and the diameter as network size
increases. To the best of our knowledge, this is the first work that uses real
data to study futures trading networks, and we argue that the research results
can shed light on the nature of real futures business.","['Junjie Wang', 'Shuigeng Zhou', 'Jihong Guan']",[],0,arXiv,http://arxiv.org/abs/1004.4402v2,False,True,False,False,False,491,Richard Chard,Washington,Completed,2006,2007.0,"This project will develop an improved method for using U.S. Census Bureau data to measure the economic impacts of the interregional flow of goods and services and by providing to the Census Bureau advice that will improve the methodologies used to collect information on the interregional flow of goods and services.  An additional purpose of this project is to analyze the shipment of manufactured goods among states and sub-state regions within the United States using Commodity Flows Survey (CFS) data. The analysis will model the patterns of trade in manufactured goods among states and BEA economic areas using widely accepted regional location theory.  Through this research, we plan to show how Census Bureau data could be better used to assess the economic impact of shocks by employing improved methods for using Census Bureau data and by suggesting changes to the collection methodologies used for the CFS. This will significantly benefit the Census Bureau through improved utility of its CFS.  The improved method employed for measuring the impact of economic shocks to localities relies on the estimation of regression-based Regional Purchase Coefficients (RPCs), based on Census Bureau microdata. We will calculate these RPCs using linked CFS, Annual Survey of Manufactures, and Census of Manufactures data. Ultimately, the RPCs will be used at BEA in two ways. First, an analysis of RPCs over time will shed light on how trade in intermediates has changed. Second, the RPCs will be used to estimate equations, which relate RPCs to characteristics of state economies."
"Social capital at venture capital firms and their financial performance:
  Evidence from China","This paper studies the extent to which social capital drives performance in
the Chinese venture capital market and explores the trend toward VC syndication
in China. First, we propose a hybrid model based on syndicated social networks
and the latent-variable model, which describes the social capital at venture
capital firms and builds relationships between social capital and performance
at VC firms. Then, we build three hypotheses about the relationships and test
the hypotheses using our proposed model. Some numerical simulations are given
to support the test results. Finally, we show that the correlations between
social capital and financial performance at venture capital firms are weak in
China and find that China's venture capital firms lack mature social capital
links.","['Qi-lin Cao', 'Hua-yun Xiang', 'You-jia Mao', 'Ben-zhang Yang']",[],0,arXiv,http://arxiv.org/abs/1810.02952v1,False,True,False,False,False,493,Rebecca E Zarutskie,Triangle,Completed,2005,2009.0," 
The primary purpose of this study is to enable the U.S. Census Bureau to better understand the role of venture capital (VC) financing in young firms and to improve the data collected in the Quarterly Financial Report (QFR) and the Survey of Business Owners (SBO) by linking these datasets to an external dataset on VC financing. The researchers will suggest ways to improve the collection of information on VC financing and note any inconsistencies in variables across the Census Bureau datasets and the external data. The researchers will be the first to link the external dataset on VC financing (called VentureXpert) with the QFR and the SBO. The researchers will therefore create a bridge file that can be accessed by future users of these data. Finally, the researchers intend to empirically investigate important economic questions relating to VC. 
Using the SSEL Name and Address files from 1975 to 1999, the researchers will link VentureXpert to the Longitudinal Business Database (LBD) using a STATA coded name matching algorithm. The researchers will link the QFR (1982, 1987, 1992, 1997), and the 1992 Characteristics of Business Owners Survey (Firms and Owners) to the merged LBD-VentureXpert dataset using EINs and CFNs. The researchers will also link to the merged LBD-VentureXpert dataset the 1977, 1982, 1987, 1992, and 1997 waves of the Census of Manufactures, Census of Services, Census of Retail Trade, Census of Wholesale Trade, and Census of Construction Industries, and the 1987, 1992, and 1997 waves of the Census of Transportation using EINs and CFNs. 
Using these data and additional data from SDC Platinum, Compustat®, CRSP, the FDIC Summary of Deposits and Call Reports, and BEA economic data, the researchers will estimate what are the determinants of receiving VC financing and what is the relationship between VC financing and a variety of firm outcomes, such as growth rates, survival rates, time to merger, and time to initial public offering. Additional analysis will be performed on the sub-sample of the LBD and VentureXpert, which can be matched to and the Annual Surveys of Manufactures (1975 to 2001). This project will improve the data collected in the QFR and the SBO, creating a VentureXpert-SSEL bridge file, and creating estimates of the fraction of firms receiving VC financing from 1975 to 1999 and the determinants and impact of this VC financing."
"The perverse incentive for insurance instruments that are derivatives:
  solving the jackpot problem with a clawback lien for default insurance notes","When an insurance note is also a derivative a serious problem arises because
a derivative must be fulfilled immediately. This feature of derivatives
prevents claims processing procedures that screen out ineligible claims. This,
in turn, creates a perverse incentive for insured holders of notes to commit
acts that result in payment. This problem first surfaced with CDS contracts,
which are part of a class of loan insurance I term a default insurance note.
  Without an address to this problem, within the average range of returns for a
large venture capital portfolio, a venture-bank makes less money the better
their investments do, in a continuous function. The highest rate of return is a
total loss, 64% more than a top portfolio.
  Here, a strategy for removing this perverse incentive is defined, consisting
of a clawback lien that returns part of the payment value as a lien on the firm
that is the beneficiary of the insurance. This is presented as the final major
component for implementing a default insurance note system so that
venture-banking can operate to maximum benefit. Removing the perverse incentive
also minimizes disincentive for underwriters to deny DIN coverage to new
venture capital firms, or to those firms that have historical earnings which
are below average.",['Brian P. Hanley'],[],0,arXiv,http://arxiv.org/abs/1711.02600v3,False,True,False,False,False,493,Rebecca E Zarutskie,Triangle,Completed,2005,2009.0," 
The primary purpose of this study is to enable the U.S. Census Bureau to better understand the role of venture capital (VC) financing in young firms and to improve the data collected in the Quarterly Financial Report (QFR) and the Survey of Business Owners (SBO) by linking these datasets to an external dataset on VC financing. The researchers will suggest ways to improve the collection of information on VC financing and note any inconsistencies in variables across the Census Bureau datasets and the external data. The researchers will be the first to link the external dataset on VC financing (called VentureXpert) with the QFR and the SBO. The researchers will therefore create a bridge file that can be accessed by future users of these data. Finally, the researchers intend to empirically investigate important economic questions relating to VC. 
Using the SSEL Name and Address files from 1975 to 1999, the researchers will link VentureXpert to the Longitudinal Business Database (LBD) using a STATA coded name matching algorithm. The researchers will link the QFR (1982, 1987, 1992, 1997), and the 1992 Characteristics of Business Owners Survey (Firms and Owners) to the merged LBD-VentureXpert dataset using EINs and CFNs. The researchers will also link to the merged LBD-VentureXpert dataset the 1977, 1982, 1987, 1992, and 1997 waves of the Census of Manufactures, Census of Services, Census of Retail Trade, Census of Wholesale Trade, and Census of Construction Industries, and the 1987, 1992, and 1997 waves of the Census of Transportation using EINs and CFNs. 
Using these data and additional data from SDC Platinum, Compustat®, CRSP, the FDIC Summary of Deposits and Call Reports, and BEA economic data, the researchers will estimate what are the determinants of receiving VC financing and what is the relationship between VC financing and a variety of firm outcomes, such as growth rates, survival rates, time to merger, and time to initial public offering. Additional analysis will be performed on the sub-sample of the LBD and VentureXpert, which can be matched to and the Annual Surveys of Manufactures (1975 to 2001). This project will improve the data collected in the QFR and the SBO, creating a VentureXpert-SSEL bridge file, and creating estimates of the fraction of firms receiving VC financing from 1975 to 1999 and the determinants and impact of this VC financing."
"Decarbonization patterns of residential building operations in China and
  India","As the two largest emerging emitters with the highest growth in operational
carbon from residential buildings, the historical emission patterns and
decarbonization efforts of China and India warrant further exploration. This
study aims to be the first to present a carbon intensity model considering
end-use performances, assessing the operational decarbonization progress of
residential building in India and China over the past two decades using the
improved decomposing structural decomposition approach. Results indicate (1)
the overall operational carbon intensity increased by 1.4% and 2.5% in China
and India, respectively, between 2000 and 2020. Household expenditure-related
energy intensity and emission factors were crucial in decarbonizing residential
buildings. (2) Building electrification played a significant role in
decarbonizing space cooling (-87.7 in China and -130.2 kilograms of carbon
dioxide (kgCO2) per household in India) and appliances (-169.7 in China and
-43.4 kgCO2 per household in India). (3) China and India collectively
decarbonized 1498.3 and 399.7 mega-tons of CO2 in residential building
operations, respectively. In terms of decarbonization intensity, India (164.8
kgCO2 per household) nearly caught up with China (182.5 kgCO2 per household) in
2020 and is expected to surpass China in the upcoming years, given the
country's robust annual growth rate of 7.3%. Overall, this study provides an
effective data-driven tool for investigating the building decarbonization
potential in China and India, and offers valuable insights for other emerging
economies seeking to decarbonize residential buildings in the forthcoming COP28
age.","['Ran Yan', 'Nan Zhou', 'Wei Feng', 'Minda Ma', 'Xiwang Xiang', 'Chao Mao']",[],0,arXiv,http://arxiv.org/abs/2306.13858v1,False,True,False,False,False,501,John Mark Ellis,UCLA,Completed,2005,2008.0,"The research has four goals that require use of the 1990 and 2000 long form data. First, we will test various definitions of mixed-race households using 2000 multiracial data with a view to maximizing compatibility for comparisons with 1990 single-race data. This testing will identify locations and scales where temporal comparisons are most sensitive to definitional issues in 2000. Second, we intend to map and analyze the neighborhood geographies of mixed-race households in 1990 and 2000. In light of concerns about disclosure risk for small populations in small areas, we are interested in developing procedures available for effective cartographic representations of mixed-race household geography that do not violate confidentiality protections. The third aim of the proposed research investigates the effect of increased rates of mixing within households on neighborhood segregation measures. The fourth aim of the proposed research centers on how racial identity is reported for the children of mixed-race couples. Specifically, to what extent does this choice reflect the particulars of household and/or neighborhood characteristics."
"High-resolution synthetic residential energy use profiles for the United
  States","Efficient energy consumption is crucial for achieving sustainable energy
goals in the era of climate change and grid modernization. Thus, it is vital to
understand how energy is consumed at finer resolutions such as household in
order to plan demand-response events or analyze the impacts of weather,
electricity prices, electric vehicles, solar, and occupancy schedules on energy
consumption. However, availability and access to detailed energy-use data,
which would enable detailed studies, has been rare. In this paper, we release a
unique, large-scale, synthetic, residential energy-use dataset for the
residential sector across the contiguous United States covering millions of
households. The data comprise of hourly energy use profiles for synthetic
households, disaggregated into Thermostatically Controlled Loads (TCL) and
appliance use. The underlying framework is constructed using a bottom-up
approach. Diverse open-source surveys and first principles models are used for
end-use modeling. Extensive validation of the synthetic dataset has been
conducted through comparisons with reported energy-use data. We present a
detailed, open, high-resolution, residential energy-use dataset for the United
States.","['Swapna Thorve', 'Young Yun Baek', 'Samarth Swarup', 'Henning Mortveit', 'Achla Marathe', 'Anil Vullikanti', 'Madhav Marathe']",[],0,arXiv,http://arxiv.org/abs/2210.08103v2,False,True,False,False,False,501,John Mark Ellis,UCLA,Completed,2005,2008.0,"The research has four goals that require use of the 1990 and 2000 long form data. First, we will test various definitions of mixed-race households using 2000 multiracial data with a view to maximizing compatibility for comparisons with 1990 single-race data. This testing will identify locations and scales where temporal comparisons are most sensitive to definitional issues in 2000. Second, we intend to map and analyze the neighborhood geographies of mixed-race households in 1990 and 2000. In light of concerns about disclosure risk for small populations in small areas, we are interested in developing procedures available for effective cartographic representations of mixed-race household geography that do not violate confidentiality protections. The third aim of the proposed research investigates the effect of increased rates of mixing within households on neighborhood segregation measures. The fourth aim of the proposed research centers on how racial identity is reported for the children of mixed-race couples. Specifically, to what extent does this choice reflect the particulars of household and/or neighborhood characteristics."
"Applying Data Synthesis for Longitudinal Business Data across Three
  Countries","Data on businesses collected by statistical agencies are challenging to
protect. Many businesses have unique characteristics, and distributions of
employment, sales, and profits are highly skewed. Attackers wishing to conduct
identification attacks often have access to much more information than for any
individual. As a consequence, most disclosure avoidance mechanisms fail to
strike an acceptable balance between usefulness and confidentiality protection.
Detailed aggregate statistics by geography or detailed industry classes are
rare, public-use microdata on businesses are virtually inexistant, and access
to confidential microdata can be burdensome. Synthetic microdata have been
proposed as a secure mechanism to publish microdata, as part of a broader
discussion of how to provide broader access to such data sets to researchers.
In this article, we document an experiment to create analytically valid
synthetic data, using the exact same model and methods previously employed for
the United States, for data from two different countries: Canada (LEAP) and
Germany (BHP). We assess utility and protection, and provide an assessment of
the feasibility of extending such an approach in a cost-effective way to other
data.","['M. Jahangir Alam', 'Benoit Dostie', 'Jörg Drechsler', 'Lars Vilhuber']",[],0,arXiv,http://arxiv.org/abs/2008.02246v1,False,True,True,False,False,503,Daniel A Bens,Chicago,Completed,2005,2008.0,"Our proposed study evaluates the degree of information aggregation selected by management in a firm’s published financial statements. This project will study the way financial data within a firm are grouped in the Compustat Database to the way they are grouped in the Longitudinal Research Database (LRD), the Longitudinal Business Database (LBD), and the Enterprise Summary Report (ES9100). Compustat is a database of information from firms’ published financial statements, whereas the LRD and the LBD contain establishment-level data reported to the U.S. Census Bureau by firms and collected from administrative records. The ES9100 provides aggregated firm information as reported by the firm to the Census Bureau. Firms have considerable discretion in how they aggregate business information when preparing their published financial statements. Thus, comparing these data to the more detailed establishment-level data will shed light on how discretion affects the aggregation of information by firms in general. Further, the project will assess the impact of this aggregation on industry classification. While many factors likely affect management’s aggregation decision, economic theory suggests two phenomena are particularly pertinent: proprietary costs and agency costs. Proprietary costs result from revealing proprietary information to competitors, suppliers, employees, customers, or other groups; our focus in this study will be on competitive proprietary costs. Shareholders incur agency costs when they delegate decision-making authority to agents (managers) whose interests are not fully aligned with those of the shareholders. Thus, investigating the impact of proprietary costs and agency costs on the aggregation of information in published financial statements is a central issue in our research proposal. "
The role of exceptional points in quantum systems,"In the present paper, first the mathematical basic properties of the
exceptional points are discussed. Then, their role in the description of real
physical quantum systems is considered. Most interesting value is the phase
rigidity of the eigenfunctions which varies between 1 (for distant
non-overlapping states) and 0 (at the exceptional point where the resonance
states completely overlap). This variation allows the system to incorporate
environmentally induced effects. In the very neighborhood of an exceptional
point, the system can be described well by a conventional nonlinear
Schr\""odinger equation. In the regime of overlapping resonances, a dynamical
phase transition takes place to which all states of the system contribute: a
few short-lived resonance states are aligned to the scattering states of the
environment by trapping the other states. The trapped resonance states show
chaotic features. Due to the alignment of a few states with the states of the
environment, observable values may be enhanced. The dynamical phase transition
allows us to understand some experimental results which remained puzzling in
the framework of conventional Hermitian quantum physics. The effects caused by
the exceptional points in physical systems allow us to manipulate them for many
different applications.",['Ingrid Rotter'],[],0,arXiv,http://arxiv.org/abs/1011.0645v1,False,True,False,False,False,505,Ingrid G Ellen,Baruch,Completed,2008,2011.0,"This project aims at improving understanding of neighborhood economic change by studying household residential choices and examining the circumstances under which households are willing to make moves into neighborhoods with incomes lower than their own. It will study whether and how these households diﬀer from house-holds making other types of residential choices and whether certain aspects of neighborhoods or their residents make such pioneering moves more attractive. The project will also investigate household exit decisions and examine whether mobility rates are higher in economically gaining neighborhoods. We will consider renters and low-income renters separately. The research will study other changes taking place in neighborhoods experiencing gains in income—such as whether housing costs increase and whether there are compensating changes in quality of the neighborhood, like increased satisfaction with neighbor-hood safety, schools, local transit, and availability of local businesses. The research will compare mobility rates and the prevalence of certain types of household turnover across different types of neighborhoods. A series of regression analyses will model pioneering moves, household exit, and various measures of household satisfaction by using the national and ﬁve metropolitan area versions of the American Housing Survey"
Implementation of Automata Theory to Improve the Learning Disability,"There are various types of disability egress in world like blindness,
deafness, and Physical disabilities. It is quite difficult to deal with people
with disability. Learning disability (LD) is types of disability totally
different from general disability. To deal children with learning disability is
difficult for both parents and teacher. As parent deal with only single child
so it bit easy. But teacher deals with different students at a time so its more
difficult to deal with group of students with learning disability. If there is
more students with learning disability so it is necessary that first all
identify the type of learning disability in group of students. Some students
have learning disability of mathematics; some have learning disability of other
subjects. By using theory of Automata it easy to analysis the level of
disability among all students then deal with them accordingly. For these
purpose deterministic automata is the best practice. Teacher deals with
deterministic students in class and check there response. In this research
deterministic automata is use to facilitated the teacher which help teacher in
identification of students with learning disability.","['Syed Asif Ali', 'Safeeullah Soomro', 'Abdul Ghafoor Memon', 'Abdul Baqi']",[],0,arXiv,http://arxiv.org/abs/1310.5474v1,False,True,False,False,False,509,Andrew J Houtenville,Cornell,Completed,2006,2009.0,"We propose to analyze restricted data from the Census 2000 Long Form and the 2000–2003 American Community Survey (ACS) to further the understanding of respondent/enumerator error in responses to the “employment disability” question in these surveys.   We propose to extend previous work to investigate the following questions: (1) What factors influence enumerator/respondent error in the employment disability question, and what groups are having difficulty with the employment disability question? (2) What is the impact of respondent/enumerator error on the estimates of employment dis-ability and overall disability; in other words, what would the Census 2000 statistics and 2000–2002 ACS statistics have looked like without respondent/enumerator error? Restricted data are needed because the Public Use Microdata Sample files do not contain enumerations information.   The benefits to the U.S. Census Bureau are an increased understanding of (a) the bene-fits (in terms of the reduction of respondent/enumerator error) of using the more advanced ACS enumeration process over the more costly Census 2000 enumeration process; (b) the types of individuals that had difficulty responding to the complex set of disability items; (c) the degree of respondent error that may still exist within the 2003 ACS; and (d) with this information, the ongoing process of developing and cognitively testing disability questions will be informed by helping refine the groups of individuals that should be targeted by cognitive testing. The ACS disability questions are in the process of being revised for the 2008 ACS."
"Offset-free model predictive control: stability under plant-model
  mismatch","We present the first general stability results for nonlinear offset-free
model predictive control (MPC). Despite over twenty years of active research,
the offset-free MPC literature has not shaken the assumption of closed-loop
stability for establishing offset-free performance. In this paper, we present a
nonlinear offset-free MPC design that is robustly stable with respect to the
tracking errors, and thus achieves offset-free performance, despite plant-model
mismatch and persistent disturbances. Key features and assumptions of this
design include quadratic costs, differentiability of the plant and model
functions, constraint backoffs at steady state, and a robustly stable state and
disturbance estimator. We first establish nominal stability and offset-free
performance. Then, robustness to state and disturbance estimate errors and
setpoint and disturbance changes is demonstrated. Finally, the results are
extended to sufficiently small plant-model mismatch. The results are
illustrated by numerical examples.","['Steven J. Kuntz', 'James B. Rawlings']",[],0,arXiv,http://arxiv.org/abs/2412.08104v1,False,True,False,False,False,514,John J Stevens,Washington,Completed,2006,2010.0,"This project expands the notion of within-industry heterogeneity in plant size beyond variation in productivity to include variation in function. The main idea is that small plants tend to do different things than large plants; in particular, they specialize in custom work or retail-like activity that is often efficiently undertaken in small plants. This project studies the relationship between plant size and plant function by 1) constructing measures of dispersion across product lines within an industry across size classes; 2) looking for evidence that small plants engage in more custom and retail-like activity; 3) looking at variation in market areas within narrowly defined industries; and 4) determining the extent to which changes in the distribution of manufacturing establishments at a location parallel changes in the retail sector. The proposed project will bene-fit the U.S. Census Bureau through the tabulation of new statistics on the population of manufacturing establishments. These statistics will contribute to a better understanding of the limitations of the industrial classifications used by the Census Bureau. The proposed analysis of industry definitions is of particular interest at this time because of the major shift from the Standard Industrial Classification (SIC) system to the North American Industry Classification System (NAICS) between the 1992 and 1997 Economic Censuses; this analysis will provide quantitative results on how the switch from SIC to NAICS affected the relationship between plant function and plant size within narrowly defined industries. In the longer run, the limitations of the industrial classification systems that we identify may aid in the design of future classification systems. The statistics tabulated in this project will also improve our understanding of the quality of the export data collected in the Census of Manufactures and Commodity Flow Survey (CFS); in particular, we will use the information in the CFS data to learn whether the well-known understatement of exports in the Census of Manufactures reflects a failure to correctly report export status or a failure to correctly report the value of exports."
"Distance Maps and Plant Development #1: Uniform Production and
  Proportional Destruction","Experimental data regarding auxin and venation formation exist at both
macroscopic and molecular scales, and we attempt to unify them into a
comprehensive model for venation formation. We begin with a set of principles
to guide an abstract model of venation formation, from which we show how
patterns in plant development are related to the representation of global
distance information locally as cellular-level signals. Venation formation, in
particular, is a function of distances between cells and their locations. The
first principle, that auxin is produced at a constant rate in all cells, leads
to a (Poisson) reaction-diffusion equation. Equilibrium solutions uniquely
codify information about distances, thereby providing cells with the signal to
begin differentiation from ground to vascular. A uniform destruction hypothesis
and scaling by cell size leads to a more biologically-relevant (Helmholtz)
model, and simulations demonstrate its capability to predict leaf and root
auxin distributions and venation patterns. The mathematical development is
centered on properties of the distance map, and provides a mechanism by which
global information about shape can be presented locally to individual cells.
The principles provide the foundation for an elaboration of these models in a
companion paper \cite{plos-paper2}, and together they provide a framework for
understanding organ- and plant-scale organization.","['Pavel Dimitrov', 'Steven W. Zucker']",[],0,arXiv,http://arxiv.org/abs/0905.4446v1,False,True,False,False,False,514,John J Stevens,Washington,Completed,2006,2010.0,"This project expands the notion of within-industry heterogeneity in plant size beyond variation in productivity to include variation in function. The main idea is that small plants tend to do different things than large plants; in particular, they specialize in custom work or retail-like activity that is often efficiently undertaken in small plants. This project studies the relationship between plant size and plant function by 1) constructing measures of dispersion across product lines within an industry across size classes; 2) looking for evidence that small plants engage in more custom and retail-like activity; 3) looking at variation in market areas within narrowly defined industries; and 4) determining the extent to which changes in the distribution of manufacturing establishments at a location parallel changes in the retail sector. The proposed project will bene-fit the U.S. Census Bureau through the tabulation of new statistics on the population of manufacturing establishments. These statistics will contribute to a better understanding of the limitations of the industrial classifications used by the Census Bureau. The proposed analysis of industry definitions is of particular interest at this time because of the major shift from the Standard Industrial Classification (SIC) system to the North American Industry Classification System (NAICS) between the 1992 and 1997 Economic Censuses; this analysis will provide quantitative results on how the switch from SIC to NAICS affected the relationship between plant function and plant size within narrowly defined industries. In the longer run, the limitations of the industrial classification systems that we identify may aid in the design of future classification systems. The statistics tabulated in this project will also improve our understanding of the quality of the export data collected in the Census of Manufactures and Commodity Flow Survey (CFS); in particular, we will use the information in the CFS data to learn whether the well-known understatement of exports in the Census of Manufactures reflects a failure to correctly report export status or a failure to correctly report the value of exports."
"Total Error and Variability Measures for the Quarterly Workforce
  Indicators and LEHD Origin-Destination Employment Statistics in OnTheMap","We report results from the first comprehensive total quality evaluation of
five major indicators in the U.S. Census Bureau's Longitudinal
Employer-Household Dynamics (LEHD) Program Quarterly Workforce Indicators
(QWI): total flow-employment, beginning-of-quarter employment, full-quarter
employment, average monthly earnings of full-quarter employees, and total
quarterly payroll. Beginning-of-quarter employment is also the main tabulation
variable in the LEHD Origin-Destination Employment Statistics (LODES) workplace
reports as displayed in OnTheMap (OTM), including OnTheMap for Emergency
Management. We account for errors due to coverage; record-level non-response;
edit and imputation of item missing data; and statistical disclosure
limitation. The analysis reveals that the five publication variables under
study are estimated very accurately for tabulations involving at least 10 jobs.
Tabulations involving three to nine jobs are a transition zone, where cells may
be fit for use with caution. Tabulations involving one or two jobs, which are
generally suppressed on fitness-for-use criteria in the QWI and synthesized in
LODES, have substantial total variability but can still be used to estimate
statistics for untabulated aggregates as long as the job count in the aggregate
is more than 10.","['Kevin L. McKinney', 'Andrew S. Green', 'Lars Vilhuber', 'John M. Abowd']",[],0,arXiv,http://arxiv.org/abs/2007.13275v1,True,True,False,False,False,535,Patrick M Kline,Michigan,Completed,2007,2012.0,"Kain’s classic paper on spatial mismatch argued that residential segregation reduces the equilibrium employment of minorities by increasing the distance to available jobs. While a substantial literature has emerged testing this hypothesis, and the more general notion that one’s distance to potential jobs might reduce employment probabilities, few studies have been able to deal adequately with the endogeneity of firm and worker location decisions. This project uses a natural experiment to infer the wage and employment effects of moving employers closer to an underemployed population. Using the federal Empowerment Zone program as an exogenous predictor of firm location, the project develops an instrumental variables approach to estimating the elasticity of labor supply with respect to job availability. The analysis will utilize data from the 1990 and 2000 Decennial Censuses, the Standard Statistical Establishment Listing (SSEL), and the Longitudinal Business Database (LBD)."
"A housing-demographic multi-layered nonlinear model to test regulation
  strategies","We propose a novel multi-layered nonlinear model that is able to capture and
predict the housing-demographic dynamics of the real-state market by simulating
the transitions of owners among price-based house layers. This model allows us
to determine which parameters are most effective to smoothen the severity of a
potential market crisis. The International Monetary Fund (IMF) has issued
severe warnings about the current real-state bubble in the United States, the
United Kingdom, Ireland, the Netherlands, Australia and Spain in the last
years. Madrid (Spain), in particular, is an extreme case of this bubble. It is,
therefore, an excellent test case to analyze housing dynamics in the context of
the empirical data provided by the Spanish National Institute of Statistics and
other sources of data. The model is able to predict the mean house occupancy,
and shows that i) the house market conditions in Madrid are unstable but not
critical; and ii) the regulation of the construction rate is more effective
than interest rate changes. Our results indicate that to accommodate the
construction rate to the total population of first-time buyers is the most
effective way to maintain the system near equilibrium conditions. In addition,
we show that to raise interest rates will heavily affect the poorest housing
bands of the population while the middle class layers remain nearly unaffected.","['Ramon Huerta', 'Fernando Corbacho', 'Luis F. Lago-Fernandez']",[],0,arXiv,http://arxiv.org/abs/0809.0979v1,False,True,False,False,False,537,Robert McMillan,Triangle,Completed,2006,2011.0,"This project has four related components.  The first component continues research begun under our previous project at the Berkeley Research Data Center. At the heart of that project was the development of a general equilibrium model of an urban housing market, using an extensive dataset built around restricted-access decennial census data for 1990. In developing this framework further, we will focus on two areas—the identification of key parameters of the model using a boundary fixed effects approach and carrying out informative counterfactual simulations using the equilibrium model in conjunction with our parameter estimates.  The second component uses two waves of decennial census data, for 1990 and 2000, to study the effects of California’s Class Size Reduction Act on local housing markets. Our goal is to measure the size of the induced effects of the reform on household sorting across schools and neighborhoods before estimating the effects of such changes on school and student performance. The third component will make use of the rich cross-sectional data for 2000 to develop and estimate a matching model that describes how workers are matched to firms in equilibrium. And the fourth component will take advantage of the two waves of decennial census data for California (used in the second component of our proposed research) to estimate a dynamic housing market model."
"Dynamic Urban Planning: an Agent-Based Model Coupling Mobility Mode and
  Housing Choice. Use case Kendall Square","As cities become increasingly populated, urban planning plays a key role in
ensuring the equitable and inclusive development of metropolitan areas. MIT
City Science group created a data-driven tangible platform, CityScope, to help
different stakeholders, such as government representatives, urban planners,
developers, and citizens, collaboratively shape the urban scenario through the
real-time impact analysis of different urban interventions. This paper presents
an agent-based model that characterizes citizens' behavioural patterns with
respect to housing and mobility choice that will constitute the first step in
the development of a dynamic incentive system for an open interactive
governance process. The realistic identification and representation of the
criteria that affect this decision-making process will help understand and
evaluate the impacts of potential housing incentives that aim to promote urban
characteristics such as equality, diversity, walkability, and efficiency. The
calibration and validation of the model have been performed in a well-known
geographic area for the Group: Kendall Square in Cambridge, MA.","['Mireia Yurrita', 'Arnaud Grignard', 'Luis Alonso', 'Yan Zhang', 'Cristian Jara-Figueroa', 'Markus Elkatsha', 'Kent Larson']",[],0,arXiv,http://arxiv.org/abs/2106.14572v1,False,True,False,False,False,537,Robert McMillan,Triangle,Completed,2006,2011.0,"This project has four related components.  The first component continues research begun under our previous project at the Berkeley Research Data Center. At the heart of that project was the development of a general equilibrium model of an urban housing market, using an extensive dataset built around restricted-access decennial census data for 1990. In developing this framework further, we will focus on two areas—the identification of key parameters of the model using a boundary fixed effects approach and carrying out informative counterfactual simulations using the equilibrium model in conjunction with our parameter estimates.  The second component uses two waves of decennial census data, for 1990 and 2000, to study the effects of California’s Class Size Reduction Act on local housing markets. Our goal is to measure the size of the induced effects of the reform on household sorting across schools and neighborhoods before estimating the effects of such changes on school and student performance. The third component will make use of the rich cross-sectional data for 2000 to develop and estimate a matching model that describes how workers are matched to firms in equilibrium. And the fourth component will take advantage of the two waves of decennial census data for California (used in the second component of our proposed research) to estimate a dynamic housing market model."
Quantifying the Invisible Labor in Crowd Work,"Crowdsourcing markets provide workers with a centralized place to find paid
work. What may not be obvious at first glance is that, in addition to the work
they do for pay, crowd workers also have to shoulder a variety of unpaid
invisible labor in these markets, which ultimately reduces workers' hourly
wages. Invisible labor includes finding good tasks, messaging requesters, or
managing payments. However, we currently know little about how much time crowd
workers actually spend on invisible labor or how much it costs them
economically. To ensure a fair and equitable future for crowd work, we need to
be certain that workers are being paid fairly for all of the work they do. In
this paper, we conduct a field study to quantify the invisible labor in crowd
work. We build a plugin to record the amount of time that 100 workers on Amazon
Mechanical Turk dedicate to invisible labor while completing 40,903 tasks. If
we ignore the time workers spent on invisible labor, workers' median hourly
wage was $3.76. But, we estimated that crowd workers in our study spent 33% of
their time daily on invisible labor, dropping their median hourly wage to
$2.83. We found that the invisible labor differentially impacts workers
depending on their skill level and workers' demographics. The invisible labor
category that took the most time and that was also the most common revolved
around workers having to manage their payments. The second most time-consuming
invisible labor category involved hyper-vigilance, where workers vigilantly
watched over requesters' profiles for newly posted work or vigilantly searched
for labor. We hope that through our paper, the invisible labor in crowdsourcing
becomes more visible, and our results help to reveal the larger implications of
the continuing invisibility of labor in crowdsourcing.","['Carlos Toxtli', 'Siddharth Suri', 'Saiph Savage']",[],0,arXiv,http://arxiv.org/abs/2110.00169v1,False,True,False,False,False,539,David M Blau,Triangle,Completed,2006,2010.0,"This project will generate new information on rigidities in the labor market for older workers by using rich longitudinal survey data on individuals matched to employment data on the firms that employ them. The individual data are from the Survey of Program Participation and the employer data are from the Longitudinal Employer-Household Dynamics files. The aims of this project are to address the following issues: (1) What accounts for differences in the age structure of employment across firms? Why do some firms employ a larger proportion of older workers than others, and why do some firms hire a larger share of older workers than others? Do differences in the age structure of employment across firms indicate the existence of labor market rigidities? (2) How does the age composition of employment and hiring in a firm affect hours worked and the rate of exit from the firm of older workers relative to younger workers, both to other firms and to nonemployment, controlling for the effects of worker characteristics? (3) What are the main factors responsible for rigidity in the labor market and its differential effects on older relative to younger workers? The main alter-native explanations that can be analyzed with matched worker-firm data are technology-based—fixed costs of hiring, training, and employment; team production considerations; costly monitoring of worker effort; and firm-specific human capital. These explanations can be studied with matched worker-firm data because technology is firm specific, even within industries. The project will address these questions by estimating regressions models explaining labor market transitions of workers as a function of the age distribution of employment in their firms, con-trolling for worker characteristics. The project will also estimate structural equilibrium models of the labor market intended to explain variation in the age structure of employment across firms."
Spatiotemporal characteristics of agricultural food import shocks,"Ensuring food supply stability is key to food security for economies, and
food imports become increasingly important to safeguard food supplies in
economies with inadequate food production. Food import shocks have significant
impacts on targeted economies. Using import trade data of four staple crops
(maize, rice, soybean, and wheat) from 1986 to 2018, this paper identifies food
import trade shocks that occurred to economies during the period of 1995--2018.
We compare the temporal evolution and spatial distribution of import shocks
occurring to different crops and analyze the shock intensity and shock recovery
in various continents based on locally weighted polynomial regression and
Cook's distance. The results reveal higher frequencies during the 2007/2008
food crisis and relatively higher shock frequencies in North America, Africa,
and Asia. Meanwhile, there are regional differences in shock recovery, with the
majority of shocks in Asia recovering in the short term. We also find that high
import diversity and a low import dependency ratio buffer economies against
import shocks, resulting in a low shock rate and a high recovery rate. These
results contribute to our understanding of the external supply risks of food,
placing emphasis on accessibility issues in food security.","['Yin-Ting Zhang', 'Duc Khuong Nguyen', 'Wei-Xing Zhou']",[],0,arXiv,http://arxiv.org/abs/2303.00919v1,False,True,False,False,False,544,J Bradford Jensen,Boston,Completed,2005,2010.0,"Measuring foreign trade in the United States, who trades, how it is conducted, where it originates, where it goes, and its impact on the U.S. economy is an important mission of the U.S. Census Bureau. A principal objective of the project is to build on prior work by linking additional years of import and export transaction data to the Longitudinal Business Database (LBD) and enhancing the existing match of 1993 and 2000 data. The additional years to link are 1992, and 1994–1999 (with the hope of obtaining (and linking) additional years of data from the Foreign Trade Division). The links are made via the EIN information on the import and export transaction files to the Standard Statistical Establishment List files (SSEL) and for exports to Canada, the link is made via business name. We propose to investigate improved matching methodologies using enhanced statistical matching algorithms. 
With the additional linked data, the project proposes to examine a number of issues to increase the Census Bureau’s understanding of the quality of data collected in Title 13, Chapter 5 programs. The topical areas to be investigated include multinational corporation import and export pricing and valuation behavior, geographic and product market entry, the impact of trade on the domestic economy, and treatment of inventory in transit. The project requests the use of all economic census and survey data for the years 1963 through the most recent available (and future years as they become available), the SSEL files (including name and address information) for 1975-most recent available (and future years as they become available), the LBD for 1975-most recent available (and future years as they become available), the Foreign Trade Division import and export transaction data for 1992–2000 (and future years should they become available). The project will also make use of a number of publicly available datasets that the research team will provide. "
Automatic Game Design via Mechanic Generation,"Game designs often center on the game mechanics---rules governing the logical
evolution of the game. We seek to develop an intelligent system that generates
computer games. As first steps towards this goal we present a composable and
cross-domain representation for game mechanics that draws from AI planning
action representations. We use a constraint solver to generate mechanics
subject to design requirements on the form of those mechanics---what they do in
the game. A planner takes a set of generated mechanics and tests whether those
mechanics meet playability requirements---controlling how mechanics function in
a game to affect player behavior. We demonstrate our system by modeling and
generating mechanics in a role-playing game, platformer game, and combined
role-playing-platformer game.","['Alexander Zook', 'Mark O. Riedl']",[],0,arXiv,http://arxiv.org/abs/1908.01420v1,False,True,False,False,False,545,Matthew A Zook,Chicago,Completed,2007,2011.0,"The blooming of e-commerce over the past decade has fostered a considerable diversity and complexity of structure, applications, and definitions. This project examines and evaluates the adoption and use of e-commerce across a diverse set of manufacturing firms.  It examines the Computer Network Use Supplement data on e-commerce gathered in the Census Bureau’s Annual Survey of Manufactures (ASM) and analyzes the characteristics of firms that are related to the use of e-commerce. The focus is on manufacturing because it is currently the sector in which e-commerce is most widely adopted.  The project examines the implementation of e-commerce across manufacturing industries and product types and focuses on how it con-tributes to firms’ competitive advantage through changes in value chains. The analysis is set in the context of how the material characteristics of firms (ranging from size to ability to adopt innovation) impact their medium to long-term viability. While this analysis masks the complex ways in which e-commerce is put to work by firms, this focus on the firm and firm-level characteristics is a first step in uncovering the larger changes at the firm and regional level engendered by e-commerce. This project will also inform the Census Bureau about the quality of the e-commerce data collected using the ASM, about new methods for collecting this type of data, and about the characteristics of firms that influence the probability that and the degree to which a firm will use e-commerce. The Census Bureau would then be able to use this information to further assess the quality of data reported as well as be able to update cur-rent methods for imputing missing data."
Utilizing a Mathematical Model to Estimate Abortion Decline Scenario,"Abortion is one of the biggest causes of maternal deaths, accounting for 15%
of maternal deaths in Southeast Asia. The increase in and effectiveness of
using contraception are still considered to be the effective method to reduce
abortion rate. Data pertaining to abortion incidence and effective efforts to
reduce abortion rate in Indonesia is limited and difficult to access. Meanwhile
such supporting information is necessary to enable the planning and evaluation
of abortion control programs. This paper exemplifies the use of a mathematical
model to explain an abortion decline scenario. The model employs determinants
proposed by Bongaarts, which include average reproductive period, contraceptive
prevalence and effectiveness, total fertility rate (TFR), and intended total
fertility rate (ITFR), as well as birth and abortion intervals. The data used
is from the 1991-2007 Indonesian Demography and Health Survey (Survei Demografi
dan Kesehatan Indonesia/SDKI), and the unit of analysis is women who had been
married and aged 15-49 years old. Based on the current contraceptive prevalence
level in Indonesia at 59-61%, the estimated total abortion rate is 1.9-2.2.
Based on the plot of this total abortion rate, an abortion decline scenario can
be estimated. At the current TFR level of 2.6, the required contraceptive
prevalence is 69% (9% increase) for a decrease of one abortion case per woman.
With a delay of one year in the age of the first marriage and a birth interval
of three years, it is estimated that the abortion rate will decline from 3.05
to 0.69 case per woman throughout her reproductive period. Based on the
assumption of contraceptive prevalence growth at 1-1.4%, it can be estimated
that abortion rate will reach nearly 0 between 2018 and 2022.",['R. Sutiawan'],[],0,arXiv,http://arxiv.org/abs/2104.02023v1,False,True,False,False,False,556,Martha J Bailey,Michigan,Completed,2006,2009.0,"Since the release of the first birth control pill in 1960, women’s fertility and work decisions have undergone a dramatic transition. By the turn of the century, the high fertility rates and low participation of the Baby Boom had evolved into high employment and high childlessness. While recent work links oral contraception to changes in fertility and marital timing and changes in the labor-force participation rates of younger women, these studies do not explore the importance of oral contraception in reshaping the career and mobility decisions of young women. Moreover, research on the changing gender gap does not consider the significance of greater fertility control on inter-mediate mobility decisions and, by extension, longer term wage and employment outcomes. The relationship of each of these outcomes with fertility control are interesting per se, but they may also provide insight as to how women were successful in “swimming upstream” in times of rising wage inequality and why changes in the gender gap appear to have stagnated since 1990.  The proposed project will explore dimensions of career mobility that have been important to women’s economic advancement since 1968—the importance of interstate mobility in determining educational, occupational, and employment paths and labor-force outcomes; how the importance of mobility has changed over time; and how changes in women’s career mobility might be related to oral contraception.  The project uses the restricted access geographic identifiers both in the March Current Population Survey and the National Longitudinal Surveys of Young and Mature Women.  These data facilitate both a comparative and descriptive analysis as well as an experimental evaluation of the origins and nature of the second demographic transition and the quality and shortcomings of the Current Population Survey in light of these population changes."
Fully Synthetic Data for Complex Surveys,"When seeking to release public use files for confidential data, statistical
agencies can generate fully synthetic data. We propose an approach for making
fully synthetic data from surveys collected with complex sampling designs. Our
approach adheres to the general strategy proposed by Rubin (1993).
Specifically, we generate pseudo-populations by applying the weighted finite
population Bayesian bootstrap to account for survey weights, take simple random
samples from those pseudo-populations, estimate synthesis models using these
simple random samples, and release simulated data drawn from the models as
public use files. To facilitate variance estimation, we use the framework of
multiple imputation with two data generation strategies. In the first, we
generate multiple data sets from each simple random sample. In the second, we
generate a single synthetic data set from each simple random sample. We present
multiple imputation combining rules for each setting. We illustrate the
repeated sampling properties of the combining rules via simulation studies,
including comparisons with synthetic data generation based on pseudo-likelihood
methods. We apply the proposed methods to a subset of data from the American
Community Survey.","['Shirley Mathur', 'Yajuan Si', 'Jerome P. Reiter']",[],0,arXiv,http://arxiv.org/abs/2309.09115v4,False,True,False,False,False,560,Jerome P Reiter,Triangle,Completed,2006,2007.0,"This project will investigate the disclosure risks and data utility associated with using partially synthetic data to limit disclosure risks for people in group quarters in the American Communities Survey.  We will first investigate criteria for deciding which records in group quarters are most at risk.  We will develop approaches to generating partially synthetic data, considering risk and utility of proposed releases.  We also will examine the risk utility tradeoffs of releasing one, two, or five imputed data sets.  We will develop approximations to the variance for single imputation datasets, if needed.  Lastly, we will consider how the survey weights interact with the data synthesis."
"Dirichlet Process Mixture Models for Modeling and Generating Synthetic
  Versions of Nested Categorical Data","We present a Bayesian model for estimating the joint distribution of
multivariate categorical data when units are nested within groups. Such data
arise frequently in social science settings, for example, people living in
households. The model assumes that (i) each group is a member of a group-level
latent class, and (ii) each unit is a member of a unit-level latent class
nested within its group-level latent class. This structure allows the model to
capture dependence among units in the same group. It also facilitates
simultaneous modeling of variables at both group and unit levels. We develop a
version of the model that assigns zero probability to groups and units with
physically impossible combinations of variables. We apply the model to estimate
multivariate relationships in a subset of the American Community Survey. Using
the estimated model, we generate synthetic household data that could be
disseminated as redacted public use files with high analytic validity and low
disclosure risks. Supplementary materials for this article are available
online.","['Jingchen Hu', 'Jerome P. Reiter', 'Quanli Wang']",[],0,arXiv,http://arxiv.org/abs/1412.2282v6,False,False,False,False,True,560,Jerome P Reiter,Triangle,Completed,2006,2007.0,"This project will investigate the disclosure risks and data utility associated with using partially synthetic data to limit disclosure risks for people in group quarters in the American Communities Survey.  We will first investigate criteria for deciding which records in group quarters are most at risk.  We will develop approaches to generating partially synthetic data, considering risk and utility of proposed releases.  We also will examine the risk utility tradeoffs of releasing one, two, or five imputed data sets.  We will develop approximations to the variance for single imputation datasets, if needed.  Lastly, we will consider how the survey weights interact with the data synthesis."
"A Latent Class Modeling Approach for Generating Synthetic Data and
  Making Posterior Inferences from Differentially Private Counts","Several algorithms exist for creating differentially private counts from
contingency tables, such as two-way or three-way marginal counts. The resulting
noisy counts generally do not correspond to a coherent contingency table, so
that some post-processing step is needed if one wants the released counts to
correspond to a coherent contingency table. We present a latent class modeling
approach for post-processing differentially private marginal counts that can be
used (i) to create differentially private synthetic data from the set of
marginal counts, and (ii) to enable posterior inferences about the confidential
counts. We illustrate the approach using a subset of the 2016 American
Community Survey Public Use Microdata Sets and the 2004 National Long Term Care
Survey.","['Michelle Pistner Nixon', 'Andrés F. Barrientos', 'Jerome P. Reiter', 'Aleksandra Slavković']",[],0,arXiv,http://arxiv.org/abs/2201.10545v1,False,True,False,False,False,560,Jerome P Reiter,Triangle,Completed,2006,2007.0,"This project will investigate the disclosure risks and data utility associated with using partially synthetic data to limit disclosure risks for people in group quarters in the American Communities Survey.  We will first investigate criteria for deciding which records in group quarters are most at risk.  We will develop approaches to generating partially synthetic data, considering risk and utility of proposed releases.  We also will examine the risk utility tradeoffs of releasing one, two, or five imputed data sets.  We will develop approximations to the variance for single imputation datasets, if needed.  Lastly, we will consider how the survey weights interact with the data synthesis."
Data Fusion for Correcting Measurement Errors,"Often in surveys, key items are subject to measurement errors. Given just the
data, it can be difficult to determine the distribution of this error process,
and hence to obtain accurate inferences that involve the error-prone variables.
In some settings, however, analysts have access to a data source on different
individuals with high quality measurements of the error-prone survey items. We
present a data fusion framework for leveraging this information to improve
inferences in the error-prone survey. The basic idea is to posit models about
the rates at which individuals make errors, coupled with models for the values
reported when errors are made. This can avoid the unrealistic assumption of
conditional independence typically used in data fusion. We apply the approach
on the reported values of educational attainments in the American Community
Survey, using the National Survey of College Graduates as the high quality data
source. In doing so, we account for the informative sampling design used to
select the National Survey of College Graduates. We also present a process for
assessing the sensitivity of various analyses to different choices for the
measurement error models. Supplemental material is available online.","['Tracy Schifeling', 'Jerome P. Reiter', 'Maria DeYoreo']",[],0,arXiv,http://arxiv.org/abs/1610.00147v1,False,False,False,False,True,560,Jerome P Reiter,Triangle,Completed,2006,2007.0,"This project will investigate the disclosure risks and data utility associated with using partially synthetic data to limit disclosure risks for people in group quarters in the American Communities Survey.  We will first investigate criteria for deciding which records in group quarters are most at risk.  We will develop approaches to generating partially synthetic data, considering risk and utility of proposed releases.  We also will examine the risk utility tradeoffs of releasing one, two, or five imputed data sets.  We will develop approximations to the variance for single imputation datasets, if needed.  Lastly, we will consider how the survey weights interact with the data synthesis."
"Discrimination and AI in insurance: what do people find fair? Results
  from a survey","Two modern trends in insurance are data-intensive underwriting and
behavior-based insurance. Data-intensive underwriting means that insurers use
and analyze more data for estimating the chance that a consumer files a claim
and calculating the premium based on that estimation. Insurers analyze the new
datasets with artificial intelligence (AI) to discover new correlations, with
which they can estimate the policyholder's expected claims cost more precisely.
Insurers also offer behavior-based insurance. For example, some car insurers
use AI to follow the driving behavior of an individual policyholder in
real-time and decide whether to offer that policyholder a discount. Similarly,
a life insurer could track a policyholder's activity with a smart watch and
offer a discount for an active lifestyle.
  In this paper, we report on a survey of the Dutch population (N=999) in which
we asked people's opinions about examples of data-intensive underwriting and
behavior-based insurance. The main results include the following. First, if
survey respondents find an insurance practice unfair, they also find the
practice unacceptable. Second, respondents find almost all modern insurance
practices that we described unfair. Third, respondents find practices fairer if
they can influence the premium. For example, respondents find behavior-based
car insurance with a car tracker relatively fair. Fourth, if respondents do not
see the logic of using a certain consumer characteristic, then respondents find
it unfair if an insurer calculates the premium based on the characteristic.
Fifth, respondents find it unfair if an insurer offers an insurance product
only to a specific group, such as car insurance specifically for family
doctors. Sixth, respondents find it unfair if an insurance practice leads to
higher prices for poorer people. We reflect on the policy implications of the
findings.","['Frederik Zuiderveen Borgesius', 'Marvin van Bekkum', 'Iris van Ooijen', 'Gabi Schaap', 'Maaike Harbers', 'Tjerk Timan']",[],0,arXiv,http://arxiv.org/abs/2501.12897v1,False,True,False,False,False,565,Philip F Cooper,Washington,Completed,2006,2013.0,"In this project, we will use the Medical Expenditure Panel Survey Insurance Component (MEPS-IC) to produce estimates of the factors affecting employer-sponsored health insurance.  We will investigate the quality of the MEPS-IC data, and we will enhance the usefulness of the MEPS-IC by matching information from several other datasets to it.   Estimates will be primarily derived from multivariate models and will focus on the following six broad areas: employers’ decisions to offer insurance; employers’ decisions on the types of plans to offer employees; employees’ health insurance enrollment decisions; employers’ decisions on the structure of their contributions towards premiums; employers’ labor market responses to employer-sponsored health insurance; employers’ decisions with respect to health insurance eligibility rules."
Strategic Innovation Through Outsourcing: A Theoretical Review,"Competition in the Information Technology Outsourcing (ITO) and Business
Process Outsourcing (BPO) industry is increasingly moving from being motivated
by cost savings towards strategic benefits that service providers can offer to
their clients. Innovation is one such benefit that is expected nowadays in
outsourcing engagements. The rising importance of innovation has been noticed
and acknowledged not only in the Information Systems (IS) literature, but also
in other management streams such as innovation and strategy. However, to date,
these individual strands of research remain largely isolated from each other.
Our theoretical review addresses this gap by consolidating and analyzing
research on strategic innovation in the ITO and BPO context. The article set
includes 95 papers published between 1998 to 2020 in outlets from the IS and
related management fields. We craft a four-phase framework that integrates
prior insights about (1) the antecedents of the decision to pursue strategic
innovation in outsourcing settings; (2) arrangement options that facilitate
strategic innovation in outsourcing relationships; (3) the generation of
strategic innovations; and (4) realized strategic innovation outcomes, as
assessed in the literature. We find that the research landscape to date is
skewed, with many studies focusing on the first two phases. The last two phases
remain relatively uncharted. We also discuss how innovation-oriented
outsourcing insights compare with established research on cost-oriented
outsourcing engagements. Finally, we offer directions for future research.","['Marfri Gambal', 'Aleksandre Asatiani', 'Julia Kotlarsky']",[],0,arXiv,http://arxiv.org/abs/2206.00982v1,False,True,False,False,False,566,Michael A Stanko,Michigan,Completed,2007,2010.0,"This project explores the firm level drivers of innovation outsourcing, such as exploratory research performed, inventory turnover, and consequences such as innovation costs and other financial outcomes. Industry level moderators of these relationships are also proposed.  This research project investigates the extent to which increased outsourcing of research and development (R&D) recategorizes innovation activities, which, if carried out internally, would be classified as manufacturing but when contracted to a specialist firm is categorized as a service.  Time series analysis of shifts from manufacturing to nonmanufacturing will illustrate the extent to which the growth of contract R&D creates (or does not create) a measurement problem and give insight into the comparability of historical data with more recent years’ data.  The project will compute Herfindahl indexes for nonmanufacturing industries. The Census Bureau currently publishes Herfindahl indexes for the manufacturing sector but not for nonmanufacturing. This project will provide estimates of how firm and industry characteristics differentially influence the propensity to outsource innovation activities, as well as the consequences of this outsourcing.  This study links the Survey of Industrial Research and Development, the Longitudinal Business Database and Compustat for the years 1972–2001. Two external, publicly available databases are also required: The National Bureau of Economic Research’s “U.S. Patent Citations Data File” as well as the Census Bureau’s “Concentration Ratios in Manufacturing” dataset.  Once assembled, these datasets will be used to test a multilevel model that examines firm and industry level factors influencing the propensity to outsource R&D as well as the outcomes of this outsourcing."
Evaluating the Impacts of Swapping on the US Decennial Census,"To meet its dual burdens of providing useful statistics and ensuring privacy
of individual respondents, the US Census Bureau has for decades introduced some
form of ""noise"" into published statistics. Initially, they used a method known
as ""swapping"" (1990-2010). In 2020, they switched to an algorithm called
TopDown that ensures a form of Differential Privacy. While the TopDown
algorithm has been made public, no implementation of swapping has been released
and many details of the deployed swapping methodology deployed have been kept
secret. Further, the Bureau has not published (even a synthetic) ""original""
dataset and its swapped version. It is therefore difficult to evaluate the
effects of swapping, and to compare these effects to those of other privacy
technologies. To address these difficulties we describe and implement a
parameterized swapping algorithm based on Census publications, court documents,
and informal interviews with Census employees. With this implementation, we
characterize the impacts of swapping on a range of statistical quantities of
interest. We provide intuition for the types of shifts induced by swapping and
compare against those introduced by TopDown. We find that even when swapping
and TopDown introduce errors of similar magnitude, the direction in which
statistics are biased need not be the same across the two techniques. More
broadly, our implementation provides researchers with the tools to analyze and
potentially correct for the impacts of disclosure avoidance systems on the
quantities they study.","['Maria Ballesteros', 'Cynthia Dwork', 'Gary King', 'Conlan Olson', 'Manish Raghavan']",[],0,arXiv,http://arxiv.org/abs/2502.01320v2,True,True,True,False,True,567,Felicia B Leclere,Michigan,Completed,2007,2008.0,"This project will revamp existing documentation for the 1990 and 2000 decennial census micro data files currently available at Inter-University Consortium for Political and Social Research (ICPSR). The improved documentation will include information on the additional variables and codes available to researchers in the Census Bureau’s Research Data Centers as well as relabel already publicly available variables to match internal documentation. Documentation also will include elements such as procedural histories, enumerator instructions, enumeration forms, and descriptive text from published Census Bureau volumes that explain how data are organized, details tabulation methods, and provides other information useful to users."
"Quantifying Privacy Risks of Public Statistics to Residents of
  Subsidized Housing","As the U.S. Census Bureau implements its controversial new disclosure
avoidance system, researchers and policymakers debate the necessity of new
privacy protections for public statistics. With experiments on both published
statistics and synthetic data, we explore a particular privacy concern:
respondents in subsidized housing may deliberately not mention unauthorized
children and other household members for fear of being evicted. By combining
public statistics from the Decennial Census and the Department of Housing and
Urban Development, we demonstrate a simple, inexpensive reconstruction attack
that could identify subsidized households living in violation of occupancy
guidelines in 2010. Experiments on synthetic data suggest that a random
swapping mechanism similar to the Census Bureau's 2010 disclosure avoidance
measures does not significantly reduce the precision of this attack, while a
differentially private mechanism similar to the 2020 disclosure avoidance
system does. Our results provide a valuable example for policymakers seeking a
trustworthy, accurate census.","['Ryan Steed', 'Diana Qing', 'Zhiwei Steven Wu']",[],0,arXiv,http://arxiv.org/abs/2407.04776v1,True,True,True,False,True,567,Felicia B Leclere,Michigan,Completed,2007,2008.0,"This project will revamp existing documentation for the 1990 and 2000 decennial census micro data files currently available at Inter-University Consortium for Political and Social Research (ICPSR). The improved documentation will include information on the additional variables and codes available to researchers in the Census Bureau’s Research Data Centers as well as relabel already publicly available variables to match internal documentation. Documentation also will include elements such as procedural histories, enumerator instructions, enumeration forms, and descriptive text from published Census Bureau volumes that explain how data are organized, details tabulation methods, and provides other information useful to users."
"Confidence-Ranked Reconstruction of Census Microdata from Published
  Statistics","A reconstruction attack on a private dataset $D$ takes as input some publicly
accessible information about the dataset and produces a list of candidate
elements of $D$. We introduce a new class of data reconstruction attacks based
on randomized methods for non-convex optimization. We empirically demonstrate
that our attacks can not only reconstruct full rows of $D$ from aggregate query
statistics $Q(D)\in \mathbb{R}^m$, but can do so in a way that reliably ranks
reconstructed rows by their odds of appearing in the private data, providing a
signature that could be used for prioritizing reconstructed rows for further
actions such as identify theft or hate crime. We also design a sequence of
baselines for evaluating reconstruction attacks. Our attacks significantly
outperform those that are based only on access to a public distribution or
population from which the private dataset $D$ was sampled, demonstrating that
they are exploiting information in the aggregate statistics $Q(D)$, and not
simply the overall structure of the distribution. In other words, the queries
$Q(D)$ are permitting reconstruction of elements of this dataset, not the
distribution from which $D$ was drawn. These findings are established both on
2010 U.S. decennial Census data and queries and Census-derived American
Community Survey datasets. Taken together, our methods and experiments
illustrate the risks in releasing numerically precise aggregate statistics of a
large dataset, and provide further motivation for the careful application of
provably private techniques such as differential privacy.","['Travis Dick', 'Cynthia Dwork', 'Michael Kearns', 'Terrance Liu', 'Aaron Roth', 'Giuseppe Vietri', 'Zhiwei Steven Wu']",[],0,arXiv,http://arxiv.org/abs/2211.03128v2,False,True,False,False,True,567,Felicia B Leclere,Michigan,Completed,2007,2008.0,"This project will revamp existing documentation for the 1990 and 2000 decennial census micro data files currently available at Inter-University Consortium for Political and Social Research (ICPSR). The improved documentation will include information on the additional variables and codes available to researchers in the Census Bureau’s Research Data Centers as well as relabel already publicly available variables to match internal documentation. Documentation also will include elements such as procedural histories, enumerator instructions, enumeration forms, and descriptive text from published Census Bureau volumes that explain how data are organized, details tabulation methods, and provides other information useful to users."
"Tax Credits and Household Behavior: The Roles of Myopic Decision-Making
  and Liquidity in a Simulated Economy","There has been a growing interest in multi-agent simulators in the domain of
economic modeling. However, contemporary research often involves developing
reinforcement learning (RL) based models that focus solely on a single type of
agents, such as households, firms, or the government. Such an approach
overlooks the adaptation of interacting agents thereby failing to capture the
complexity of real-world economic systems. In this work, we consider a
multi-agent simulator comprised of RL agents of numerous types, including
heterogeneous households, firm, central bank and government. In particular, we
focus on the crucial role of the government in distributing tax credits to
households. We conduct two broad categories of comprehensive experiments
dealing with the impact of tax credits on 1) households with varied degrees of
myopia (short-sightedness in spending and saving decisions), and 2) households
with diverse liquidity profiles. The first category of experiments examines the
impact of the frequency of tax credits (e.g. annual vs quarterly) on
consumption patterns of myopic households. The second category of experiments
focuses on the impact of varying tax credit distribution strategies on
households with differing liquidities. We validate our simulation model by
reproducing trends observed in real households upon receipt of unforeseen,
uniform tax credits, as documented in a JPMorgan Chase report. Based on the
results of the latter, we propose an innovative tax credit distribution
strategy for the government to reduce inequality among households. We
demonstrate the efficacy of this strategy in improving social welfare in our
simulation results.","['Kshama Dwarakanath', 'Jialin Dong', 'Svitlana Vyetrenko']",[],0,arXiv,http://arxiv.org/abs/2408.10391v2,False,True,False,False,False,575,Alicia Robb,Berkeley,Completed,2006,2007.0,"The purpose of this project is to evaluate survival patterns of firms in the 1992 Characteristics of Business Owners (CBO), particularly minority-owned firms in metropolitan areas. The project results will provide information that can be used to improve measurement of business survival in the new Survey of Business Owners and Self-Employed Persons (SBO).  This project proposes to use the most current (1992) version of the CBO to revisit earlier findings regarding firm survival patterns.  It will investigate the robustness of the CBO database for portraying small-business survival patterns for employer businesses, using the Longitudinal Business Database (LBD) to track CBO firm survival patterns.  This research also is relevant to the new SBO. Information from this will increase the U.S. Census Bureau’s knowledge base regarding business survival dynamics. The SBO survey staff can use this knowledge to improve SBO response rates by tailoring their sample designs to businesses based on their likelihood of still being in business.  This could help reduce the number of out-of-scope cases that occur when a portion of the sample is selected from the previous year business register, but responses are only used when the business remains active for the next year. The results from this research can also assist the Census Bureau in the construction or improvement of the sampling frame for the SBO survey."
"Precision Health Data: Requirements, Challenges and Existing Techniques
  for Data Security and Privacy","Precision health leverages information from various sources, including omics,
lifestyle, environment, social media, medical records, and medical insurance
claims to enable personalized care, prevent and predict illness, and precise
treatments. It extensively uses sensing technologies (e.g., electronic health
monitoring devices), computations (e.g., machine learning), and communication
(e.g., interaction between the health data centers). As health data contain
sensitive private information, including the identity of patient and carer and
medical conditions of the patient, proper care is required at all times.
Leakage of these private information affects the personal life, including
bullying, high insurance premium, and loss of job due to the medical history.
Thus, the security, privacy of and trust on the information are of utmost
importance. Moreover, government legislation and ethics committees demand the
security and privacy of healthcare data. Herein, in the light of precision
health data security, privacy, ethical and regulatory requirements, finding the
best methods and techniques for the utilization of the health data, and thus
precision health is essential. In this regard, firstly, this paper explores the
regulations, ethical guidelines around the world, and domain-specific needs.
Then it presents the requirements and investigates the associated challenges.
Secondly, this paper investigates secure and privacy-preserving machine
learning methods suitable for the computation of precision health data along
with their usage in relevant health projects. Finally, it illustrates the best
available techniques for precision health data security and privacy with a
conceptual system model that enables compliance, ethics clearance, consent
management, medical innovations, and developments in the health domain.","['Chandra Thapa', 'Seyit Camtepe']",[],0,arXiv,http://arxiv.org/abs/2008.10733v1,False,True,False,False,False,577,Jennifer S Schultz,Chicago,Completed,2006,2009.0,"We propose to analyze the effects of health insurance benefit costs on employer demand for part-time employees, availability of retiree health insurance bene-fits, and the effects of unionization on health benefit offers and cost sharing arrangements by employers. To address these issues, we propose to use the Medical Expenditure Panel Survey-Insurance Component (MEPS-IC) List Sample matched with the Longitudinal Business Database (LBD) and supplemental economic data from the U.S. Bureau of Labor Statistics (BLS) and the Area Resource File.   This project will benefit the U.S. Census Bureau by contributing to the understanding of the quality of the data collected in the MEPS-IC by comparing variables reported by establishments and firms in MEPS-IC and the LBD (e.g., tenure/age of firm). This project will also look for variability in reporting by establishments of the same firm and will derive methods to address inconsistencies. In addition, this project will benefit the Census Bureau by reporting estimates on the effects of rising health insurance on labor demand."
"Mobility Gaps between Low-Income and Not Low-Income Households: A Case
  Study in New York State","Understanding the travel challenges faced by low-income residents has always
been and continues to be one of the most important transportation equity
topics. This study aims to explore the mobility gaps between low-income
households (HHs) and not low-income HHs, and how the gaps vary within different
socio-demographic population groups in New York State (NYS). The latest
National Household Travel Survey data was used as the primary data source for
the analysis. The study first employed the K-prototype clustering algorithm to
categorize the HHs in NYS based on their socio-demographic attributes. Five
population groups were identified based on nine different household (HH)
features such as HH size, vehicle ownership, and elderly status of its members.
Then, the mobility differences, measured by trip frequency, trip distance,
travel time, and person miles traveled, were examined among the five population
groups. Results suggest that the individuals in low-income HHs consistently
took fewer trips and made shorter trips compared to their not low-income
counterparts in NYS. The travel distance gaps were most obvious among white HHs
with more vehicles than drivers. In addition, while the population from
low-income HHs made shorter trips on average (2.7 mi shorter per trip), they
experienced longer travel time than those from not low-income HHs (1.8 min
longer per trip). These key findings provide a deeper understanding of the
travel behavior disparities between low-income and not low-income households.
The findings could also support policymakers and transportation planners in
addressing the critical needs of residents in low-income households in NYS and
provide inputs for designing a more equitable transportation system.","['Yuandong Liu', 'Majbah Uddin']",[],0,arXiv,http://arxiv.org/abs/2402.04018v1,False,True,False,False,False,582,Michael H Riordan,Baruch,Completed,2007,2011.0,"Telephone penetration—the percentage of households with telephone service—is the accepted measure of universal service in the United States. This research studies the telephone penetration of low-income households in the United States. One purpose of the study is to measure the determinants of telephone penetration of low-income households, including the effects of universal service policies that reduce the prices these households pay for telephone service. Another objective is to compare predictions generated by the econometric model with standard hotdeck imputations used to assign responses for households that do not respond to the telephone availability question. The study uses cross-section and panel econometric methods to estimate the demand for telephone service by low-income households. The explanatory variables are demographic and location characteristics, including the characteristics of the telephone service plans offered to low-income households. The econometric analysis estimates the price elasticity of demand for telephone service for different demographic groups. Predicted household demands are aggregated to explain the determinants of changes in telephone penetration of low-income households between 1990 and 2000. Predictions from the econometric model are compared to imputations from standard hotdeck methods used for dealing with nonresponses to the telephone availability question. The study estimates the price elasticity of demand for telephone service of different demographic groups and measures the determinants of changes in telephone penetration between 1990 and 2000, including the effects of universal service policies that reduced the prices low-income households pay for telephone service."
"Machine learning to assess relatedness: the advantage of using
  firm-level data","The relatedness between a country or a firm and a product is a measure of the
feasibility of that economic activity. As such, it is a driver for investments
at a private and institutional level. Traditionally, relatedness is measured
using networks derived by country-level co-occurrences of product pairs, that
is counting how many countries export both. In this work, we compare networks
and machine learning algorithms trained not only on country-level data, but
also on firms, that is something not much studied due to the low availability
of firm-level data. We quantitatively compare the different measures of
relatedness, by using them to forecast the exports at the country and
firm-level, assuming that more related products have a higher likelihood to be
exported in the future. Our results show that relatedness is scale-dependent:
the best assessments are obtained by using machine learning on the same
typology of data one wants to predict. Moreover, we found that while
relatedness measures based on country data are not suitable for firms,
firm-level data are very informative also for the development of countries. In
this sense, models built on firm data provide a better assessment of
relatedness. We also discuss the effect of using parameter optimization and
community detection algorithms to identify clusters of related companies and
products, finding that a partition into a higher number of blocks decreases the
computational time while maintaining a prediction performance well above the
network-based benchmarks.","['Giambattista Albora', 'Andrea Zaccaria']",[],0,arXiv,http://arxiv.org/abs/2202.00458v3,False,True,False,False,False,585,Chang-Tai Hsieh,Chicago,Completed,2006,2009.0,"It is well established that there are large differences in productivity across firms, industries, and countries. Motivated by this fact, the purpose of this project is to use the Census of Manufactures (from 1963, 1967, 1972, 1977, 1982, 1987, 1992, 1997, and 2002) and the Annual Survey of Manufacturers (1973-2001) to develop a methodology for two new series for potential public release. These series help shed light on the underlying sources of productivity differences. First, for 4-digit Standard Industrial Classification (SIC) and 5-digit North American Industry Classification System industries, and for state and metropolitan geographic areas, we will construct and document industry series on the quality of products made. This quality index will exploit the unit price data provided for many of the 7-digit SIC products in the Census of Manufactures to measure the extent to which differences in productivity across establishments show up as differences in product quality. Our second contribution will be to provide new geographic area and industry series on the extent to which factor inputs are misallocated across plants in a geographic area or in a given industry. This “inefficiency” index will measure the potential gains in output if factor inputs were to be allocated efficiently across plants in the industry and area."
"Structure of global buyer-supplier networks and its implications for
  conflict minerals regulations","We investigate the structure of global inter-firm linkages using a dataset
that contains information on business partners for about 400,000 firms
worldwide, including all the firms listed on the major stock exchanges. Among
the firms, we examine three networks, which are based on customer-supplier,
licensee-licensor, and strategic alliance relationships. First, we show that
these networks all have scale-free topology and that the degree distribution
for each follows a power law with an exponent of 1.5. The shortest path length
is around six for all three networks. Second, we show through community
structure analysis that the firms comprise a community with those firms that
belong to the same industry but different home countries, indicating the
globalization of firms' production activities. Finally, we discuss what such
production globalization implies for the proliferation of conflict minerals
(i.e., minerals extracted from conflict zones and sold to firms in other
countries to perpetuate fighting) through global buyer-supplier linkages. We
show that a limited number of firms belonging to some specific industries and
countries plays an important role in the global proliferation of conflict
minerals. Our numerical simulation shows that regulations on the purchases of
conflict minerals by those firms would substantially reduce their worldwide
use.","['Takayuki Mizuno', 'Takaaki Ohnishi', 'Tsutomu Watanabe']",[],0,arXiv,http://arxiv.org/abs/1505.02274v1,False,True,False,False,False,585,Chang-Tai Hsieh,Chicago,Completed,2006,2009.0,"It is well established that there are large differences in productivity across firms, industries, and countries. Motivated by this fact, the purpose of this project is to use the Census of Manufactures (from 1963, 1967, 1972, 1977, 1982, 1987, 1992, 1997, and 2002) and the Annual Survey of Manufacturers (1973-2001) to develop a methodology for two new series for potential public release. These series help shed light on the underlying sources of productivity differences. First, for 4-digit Standard Industrial Classification (SIC) and 5-digit North American Industry Classification System industries, and for state and metropolitan geographic areas, we will construct and document industry series on the quality of products made. This quality index will exploit the unit price data provided for many of the 7-digit SIC products in the Census of Manufactures to measure the extent to which differences in productivity across establishments show up as differences in product quality. Our second contribution will be to provide new geographic area and industry series on the extent to which factor inputs are misallocated across plants in a geographic area or in a given industry. This “inefficiency” index will measure the potential gains in output if factor inputs were to be allocated efficiently across plants in the industry and area."
The Similarity of Global Value Chains: A Network-Based Measure,"International trade has been increasingly organized in the form of global
value chains (GVCs) where different stages of production are located in
different countries. This recent phenomenon has substantial consequences for
both trade policy design at the national or regional level and business
decision making at the firm level. In this paper, we provide a new method for
comparing GVCs across countries and over time. First, we use the World
Input-Output Database (WIOD) to construct both the upstream and downstream
global value networks, where the nodes are individual sectors in different
countries and the links are the value-added contribution relationships. Second,
we introduce a network-based measure of node similarity to compare the GVCs
between any pair of countries for each sector and each year available in the
WIOD. Our network-based similarity is a better measure for node comparison than
the existing ones because it takes into account all the direct and indirect
relationships between country-sector pairs, is applicable to both directed and
weighted networks with self-loops, and takes into account externally defined
node attributes. As a result, our measure of similarity reveals the most
intensive interactions among the GVCs across countries and over time. From 1995
to 2011, the average similarity between sectors and countries have clear
increasing trends, which are temporarily interrupted by the recent economic
crisis. This measure of the similarity of GVCs provides quantitative answers to
important questions about dependency, sustainability, risk, and competition in
the global production system.","['Zhen Zhu', 'Greg Morrison', 'Michelangelo Puliga', 'Alessandro Chessa', 'Massimo Riccaboni']",[],0,arXiv,http://arxiv.org/abs/1508.04392v1,False,True,False,False,False,585,Chang-Tai Hsieh,Chicago,Completed,2006,2009.0,"It is well established that there are large differences in productivity across firms, industries, and countries. Motivated by this fact, the purpose of this project is to use the Census of Manufactures (from 1963, 1967, 1972, 1977, 1982, 1987, 1992, 1997, and 2002) and the Annual Survey of Manufacturers (1973-2001) to develop a methodology for two new series for potential public release. These series help shed light on the underlying sources of productivity differences. First, for 4-digit Standard Industrial Classification (SIC) and 5-digit North American Industry Classification System industries, and for state and metropolitan geographic areas, we will construct and document industry series on the quality of products made. This quality index will exploit the unit price data provided for many of the 7-digit SIC products in the Census of Manufactures to measure the extent to which differences in productivity across establishments show up as differences in product quality. Our second contribution will be to provide new geographic area and industry series on the extent to which factor inputs are misallocated across plants in a geographic area or in a given industry. This “inefficiency” index will measure the potential gains in output if factor inputs were to be allocated efficiently across plants in the industry and area."
"High-resolution estimates of the foreign-born population and
  international migration for the United States","Detailed estimates of migration stocks and flows provides evidence for
understanding population dynamics, and the impact of economic and political
changes that influence migration. Using data from the 2000 decennial census and
2001-2016 American Community Survey (ACS), this study derives
highly-disaggregated estimates of the foreign-born population residing in the
United States for the period 2000-2018, and annual foreign-born entries to the
ACS population as a measure of immigration volume. These estimates are derived
from an evidence synthesis combining pooled survey data with auxiliary data on
potential biases in raw survey estimates and other trends affecting the
foreign-born population. For an individual population stratum (defined by
current age, entry year, country of origin, and calendar year) direct estimates
using survey data can have substantial sampling uncertainty. By imposing
logical and probabilistic constraints, data are pooled across survey years to
produce more precise estimates. Corrections are implemented for respondent
misreporting of demographic information, and undercount of the foreign-born
population in the ACS. This paper describes the statistical approach used to
model population change, demonstrates the validity of the approach via in- and
out-of-sample predictive performance, provides the population estimates, and
highlights potential applications.",['Nicolas A Menzies'],[],0,arXiv,http://arxiv.org/abs/1906.01716v1,False,True,False,False,True,586,Mary M Kritz,Cornell,Completed,2007,2014.0,"This research employs census long form sample and American Community Survey (ACS) confidential data to analyze the dynamics underlying the increasing dispersal of the foreign-born population in the United States. The project focuses on three dimensions of this process: 1) The estimation of the individual and context characteristics that underlie internal migration to nontraditional destinations; 2) The analysis of place and individual characteristics associated with both residential stability and residential churning for foreign-born persons residing in non-traditional destinations; and 3) The examination of the process of selection of destinations for those departing from nontraditional settlement areas. The analysis utilizes McFadden choice models to estimate the role of different destination contexts in attracting foreign born. Multilevel logit models estimate the processes of departure from gateway and nontraditional places. This project will enhance census and ACS data by generating knowledge on cohort residential trajectories between the 1980s and early 2000s. The research on destination choices will also provide a detailed picture of the migration links between specific types of places. In addition to describing the nature of these linkages, the research will shed light on the dynamics underlying emerging trends in the internal migration and settlement behaviors of the growing foreign-born population."
"Sources of HIV infections among MSM with a migration background: a viral
  phylogenetic case study in Amsterdam, the Netherlands","Background: Men and women with a migration background comprise an increasing
proportion of incident HIV cases across Western Europe. Several studies
indicate a substantial proportion acquire HIV post-migration.
  Methods: We used partial HIV consensus sequences with linked demographic and
clinical data from the opt-out ATHENA cohort of people with HIV in the
Netherlands to quantify population-level sources of transmission to Dutch-born
and foreign-born Amsterdam men who have sex with men (MSM) between 2010-2021.
We identified phylogenetically and epidemiologically possible transmission
pairs in local transmission chains and interpreted these in the context of
estimated infection dates, quantifying transmission dynamics between
sub-populations by world region of birth.
  Results: We estimate the majority of Amsterdam MSM who acquired their
infection locally had a Dutch-born Amsterdam MSM source (56% [53-58%]).
Dutch-born MSM were the predominant source population of infections among
almost all foreign-born Amsterdam MSM sub-populations. Stratifying by two-year
intervals indicated shifts in transmission dynamics, with a majority of
infections originating from foreign-born MSM since 2018, although uncertainty
ranges remained wide.
  Conclusions: In the context of declining HIV incidence among Amsterdam MSM,
our data suggest whilst native-born MSM have predominantly driven transmissions
in 2010-2021, the contribution from foreign-born MSM living in Amsterdam is
increasing.","['Alexandra Blenkinsop', 'Nikos Pantazis', 'Evangelia Georgia Kostaki', 'Lysandros Sofocleous', 'Ard van Sighem', 'Daniela Bezemer', 'Thijs van de Laar', 'Marc van der Valk', 'Peter Reiss', 'Godelieve de Bree', 'Oliver Ratmann']",[],0,arXiv,http://arxiv.org/abs/2401.08308v1,False,True,False,False,False,586,Mary M Kritz,Cornell,Completed,2007,2014.0,"This research employs census long form sample and American Community Survey (ACS) confidential data to analyze the dynamics underlying the increasing dispersal of the foreign-born population in the United States. The project focuses on three dimensions of this process: 1) The estimation of the individual and context characteristics that underlie internal migration to nontraditional destinations; 2) The analysis of place and individual characteristics associated with both residential stability and residential churning for foreign-born persons residing in non-traditional destinations; and 3) The examination of the process of selection of destinations for those departing from nontraditional settlement areas. The analysis utilizes McFadden choice models to estimate the role of different destination contexts in attracting foreign born. Multilevel logit models estimate the processes of departure from gateway and nontraditional places. This project will enhance census and ACS data by generating knowledge on cohort residential trajectories between the 1980s and early 2000s. The research on destination choices will also provide a detailed picture of the migration links between specific types of places. In addition to describing the nature of these linkages, the research will shed light on the dynamics underlying emerging trends in the internal migration and settlement behaviors of the growing foreign-born population."
"Morphology and Electronic Properties of Incipient Soot by Scanning
  Tunneling Microscopy and Spectroscopy","Soot nucleation is one of the most complex and debated steps of the soot
formation process in combustion. In this work, we used scanning tunneling
microscopy (STM) and spectroscopy (STS) to probe morphological and electronic
properties of incipient soot particles formed right behind the flame front of a
lightly sooting laminar premixed flame of ethylene and air. Particles were
thermophoretically sampled on an atomically flat gold film on a mica substrate.
High-resolution STM images of incipient soot particles were obtained for the
first time showing the morphology of sub-5 nm incipient soot particles.
High-resolution single-particle spectroscopic properties were measured
confirming the semiconductor behavior of incipient soot particles with an
electronic band gap ranging from 1.5 to 2 eV, consistent with earlier optical
and spectroscopic observations.","['Stefano Veronesi', 'Mario Commodo', 'Luca Basta', 'Gianluigi De Falco', 'Patrizia Minutolo', 'Nikolaos Kateris', 'Hai Wang', ""Andrea D'Anna"", 'Stefan Heun']",[],0,arXiv,http://arxiv.org/abs/2201.01743v1,False,True,False,False,False,588,Siim Soot,Chicago,Completed,2007,2009.0,"This project centers on the journey-to-work data from the American Community Survey to analyze the impact of population growth, especially for special population groups, on demand for transportation services.  Focus is on seasonality in these data and the feasibility/reliability of producing small area estimates (e.g., transportation analysis zones) using these data. The project will produce estimates of social, economic, and demographic differentials among special population groups (specifically those that relate to transportation demand). It will inform the Census Bureau about the seasonality of the underlying data used in these analyses. It examines the feasibility and reliability of producing small area estimates, like transportation analysis zones, that are useful to the transportation planning community. The research focuses on the differences in commuting behavior among the major population groups and how these patterns may change in the long term and the short term through seasonal cycles. These research questions have major implications for decision makers and transportation planners."
"Effect of Precursors and Radiation on Soot Formation in Turbulent
  Diffusion Flame","Soot formation in Delft flame III, a pilot stabilized turbulent diffusion
flame burning natural gas/air, is investigated using ANSYS FLUENT by
considering two different approaches for soot inception. In the first approach
soot inception is based on the formation rate of acetylene, while the second
approach considers the formation rate of two and three-ringed aromatics to
describe the soot inception [1]. Transport equations are solved for soot mass
fraction and radical nuclei concentration to describe inception, coagulation,
surface growth, and oxidation processes. The turbulent-chemistry interactions
and soot precursors are described by the steady laminar flamelet model (SLFM).
Two chemical mechanisms GRI 3.0 [2] and POLIMI [3] are used to represent the
effect of species concentration on soot formation. The radiative properties of
the medium are included based on the non-gray modeling approach by considering
four factious gases; the weighted sum of gray gas (WSGGM) approach is used to
model the absorption coefficient. The effect of soot on radiative transfer is
modeled in terms of effective absorption coefficient of the medium. A beta
probability density function (\b{eta}-PDF) in terms of normalized temperature
is used to describe the effect of turbulence on soot formation. The results
clearly elucidate the strong effect of radiation and species concentration on
soot volume fraction predictions. Due to increase in radiative heat loss with
soot, flame temperature decreases slightly. The inclusion of ethylene has less
synergic effect than that of both benzene and ethylene. Both cases have less
impact on the nucleation of soot. The increase in soot volume fraction with
soot-turbulence interaction is in consistence with the DNS predictions.","['Manedhar Reddy B', 'Ashoke De', 'Rakesh Yadav']",[],0,arXiv,http://arxiv.org/abs/2102.11060v1,False,True,False,False,False,588,Siim Soot,Chicago,Completed,2007,2009.0,"This project centers on the journey-to-work data from the American Community Survey to analyze the impact of population growth, especially for special population groups, on demand for transportation services.  Focus is on seasonality in these data and the feasibility/reliability of producing small area estimates (e.g., transportation analysis zones) using these data. The project will produce estimates of social, economic, and demographic differentials among special population groups (specifically those that relate to transportation demand). It will inform the Census Bureau about the seasonality of the underlying data used in these analyses. It examines the feasibility and reliability of producing small area estimates, like transportation analysis zones, that are useful to the transportation planning community. The research focuses on the differences in commuting behavior among the major population groups and how these patterns may change in the long term and the short term through seasonal cycles. These research questions have major implications for decision makers and transportation planners."
"Soot particle size distribution reconstruction in a turbulent sooting
  flame with the split-based extended quadrature method of moments","The Method of Moments (MOM) has largely been applied to investigate sooting
laminar and turbulent flames. However, the classical MOM is not able to
characterize a continuous particle size distribution (PSD). Without access to
information on the PSD, it is difficult to accurately take into account
particle oxidation, which is crucial for shrinking and eliminating soot
particles. Recently, the Split-based Extended Quadrature Method of Moments
(S-EQMOM) has been proposed as a numerically robust alternative to overcome
this issue (Salenbauch et al., 2019). The main advantage is that a continuous
particle number density function can be reconstructed by superimposing kernel
density functions (KDF). Moreover, the S-EQMOM primary nodes are determined
individually for each KDF, improving the moment realizability.
  In this work, the S-EQMOM is combined with a Large Eddy
Simulation/presumed-PDF flamelet/progress variable approach for predicting soot
formation in the Delft Adelaide Flame III. The target flame features low/high
sooting propensity/intermittency and comprehensive flow/scalar/soot data are
available for model validation. Simulation results are compared with the
experimental data for both the gas phase and the particulate phase. A good
quantitative agreement has been obtained especially in terms of the soot volume
fraction. The reconstructed PSD reveals predominantly unimodal/bimodal
distributions in the first/downstream portion of this flame, with particle
diameters smaller than 100 nm. By investigating the instantaneous and
statistical sooting behavior at the flame tip, it has been found that the
experimentally observed soot intermittency is linked to mixture fraction
fluctuations around its stoichiometric value that exhibit a bimodal probability
density function.","['Federica Ferraro', 'Sandro Gierth', 'Steffen Salenbauch', 'Wang Han', 'Christian Hasse']",[],0,arXiv,http://arxiv.org/abs/2210.02219v2,False,True,False,False,False,588,Siim Soot,Chicago,Completed,2007,2009.0,"This project centers on the journey-to-work data from the American Community Survey to analyze the impact of population growth, especially for special population groups, on demand for transportation services.  Focus is on seasonality in these data and the feasibility/reliability of producing small area estimates (e.g., transportation analysis zones) using these data. The project will produce estimates of social, economic, and demographic differentials among special population groups (specifically those that relate to transportation demand). It will inform the Census Bureau about the seasonality of the underlying data used in these analyses. It examines the feasibility and reliability of producing small area estimates, like transportation analysis zones, that are useful to the transportation planning community. The research focuses on the differences in commuting behavior among the major population groups and how these patterns may change in the long term and the short term through seasonal cycles. These research questions have major implications for decision makers and transportation planners."
Nursing Home Staff Networks and COVID-19,"Nursing homes and other long term-care facilities account for a
disproportionate share of COVID-19 cases and fatalities worldwide. Outbreaks in
U.S. nursing homes have persisted despite nationwide visitor restrictions
beginning in mid-March. An early report issued by the Centers for Disease
Control and Prevention identified staff members working in multiple nursing
homes as a likely source of spread from the Life Care Center in Kirkland,
Washington to other skilled nursing facilities. The full extent of staff
connections between nursing homes---and the crucial role these connections
serve in spreading a highly contagious respiratory infection---is currently
unknown given the lack of centralized data on cross-facility nursing home
employment. In this paper, we perform the first large-scale analysis of nursing
home connections via shared staff using device-level geolocation data from 30
million smartphones, and find that 7 percent of smartphones appearing in a
nursing home also appeared in at least one other facility---even after visitor
restrictions were imposed. We construct network measures of nursing home
connectedness and estimate that nursing homes have, on average, connections
with 15 other facilities. Controlling for demographic and other factors, a
home's staff-network connections and its centrality within the greater network
strongly predict COVID-19 cases. Traditional federal regulatory metrics of
nursing home quality are unimportant in predicting outbreaks, consistent with
recent research. Results suggest that eliminating staff linkages between
nursing homes could reduce COVID-19 infections in nursing homes by 44 percent.","['M. Keith Chen', 'Judith A. Chevalier', 'Elisa F. Long']",[],0,arXiv,http://arxiv.org/abs/2007.11789v2,False,True,False,False,False,589,Orna K Intrator,Boston,Completed,2007,2012.0,"This project examines the relationship between nursing home labor turnover and retention rates and the quality of care provided in nursing homes across the United States and determine how those relation-ships are altered in the face of changes in state mandates affecting nursing staffing or wages. Several Census Bureau datasets provide longitudinally linkable information about staff turnover and retention in all U.S. nursing homes.  Census datasets for the years 1990–2006 are linked with the Online Survey Certification of Automated Reporting (OSCAR) annual data on nursing home structure, staffing and regulatory compliance, facility case mix acuity and resident quality indicators, and a survey of state regulations and initiatives regarding nursing home staffing standards and wages. The purpose of this project is to evaluate the quality of census nursing home data as collected in the economic census and business register; to examine the relationship between nursing home labor turnover, wages, and the quality of care provided in nursing homes; and to deter-mine how those relationships are altered in the face of state legal changes affecting staffing or wage mandates."
"A Latent Survival Analysis Enabled Simulation Platform For Nursing Home
  Staffing Strategy Evaluation","Nursing homes are critical facilities for caring frail older adults with
round-the-clock formal care and personal assistance. To ensure quality care for
nursing home residents, adequate staffing level is of great importance. Current
nursing home staffing practice is mainly based on experience and regulation.
The objective of this paper is to investigate the viability of experience-based
and regulation-based strategies, as well as alternative staffing strategies to
minimize labor costs subject to heterogeneous service demand of nursing home
residents under various scenarios of census. We propose a data-driven analysis
framework to model heterogeneous service demand of nursing home residents and
further identify appropriate staffing strategies by combing survival model and
computer simulation techniques as well as domain knowledge. Specifically, in
the analysis, we develop an agent-based simulation tool consisting of four main
modules, namely individual length of stay predictor, individual daily staff
time generator, facility level staffing strategy evaluator, and graphical user
interface. We use real nursing home data to validate the proposed model, and
demonstrate that the identified staffing strategy significantly reduces the
total labor cost of certified nursing assistants compared to the benchmark
strategies. Additionally, the proposed length of stay predictive model that
considers multiple discharge dispositions exhibits superior accuracy and offers
better staffing decisions than those without the consideration. Further, we
construct different census scenarios of nursing home residents to demonstrate
the capability of the proposed framework in helping adjust staffing decisions
of nursing home administrators in various realistic settings.","['Xuxue Sun', 'Nan Kong', 'Nazmus Sakib', 'Chao Meng', 'Kathryn Hyer', 'Hongdao Meng', 'Chris Masterson', 'Mingyang Li']",[],0,arXiv,http://arxiv.org/abs/2101.03254v2,False,True,False,False,False,589,Orna K Intrator,Boston,Completed,2007,2012.0,"This project examines the relationship between nursing home labor turnover and retention rates and the quality of care provided in nursing homes across the United States and determine how those relation-ships are altered in the face of changes in state mandates affecting nursing staffing or wages. Several Census Bureau datasets provide longitudinally linkable information about staff turnover and retention in all U.S. nursing homes.  Census datasets for the years 1990–2006 are linked with the Online Survey Certification of Automated Reporting (OSCAR) annual data on nursing home structure, staffing and regulatory compliance, facility case mix acuity and resident quality indicators, and a survey of state regulations and initiatives regarding nursing home staffing standards and wages. The purpose of this project is to evaluate the quality of census nursing home data as collected in the economic census and business register; to examine the relationship between nursing home labor turnover, wages, and the quality of care provided in nursing homes; and to deter-mine how those relationships are altered in the face of state legal changes affecting staffing or wage mandates."
Status of the Cylindical-GEM project for the KLOE-2 Inner Tracker,"The status of the R&D on the Cylindrical-GEM (CGEM) detector foreseen as
Inner Tracker for KLOE-2, the upgrade of the KLOE experiment at the DAFNE
phi-factory, will be presented. The R&D includes several activities: i) the
construction and complete characterization of the full-size CGEM prototype,
equipped with 650 microns pitch 1-D longitudinal strips; ii) the study of the
2-D readout with XV patterned strips and operation in magnetic field (up to
1.5T), performed with small planar prototypes in a dedicated test at the H4-SPS
beam facility; iii) the characterization of the single-mask GEM technology for
the realization of large-area GEM foils.","['A. Balla', 'G. Bencivenni', 'S. Cerioni', 'P. Ciambrone', 'E. De Lucia', 'G. De Robertis', 'D. Domenici', 'G. Felici', 'M. Gatta', 'M. Jacewicz', 'N. Lacalamita', 'S. Lauciani', 'R. Liuzzi', 'F. Loddo', 'M. Mongelli', 'G. Morello', 'A. Pelosi', 'M. Pistilli', 'L. Quintieri', 'A. Ranieri', 'V. Valentino']",[],0,arXiv,http://arxiv.org/abs/1003.3770v1,False,True,False,False,False,590,Lucia S Foster,Washington,Completed,2006,2007.0,The project will examine total costs incurred for research and development (by Federal and by company) from the RD-1 survey for the entire time period available (1972-latest year available). The expenditure data will be used for two purposes: (1) to improve the industry performed R&D capital stock and (2) to generate a government funded R&D capital stock. The stocks will be included as variables in an industry-level translog variable cost function estimation. The returns to total R&D investment and the returns to capital stock of government funded R&D will be estimated. 
"Are deep learning models superior for missing data imputation in large
  surveys? Evidence from an empirical comparison","Multiple imputation (MI) is a popular approach for dealing with missing data
arising from non-response in sample surveys. Multiple imputation by chained
equations (MICE) is one of the most widely used MI algorithms for multivariate
data, but it lacks theoretical foundation and is computationally intensive.
Recently, missing data imputation methods based on deep learning models have
been developed with encouraging results in small studies. However, there has
been limited research on evaluating their performance in realistic settings
compared to MICE, particularly in big surveys. We conduct extensive simulation
studies based on a subsample of the American Community Survey to compare the
repeated sampling properties of four machine learning based MI methods: MICE
with classification trees, MICE with random forests, generative adversarial
imputation networks, and multiple imputation using denoising autoencoders. We
find the deep learning imputation methods are superior to MICE in terms of
computational time. However, with the default choice of hyperparameters in the
common software packages, MICE with classification trees consistently
outperforms, often by a large margin, the deep learning imputation methods in
terms of bias, mean squared error, and coverage under a range of realistic
settings.","['Zhenhua Wang', 'Olanrewaju Akande', 'Jason Poulos', 'Fan Li']",[],0,arXiv,http://arxiv.org/abs/2103.09316v3,False,False,False,False,True,592,Adriana Perez,Washington,Completed,2006,2009.0,"The evaluation of the effect on estimates after imputation techniques have been applied and accounting for its uncertainty is an important enterprise in any survey. This research project seeks to carry out an in-depth evaluation of the effect of the current implemented imputation techniques in the annual Survey of Industrial Research and Development (SIRD).  Since the early 1990s, there has not been a systematic evaluation of the current imputation procedures and their effects on survey estimates. The purpose of this project is to evaluate and recommend improvements to the current imputation methods in the SIRD. Specifically, we will use the 1999-2003 SIRD datasets at the Center for Economic Studies to evaluate the effects of current imputation methods on survey estimates in the SIRD. This project has three aims: (1) to describe the current imputation methods currently used in the SIRD; (2) to evaluate the effectiveness of the current imputation methods through precision and accuracy measures; and (3) to compare current imputation methods with alter-native imputation methods and formulate recommendations for improvement. The overall goal is to assess the effect of the imputation techniques on the quality of this survey data, including variance estimates. Simulations will be carried out using standard precision and accuracy measures (bias, variance, and mean square error) for evaluating the current imputation methods. Multivariate distributional patterns of missing-ness will be described during implementation of simulations. Sensitivity analysis will be con-ducted to describe worst and best case scenarios on departures from current observed percentages of missingness."
"An Empirical Comparison of Multiple Imputation Methods for Categorical
  Data","Multiple imputation is a common approach for dealing with missing values in
statistical databases. The imputer fills in missing values with draws from
predictive models estimated from the observed data, resulting in multiple,
completed versions of the database. Researchers have developed a variety of
default routines to implement multiple imputation; however, there has been
limited research comparing the performance of these methods, particularly for
categorical data. We use simulation studies to compare repeated sampling
properties of three default multiple imputation methods for categorical data,
including chained equations using generalized linear models, chained equations
using classification and regression trees, and a fully Bayesian joint
distribution based on Dirichlet Process mixture models. We base the simulations
on categorical data from the American Community Survey. In the circumstances of
this study, the results suggest that default chained equations approaches based
on generalized linear models are dominated by the default regression tree and
Bayesian mixture model approaches. They also suggest competing advantages for
the regression tree and Bayesian mixture model approaches, making both
reasonable default engines for multiple imputation of categorical data. A
supplementary material for this article is available online.","['Olanrewaju Akande', 'Fan Li', 'Jerome Reiter']",[],0,arXiv,http://arxiv.org/abs/1508.05918v2,False,False,False,False,True,592,Adriana Perez,Washington,Completed,2006,2009.0,"The evaluation of the effect on estimates after imputation techniques have been applied and accounting for its uncertainty is an important enterprise in any survey. This research project seeks to carry out an in-depth evaluation of the effect of the current implemented imputation techniques in the annual Survey of Industrial Research and Development (SIRD).  Since the early 1990s, there has not been a systematic evaluation of the current imputation procedures and their effects on survey estimates. The purpose of this project is to evaluate and recommend improvements to the current imputation methods in the SIRD. Specifically, we will use the 1999-2003 SIRD datasets at the Center for Economic Studies to evaluate the effects of current imputation methods on survey estimates in the SIRD. This project has three aims: (1) to describe the current imputation methods currently used in the SIRD; (2) to evaluate the effectiveness of the current imputation methods through precision and accuracy measures; and (3) to compare current imputation methods with alter-native imputation methods and formulate recommendations for improvement. The overall goal is to assess the effect of the imputation techniques on the quality of this survey data, including variance estimates. Simulations will be carried out using standard precision and accuracy measures (bias, variance, and mean square error) for evaluating the current imputation methods. Multivariate distributional patterns of missing-ness will be described during implementation of simulations. Sensitivity analysis will be con-ducted to describe worst and best case scenarios on departures from current observed percentages of missingness."
"Viability of Mobile Forms for Population Health Surveys in Low Resource
  Areas","Population health surveys are an important tool to effectively allocate
limited resources in low resource communities. In such an environment, surveys
are often done by local population with pen and paper. Data thus collected is
difficult to tabulate and analyze. We conducted a series of interviews and
experiments in the Philippines to assess if mobile forms can be a viable and
more efficient survey method. We first conducted pilot interviews and found 60%
of the local surveyors actually preferred mobile forms over paper. We then
built a software that can generate mobile forms that are easy to use, capable
of working offline, and able to track key metrics such as time to complete
questions. Our mobile form was field tested in three locations in the
Philippines with 33 surveyors collecting health survey responses from 266
subjects. The percentage of surveyors preferring mobile forms increased to 76%
after just using the form a few times. The results demonstrate our mobile form
is a viable method to conduct large scale population health surveys in a low
resource environment.","['Alexander Davis', 'Aidan Chen', 'Milton Chen', 'James Davis']",[],0,arXiv,http://arxiv.org/abs/2310.07888v1,False,True,False,False,False,595,Lucas W Davis,Michigan,Completed,2007,2012.0,"This project describes and estimates a model of neighborhood choice in which environmental health risks vary across neighborhoods. The model is estimated using household-level data from a restricted version of the U.S. Decennial Census 1990 and 2000. The analysis focuses on neighborhoods near waste incinerators, coal-burning power plants, nuclear power plants, and other facilities. The empirical strategy exploits the opening and closing of these facilities to control for unobserved differences across neighbor-hoods. The first objective of this project is to generate new estimates about the causal impact of environmental health risks on geographic mobility and home values, with particular emphasis on patterns for different ethnic and racial groups. A second objective is to assess the environmental-related questions in the American Housing Survey (AHS) and to make available a database of facilities that can be merged with the AHS."
Residential income segregation: A behavioral model of the housing market,"We represent the functioning of the housing market and study the relation
between income segregation, income inequality and house prices by introducing a
spatial Agent-Based Model (ABM). Differently from traditional models in urban
economics, we explicitly specify the behavior of buyers and sellers and the
price formation mechanism. Buyers who differ by income select among
heterogeneous neighborhoods using a probabilistic model of residential choice;
sellers employ an aspiration level heuristic to set their reservation offer
price; prices are determined through a continuous double auction. We first
provide an approximate analytical solution of the ABM, shedding light on the
structure of the model and on the effect of the parameters. We then simulate
the ABM and find that: (i) a more unequal income distribution lowers the prices
globally, but implies stronger segregation; (ii) a spike of the demand in one
part of the city increases the prices all over the city; (iii) subsidies are
more efficient than taxes in fostering social mixing.","['Marco Pangallo', 'Jean Pierre Nadal', 'Annick Vignes']",[],0,arXiv,http://arxiv.org/abs/1606.00424v3,False,True,False,False,False,596,Jeffrey E Zabel,Boston,Completed,2006,2006.0,"The increase in income inequality in the 1990s, among other reasons, has led to a new focus on measuring the impact of social effects on economic behavior. One important component of social effects is the impact of one's place of residence, or neighborhood effects. We propose to extend earlier work (Ioannides and Zabel 1999) to include neighborhood choice in our model of housing demand with neighborhood effects. We will model the choice of community via the choice of census tract and neighborhood (cluster) of residence. This requires that we have knowledge of the census tract in which each household in the AHS resides. This is an important component of our analysis of neighborhood effects since the choice of neighborhood is not random and hence estimation of these effects should take this nonrandom choice into consideration."
The William Kruskal Legacy: 1919--2005,"William Kruskal (Bill) was a distinguished statistician who spent virtually
his entire professional career at the University of Chicago, and who had a
lasting impact on the Institute of Mathematical Statistics and on the field of
statistics more broadly, as well as on many who came in contact with him. Bill
passed away last April following an extended illness, and on May 19, 2005, the
University of Chicago held a memorial service at which several of Bill's
colleagues and collaborators spoke along with members of his family and other
friends. This biography and the accompanying commentaries derive in part from
brief presentations on that occasion, along with recollections and input from
several others. Bill was known personally to most of an older generation of
statisticians as an editor and as an intellectual and professional leader. In
1994, Statistical Science published an interview by Sandy Zabell (Vol. 9,
285--303) in which Bill looked back on selected events in his professional
life. One of the purposes of the present biography and accompanying
commentaries is to reintroduce him to old friends and to introduce him for the
first time to new generations of statisticians who never had an opportunity to
interact with him and to fall under his influence.","['Stephen E. Fienberg', 'Stephen M. Stigler', 'Judith M. Tanur']",[],0,arXiv,http://arxiv.org/abs/0710.5063v1,False,True,False,False,False,596,Jeffrey E Zabel,Boston,Completed,2006,2006.0,"The increase in income inequality in the 1990s, among other reasons, has led to a new focus on measuring the impact of social effects on economic behavior. One important component of social effects is the impact of one's place of residence, or neighborhood effects. We propose to extend earlier work (Ioannides and Zabel 1999) to include neighborhood choice in our model of housing demand with neighborhood effects. We will model the choice of community via the choice of census tract and neighborhood (cluster) of residence. This requires that we have knowledge of the census tract in which each household in the AHS resides. This is an important component of our analysis of neighborhood effects since the choice of neighborhood is not random and hence estimation of these effects should take this nonrandom choice into consideration."
Endogenous Growth Under Multiple Uses of Data,"We model a dynamic data economy with fully endogenous growth where agents
generate data from consumption and share them with innovation and production
firms. Different from other productive factors such as labor or capital, data
are nonrival in their uses across sectors which affect both the level and
growth of economic outputs. Despite the vertical nonrivalry, the innovation
sector dominates the production sector in data usage and contribution to growth
because (i) data are dynamically nonrival and add to knowledge accumulation,
and (ii) innovations ""desensitize"" raw data and enter production as knowledge,
which allays consumers' privacy concerns. Data uses in both sectors interact to
generate spillover of allocative distortion and exhibit an apparent
substitutability due to labor's rivalry and complementarity with data.
Consequently, growth rates under a social planner and a decentralized
equilibrium differ, which is novel in the literature and has policy
implications. Specifically, consumers' failure to fully internalize knowledge
spillover when bearing privacy costs, combined with firms' market power,
underprice data and inefficiently limit their supply, leading to
underemployment in the innovation sector and a suboptimal long-run growth.
Improving data usage efficiency is ineffective in mitigating the
underutilization of data, but interventions in the data market and direct
subsidies hold promises.","['Lin William Cong', 'Wenshi Wei', 'Danxia Xie', 'Longtian Zhang']",[],0,arXiv,http://arxiv.org/abs/2109.10027v1,False,True,False,False,False,598,William R Kerr,Boston,Completed,2007,2012.0,"This study characterizes the innovative and entrepreneurial efforts of firms in the U.S. economy. It begins with a detailed analysis of the R&D-to-patenting inventive process and further delineates how this innovation translates into within-firm productivity growth and across-firm knowledge spillovers. It also considers how U.S. national and local governments influence these rates of innovation and entrepreneurship and firm entry more generally. These innovative forces will be finally linked to concomitant technological change, productivity growth, and changes in industrial structure. Specific topics addressed include immigration admissions of foreign-born scientists and engineers, labor market regulations, federal funding of R&D undertaken in private firms, government-backed loans to entrepreneurs and small businesses, banking and financial market structures and regulations, foreign direct investments by multinationals, and patent and trademark laws. Detailed firm-level and establishment-level data are employed to pair a firm’s R&D and patenting efforts with its productivity out-comes, for considering federal support of R&D at the firm-level, and for looking at knowledge spillovers from a research-oriented firm to other businesses within the firm’s state or industry. Establishment data characterize industrial and financial market structures in local areas, including entrepreneurial entry and exit rates, the firm size distribution, and market concentration and agglomeration indexes."
Innovation and informal knowledge exchanges between firms,"Firm clusters are seen as having a positive effect on innovations, what can
be interpreted as economies of scale or knowledge spillovers. The processes
underlying the success of these clusters remain difficult to isolate. We
propose in this paper a stylised agent-based model to test the role of
geographical proximity and informal knowledge exchanges between firms on the
emergence of innovations. The model is run on synthetic firm clusters.
Sensitivity analysis and systematic model exploration unveil a strong impact of
interaction distance on innovations, with a qualitative shift when spatial
interactions are more intense. Model bi-objective optimisation shows a
compromise between innovation and product diversity, suggesting trade-offs for
clusters in practice. This model provides thus a first basis to systematically
explore the interplay between firm cluster geography and innovation, from an
evolutionary perspective.",['Juste Raimbault'],[],0,arXiv,http://arxiv.org/abs/2208.14719v1,False,True,False,False,False,598,William R Kerr,Boston,Completed,2007,2012.0,"This study characterizes the innovative and entrepreneurial efforts of firms in the U.S. economy. It begins with a detailed analysis of the R&D-to-patenting inventive process and further delineates how this innovation translates into within-firm productivity growth and across-firm knowledge spillovers. It also considers how U.S. national and local governments influence these rates of innovation and entrepreneurship and firm entry more generally. These innovative forces will be finally linked to concomitant technological change, productivity growth, and changes in industrial structure. Specific topics addressed include immigration admissions of foreign-born scientists and engineers, labor market regulations, federal funding of R&D undertaken in private firms, government-backed loans to entrepreneurs and small businesses, banking and financial market structures and regulations, foreign direct investments by multinationals, and patent and trademark laws. Detailed firm-level and establishment-level data are employed to pair a firm’s R&D and patenting efforts with its productivity out-comes, for considering federal support of R&D at the firm-level, and for looking at knowledge spillovers from a research-oriented firm to other businesses within the firm’s state or industry. Establishment data characterize industrial and financial market structures in local areas, including entrepreneurial entry and exit rates, the firm size distribution, and market concentration and agglomeration indexes."
"Janus monolayer TaNF: a new ferrovalley material with large valley
  splitting and tunable magnetic properties","Materials with large intrinsic valley splitting and high Curie temperature
are a huge advantage for studying valleytronics and practical applications. In
this work, using first-principles calculations, a new Janus TaNF monolayer is
predicted to exhibit excellent piezoelectric properties and intrinsic valley
splitting, resulting from the spontaneous spin polarization, the spatial
inversion symmetry breaking and strong spin-orbit coupling (SOC). TaNF is also
a potential two-dimensional (2D) magnetic material due to its high Curie
temperature and huge magnetic anisotropy energy. The effective control of the
band gap of TaNF can be achieved by biaxial strain, which can transform TaNF
monolayer from semiconductor to semi-metal. The magnitude of valley splitting
at the CBM can be effectively tuned by biaxial strain due to the changes of
orbital composition at the valleys. The magnetic anisotropy energy (MAE) can be
manipulated by changing the energy and occupation (unoccupation) states of d
orbital compositions through biaxial strain. In addition, Curie temperature
reaches 373 K under only -3% biaxial strain, indicating that Janus TaNF
monolayer can be used at high temperatures for spintronic and valleytronic
devices.","['Guibo Zheng', 'Shuixian Qu', 'Wenzhe Zhou', 'Fangping Ouyang']",[],0,arXiv,http://arxiv.org/abs/2304.05670v1,False,True,False,False,False,606,Allison G Harris,Chicago,Completed,2006,2010.0,"The disparity between persons enrolled in U.S. federal poverty programs and persons who respond to U.S. Census Bureau surveys saying they are enrolled appears to be systemic across programs. This research will analyze the child care subsidy (CCS) take-up decision and a range of employment and welfare outcomes among all low-income families in Illinois, Maryland, and Texas. This project will improve the Census Bureau’s understanding of who uses the child care subsidy and how the subsidy aids different groups of low-income families in their quest for economic independence. The groups we distinguish are those who are currently receiving cash assistance through the Temporary Assistance for Needy Families (TANF) program, those who have recently left TANF, and those who have had no recent contact with the TANF program (frequently referred to as the working poor).  This proposed research will further benefit the Census Bureau’s data programs by prototyping an eligibility microsimulation model for a specific federal poverty program (the Child Care and Development Fund in this case) that can be tailored for other programs. Since federal poverty programs are dependent on current surveys for program administration and program size estimates, the quality of the surveys is of great interest to the Census Bureau as well as to federal poverty programs. The American Community Survey, while not providing significant detail on program utilization, has a large sample size, thus affording an opportunity to use it in concert with other smaller more detailed surveys, like the CPS and SIPP, to improve eligibility modeling. The primary dataset to be used for analysis, the Social Services Analysis File (SSAF), is an output of an internal Census Bureau project (TANF/Child Care Subsidy Research)."
Mathematical Modeling of Competition in Sponsored Search Market,"Sponsored search mechanisms have drawn much attention from both academic
community and industry in recent years since the seminal papers of [13] and
[14]. However, most of the existing literature concentrates on the mechanism
design and analysis within the scope of only one search engine in the market.
In this paper we propose a mathematical framework for modeling the interaction
of publishers, advertisers and end users in a competitive market. We first
consider the monopoly market model and provide optimal solutions for both ex
ante and ex post cases, which represents the long-term and short-term revenues
of search engines respectively. We then analyze the strategic behaviors of end
users and advertisers under duopoly and prove the existence of equilibrium for
both search engines to co-exist from ex-post perspective. To show the more
general ex ante results, we carry out extensive simulations under different
parameter settings. Our analysis and observation in this work can provide
useful insight in regulating the sponsored search market and protecting the
interests of advertisers and end users.","['Jian Liu', 'Dah Ming Chiu']",[],0,arXiv,http://arxiv.org/abs/1006.1019v2,False,True,False,False,False,607,Alvin J Silk,Boston,Completed,2007,2012.0,"This research aims to improve understanding of the quality of data on advertising agencies collected by the Census Bureau; assess the importance of scale and scope economies in the supply of advertising and marketing service industries; explore the relation of such economies to the overall organization of this industry in terms of the distribution of revenue and employment among single and multiestablishment firms and holding companies; examine employment turnover and advertising firm entry, survival, and exit patterns over time; and document the geographical distribution of advertising agencies and their economic activities. Census data are linked to external data on advertising agencies, specifically the Advertising Red Book.  The project compares census data coverage with publicly available data on advertising agencies.  The project will produce estimates of the population of advertising establishments, which will inform the Census Bureau’s knowledge base on the extent of scale and scope economies in the supply of advertising and marketing service industries and the relation of such economies to the overall organizational structure of firms in this industry. It will examine entry, exit, and acquisitions at various levels of aggregation—individual establishment, the multi-establishment advertising agency, and the global holding companies. And it will investigate the role of mergers and acquisitions on the extent of outsourcing and measurement of output, price, average labor productivity, and labor turnover."
Aligning Software-related Strategies in Multi-Organizational Settings,"Aligning the activities of an organization with its business goals is a
challenging task that is critical for success. Alignment in a
multi-organizational setting requires the integration of different internal or
external organizational units. The anticipated benefits of multi-organizational
alignment consist of clarified contributions and increased transparency of the
involved organizational units. The GQM+Strategies approach provides mechanisms
for explicitly linking goals and strategies within an organization and is based
on goal-oriented measurement. This paper presents the process and first-hand
experience of applying GQM+Strategies in a multi-organizational setting from
the aerospace industry. Additionally, the resulting GQM+Strategies grid is
sketched and selected parts are discussed. Finally, the results are reflected
on and an overview of future work is given.","['Martin Kowalczyk', 'Jürgen Münch', 'Masafumi Katahira', 'Tatsuya Kaneko', 'Yuko Miyamoto', 'Yumi Koishi']",[],0,arXiv,http://arxiv.org/abs/1401.1910v1,False,True,False,False,False,610,Gordon M Phillips,Washington,Completed,2007,2011.0,"This project studies innovation and research and development in the spirit of Schumpeter where innovation and research and development (R&D) are part of the process of creative destruction in the economy. Using data on both public and private firms, it examines how firm organization changes following innovation that is initiated by itself, or by firms in its own upstream and downstream industries. It also studies the impact of firm organization on the exploitation of R&D expenditures and patents. The central premise of this study is that organizations adjust to industry technological change as both additional research and the development of ideas after patents require extensive organizational and financial resources. It focuses on two types of organizational adaptations: the decision to be private or publicly listed and changes in firm boundaries through mergers and acquisitions. Firms differ in the extent of their operations across multiple industries. Some firms choose to be focused and produce in single industries. Other firms produce in multiple industries. The central question is the extent that these two dimensions are significant in explaining which firms conduct and then commercialize research and patent activity. This project will provide evidence on the importance of research and the development of that research in determining the boundaries of the firm by examining three different aspects of innovative activity. These include patent activity, patent citations, and R&D expenditures."
"A novel association and ranking approach identifies factors affecting
  educational outcomes of STEM majors","Improving undergraduate success in STEM requires identifying actionable
factors that impact student outcomes, allowing institutions to prioritize key
leverage points for change. We examined academic, demographic, and
institutional factors that might be associated with graduation rates at two
four-year colleges in the northeastern United States using a novel association
algorithm called D-basis to rank attributes associated with graduation.
Importantly, the data analyzed included tracking data from the National Student
Clearinghouse on students who left their original institutions to determine
outcomes following transfer.
  Key predictors of successful graduation include performance in introductory
STEM courses, the choice of first mathematics class, and flexibility in major
selection. High grades in introductory biology, general chemistry, and
mathematics courses were strongly correlated with graduation. At the same time,
students who switched majors - especially from STEM to non-STEM - had higher
overall graduation rates. Additionally, Pell eligibility and demographic
factors, though less predictive overall, revealed disparities in time to
graduation and retention rates.
  The findings highlight the importance of early academic support in STEM
gateway courses and the implementation of institutional policies that provide
flexibility in major selection. Enhancing student success in introductory
mathematics, biology, and chemistry courses could greatly influence graduation
rates. Furthermore, customized mathematics pathways and focused support for
STEM courses may assist institutions in optimizing student outcomes. This study
offers data-driven insights to guide strategies to increase STEM degree
completion.","['Kira Adaricheva', 'Jonathan T. Brockman', 'Gillian Z. Elston', 'Lawrence Hobbie', 'Skylar Homan', 'Mohamad Khalefa', 'Jiyun V. Kim', 'Rochelle K. Nelson', 'Sarah Samad', 'Oren Segal']",[],0,arXiv,http://arxiv.org/abs/2503.12321v1,False,True,False,False,False,611,Abigail K Wozniak,Chicago,Completed,2011,2014.0,"This project uses detailed longitudinal information from all waves of the National Longitudinal Surveys to examine lifecycle migration patterns across education and gender groups.  It extends earlier work examining the causal role of a college education in subsequent geographic mobility to answer the important questions of why and how college going increases long distance mobility.  Educational differences in migration primarily occur between the college educated and everyone else.  The project studies gender differences in lifetime migration patterns, particularly the manner in which these have evolved across cohorts.  Migration patterns for women may have changed along with the dramatic increase in education and labor force participation that women experience over the latter half of the twentieth century.  The research involves two stages. The first employs longitudinal data to construct complete migration histories of a representative sample of U.S. residents. The second stage builds on the first, using information in the migration histories to test ideas about which mechanisms explain the different rates of long distance moves across education and gender groups."
Occupational mobility network of the Romanian higher education graduates,"Although there is a rich literature on the rate of occupational mobility,
there are important gaps in understanding patterns of movement among
occupations. We employ a network based approach to explore occupational
mobility of the Romanian university graduates in the first years after
graduation (2003 - 2008). We use survey data on their career mobility to build
an empirical occupational mobility network (OMN) that covers all their job
movements in the considered period. We construct the network as directed and
weighted. The nodes are represented by the occupations (post coded at 3 digits
according to ISCO-88) and the links are weighted with the number of persons
switching from one occupation to another. This representation of data permits
us to use the novel statistical techniques developed in the framework of
weighted directed networks in order to extract a set of stylized facts that
highlight patterns of occupational mobility: centrality, network motifs.","['Eliza-Olivia Lungu', 'Ana-Maria Zamfir', 'Eva Militaru', 'Cristina Mocanu']",[],0,arXiv,http://arxiv.org/abs/1202.0404v1,False,True,False,False,False,611,Abigail K Wozniak,Chicago,Completed,2011,2014.0,"This project uses detailed longitudinal information from all waves of the National Longitudinal Surveys to examine lifecycle migration patterns across education and gender groups.  It extends earlier work examining the causal role of a college education in subsequent geographic mobility to answer the important questions of why and how college going increases long distance mobility.  Educational differences in migration primarily occur between the college educated and everyone else.  The project studies gender differences in lifetime migration patterns, particularly the manner in which these have evolved across cohorts.  Migration patterns for women may have changed along with the dramatic increase in education and labor force participation that women experience over the latter half of the twentieth century.  The research involves two stages. The first employs longitudinal data to construct complete migration histories of a representative sample of U.S. residents. The second stage builds on the first, using information in the migration histories to test ideas about which mechanisms explain the different rates of long distance moves across education and gender groups."
"Improving On-Time Undergraduate Graduation Rate For Undergraduate
  Students Using Predictive Analytics","The on-time graduation rate among universities in Puerto Rico is
significantly lower than in the mainland United States. This problem is
noteworthy because it leads to substantial negative consequences for the
student, both socially and economically, the educational institution and the
local economy. This project aims to develop a predictive model that accurately
detects students early in their academic pursuit at risk of not graduating on
time. Various predictive models are developed to do this, and the best model,
the one with the highest performance, is selected. Using a dataset containing
information from 24432 undergraduate students at the University of Puerto Rico
at Mayaguez, the predictive performance of the models is evaluated in two
scenarios: Group I includes both the first year of college and pre-college
factors, and Group II only considers pre-college factors. Overall, for both
scenarios, the boosting model, trained on the oversampled dataset, is the most
successful at predicting who will not graduate on time.","['Ramineh Lopez-Yazdani', 'Roberto Rivera']",[],0,arXiv,http://arxiv.org/abs/2407.10253v1,False,True,False,False,False,611,Abigail K Wozniak,Chicago,Completed,2011,2014.0,"This project uses detailed longitudinal information from all waves of the National Longitudinal Surveys to examine lifecycle migration patterns across education and gender groups.  It extends earlier work examining the causal role of a college education in subsequent geographic mobility to answer the important questions of why and how college going increases long distance mobility.  Educational differences in migration primarily occur between the college educated and everyone else.  The project studies gender differences in lifetime migration patterns, particularly the manner in which these have evolved across cohorts.  Migration patterns for women may have changed along with the dramatic increase in education and labor force participation that women experience over the latter half of the twentieth century.  The research involves two stages. The first employs longitudinal data to construct complete migration histories of a representative sample of U.S. residents. The second stage builds on the first, using information in the migration histories to test ideas about which mechanisms explain the different rates of long distance moves across education and gender groups."
"Modelling an electricity market oligopoly with a competitive fringe and
  generation investments","Market power behaviour often occurs in modern wholesale electricity markets.
Mixed Complementarity Problems (MCPs) have been typically used for
computational modelling of market power when it is characterised by an
oligopoly with competitive fringe. However, such models can lead to myopic and
contradictory behaviour. Previous works in the literature have suggested using
conjectural variations to overcome this modelling issue. We first show however,
that an oligopoly with competitive fringe where all firms have investment
decisions, will also lead to myopic and contradictory behaviour when modelled
using conjectural variations. Consequently, we develop an Equilibrium Problem
with Equilibrium Constraints (EPEC) to model such an electricity market
structure. The EPEC models two types of players: price-making firms, who have
market power, and price-taking firms, who do not. In addition to generation
decisions, all firms have endogenous investment decisions for multiple new
generating technologies. The results indicate that, when modelling an oligopoly
with a competitive fringe and generation investment decisions, an EPEC model
can represent a more realistic market structure and overcome the myopic
behaviour observed in MCPs. The EPEC considered found multiple equilibria for
investment decisions and firms' profits. However, market prices and consumer
costs were found to remain relatively constant across the equilibria. In
addition, the model shows how it may be optimal for price-making firms to
occasionally sell some of their electricity below marginal cost in order to
de-incentivize price-taking firms from investing further into the market. Such
strategic behaviour would not be captured by MCP or cost-minimisation models.","['Mel T. Devine', 'Sauleh Siddiqui']",[],0,arXiv,http://arxiv.org/abs/2001.03526v1,False,True,False,False,False,615,Kristina S Steffenson McElheran,Chicago,Completed,2007,2013.0,"This research project links recent survey data on the digital economy with other Census Bureau datasets and with proprietary data to understand how productivity, supply chain structures, and investment in information technology (IT) are coevolving in the U.S. manufacturing sector.  It investigates the drivers of firm IT investment that are likely to be endogenous in standard models of how IT affects firm behavior. The primary outcome of this research and main benefit will be more accurate and more-nuanced estimates of firm populations that invest in IT, as well as insights into the characteristics and distributions of these different subpopulations in the U.S. manufacturing sector. Another central outcome will be an assessment of the quality and consistency of existing census data on the digital economy. Directly linking and comparing datasets, as well as improving the Census Bureau’s under-standing of the interaction between IT, the infrastructure needed to support e-business, and evolving supply chain relationships in the U.S. economy, will reveal observable patterns in nonresponse, highlight inconsistencies across surveys and years, verify the stability of important empirical relation-ships across time, and suggest ways to improve future surveys."
Strategic Formation and Reliability of Supply Chain Networks,"Supply chains are the backbone of the global economy. Disruptions to them can
be costly. Centrally managed supply chains invest in ensuring their resilience.
Decentralized supply chains, however, must rely upon the self-interest of their
individual components to maintain the resilience of the entire chain.
  We examine the incentives that independent self-interested agents have in
forming a resilient supply chain network in the face of production disruptions
and competition. In our model, competing suppliers are subject to yield
uncertainty (they deliver less than ordered) and congestion (lead time
uncertainty or, ""soft"" supply caps). Competing retailers must decide which
suppliers to link to based on both price and reliability. In the presence of
yield uncertainty only, the resulting supply chain networks are sparse.
Retailers concentrate their links on a single supplier, counter to the idea
that they should mitigate yield uncertainty by diversifying their supply base.
This happens because retailers benefit from supply variance. It suggests that
competition will amplify output uncertainty. When congestion is included as
well, the resulting networks are denser and resemble the bipartite expander
graphs that have been proposed in the supply chain literature, thereby,
providing the first example of endogenous formation of resilient supply chain
networks, without resilience being explicitly encoded in payoffs. Finally, we
show that a supplier's investments in improved yield can make it worse off.
This happens because high production output saturates the market, which, in
turn lowers prices and profits for participants.","['Victor Amelkin', 'Rakesh Vohra']",[],0,arXiv,http://arxiv.org/abs/1909.08021v2,False,True,False,False,False,615,Kristina S Steffenson McElheran,Chicago,Completed,2007,2013.0,"This research project links recent survey data on the digital economy with other Census Bureau datasets and with proprietary data to understand how productivity, supply chain structures, and investment in information technology (IT) are coevolving in the U.S. manufacturing sector.  It investigates the drivers of firm IT investment that are likely to be endogenous in standard models of how IT affects firm behavior. The primary outcome of this research and main benefit will be more accurate and more-nuanced estimates of firm populations that invest in IT, as well as insights into the characteristics and distributions of these different subpopulations in the U.S. manufacturing sector. Another central outcome will be an assessment of the quality and consistency of existing census data on the digital economy. Directly linking and comparing datasets, as well as improving the Census Bureau’s under-standing of the interaction between IT, the infrastructure needed to support e-business, and evolving supply chain relationships in the U.S. economy, will reveal observable patterns in nonresponse, highlight inconsistencies across surveys and years, verify the stability of important empirical relation-ships across time, and suggest ways to improve future surveys."
"Deep Learning Framework for Measuring the Digital Strategy of Companies
  from Earnings Calls","Companies today are racing to leverage the latest digital technologies, such
as artificial intelligence, blockchain, and cloud computing. However, many
companies report that their strategies did not achieve the anticipated business
results. This study is the first to apply state of the art NLP models on
unstructured data to understand the different clusters of digital strategy
patterns that companies are Adopting. We achieve this by analyzing earnings
calls from Fortune Global 500 companies between 2015 and 2019. We use
Transformer based architecture for text classification which show a better
understanding of the conversation context. We then investigate digital strategy
patterns by applying clustering analysis. Our findings suggest that Fortune 500
companies use four distinct strategies which are product led, customer
experience led, service led, and efficiency led. This work provides an
empirical baseline for companies and researchers to enhance our understanding
of the field.","['Ahmed Ghanim Al-Ali', 'Robert Phaal', 'Donald Sull']",[],0,arXiv,http://arxiv.org/abs/2010.12418v2,False,True,False,False,False,621,Thomas Hubbard,Chicago,Completed,2007,2010.0,"Wage inequality increased substantially in the United States during the past quarter century. The sources of this increase and its public policy implications have been controversial, both within academia and among policy makers. Much of the debate has surrounded whether this increase was due to technological factors, such as the diffusion of information technology, or to policy changes, such as reductions in the minimum wage (in real terms).  Economists have proposed that organizational structure affects wage inequality and can amplify the effect of technological factors, especially in contexts where production is human-capital intensive. Understanding what affects wage inequality in human-capital-intensive sectors is particularly important because these sectors occupy a high and growing share of U.S. economy and because many government policies aimed at raising wages at the low end do so by increasing these workers’ human capital. If wages are affected not just by individuals’ human capital, but the organizational structure in which individuals’ work, one can make these policies more productive by applying them in organizational contexts where they are likely to have the greatest impact on wages. This proposal examines the quality of the 2002 Census of Services data for legal services firms, compares their quality to that of previous census of Services, and produces estimates of number of lawyers that extend a series that the Census Bureau published for 20 years but failed to publish in 2002. The research also investigates how the organization of legal services—in particular, firms’ hierarchical structure—has changed over time, characterizes the distribution of wages in this industry and how it has changed over time, and analyzes relationships between changes in hierarchies and changes in the wage distribution. The latter will lead to a better understanding of wage inequality not only in legal services, but also in human-capital-intensive sectors (such as services) more broadly."
"Earning Maximization with Quality of Charging Service Guarantee for IoT
  Devices","Resonant Beam Charging (RBC) is a promising Wireless Power Transfer (WPT)
technology to provide long-range, high-power, mobile and safe wireless power
for the Internet of Things (IoT) devices. The Point-to-Multipoint (PtMP) RBC
system can charge multiple receivers simultaneously similar to WiFi
communications. To guarantee the Quality of Charging Service (QoCS) for each
receiver and maximize the overall earning in the PtMP RBC service, we specify
the Charging Pricing Strategy (CPS) and develop the High Priority Charge (HPC)
scheduling algorithm to control the charging order and power allocation. Each
receiver is assigned a priority, which is updated dynamically based on its
State of Charging (SOC) and specified charging power. The receivers with high
priorities are scheduled to be charged in each time slot. We present the pseudo
code of the HPC algorithm based on quantifying the receiver's SOC, discharging
energy and various relevant parameters. Relying on simulation analysis, we
demonstrate that the HPC algorithm can achieve better QoCS and earning than the
Round-Robin Charge (RRC) scheduling algorithm. Based on the performance
evaluation, we illustrate that the methods to improve the PtMP RBC service are:
1) limiting the receiver number within a reasonable range and 2) prolonging the
charging duration as long as possible. In summary, the HPC scheduling algorithm
provides a practical strategy to maximize the earning of the PtMP RBC service
with each receiver's QoCS guarantee.","['Wen Fang', 'Qingqing Zhang', 'Mingqing Liu', 'Qingwen Liu', 'Pengfei Xia']",[],0,arXiv,http://arxiv.org/abs/1809.09520v1,False,True,False,False,False,621,Thomas Hubbard,Chicago,Completed,2007,2010.0,"Wage inequality increased substantially in the United States during the past quarter century. The sources of this increase and its public policy implications have been controversial, both within academia and among policy makers. Much of the debate has surrounded whether this increase was due to technological factors, such as the diffusion of information technology, or to policy changes, such as reductions in the minimum wage (in real terms).  Economists have proposed that organizational structure affects wage inequality and can amplify the effect of technological factors, especially in contexts where production is human-capital intensive. Understanding what affects wage inequality in human-capital-intensive sectors is particularly important because these sectors occupy a high and growing share of U.S. economy and because many government policies aimed at raising wages at the low end do so by increasing these workers’ human capital. If wages are affected not just by individuals’ human capital, but the organizational structure in which individuals’ work, one can make these policies more productive by applying them in organizational contexts where they are likely to have the greatest impact on wages. This proposal examines the quality of the 2002 Census of Services data for legal services firms, compares their quality to that of previous census of Services, and produces estimates of number of lawyers that extend a series that the Census Bureau published for 20 years but failed to publish in 2002. The research also investigates how the organization of legal services—in particular, firms’ hierarchical structure—has changed over time, characterizes the distribution of wages in this industry and how it has changed over time, and analyzes relationships between changes in hierarchies and changes in the wage distribution. The latter will lead to a better understanding of wage inequality not only in legal services, but also in human-capital-intensive sectors (such as services) more broadly."
Dilepton production in ultrarelativistic heavy ion collisions,"We review the production of lepton pairs during high energy nuclear
collisions. We highlight the information being carried out of the hot and dense
strongly interacting medium. We describe the phenomenon of scalar-vector mixing
that can take place in a dense medium and suggest possible measurable
signatures of this effect in high energy heavy ion collisions.",['Charles Gale'],[],0,arXiv,http://arxiv.org/abs/hep-ph/0102214v1,False,True,False,False,False,623,Gale A Boyd,Triangle,Completed,2007,2012.0,"This project extends the time frame and scope of the projects Comparison of the Distributions of Production and Energy Efficiency in Manufacturing: Phase 1 and Phase 2, respectively. Those projects successfully implemented the methods described in prior project proposals for a few selected industrial sectors. This project will continue to expand the scope of phases 1 and 2 via additional industry specific analysis. The principal analytic approach is the application of the frontier production function. The project will enhance the Census Bureau’s knowledge base regarding the specific area of investigation, which is the distribution of energy output ratios specifically and in relationship to the distribution of total factor productivity. This understanding could lead to improved editing and screening procedures, ultimately improving the overall Economic Census program. This project will compare energy related data, including census materials and product data, with external sources of information, including industry and trade group data and process specific information. The expanded project scope will include a wide range of industrial sectors including, but not limited to, pulp/paper/paperboard and petrochemicals sector."
"How residence permits affect the labor market attachment of foreign
  workers: Evidence from a migration lottery in Liechtenstein","We analyze the impact of obtaining a residence permit on foreign workers'
labor market and residential attachment. To overcome the usually severe
selection issues, we exploit a unique migration lottery that randomly assigns
access to otherwise restricted residence permits in Liechtenstein (situated
between Austria and Switzerland). Using an instrumental variable approach, our
results show that lottery compliers (whose migration behavior complies with the
assignment in their first lottery) raise their employment probability in
Liechtenstein by on average 24 percentage points across outcome periods (2008
to 2018) as a result of receiving a permit. Relatedly, their activity level and
employment duration in Liechtenstein increase by on average 20 percentage
points and 1.15 years, respectively, over the outcome window. These substantial
and statistically significant effects are mainly driven by individuals not
(yet) working in Liechtenstein prior to the lottery rather than by previous
cross-border commuters. Moreover, we find both the labor market and residential
effects to be persistent even several years after the lottery with no sign of
fading out. These results suggest that granting resident permits to foreign
workers can be effective to foster labor supply even beyond the effect of
cross-border commuting from adjacent regions.","['Berno Buechel', 'Selina Gangl', 'Martin Huber']",[],0,arXiv,http://arxiv.org/abs/2105.11840v1,False,True,False,False,False,628,Jennifer A Heerwig,Baruch,Completed,2008,2010.0,"The goal of our proposed project is to examine important economic, family, health, and residential outcomes in the Vietnam-era service cohort through an instrumental variable estimation. By using this statistical technique, we hope to provide the Census Bureau with estimates of the eﬀect of military service on the veteran population purged of selection bias. These estimates will demonstrate how Vietnam-era males, chosen randomly through the draft lottery, compare with the nonservice population on variables such as income, wealth accumulation, marital stability, and residential mobility. The analysis will provide important information about the later life characteristics of the male Vietnam-era population while highlighting the needs of aging Vietnam veterans."
"Gender gaps in frontier entrepreneurship? Evidence from 1901 Oklahoma
  land lottery winners","The paper investigates gender differences in entrepreneurship by exploiting a
large-scale land lottery in Oklahoma at the turn of the 20$^{\text{th}}$
century. Lottery winners claimed land in the order in which their names were
drawn, so the draw number is an approximate rank ordering of lottery wealth.
This mechanism allows for the estimation of a dose-response function, which
relates each draw number to the expected outcome under each draw. I estimate
dose-response functions on a linked dataset of lottery winners and land patent
records, and find the probability of purchasing land from the government to be
decreasing as a function of lottery wealth, which is evidence for the presence
of liquidity constraints. I find female winners were more effective in
leveraging lottery wealth to purchase additional land, as evidenced by
significantly higher median dose-responses compared to those of male winners.
For a sample of winners linked to the 1910 Census, I find that male winners
have higher median dose-responses compared to female winners in terms of farm
or home ownership. These results suggest that liquidity constraints may have
been more binding for female entrepreneurs in the market economy.",['Jason Poulos'],[],0,arXiv,http://arxiv.org/abs/2206.14922v3,False,True,False,False,False,628,Jennifer A Heerwig,Baruch,Completed,2008,2010.0,"The goal of our proposed project is to examine important economic, family, health, and residential outcomes in the Vietnam-era service cohort through an instrumental variable estimation. By using this statistical technique, we hope to provide the Census Bureau with estimates of the eﬀect of military service on the veteran population purged of selection bias. These estimates will demonstrate how Vietnam-era males, chosen randomly through the draft lottery, compare with the nonservice population on variables such as income, wealth accumulation, marital stability, and residential mobility. The analysis will provide important information about the later life characteristics of the male Vietnam-era population while highlighting the needs of aging Vietnam veterans."
Payday loans -- blessing or growth suppressor? Machine Learning Analysis,"The upsurge of real estate involves a variety of factors that have got
influenced by many domains. Indeed, the unrecognized sector that would affect
the economy for which regulatory proposals are being drafted to keep this in
control is the payday loans. This research paper revolves around the impact of
payday loans in the real estate market. The research paper draws a first-hand
experience of obtaining the index for the concentration of real estate in an
area of reference by virtue of payday loans in Toronto, Ontario in particular,
which sets out an ideology to create, evaluate and demonstrate the scenario
through research analysis. The purpose of this indexing via payday loans is the
basic - debt: income ratio which states that when the income of the person
bound to pay the interest of payday loans increases, his debt goes down
marginally which hence infers that the person invests in fixed assets like real
estate which hikes up its growth.","['Rohith Mahadevan', 'Sam Richard', 'Kishore Harshan Kumar', 'Jeevitha Murugan', 'Santhosh Kannan', 'Saaisri', 'Tarun', 'Raja CSP Raman']",[],0,arXiv,http://arxiv.org/abs/2205.15320v1,False,True,False,False,False,640,Brian T Melzer,Chicago,Completed,2008,2012.0,"This project uses nonpublic data from the Survey of Income and Program Participation (SIPP) to study the eﬀect of access to short-term personal loans on household welfare, particularly among low-income populations. Speciﬁcally, it will estimate the impact of loan access on the following outcomes: diﬃculty paying rent and utilities, eviction, termination of utilities due to default, delay of needed health care, and household debt. The measure of loan access, geographic proximity to payday loan stores, depends upon fairly detailed information on house-hold location (census tract and county), necessitating access to nonpublic geographic identiﬁers in the SIPP. By investigating a determinant of economic well-being among low-income individuals, this project fulﬁlls one of the Census Bureau’s goals in conducting the SIPP: to provide improved measures of economic well-being. If short-term borrowing does inﬂuence welfare, then this would suggest a useful revision to the SIPP, whereby the Census Bureau could oﬀer a clearer picture of the ﬁnancial situation of low-income individuals by inquiring about payday loan usage among SIPP respondents."
"Credit Cycles, Securitization, and Credit Default Swaps","We present a limits-to-arbitrage model to study the impact of securitization,
leverage and credit risk protection on the cyclicity of bank credit. In a
stable bank credit situation, no cycles of credit expansion or contraction
appear. Unlevered securitization together with mis-pricing of securitized
assets increases lending cyclicality, favoring credit booms and busts. Leverage
changes the state of affairs with respect to the simple securitization. First,
the volume of real activity and banking profits increases. Second, banks sell
securities when markets decline. This selling puts further pressure on falling
prices. The mis-pricing of credit risk protection or securitized assets
influences the real economy. Trading in these contracts reduces the amount of
funding available to entrepreneurs, particularly to high-credit-risk borrowers.
This trading decreases the liquidity of the securitized assets, and especially
those based on investments with high credit risk.",['Juan Ignacio Peña'],[],0,arXiv,http://arxiv.org/abs/1901.00177v1,False,True,False,False,False,640,Brian T Melzer,Chicago,Completed,2008,2012.0,"This project uses nonpublic data from the Survey of Income and Program Participation (SIPP) to study the eﬀect of access to short-term personal loans on household welfare, particularly among low-income populations. Speciﬁcally, it will estimate the impact of loan access on the following outcomes: diﬃculty paying rent and utilities, eviction, termination of utilities due to default, delay of needed health care, and household debt. The measure of loan access, geographic proximity to payday loan stores, depends upon fairly detailed information on house-hold location (census tract and county), necessitating access to nonpublic geographic identiﬁers in the SIPP. By investigating a determinant of economic well-being among low-income individuals, this project fulﬁlls one of the Census Bureau’s goals in conducting the SIPP: to provide improved measures of economic well-being. If short-term borrowing does inﬂuence welfare, then this would suggest a useful revision to the SIPP, whereby the Census Bureau could oﬀer a clearer picture of the ﬁnancial situation of low-income individuals by inquiring about payday loan usage among SIPP respondents."
"Modelling air pollution abatement in deep street canyons by means of air
  scrubbers","Deep street canyons are characterized by weak ventilation and recirculation
of air. In such environment, the exposure to particulate matter and other air
pollutants is enhanced, with a consequent worsening of both safety and health.
The main solution adopted by the international community is aimed at the
reduction of the emissions. In this theoretical study, we test a new solution:
the removal of air pollutants close to their sources by a network of Air
Pollution Abatement (APA) devices. The APA technology depletes gaseous and
particulate air pollutants by a portable and low-consuming scrubbing system,
that mimics the processes of wet and dry deposition. We estimate the potential
pollutant abatement efficacy of a single absorber by Computational Fluid
Dynamics (CFD) method. The presence of the scrubber effectively creates an
additional sink at the bottom of the canyon, accelerating its cleaning process
by up to 70%, when an almost perfect scrubber (90% efficiency) is simulated.
The efficacy of absorber is not proportional to its internal abatement
efficiency, but it increases rapidly at low efficiencies, then tends to
saturate. In the particular configuration of the canyon we choose (aspect ratio
of 3) the upwind corner is the most favourable for the absorber application. In
the downwind corner the maximum pollutant abatement is -24%, while in the
upwind corner the maximum abatement is twice as much at -51%. The efficacy of
the absorber increases much faster at low efficiencies: at 25% efficiency, the
abatement is already about half that obtained with the 90% efficiency. These
first results, suggest strategies for the real-world application of a network
of absorbers, and motivates further theoretical study to better characterize
the details of the air flow inside and around the absorber.","['Marina De Giovanni', 'Gabriele Curci', 'Alessandro Avveduto', 'Lorenzo Pace', 'Cesare Dari Salisburgo', 'Franco Giammaria', 'Alessio Monaco', 'Giuseppe Spanto', 'Paolo Tripodi']",[],0,arXiv,http://arxiv.org/abs/1506.06088v1,False,True,False,False,False,642,Wayne Gray,Boston,Completed,2008,2013.0,"This project seeks to improve the understanding and the quality of the plant-level data on environmental spending collected in the Census Bureau’s Pollution Abatement Costs and Expenditures (PACE) survey. The project combines PACE data with other Census Bureau datasets and with external data to model the impact of pollution abatement spending on economic factors, such as the plant’s production costs and productivity, as well as its pollution emissions. The research will test accuracy of reported abatement expenditures by modeling their impact on total factor productivity levels, which should decrease productivity on a one-for-one basis if abatement activities contribute nothing to production. It also models the plant’s production function, testing whether the productivity of speciﬁc types of inputs are more seriously aﬀected by pollution abatement activities. Analyses include tests for differences across plants in abatement costs and in their impact on productivity, based on plant size, age, and other observed characteristics. The project will also model the impact of reported abatement costs on a variety of business decisions, including shifts in economic activity and investment, providing an indirect test for the reality of abatement costs. The project will beneﬁt the Census Bureau in several ways. The PACE survey has been recently resumed after an extended hiatus, so information about its data quality and comparisons to data from earlier versions of the survey are valuable. Our models of the impact of reported PACE spending on productivity and emissions test their validity in two ways: are they costs, and do they abate pollution? Our external datasets provide information on production technology and material use that will be used to assess the quality of comparable Census Bureau-collected information. Finally, the external datasets (particularly the Environmental Protection Agency’s Facility Registry System) provide care-fully maintained name-address, latitude-longitude, and plant ownership data that will provide information about the quality of the comparable Census Bureau information and how quickly that information is updated."
"Identification of Key Companies for International Profit Shifting in the
  Global Ownership Network","In the global economy, the intermediate companies owned by multinational
corporations are becoming an important policy issue as they are likely to cause
international profit shifting and diversion of foreign direct investments. The
purpose of this analysis is to call the intermediate companies with high risk
of international profit shifting as key firms and to identifying and clarify
them. For this aim, we propose a model that focuses on each affiliate's
position on the ownership structure of each multinational corporation. Based on
the information contained in the Orbis database, we constructed the Global
Ownership Network, reflecting the relationship that can give significant
influence to a firm, and analyzed for large multinational corporations listed
in Fortune Global 500. In this analysis, first, we confirmed the validity of
this model by identifying affiliates playing an important role in international
tax avoidance at a certain degree. Secondly, intermediate companies are mainly
found in the Netherlands and the United Kingdom, etc., and tended to be located
in jurisdictions favorable to treaty shopping. And it was found that such key
firms are concentrated on the IN component of the bow-tie structure that the
giant weakly connected component of the Global Ownership Network consist of.
Therefore, it clarifies that the key firms are geographically located in
specific jurisdictions, and concentrates on specific components in the Global
Ownership Network. The location of key firms are related with the ease of
treaty shopping, and there is a difference in the jurisdiction where key firms
are located depending on the location of the multinational corporations.","['Tembo Nakamoto', 'Abhijit Chakraborty', 'Yuichi Ikeda']",[],0,arXiv,http://arxiv.org/abs/1904.12397v1,False,True,False,False,False,655,Maria Borga,Washington,Completed,2008,2012.0,"This project examines the impact of trade and globalization on U.S. businesses. The project has three objectives. It will study the regional impacts of rapidly growing U.S. foreign exports of services by developing estimates of exports of services at the state level. It will examine the impact of globalization by U.S. multinational companies through direct investment, trade, and the oﬀ -shoring of services on the U.S. economy. It will examine the relationship between an enterprise and its establishments and explore methods of allocating enterprise-level data to establishments in various industries. All three components of this project will involve supplementing data collected at the Census Bureau on U.S. business activities with data collected at the U.S. Bureau of Economic Analysis (BEA) on the operations of multinational ﬁrms and on trade in services. Most Census Bureau business data are collected at the establishment level, while BEA’s surveys are conducted on a consolidated business enterprise basis. Linking these datasets greatly increases their analytic value by allowing questions about decisions made at the enterprise level—such as where to locate production to serve foreign customers—to be examined using the establishment-level data best suited to answering them. This project will build on previous projects that have linked Census data with BEA enterprise data. These previous linking projects have provided beneﬁts to the Census Bureau, and this project will expand on these beneﬁts. For example, it will enable the Census Bureau to improve its sample frames, verify and improve the accuracy of data reported on its surveys, and improve the industry classiﬁcations of enterprises and establishments. In addition, it will greatly increase the utility of the Census Bureau datasets for examining the impact that globalization—in the form of direct investment and trade in services—is having on the U.S. economy."
"How should we proxy for race/ethnicity? Comparing Bayesian improved
  surname geocoding to machine learning methods","Bayesian Improved Surname Geocoding (BISG) is the most popular method for
proxying race/ethnicity in voter registration files that do not contain it.
This paper benchmarks BISG against a range of previously untested machine
learning alternatives, using voter files with self-reported race/ethnicity from
California, Florida, North Carolina, and Georgia. This analysis yields three
key findings. First, machine learning consistently outperforms BISG at
individual classification of race/ethnicity. Second, BISG and machine learning
methods exhibit divergent biases for estimating regional racial composition.
Third, the performance of all methods varies substantially across states. These
results suggest that pre-trained machine learning models are preferable to BISG
for individual classification. Furthermore, mixed results across states
underscore the need for researchers to empirically validate their chosen
race/ethnicity proxy in their populations of interest.",['Ari Decter-Frain'],[],0,arXiv,http://arxiv.org/abs/2206.14583v2,False,True,False,False,False,657,Qingfang Wang,Triangle,Completed,2009,2014.0,"This study investigates how place, race, and ethnicity intertwine to produce the spatial division of ethnic enterprises in different types of metropolitan areas in the U.S.—established immigration gateways versus newly emergent immigration destinations. The project uses the Survey of Business Owners (SBO), Business Register, Longitudinal Business Database, and Decennial Long Form data to address the following questions: (1) what are the characteristics of ethnic entrepreneurs and ethnic enterprises and how do they differ by group and by region, and (2) how are ethnic minority owned enterprises socially and spatially embedded in each urban context, and (3) what are the impacts of place on the presence and performance of ethnic enterprises. This project will benefit U.S. Census Bureau programs by an investigation of variation in survey nonresponse in the SBO across ethnic groups (sampling frames), across economic charac­teristics of firms, and across geographic areas. Thus this study will increase the Census Bureau’s understanding of the quality of the SBO data, help improve imputations for nonresponse, and potentially help improve the sampling frame for the SBO. The study will also aid the preparation of estimates and characteristics of the Hispanic and Asian sub-group populations and help minimize problems of missed and inaccurately represented subpopulations in the decennial census. Results from this project will help the Census Bureau to design and appropriately target bilingual forms, provide telephone assistance and telephone self-response options, and will thereby improve the accuracy and reduce the costs of conducting the census."
"Race, Ethnicity and National Origin-based Discrimination in Social Media
  and Hate Crimes Across 100 U.S. Cities","We study malicious online content via a specific type of hate speech: race,
ethnicity and national-origin based discrimination in social media, alongside
hate crimes motivated by those characteristics, in 100 cities across the United
States. We develop a spatially-diverse training dataset and classification
pipeline to delineate targeted and self-narration of discrimination on social
media, accounting for language across geographies. Controlling for census
parameters, we find that the proportion of discrimination that is targeted is
associated with the number of hate crimes. Finally, we explore the linguistic
features of discrimination Tweets in relation to hate crimes by city, features
used by users who Tweet different amounts of discrimination, and features of
discrimination compared to non-discrimination Tweets. Findings from this
spatial study can inform future studies of how discrimination in physical and
virtual worlds vary by place, or how physical and virtual world discrimination
may synergize.","['Kunal Relia', 'Zhengyi Li', 'Stephanie H. Cook', 'Rumi Chunara']",[],0,arXiv,http://arxiv.org/abs/1902.00119v1,False,True,False,False,False,657,Qingfang Wang,Triangle,Completed,2009,2014.0,"This study investigates how place, race, and ethnicity intertwine to produce the spatial division of ethnic enterprises in different types of metropolitan areas in the U.S.—established immigration gateways versus newly emergent immigration destinations. The project uses the Survey of Business Owners (SBO), Business Register, Longitudinal Business Database, and Decennial Long Form data to address the following questions: (1) what are the characteristics of ethnic entrepreneurs and ethnic enterprises and how do they differ by group and by region, and (2) how are ethnic minority owned enterprises socially and spatially embedded in each urban context, and (3) what are the impacts of place on the presence and performance of ethnic enterprises. This project will benefit U.S. Census Bureau programs by an investigation of variation in survey nonresponse in the SBO across ethnic groups (sampling frames), across economic charac­teristics of firms, and across geographic areas. Thus this study will increase the Census Bureau’s understanding of the quality of the SBO data, help improve imputations for nonresponse, and potentially help improve the sampling frame for the SBO. The study will also aid the preparation of estimates and characteristics of the Hispanic and Asian sub-group populations and help minimize problems of missed and inaccurately represented subpopulations in the decennial census. Results from this project will help the Census Bureau to design and appropriately target bilingual forms, provide telephone assistance and telephone self-response options, and will thereby improve the accuracy and reduce the costs of conducting the census."
"Do Large Language Models Discriminate in Hiring Decisions on the Basis
  of Race, Ethnicity, and Gender?","We examine whether large language models (LLMs) exhibit race- and
gender-based name discrimination in hiring decisions, similar to classic
findings in the social sciences (Bertrand and Mullainathan, 2004). We design a
series of templatic prompts to LLMs to write an email to a named job applicant
informing them of a hiring decision. By manipulating the applicant's first
name, we measure the effect of perceived race, ethnicity, and gender on the
probability that the LLM generates an acceptance or rejection email. We find
that the hiring decisions of LLMs in many settings are more likely to favor
White applicants over Hispanic applicants. In aggregate, the groups with the
highest and lowest acceptance rates respectively are masculine White names and
masculine Hispanic names. However, the comparative acceptance rates by group
vary under different templatic settings, suggesting that LLMs' race- and
gender-sensitivity may be idiosyncratic and prompt-sensitive.","['Haozhe An', 'Christabel Acquaye', 'Colin Wang', 'Zongxia Li', 'Rachel Rudinger']",[],0,arXiv,http://arxiv.org/abs/2406.10486v1,False,True,False,False,False,657,Qingfang Wang,Triangle,Completed,2009,2014.0,"This study investigates how place, race, and ethnicity intertwine to produce the spatial division of ethnic enterprises in different types of metropolitan areas in the U.S.—established immigration gateways versus newly emergent immigration destinations. The project uses the Survey of Business Owners (SBO), Business Register, Longitudinal Business Database, and Decennial Long Form data to address the following questions: (1) what are the characteristics of ethnic entrepreneurs and ethnic enterprises and how do they differ by group and by region, and (2) how are ethnic minority owned enterprises socially and spatially embedded in each urban context, and (3) what are the impacts of place on the presence and performance of ethnic enterprises. This project will benefit U.S. Census Bureau programs by an investigation of variation in survey nonresponse in the SBO across ethnic groups (sampling frames), across economic charac­teristics of firms, and across geographic areas. Thus this study will increase the Census Bureau’s understanding of the quality of the SBO data, help improve imputations for nonresponse, and potentially help improve the sampling frame for the SBO. The study will also aid the preparation of estimates and characteristics of the Hispanic and Asian sub-group populations and help minimize problems of missed and inaccurately represented subpopulations in the decennial census. Results from this project will help the Census Bureau to design and appropriately target bilingual forms, provide telephone assistance and telephone self-response options, and will thereby improve the accuracy and reduce the costs of conducting the census."
'Entanglement' -- A new dynamic metric to measure team flow,"We introduce ""entanglement"", a novel metric to measure how synchronized
communication between team members is. This measure calculates the Euclidean
distance among team members' social network metrics timeseries. We validate the
metric with four case studies. The first case study uses entanglement of 11
medical innovation teams to predict team performance and learning behavior. The
second case looks at the e-mail communication of 113 senior executives of an
international services firm, predicting employee turnover through lack of
entanglement of an employee. The third case analyzes the individual employee
performance of 81 managers. The fourth case study predicts performance of 13
customer-dedicated teams at a big international company by comparing
entanglement in the e-mail interactions with satisfaction of their customers
measured through Net Promoter Score (NPS). While we can only speculate about
what is causing the entanglement effect, we find that it is a new and versatile
indicator for the analysis of employees' communication, analyzing the hitherto
underused temporal dimension of online social networks which could be used as a
powerful predictor of employee and team performance, employee turnover, and
customer satisfaction.","['P. A. Gloor', 'M. P. Zylka', 'A. Fronzetti Colladon', 'M. Makai']",[],0,arXiv,http://arxiv.org/abs/2112.00538v1,False,True,False,False,False,662,Paige P Ouimet,Michigan,Completed,2008,2013.0,"This project examines whether broad-based equity-based incentives are eﬀective at aligning incentives and the potential costs associated with providing these incentives. If employee ownership provides appropriate incentives, worker productivity should be higher and employee turnover should be lower. However, employee ownership also gives employees voting rights, which can be used to extract employee beneﬁts at the expense of other stakeholders in the ﬁrm. For example, workers with voting rights may be more successful at obtaining above-market wages or in delaying or preventing layoﬀ s or plant closures. Examining the eﬀects of employee ownership on ﬁrm and establishment performance measures—such as productivity and wages—which in turn aﬀect ﬁrm value, will provide important insights into the beneﬁts and limitations of equity-based compensation."
Did Hurricane Katrina Reduce Mortality?,"In a recent article in the American Economic Review, Tatyana Deryugina and
David Molitor (DM) analyzed the effect of Hurricane Katrina on the mortality of
elderly and disabled residents of New Orleans. The authors concluded that
Hurricane Katrina improved the eight-year survival rate of elderly and disabled
residents of New Orleans by 3% and that most of this decline in mortality was
due to declines in mortality among those who moved to places with lower
mortality. In this article, I provide a critical assessment of the evidence
provided by DM to support their conclusions. There are three main problems.
First, DM generally fail to account for the fact that people of different ages,
races or sex will have different probabilities of dying as time goes by, and
when they do allow for this, results change markedly. Second, DM do not account
for the fact that residents in New Orleans are likely to be selected
non-randomly on the basis of health because of the relatively high mortality
rate in New Orleans compared to the rest of the country. Third, there is
considerable evidence that among those who moved from New Orleans, the
destination chosen was non-random. Finally, DM never directly assessed changes
in mortality of those who moved, or stayed, in New Orleans before and after
Hurricane Katrina. These problems lead me to conclude that the evidence
presented by DM does not support their inferences.",['Robert Kaestner'],[],0,arXiv,http://arxiv.org/abs/2011.03392v2,False,True,False,False,False,664,Narayan Sastry,Michigan,Completed,2008,2012.0,"This project employs data from the Census Bureau’s American Community Survey (ACS) to examine the current location and well-being of residents of New Orleans in the year after Hurricane Katrina struck the city. The aims of this project are to describe the return or resettlement in the year following Hurricane Katrina of people who resided in New Orleans before the storm and to examine the well-being of the pre-Katrina New Orleans population in the year after the hurricane, compared to a matched population from the prior year. The ACS data provide a unique opportunity to examine the geographic dispersion of New Orleans residents throughout the United States in the after-math of Hurricane Katrina and to assess several important dimensions of well-being. This project addresses a number of unanswered research questions about the eﬀects of Hurricane Katrina on the New Orleans’ population. It also explores the strengths and weaknesses of the ACS data for examining the eﬀects of Hurricane Katrina and for future studies of the effects of large-scale natural and man-made disasters. The researchers will evaluate the suitability of propensity-score reweighting techniques for studying these topics using the ACS by, for example, examining which population groups are underrepresented and overrepresented between the two ACS cross-sections."
"Design of the Millennium Villages Project Sampling Plan: a simulation
  study for a multi-module survey","The Millennium Villages Project (MVP) is a ten-year integrated rural
development project implemented in ten sub-Saharan African sites. At its
conclusion we will conduct an evaluation of its causal effect on a variety of
development outcomes, measured via household surveys in treatment and
comparison areas. Outcomes are measured by six survey modules, with sample
sizes for each demographic group determined by budget, logistics, and the
group's vulnerability. We design a sampling plan that aims to reduce effort for
survey enumerators and maximize precision for all outcomes. We propose
two-stage sampling designs, sampling households at the first stage, followed by
a second stage sample that differs across demographic groups. Two-stage designs
are usually constructed by simple random sampling (SRS) of households and
proportional within-household sampling, or probability proportional to size
sampling (PPS) of households with fixed sampling within each. No measure of
household size is proportional for all demographic groups, putting PPS schemes
at a disadvantage. The SRS schemes have the disadvantage that multiple
individuals sampled per household decreases efficiency due to intra-household
correlation. We conduct a simulation study (using both design- and model-based
survey inference) to understand these tradeoffs and recommend a sampling plan
for the Millennium Villages Project. Similar design issues arise in other
studies with surveys that target different demographic groups.","['Shira Mitchell', 'Rebecca Ross', 'Susanna Makela', 'Elizabeth A. Stuart', 'Avi Feller', 'Alan M. Zaslavsky', 'Andrew Gelman']",[],0,arXiv,http://arxiv.org/abs/1507.02739v1,False,True,False,False,False,669,Jason E Devine,Cornell,Completed,2007,2008.0,"The Census Bureau produces population estimates at the national, state, county, and sub-county levels and housing unit estimates at the state and county levels. The estimates are used to distribute federal funds, by state and local governments, and as controls for Census Bureau and other surveys. As part of an effort to develop county-level housing unit-based population estimates, the Census Bureau is undertaking a series of research projects. These research projects are being coordinated by members of Population Division as part of the Housing Unit-Based Estimates Research Team (HUBERT). This research will provide input into the overall HUBERT research project that will be used by the Census Bureau to make decisions about the methodology that will be used to produce an experimental series of housing unit-based population estimates."
Community Structure in the United States House of Representatives,"We investigate the networks of committee and subcommittee assignments in the
United States House of Representatives from the 101st--108th Congresses, with
the committees connected by ``interlocks'' or common membership. We examine the
community structure in these networks using several methods, revealing strong
links between certain committees as well as an intrinsic hierarchical structure
in the House as a whole. We identify structural changes, including additional
hierarchical levels and higher modularity, resulting from the 1994 election, in
which the Republican party earned majority status in the House for the first
time in more than forty years. We also combine our network approach with
analysis of roll call votes using singular value decomposition to uncover
correlations between the political and organizational structure of House
committees.","['Mason A. Porter', 'Peter J. Mucha', 'M. E. J. Newman', 'A. J. Friend']",[],0,arXiv,http://arxiv.org/abs/physics/0602033v3,False,True,False,False,False,670,Jason E Devine,Cornell,Completed,2008,2009.0,The purpose of this work is to develop and evaluate a procedure to more precisely estimate the loss of housing units for all counties by using a model developed with data from the American Housing Survey in combination with annual estimates from the American Community Survey.
"Time Varying Risk in U.S. Housing Sector and Real Estate Investment
  Trusts Equity Return","This study examines how housing sector volatilities affect real estate
investment trust (REIT) equity return in the United States. I argue that
unexpected changes in housing variables can be a source of aggregate housing
risk, and the first principal component extracted from the volatilities of U.S.
housing variables can predict the expected REIT equity returns. I propose and
construct a factor-based housing risk index as an additional factor in asset
price models that uses the time-varying conditional volatility of housing
variables within the U.S. housing sector. The findings show that the proposed
housing risk index is economically and theoretically consistent with the
risk-return relationship of the conditional Intertemporal Capital Asset Pricing
Model (ICAPM) of Merton (1973), which predicts an average maximum of 5.6
percent of risk premium in REIT equity return. In subsample analyses, the
positive relationship is not affected by sample periods' choice but shows
higher housing risk beta values for the 2009-18 sample period. The relationship
remains significant after controlling for VIX, Fama-French three factors, and a
broad set of macroeconomic and financial variables. Moreover, the proposed
housing beta also accurately forecasts U.S. macroeconomic and financial
conditions.",['Masud Alam'],[],0,arXiv,http://arxiv.org/abs/2107.10455v1,False,True,False,False,False,670,Jason E Devine,Cornell,Completed,2008,2009.0,The purpose of this work is to develop and evaluate a procedure to more precisely estimate the loss of housing units for all counties by using a model developed with data from the American Housing Survey in combination with annual estimates from the American Community Survey.
A Distributed Trust Framework for Privacy-Preserving Machine Learning,"When training a machine learning model, it is standard procedure for the
researcher to have full knowledge of both the data and model. However, this
engenders a lack of trust between data owners and data scientists. Data owners
are justifiably reluctant to relinquish control of private information to third
parties. Privacy-preserving techniques distribute computation in order to
ensure that data remains in the control of the owner while learning takes
place. However, architectures distributed amongst multiple agents introduce an
entirely new set of security and trust complications. These include data
poisoning and model theft. This paper outlines a distributed infrastructure
which is used to facilitate peer-to-peer trust between distributed agents;
collaboratively performing a privacy-preserving workflow. Our outlined
prototype sets industry gatekeepers and governance bodies as credential
issuers. Before participating in the distributed learning workflow, malicious
actors must first negotiate valid credentials. We detail a proof of concept
using Hyperledger Aries, Decentralised Identifiers (DIDs) and Verifiable
Credentials (VCs) to establish a distributed trust architecture during a
privacy-preserving machine learning experiment. Specifically, we utilise secure
and authenticated DID communication channels in order to facilitate a federated
learning workflow related to mental health care data.","['Will Abramson', 'Adam James Hall', 'Pavlos Papadopoulos', 'Nikolaos Pitropakis', 'William J Buchanan']",[],0,arXiv,http://arxiv.org/abs/2006.02456v1,False,True,False,False,False,674,William D Bradford,Michigan,Completed,2009,2013.0,"This study uses the Characteristics of Business Ownership (CBO), the Survey of Business Ownership (SBO), the Integrated Longitudinal Business Database (ILBD), and the Business Register (BR) to examine the relationship between age and entrepreneurship. This research derives from a common question among scholars and practitioners: What owner traits predict business performance (e.g., employment, sales, profits, survival)? This study contributes to this research by using census data to test the strength of owner age in predicting business performance and by measuring the extent to which this relationship changed between 1982 and 2002. This research will obtain sales, employees, other available firm data for 2002, and the survival experience of the firm through 2006, or the latest available year. This study will also link the firms to improve the measures of business survival beyond the snapshots taken in 1986 and 1996. This research will use the linked data for longitudinal analyses of owner age and business performance. This will contribute to the understanding of business formation, early lifecycle dynamics of firms and the precursors to job creation in the U.S. economy. The project will create estimates of the impact of age of business owners on the performance of small businesses. It will also develop linkages between the SBO, the CBO, and the newly created ILBD. Using those linkages, it will enrich the 2002 SBO with variables which were not included in that survey, but were included in the earlier CBO surveys. This will enhance both the value of the 2002 SBO because of the inclusion of the new variables and the value of the earlier CBO surveys because of the existence of comparable data in another year. It will also enhance all three surveys by providing measures of firm survival and the transition from nonemployer to employer status."
What You See is What You Get: Local Labor Markets and Skill Acquisition,"This paper highlights the potential for negative dynamic consequences of
recent trends towards the formation of ""skill-hubs"". I first show evidence that
skill acquisition is biased towards skills which are in demand in local labor
markets. This fact along with large heterogeneity in outcomes by major and
recent reductions in migration rates implies a significant potential for
inefficient skill upgrading over time. To evaluate the impact of local bias in
education in the context of standard models which focus on agglomeration
effects, I develop a structural spatial model which includes educational
investment. The model focuses on two sources of externalities: productivity
through agglomeration and signaling. Both of these affect educational decisions
tilting the balance of aggregate skill composition. Signaling externalities can
provide a substantial wedge in the response to changes in skill demand and
skill concentration with the potential for substantial welfare gains from a
more equal distribution of skills.",['Benjamin Niswonger'],[],0,arXiv,http://arxiv.org/abs/2209.03892v1,False,True,False,False,False,678,Joshua W Mitchell,Boston,Completed,2008,2012.0,"This project uses the Quarterly Workforce Indicators (QWI) and human capital ﬁ les to better understand how workforce and production aggregation choices aﬀect estimates of labor demand for skill and inﬂuence our understanding of recent changes in wage inequality. Decompositions of changes over time in employment and wage shares of skill and demographic groups, formal modeling and estimation of labor demand parameters, and correlations between demand and indicators of technology, capital, and trade from public-use datasets will be performed. These estimates will be generated at distinct levels of aggregated producer data: establishment, detailed industry, industry group, and economy-wide. The results will be used to reconcile microeconomic and macroeconomic trends in the wage structure and help evaluate competing explanations for the evolution of wage inequality."
"Cost-benefit Analysis And Comparisons For Different Offshore Wind Energy
  Transmission Systems","This paper investigates how to efficiently and economically deliver offshore
energy generated by offshore wind turbines to onshore. Both power cables and
hydrogen energy facilities are analyzed. Each method is examined with the
associated proposed optimal sizing model. A cost-benefit analysis is conducted,
and these methods are compared under different scenarios. Three long-distance
energy transmission methods are proposed to transmit offshore energy onshore,
considering the capital/operation costs for related energy conversion and
transmission facilities. The first method that deploys power cables only is a
benchmark method. The other two methods utilize a hydrogen supercenter (HSC)
and transmit offshore energy to and onshore substation through hydrogen
pipelines. For the second method, electrolyzers are placed at offshore wind
farms connected to HSC with low-pressure hydrogen pipelines. For the third
method, offshore wind power is distributed to the HSC and then converted into
hydrogen using electrolyzers placed at the HSC. An offshore scenario including
three wind farms located in the Gulf of Mexico and one onshore substation is
used as the base test case. Numerical simulations are conducted over a planning
period of 30 years using the proposed optimization models for each method
separately. Simulation results show that the proposed hydrogen-based methods
can effectively transmit offshore energy to the onshore substation. The
distances between the wind farms can influence the selection and configuration
of the offshore wind energy system. Moreover, the proposed study analyzes the
cost and benefits for various systems with different energy carriers. It can
also suggest which method could be the best candidate for offshore energy
delivery system under which scenario and demonstrate a promising future for
offshore wind power generation.","['Jesus Silva-Rodriguez', 'Jin Lu', 'Xingpeng Li']",[],0,arXiv,http://arxiv.org/abs/2301.11208v1,False,True,False,False,False,682,Jooyoun Park,Michigan,Completed,2009,2014.0,"This project will involve a reduced form analysis of the final outcome of outsourcing, including sales and employment changes. It investigates outsourcing’s effects on prices and price-cost margins within the plants of outsourcing firms. Finally, it tests the hypothesis that offshore outsourcing decisions are driven by profit maximization. These effects and relationships are examined using U.S. Census Bureau data (Longitudinal Business Database, Census of Manufactures, Annual Survey of Manufactures, Business Register, and the Auxiliary Establishment File), merged with Trade Adjustment Assistance (TAA) and Compustat North America information on outsourcing and a range of firm-level characteristics. Research on the effects of offshore outsourcing will provide benefits to both the Census Bureau and its pro­grams in three significant areas. The analysis of the correlation between census plant death dates and TAA plant closing data will make available new information on and improved census capabilities in both measuring and predicting plant deaths. The research on offshore outsourcing necessitates the creation of a bridge between TAA data and census business datasets. Through linking census data to previously unavailable TAA variables, such as employee layoffs and plant closing dates, this bridge enhances census data. Lastly, the research will estimate the effects of offshore outsourcing on various plant-level characteristics, such as sales and employment and within-firm prices and price-cost margins."
Influence of primary particle density in the morphology of agglomerates,"Agglomeration processes occur in many different realms of science such as
colloid and aerosol formation or formation of bacterial colonies. We study the
influence of primary particle density in agglomerate structure using
diffusion-controlled Monte Carlo simulations with realistic space scales
through different regimes (DLA and DLCA). The equivalence of Monte Carlo time
steps to real time scales is given by Hirsch's hydrodynamical theory of
Brownian motion. Agglomerate behavior at different time stages of the
simulations suggests that three indices (fractal exponent, coordination number
and eccentricity index) characterize agglomerate geometry. Using these indices,
we have found that the initial density of primary particles greatly influences
the final structure of the agglomerate as observed in recent experimental
works.","['M. D. Camejo', 'D. R. Espeso', 'L. L. Bonilla']",[],0,arXiv,http://arxiv.org/abs/1407.0974v1,False,True,False,False,False,684,Matthew L Freedman,Cornell,Completed,2008,2012.0,"This research investigates the extent to which ﬁrm learning and selection account for observed geographic agglomeration eﬀects. A vast literature documents positive relationships between the wages and productivity of ﬁrms and various measures of agglomeration, eﬀects that persist even after control-ling for a broad array of worker, ﬁrm, and other local area characteristics. In many cases, researchers attribute the positive observed eﬀects of spatial agglomeration on diﬀerent outcome variables as evidence of agglomeration economies, suggesting that there may be knowledge spillovers or externalities associated with the geographic clustering of economic activity. This project explores the role of selection in explaining observed agglomeration eﬀects using the Census Bureau’s Longitudinal Business Database (LBD). The LBD provides detailed geographic classiﬁcations that permit construction of measures of agglomeration at ﬁne geographic levels. This research will shed light on the sources of observed agglomeration eﬀects, the drivers behind diﬀerential rates of ﬁrm turnover and economic growth across geo-graphic areas, and the potential ramiﬁcations of policies aimed at encouraging or discouraging clustered business activity. This work will also yield a number of benefits to the Census Bureau by identifying shortcomings of existing data and methodologies, as well by improving the quality and utility of the data through longitudinal editing and the preparation of new population estimates."
Bayesian Agency: Linear versus Tractable Contracts,"We study principal-agent problems in which a principal commits to an
outcome-dependent payment scheme (a.k.a. contract) so as to induce an agent to
take a costly, unobservable action. We relax the assumption that the principal
perfectly knows the agent by considering a Bayesian setting where the agent's
type is unknown and randomly selected according to a given probability
distribution, which is known to the principal. Each agent's type is
characterized by her own action costs and action-outcome distributions. In the
literature on non-Bayesian principal-agent problems, considerable attention has
been devoted to linear contracts, which are simple, pure-commission payment
schemes that still provide nice approximation guarantees with respect to
principal-optimal (possibly non-linear) contracts. While in non-Bayesian
settings an optimal contract can be computed efficiently, this is no longer the
case for our Bayesian principal-agent problems. This further motivates our
focus on linear contracts, which can be optimized efficiently given their
single-parameter nature. Our goal is to analyze the properties of linear
contracts in Bayesian settings, in terms of approximation guarantees with
respect to optimal contracts and general tractable contracts (i.e.,
efficiently-computable ones). First, we study the approximation guarantees of
linear contracts with respect to optimal ones, showing that the former suffer
from a multiplicative loss linear in the number of agent's types. Nevertheless,
we prove that linear contracts can still provide a constant multiplicative
approximation $\rho$ of the optimal principal's expected utility, though at the
expense of an exponentially-small additive loss $2^{-\Omega(\rho)}$. Then, we
switch to tractable contracts, showing that, surprisingly, linear contracts
perform well among them.","['Matteo Castiglioni', 'Alberto Marchesi', 'Nicola Gatti']",[],0,arXiv,http://arxiv.org/abs/2106.00319v1,False,True,False,False,False,686,Sandra Campo,Triangle,Completed,2010,2015.0,"This project will create a new, matched employee-employer dataset for use by researchers in the process of studying the determinants of employment-based insurance supply by firms. Theory suggests that, if a firm is not fully aware of the cost efficiency of an insurance company health care network, it will have to pay a premium to distinguish between the more and less efficient networks. Either asymmetric information or adverse selection exists since the insurance company knows its own quality or cost efficiency while its client(s) may not. If the insurance company charges too high a ""quality"" premium on some of its plans, thereby revealing its affordability and efficiency types, firms need to decide which plans continue to meet a minimum efficiency level and remain affordable based on the revealed insurance company type. This project will measure the efficiency of different plan options and estimate the cut-off efficiency value that firms from different sectors can afford to finance. In the process, this study will promote an understanding of why some sectors with high demand for health care and a given premium structure do not offer health insurance plans, whereas other sectors offer multiple options to their employees despite low demand."
"Mediating role of managing information technology and its impact on firm
  performance: insight from China","Purpose: Managing IT with firm performance has always been a debatable topic
in literature and practice. Prior studies examining the above relationship have
reported mixed results and have yet ignored the eminent managing IT practices.
The purpose of this paper is to empirically investigate the relevance of ValIT
2.0 practice in managing IT investment, and its mediating role in the firm
performance context. Design,methodology,approach:This paper developed on two
themes of literature. First managing IT as a firm's IT capability in order to
generate value from IT investment. Second IT as a firm's resource under
resource-based view offers firm's competence that deploys potentials in
achieving firm performance. The structural equation modeling with PLS
techniques used for analyzing data collected from 176 organization's IT, and
business executives in China. Findings: The results of this study show
empirical evidence that Val-IT's components (value governance, portfolio
management, and investment management) are significantly linked to the
management of IT, and it found to be a significant mediator between Val-IT
components and firm performance. Research implications: This research
contributes to the literature and practice by way of highlighting the value
generation through managing IT on firm performance. Originality: This study is
fully based on ValIT 2.0 with the firm performance where the managing IT
mediate this relationship in a country-specific study in China. This study adds
to the Chinese information system literature which suffers the lack of
empirical studies in the context of management of IT research.","['Aboobucker Ilmudeen', 'Yukun Bao']",[],0,arXiv,http://arxiv.org/abs/1903.08934v1,False,True,False,False,False,695,G. Andrew Bernat,Washington,Completed,2008,2013.0,"This project will reclassify the estimates of research and development (R&D) performance in the Survey of Industrial Research and Development (SIRD) from ﬁrm-based industries to establishment-based industries using a consistent industry classiﬁcation (NAICS) for the entire time series of the SIRD. The research will convey beneﬁts to the Census Bureau along three dimensions by producing estimates of R&D using establishment-based industry codes; by conducting data quality assessments of the SIRD data, which exploit the longitudinal nature of the data; and by analyzing ﬁrm-establishment relationships. The Census Bureau is currently in the process of a major redesign of the SIRD, so the insights gained from this project will be particularly timely in contributing to the improvement of the future versions of the SIRD. The project will make an important contribution to the Census Bureau’s eﬀ orts to classify economic information on an establishment basis by identifying the establishments within each ﬁrm that are most likely to perform the ﬁrm’s R&D. Identifying these establishments within each ﬁrm will also substantially improve the Census Bureau’s eﬀ orts to accurately measure economic activity within states because many R&D-performing companies have establishments in more than one state."
Socioeconomic biases in urban mixing patterns of US metropolitan areas,"Urban areas serve as melting pots of people with diverse socioeconomic
backgrounds, who may not only be segregated but have characteristic mobility
patterns in the city. While mobility is driven by individual needs and
preferences, the specific choice of venues to visit is usually constrained by
the socioeconomic status of people. The complex interplay between people and
places they visit, given their personal attributes and homophily leaning, is a
key mechanism behind the emergence of socioeconomic stratification patterns
ultimately leading to urban segregation at large. Here we investigate mixing
patterns of mobility in the twenty largest cities of the United States by
coupling individual check-in data from the social location platform Foursquare
with census information from the American Community Survey. We find strong
signs of stratification indicating that people mostly visit places in their own
socioeconomic class, occasionally visiting locations from higher classes. The
intensity of this `upwards bias' increases with socioeconomic status and
correlates with standard measures of racial residential segregation. Our
results indicate an even stronger socioeconomic segregation in individual
mobility than one would expect from system-level distributions, shedding
further light on uneven mobility mixing patterns in cities.","['Rafiazka Millanida Hilman', 'Gerardo Iñiguez', 'Márton Karsai']",[],0,arXiv,http://arxiv.org/abs/2110.04183v1,False,True,False,False,True,704,Melissa N Scopilliti,Washington,Completed,2008,2011.0,"Immigration of Asians and Hispanics has fueled recent growth in the non-White population in the United States. As of 2000, 31 percent of the population was of a group other than non-Hispanic White, up from nearly 25 percent a decade earlier. The inﬂux of immigrants, particularly to metropolitan areas, is changing the demographics of America’s neighborhoods. The ﬁrst part of this project examines the relationship between individual race/ethnicity, nativity, and human capital characteristics with levels of neighborhood economic advantage and racial diversity. Often termed residential or locational attainment, this research investigates the eﬀectiveness of spatial assimilation and place stratiﬁcation theories for understanding racial and ethnic stratiﬁcation across neighborhoods. The second portion of the analysis explores the relationship between metropolitan context and locational attainment. Metropolitan-level residential segregation indexes by race and ethnicity will be developed, and the relationship between metropolitan characteristics (segregation and ecological factors) and locational attainment will be examined."
"Every Corporation Owns Its Structure: Corporate Credit Ratings via Graph
  Neural Networks","Credit rating is an analysis of the credit risks associated with a
corporation, which reflects the level of the riskiness and reliability in
investing, and plays a vital role in financial risk. There have emerged many
studies that implement machine learning and deep learning techniques which are
based on vector space to deal with corporate credit rating. Recently,
considering the relations among enterprises such as loan guarantee network,
some graph-based models are applied in this field with the advent of graph
neural networks. But these existing models build networks between corporations
without taking the internal feature interactions into account. In this paper,
to overcome such problems, we propose a novel model, Corporate Credit Rating
via Graph Neural Networks, CCR-GNN for brevity. We firstly construct individual
graphs for each corporation based on self-outer product and then use GNN to
model the feature interaction explicitly, which includes both local and global
information. Extensive experiments conducted on the Chinese public-listed
corporate rating dataset, prove that CCR-GNN outperforms the state-of-the-art
methods consistently.","['Bojing Feng', 'Haonan Xu', 'Wenfang Xue', 'Bindang Xue']",[],0,arXiv,http://arxiv.org/abs/2012.01933v1,False,True,False,False,False,705,Xavier A Giroud,Boston,Completed,2009,2013.0,"This project will investigate how internal corporate gover­nance, measured by the quality of headquarters’ monitoring of individual plants, affects plant productivity and other plant-level attributes. The plant-level data used in this project are obtained from the Census of Manufactures, Annual Survey of Manufactures, and Longitudinal Business Database. The researchers will develop two measures of internal corporate governance. The first measure is the distance (physical distance or traveling time) between headquarters and individual plants. The second measure for the quality of monitoring is the “industry closeness” between the plant and headquarters. This measure reflects the idea that headquarters understands better, and thus finds it easier to monitor, a plant that operates in an industry with which headquarters is familiar. Plant productivity will be measured by total factor productiv­ity, operating margin, and labor productivity. Three main types of regressions will be estimated. The first type of regression looks at the direct (cross-sectional) relationship between productivity and internal corporate governance. The second type of regression examines whether the productivity gains (or losses) after a change of ownership can be explained by the difference in internal cor­porate governance. The third type of regression investigates if and how internal corporate governance amplifies (or mitigates) plant-level productivity shocks (state labor laws, natural disasters, oil and electricity shocks, opening of airports and golf courses in a neighborhood of the plant). Finally, plant-level measures of internal corporate governance will be aggregated into firm-level measures of internal corporate governance (e.g., the “weighted average distance” between headquarters and the firm’s plants) that can be used to investigate the role of internal corporate governance at the firm level (e.g., for equity prices or the conglomerate discount)."
Dynamic Windows Scheduling with Reallocation,"We consider the Windows Scheduling problem. The problem is a restricted
version of Unit-Fractions Bin Packing, and it is also called Inventory
Replenishment in the context of Supply Chain. In brief, the problem is to
schedule the use of communication channels to clients. Each client ci is
characterized by an active cycle and a window wi. During the period of time
that any given client ci is active, there must be at least one transmission
from ci scheduled in any wi consecutive time slots, but at most one
transmission can be carried out in each channel per time slot. The goal is to
minimize the number of channels used. We extend previous online models, where
decisions are permanent, assuming that clients may be reallocated at some cost.
We assume that such cost is a constant amount paid per reallocation. That is,
we aim to minimize also the number of reallocations. We present three online
reallocation algorithms for Windows Scheduling. We evaluate experimentally
these protocols showing that, in practice, all three achieve constant amortized
reallocations with close to optimal channel usage. Our simulations also expose
interesting trade-offs between reallocations and channel usage. We introduce a
new objective function for WS with reallocations, that can be also applied to
models where reallocations are not possible. We analyze this metric for one of
the algorithms which, to the best of our knowledge, is the first online WS
protocol with theoretical guarantees that applies to scenarios where clients
may leave and the analysis is against current load rather than peak load. Using
previous results, we also observe bounds on channel usage for one of the
algorithms.","['Martin Farach-Colton', 'Katia Leal', 'Miguel A. Mosteiro', 'Christopher Thraves']",[],0,arXiv,http://arxiv.org/abs/1404.1087v1,False,True,False,False,False,707,Allan G Collard-Wexler,Baruch,Completed,2009,2014.0,"Reallocation can take several forms, such as greater investment in more productive plants, entry of plants using a newer technology such as mini-mills, or exit of plants exposed to international trade. Economists care about reallocation at the plant level because of its role in generating improvement in aggregate productivity. In contrast to previous work on reallocation, this project will explicitly model the forward-looking choices of firms. When a firm decides to open a new ready-mix concrete plant, or shut down a steel mill because of foreign competition, it does so because it’s expectation of the net present value of profits are greater than the cost of either shutting down the plant or opening a new one. This project is composed of several separate subprojects designed to look at the forward-looking choices made by firms in different industries and how these generate reallocation. Changes in the ownership of establishments and establishing longitudinal links for firms over time will be used to evaluate the quality of the Master Business Register. The proposed analysis of changes in productivity will inform the quality of measurement of inputs and outputs in the economic census. In addition, several new estimates will be produced that relate to measurement error in productivity and the economic mechanisms driving reallocation of production towards more efficient units. This study will document the role of reallocation and entry and exit in shaping the productivity dispersion in the ready-mix concrete sector, as well as the role of measurement error of output and inputs in economic census questionnaires. Changes in the ownership of plants will be examined to identify the quality of Employer Identification Numbers (EIN) in census data. Since multi-plant ownership is associated with lower exit rates, the ownership of plants matters. This project will provide insight into the decision of firms to sell off assets, as well as the entry and exit decisions of firms rather than plants. It will analyze the effect of trade on plants in the textile and steel sectors. These sectors have been exposed to substantial changes in the strength of international competition over the last 30 years, due principally to the elimination of tariffs and quotas. This analysis will shed light on how the trade environment affected the speed of reallocation of resources such as employment and physical capital across plants in these sectors of the economy and whether this resulted in aggregate productivity gains. Prices of imported steel and textiles will permit the researchers to decompose changes in sales to changes in both the price and quantity produced."
"Skill-Based Labor Market Polarization in the Age of AI: A Comparative
  Analysis of India and the United States","This paper examines labor market polarization through a comparative analysis
of skill-based employment and wage distributions in India and the United States
during 2018-2023, with particular attention to differential automation risks
and AI preparedness. Using detailed occupation-level data, automation risk
metrics, and a series of statistical tests including wage premium analysis,
employment share tests, and wage-employment regressions, we document
significant structural differences in labor markets between developing and
developed economies. Our analysis yields four key findings. First, we find
statistically significant differences in employment distribution patterns, with
India showing disproportionate concentration in low-skill employment compared
to the US, particularly in occupations with high automation risk. Second,
regression analysis reveals that wage premiums differ systematically between
the two countries, with significantly larger skill-based wage gaps in India.
Third, we find robust evidence of a negative relationship between employment
size and wages, suggesting stronger labor supply effects in developing
economies. Fourth, analysis of occupation-specific automation risk reveals that
developing economies face a ""double vulnerability"" - concentration of
employment in both low-skill occupations and jobs with higher automation
potential, complicated by lower AI preparedness scores. These findings provide
novel empirical evidence on how development stages influence labor market
polarization patterns and carry important implications for skill development
and technology adoption policies in developing economies. Our results suggest
that traditional approaches to labor market development may need significant
modification to account for the differential impacts of AI across development
stages.","['Venkat Ram Reddy Ganuthula', 'Krishna Kumar Balaraman']",[],0,arXiv,http://arxiv.org/abs/2501.15809v2,False,True,False,False,False,708,Robin M Leichenko,Baruch,Completed,2010,2013.0,This project seeks to improve the utility of Census Bureau data by providing new estimates of the impacts of international trade on wages and employment opportunities in rural areas of the United States. The project is particularly concerned with the effects of international trade on wages and employment for rural women. The project will achieve this objective through 1) assessment of the impacts of international trade involvement on male and female wages and wage inequality in rural areas; 2) assessment of the impacts of international trade involvement on employment for both men and women in rural areas.
"Business Cycle Synchronization in the EU: A Regional-Sectoral Look
  through Soft-Clustering and Wavelet Decomposition","This paper elaborates on the sectoral-regional view of the business cycle
synchronization in the EU -- a necessary condition for the optimal currency
area. We argue that complete and tidy clustering of the data improves the
decision maker's understanding of the business cycle and, by extension, the
quality of economic decisions. We define the business cycles by applying a
wavelet approach to drift-adjusted gross value added data spanning over 2000Q1
to 2021Q2. For the application of the synchronization analysis, we propose the
novel soft-clustering approach, which adjusts hierarchical clustering in
several aspects. First, the method relies on synchronicity dissimilarity
measures, noting that, for time series data, the feature space is the set of
all points in time. Then, the ``soft'' part of the approach strengthens the
synchronization signal by using silhouette measures. Finally, we add a
probabilistic sparsity algorithm to drop out the most asynchronous ``noisy''
data improving the silhouette scores of the most and less synchronous groups.
The method, hence, splits the sectoral-regional data into three groups: the
synchronous group that shapes the EU business cycle; the less synchronous group
that may hint at cycle forecasting relevant information; the asynchronous group
that may help investors to diversify through-the-cycle risks of the investment
portfolios. The results support the core-periphery hypothesis.","['Saulius Jokubaitis', 'Dmitrij Celov']",[],0,arXiv,http://arxiv.org/abs/2206.14128v1,False,True,False,False,False,712,Nicholas A Bloom,Berkeley,Completed,2009,2014.0,"This project investigates the role of variations in the level of uncertainty as a key driver of the business cycle. The most important piece of this research is construction of time series measures of uncertainty. This project is to inform the U.S. Census Bureau on four issues. One, the project will help to improve the quality of data produced by the Census Bureau. By constructing measures of volatility from census micro datasets and comparing these with external measures of volatility, the likely extent of measurement error in the census datasets can be evaluated. Two, it will lead to an improved methodology for tabulating aggregate statistics derived from census data. The analysis will aim to highlight the significance of time variation in the dispersion of establishment-level data. The project would then lead to a recommendation to display aggregate variances at yearly intervals. Three, it will help to prepare estimates of nonresponse over the business cycle that are driven by fluctuations in volatility. To estimate changes in volatility over the business cycle, changes in data imputation must be controlled for in the analysis. This project will spend considerable time evaluating the cyclical and industrial properties of nonresponse. Four, the project will help to improve survey estimation techniques by identifying periods of time and industries with higher levels of underlying volatility. If recessions change the response rate of establishments to the surveys, then the requisite increase in the amount of imputed data could generate cyclical bias."
Testing Business Cycle Theories: Evidence from the Great Recession,"Empirical business cycle studies using cross-country data usually cannot
achieve causal relationships while within-country studies mostly focus on the
bust period. We provide the first causal investigation into the boom period of
the 1999-2010 U.S. cross-metropolitan business cycle. Using a novel research
design, we show that credit expansion in private-label mortgages causes a
differentially stronger boom (2000-2006) and bust (2007-2010) cycle in the
house-related industries in the high net-export-growth areas. Most importantly,
our unique research design enables us to perform the most comprehensive tests
on theories (hypotheses) regarding the business cycle. We show that the
following theories (hypotheses) cannot explain the cause of the 1999-2010 U.S.
business cycle: the speculative euphoria hypothesis, the real business cycle
theory, the collateral-driven credit cycle theory, the business uncertainty
theory, and the extrapolative expectation theory.",['Bo Li'],[],0,arXiv,http://arxiv.org/abs/2403.04104v1,False,True,False,False,False,712,Nicholas A Bloom,Berkeley,Completed,2009,2014.0,"This project investigates the role of variations in the level of uncertainty as a key driver of the business cycle. The most important piece of this research is construction of time series measures of uncertainty. This project is to inform the U.S. Census Bureau on four issues. One, the project will help to improve the quality of data produced by the Census Bureau. By constructing measures of volatility from census micro datasets and comparing these with external measures of volatility, the likely extent of measurement error in the census datasets can be evaluated. Two, it will lead to an improved methodology for tabulating aggregate statistics derived from census data. The analysis will aim to highlight the significance of time variation in the dispersion of establishment-level data. The project would then lead to a recommendation to display aggregate variances at yearly intervals. Three, it will help to prepare estimates of nonresponse over the business cycle that are driven by fluctuations in volatility. To estimate changes in volatility over the business cycle, changes in data imputation must be controlled for in the analysis. This project will spend considerable time evaluating the cyclical and industrial properties of nonresponse. Four, the project will help to improve survey estimation techniques by identifying periods of time and industries with higher levels of underlying volatility. If recessions change the response rate of establishments to the surveys, then the requisite increase in the amount of imputed data could generate cyclical bias."
"Planning Optimal From the Firm Value Creation Perspective Levels of
  Operating Cash Investments","The basic financial purpose of corporation is creation of its value.
Liquidity management should also contribute to realization of this fundamental
aim. Many of the current asset management models that are found in financial
management literature assume book profit maximization as the basic financial
purpose. These book profit based models could be lacking in what relates to
another aim like maximization of enterprise value. The corporate value creation
strategy is executed with a focus on risk and uncertainty. Firms hold cash for
a variety of reasons. Generally, cash balances held in a firm can be called
considered, precautionary, speculative, transactional and intentional. The
first are the result of management anxieties. Managers fear the negative part
of the risk and hold cash to hedge against it. Second, cash balances are held
to use chances that are created by the positive part of the risk equation.
Next, cash balances are the result of the operating needs of the firm. In this
article, we analyze the relation between these types of cash balances and risk.
This article presents the discussion about relations between firm net working
investment policy and as result operating cash balances and firm value. This
article also contains propositions for marking levels of precautionary cash
balances and speculative cash balances. Application of these propositions
should help managers to make better decisions to maximize the value of a firm.",['Grzegorz Michalski'],[],0,arXiv,http://arxiv.org/abs/1301.3824v1,False,True,False,False,False,716,Kandice A Kapinos,Michigan,Completed,2009,2013.0,"This study investigates the determinants of private pension plan conversions from traditional defined benefit plans to hybrid cash balance plans during the 1990s. It examines whether there are systematic differences in the types of firms and plans that converted. It also examines the endogenous rela­tionship between the likelihood of conversion (on pension plan type, in general) and employee turnover. The project utilizes data from several sources for this analysis. Information on pension plan characteristics comes from the annual 5500 forms which most private sector firms file each year in accordance with the Internal Revenue Service, Department of Labor, and Pension Benefit Guaranty Corporation (PBGC). From the PBGC also comes information on plan terminations. These data are linked to the Longitudinal Employer-Household Dynamics data, which provide information about establishment-level characteristics of the workforce. Firm level data from Compustat provide several important firm-level characteristics. The research intends to improve the link between the Form 5500 data and the Business Register. It will also compare data from the Longitudinal Business Database and Economic Surveys to Form 5500 data in order to evaluate the strengths and weaknesses of each dataset in studying firm human resource policies. It will also establish evidence on the determinants of employer provided pension plan policy changes and provide estimates of population characteristics relating to changes in employer provided pension plan coverage rates."
"Engines of Power: Electricity, AI, and General-Purpose Military
  Transformations","Major theories of military innovation focus on relatively narrow
technological developments, such as nuclear weapons or aircraft carriers.
Arguably the most profound military implications of technological change,
however, come from more fundamental advances arising from general purpose
technologies, such as the steam engine, electricity, and the computer. With few
exceptions, political scientists have not theorized about GPTs. Drawing from
the economics literature on GPTs, we distill several propositions on how and
when GPTs affect military affairs. We call these effects general-purpose
military transformations. In particular, we argue that the impacts of GMTs on
military effectiveness are broad, delayed, and shaped by indirect productivity
spillovers. Additionally, GMTs differentially advantage those militaries that
can draw from a robust industrial base in the GPT. To illustrate the
explanatory value of our theory, we conduct a case study of the military
consequences of electricity, the prototypical GPT. Finally, we apply our
findings to artificial intelligence, which will plausibly cause a profound
general-purpose military transformation.","['Jeffrey Ding', 'Allan Dafoe']",[],0,arXiv,http://arxiv.org/abs/2106.04338v1,False,True,False,False,False,717,Jeffrey Wenger,UCLA,Completed,2015,2024.0,"This project will study the local labor markets dynamics related to two understudied segments of the U.S. population which likely differ from the rest of the population, particularly in recent years. These two subpopulations are reservists and the civilian spouses of military service members. The project will analyze the effects on reservists of leaving the civilian labor market temporarily; the cumulative effects of reservist activations on local labor markets and firms that employ reservists; and the effects of deployments on the labor market experiences of civilian spouses of military service members. "
"Joint Modeling of Multiple Longitudinal Biomarkers and Survival Outcomes
  via Threshold Regression: Variability as a Predictor","Longitudinal biomarker data and health outcomes are routinely collected in
many studies to assess how biomarker trajectories predict health outcomes.
Existing methods primarily focus on mean biomarker profiles, treating
variability as a nuisance. However, excess variability may indicate system
dysregulations that may be associated with poor outcomes. In this paper, we
address the long-standing problem of using variability information of multiple
longitudinal biomarkers in time-to-event analyses by formulating and studying a
Bayesian joint model. We first model multiple longitudinal biomarkers, some of
which are subject to limit-of-detection censoring. We then model the survival
times by incorporating random effects and variances from the longitudinal
component as predictors through threshold regression that admits
non-proportional hazards. We demonstrate the operating characteristics of the
proposed joint model through simulations and apply it to data from the Study of
Women's Health Across the Nation (SWAN) to investigate the impact of the mean
and variability of follicle-stimulating hormone (FSH) and anti-Mullerian
hormone (AMH) on age at the final menstrual period (FMP).","['Mingyan Yu', 'Zhenke Wu', 'Michelle M. Hood', 'Carrie A. Karvonen-Gutierrez', 'Sioban D. Harlow', 'Michael R. Elliott']",[],0,arXiv,http://arxiv.org/abs/2503.24146v1,False,True,False,False,False,729,Lucas W Davis,Michigan,Completed,2009,2013.0,"Exposure to environmental emissions may be an important determinant of health outcomes, but it is difficult to obtain scientific evidence on this point given the infeasibility of conducting randomized experiments. This research examines the relationship between emis­sions and health outcomes using establishment-level data in the Longitudinal Business Database (LBD) and linked establishment-level surveys (the Census of Manufactures, the Annual Survey of Manufactures, the Census of Mining, the Census of Transportation, Communications, and Utilities, and the Census of Wholesale Trade) merged with publicly-available information about emissions. The research design will exploit the precise record of establishment openings and closings in the LBD to identify effects on health outcomes. The first objective of this project is to evaluate the accuracy of establishment openings and closings in the LBD by merg­ing the LBD with establishments in the Environmental Protection Agency’s Toxic Release Inventory (TRI). The second objective of the project is to generate new estimates of the effect of indus­trial emissions on human mortality. The U.S. Census Bureau’s National Longitudinal Mortality Study (NLMS) is a database designed for studying the effects of demographic and socio-economic characteristics on mortality rates. The estimates will help realize the objective of the NLMS, helping the Census Bureau better understand the role of environmental factors as a determinant of human mortality, as well as to better understand the extent to which industrial emissions have been a confounding factor in existing studies of mortality using the NLMS."
Zero-Leverage Puzzle,"In this paper, I examine why some firms have zero leverage. I fail to find
evidence that firms are unlevered because of managerial entrenchment since
these firms do not have weaker corporate governance. I reject the hypothesis
that firms become zero-leverage after prolonged periods of high market
valuation, since before levering these firms do not suffer from declining
valuations and continue to issue large amounts of equity. I find strong
evidence in favor of the financial constraints explanation of the zero-leverage
puzzle. Zero-leverage firms appear to be financially constrained using three
different measures of financial constraints. I obtain mixed evidence on the
financial flexibility hypothesis since all-equity firms increase investments
and acquisitions after levering, but the probability of their levering
decreased during the financial crisis. My results suggest that financial
constraints are the first-order the driver of zero-leverage behavior and are
more important than less obvious explanations such as managerial entrenchment.",['Mykola Pinchuk'],[],0,arXiv,http://arxiv.org/abs/2302.00761v1,False,True,False,False,False,730,Katharina A Lewellen,Boston,Completed,2010,2014.0,"The project will merge the Longitudinal Business Database (LBD), Annual Survey of Manufacturers (ASM/LRD), and Census of Manufacturers (CMF/LRD) databases with the SDC Platinum database provided by the Securities Data Corporation (SDC) to assess the quality of merger and acquisition information in Title 13, Chapter 5 Census data, and to investigate investment and productivity changes surrounding managerial successions. Senior management turnover events are typically followed by operational changes as well as asset restructurings, such as plant sales and acquisitions. Matched information from SDC will allow us to identify plants that change hands around the time of turnover in the management team, and to track factor productivity for both the sold and the remaining assets. We will study managers' incentives to divest underperforming assets, and whether better corporate governance leads to more timely and efficient management replacement decisions. We will also explore long-run trends in corporate governance, investment, and firms' organizational structures.
We will estimate the overall change in plant performance, as measured by total factor productivity (TFP), following managerial turnover. We will also use the Quarterly Financial Reports (QFR) to examine if the financial position of the firm improves following management change, and what the interplay between changes in TFP and changes in firm financial performance might be. Detail drivers behind any pre- and post-turnover performance changes will be explored, such as plant factor efficiency, employment changes using Longitudinal Employer-Household Dynamics (LEHD-ECF/QWI) data, plant utilization from the Survey of Plant Capacity Utilization (PCU), asset sales, including the sale of plants or segments of the firm, capital investment from the Annual Capital Expenditures Survey (ACES), research and development investment from the Research and Development (R&D) survey, or the acquisition of other plants as captured in the LBD and Ownership Change Database (OCD). Census and SDC files will be linked using the Compustat-SSEL bridge file, the Standard Statistical Establishment List (SSEL-NA), and the LEHD bridge files (BRB/GAL). Firms' asset management decisions affect various measures collected in Census Bureau data programs, including measures of investment and the purchase and sale of establishments and other firms. This project will inform Census program staff of underlying mechanisms that drive the values of reported measures, and the extent to which Census data programs capture firms' organizational changes. Variation in the quality of Census data on merger and acquisition activity is expected t+H264o be related to firm characteristics, such as acquisition ownership share, firm size, legal form of organization, and family ownership. We will explore if misreporting of acquisition activity is related to information uncertainty due to the duration of time elapsed between when an acquisition is announced and when it is completed, and discrepancies between the expected date and actual date as reported in the SDC data."
Only-child matching penalty in the marriage market,"This study explores the marriage matching of only-child individuals and its
outcome. Specifically, we analyze two aspects. First, we investigate how
marital status (i.e., marriage with an only child, that with a non-only child
and remaining single) differs between only children and non-only children. This
analysis allows us to know whether people choose mates in a positive or a
negative assortative manner regarding only-child status, and to predict whether
only-child individuals benefit from marriage matching premiums or are subject
to penalties regarding partner attractiveness. Second, we measure the
premium/penalty by the size of the gap in partner's socio economic status (SES,
here, years of schooling) between only-child and non--only-child individuals.
The conventional economic theory and the observed marriage patterns of positive
assortative mating on only-child status predict that only-child individuals are
subject to a matching penalty in the marriage market, especially when their
partner is also an only child. Furthermore, our estimation confirms that among
especially women marrying an only-child husband, only children are penalized in
terms of 0.57-years-lower educational attainment on the part of the partner.","['Keisuke Kawata', 'Mizuki Komura']",[],0,arXiv,http://arxiv.org/abs/2307.15336v1,False,True,False,False,False,736,Maria D Fitzpatrick,Berkeley,Completed,2009,2014.0,"Early childhood activities such as preschool and day care have been receiving considerable attention in recent years as avenues for providing child care and promoting school readiness. In part, this is because female labor force participation has changed dramatically in recent decades, fostering interest in the role of children in female decisions about work. This research project studies the child care market in the United States. It links existing datasets to create a unique resource for examining the supply and demand of child care, the labor market for child care workers, how parental decisions about investments of resources (such as labor supply, fertility and education) interact with decisions about child care, and how government involvement (e.g., through regulation and funding intervention) affects the market for child care. The analysis uses multivariate regression and other descriptive statistical procedures (such as cross tabulations and means) to investigate the market for child care."
"A Review on Mechanics and Mechanical Properties of 2D Materials -
  Graphene and Beyond","Since the first successful synthesis of graphene just over a decade ago, a
variety of two-dimensional (2D) materials (e.g., transition
metal-dichalcogenides, hexagonal boron-nitride, etc.) have been discovered.
Among the many unique and attractive properties of 2D materials, mechanical
properties play important roles in manufacturing, integration and performance
for their potential applications. Mechanics is indispensable in the study of
mechanical properties, both experimentally and theoretically. The coupling
between the mechanical and other physical properties (thermal, electronic,
optical) is also of great interest in exploring novel applications, where
mechanics has to be combined with condensed matter physics to establish a
scalable theoretical framework. Moreover, mechanical interactions between 2D
materials and various substrate materials are essential for integrated device
applications of 2D materials, for which the mechanics of interfaces (adhesion
and friction) has to be developed for the 2D materials. Here we review recent
theoretical and experimental works related to mechanics and mechanical
properties of 2D materials. While graphene is the most studied 2D material to
date, we expect continual growth of interest in the mechanics of other 2D
materials beyond graphene.","['Deji Akinwande', 'Christopher J. Brennan', 'J. Scott Bunch', 'Philip Egberts', 'Jonathan R. Felts', 'Huajian Gao', 'Rui Huang', 'Joon-Seok Kim', 'Teng Li', 'Yao Li', 'Kenneth M. Liechti', 'Nanshu Lu', 'Harold S. Park', 'Evan J. Reed', 'Peng Wang', 'Boris I. Yakobson', 'Teng Zhang', 'Yong-Wei Zhang', 'Yao Zhou', 'Yong Zhu']",[],0,arXiv,http://arxiv.org/abs/1611.01555v1,False,True,False,False,False,749,Christopher J Ordowich,Washington,Completed,2009,2012.0,"This research project aims to improve the quality of U.S. Census Bureau data and to increase knowledge about the determinants of manufacturing establishment performance. These objectives will be accomplished by linking an external dataset of establishments receiving business assistance between 1999 and 2007 to census datasets. The project uses the external dataset of business assistance recipients to both validate and improve the quality of census data. The data will be used to identify limitations of the census sampling frame, measure data quality, estimate nonresponse bias, and improve imputations for nonresponse in census datasets. The business assistance dataset is valuable because it contains data at the same level of resolution and contains some of the same data elements as census datasets. The project uses the linked datasets to explore determinants of establishment productivity not currently measured in census datasets. Specifically, the research explores how manufacturing establishment performance is affected by business assistance and how the effects vary across measurable dimensions. The analysis focuses on business assistance provided by the National Institute of Standards and Technology’s Manufacturing Extension Partnership between 1999 and 2007. The project employs a variety of econometric approaches, primarily relying on an instrumental variables approach to estimate how measures of establishment performance such as productivity, output, and employment growth are affected by different types and levels of business assistance. Access to the census datasets is required to provide a valid control group and to provide key performance and control variables for this analysis."
"Effect of infill pattern and build orientation on mechanical properties
  of FDM printed parts: An experimental modal analysis approach","Fused Deposition Modeling (FDM) is one of the most widely explored additive
manufacturing method that uses thermoplastic materials to manufacture products.
Mechanical properties of parts manufactured using FDM are influenced by
different process parameters involved during manufacturing as they impact the
bonding among different layers of cross-section. In this study, the effect of
infill patterns and build orientations on the mechanical properties of PLA
based parts manufactured using FDM method is studied. Six different infill
patterns (triangles, cubic, concentric, tetrahedral, lines, and zigzag) and
three different orientations (flatwise, edgewise, and upright) are considered
for rectangular beam type parts. For determining mechanical properties of parts
with different infill patterns and orientations, flexural bending test is
performed. Experimental modal analysis of manufactured parts is performed to
observe the relation of natural frequencies with elastic modulus of the parts
obtained from flexural bending test. The possibility of experimental modal
analysis as an alternative non-destructive method for testing mechanical
properties of FDM 3D printed parts is explored.",['Santosh Rajkumar'],[],0,arXiv,http://arxiv.org/abs/2202.05692v1,False,True,False,False,False,749,Christopher J Ordowich,Washington,Completed,2009,2012.0,"This research project aims to improve the quality of U.S. Census Bureau data and to increase knowledge about the determinants of manufacturing establishment performance. These objectives will be accomplished by linking an external dataset of establishments receiving business assistance between 1999 and 2007 to census datasets. The project uses the external dataset of business assistance recipients to both validate and improve the quality of census data. The data will be used to identify limitations of the census sampling frame, measure data quality, estimate nonresponse bias, and improve imputations for nonresponse in census datasets. The business assistance dataset is valuable because it contains data at the same level of resolution and contains some of the same data elements as census datasets. The project uses the linked datasets to explore determinants of establishment productivity not currently measured in census datasets. Specifically, the research explores how manufacturing establishment performance is affected by business assistance and how the effects vary across measurable dimensions. The analysis focuses on business assistance provided by the National Institute of Standards and Technology’s Manufacturing Extension Partnership between 1999 and 2007. The project employs a variety of econometric approaches, primarily relying on an instrumental variables approach to estimate how measures of establishment performance such as productivity, output, and employment growth are affected by different types and levels of business assistance. Access to the census datasets is required to provide a valid control group and to provide key performance and control variables for this analysis."
"How does stock market reflect the change in economic demand? A study on
  the industry-specific volatility spillover networks of China's stock market
  during the outbreak of COVID-19","Using the carefully selected industry classification standard, we divide 102
industry securities indices in China's stock market into four demand-oriented
sector groups and identify demand-oriented industry-specific volatility
spillover networks. The ""deman-oriented"" is a new idea of reconstructing the
structure of the networks considering the relationship between industry sectors
and the economic demand their outputs meeting. Networks with the new structure
help us improve the understanding of the economic demand change, especially
when the macroeconomic is dramatically influenced by exogenous shocks like the
outbreak of COVID-19. At the beginning of the outbreak of COVID-19, in China's
stock market, spillover effects from industry indices of sectors meeting the
investment demand to those meeting the consumption demands rose significantly.
However, these spillover effects fell after the outbreak containment in China
appeared to be effective. Besides, some services sectors including utility,
transportation and information services have played increasingly important
roles in the networks of industry-specific volatility spillovers as of the
COVID-19 out broke. By implication, firstly, being led by Chinese government,
the COVID-19 is successfully contained and the work resumption is organized
with a high efficiency in China. The risk of the investment demand therefore
was controlled and eliminated relatively fast. Secondly, the intensive using of
non-pharmaceutical interventions (NPIs) led to supply restriction in services
in China. It will still be a potential threat for the Chinese economic recovery
in the next stage.","['Fu Qiao', 'Yan Yan']",[],0,arXiv,http://arxiv.org/abs/2007.07487v1,False,True,False,False,False,751,Evan T Rawley,Baruch,Completed,2010,2014.0,"This project studies how spatial location and firm organization influence productivity and technological spillovers.  The project proposes and tests that regional clustering increases within-firm spillovers and decreases the extent to which firms are vertically integrated.  The project also explicitly tests for evidence of worker flow spillovers across industries in a pair of economically related high-tech localized industries that experienced regulatory shocks influencing entry.  The project benefits the Census Bureau by verifying the accuracy of the Business Register, merging in new establishment-level technology and customer data, and by informing the Census about supply chains and organizations.  "
"Investigating Mode Effects in Interviewer Variances Using Two
  Representative Multi-mode Surveys","This study examines whether interviewer variances remain consistent across
different modes in mixed-mode studies, using data from two distinct designs. In
the first design, when interviewers are responsible for either face-to-face or
telephone mode, we examine whether there are mode differences in interviewer
variances for 1) sensitive political questions, 2) international items, 3) and
item missing indicators on international items, using the Arab Barometer wave 6
Jordan data. In the second design, we draw on Health and Retirement Study (HRS)
2016 core survey data to examine the question on three topics when interviewers
are responsible for both modes. The topics cover 1) the CESD depression scale,
2) interviewer observations, and 3) the physical activity scale. To account for
the lack of interpenetrated designs in both data sources, we include
respondent-level covariates in our models. We find significant differences in
interviewer variances on one item (twelve items in total) in the Arab Barometer
study; whereas for HRS, the results are three out of eighteen. Overall, we find
the magnitude of the interviewer variances larger in FTF than TEL on sensitive
items. We conduct simulations to understand the power to detect mode effects in
the typically modest interviewer sample sizes.","['Wenshan Yu', 'Michael R. Elliott', 'Trivellore E. Raghunathan']",[],0,arXiv,http://arxiv.org/abs/2408.11874v1,False,True,False,False,False,755,Trivellore E Raghunathan,Michigan,Completed,2009,2012.0,"Sample surveys are a crucial source of information about the state of public health and people’s quality of life. Moreover, they provide an efficient way to identify and monitor illness and disability trends and track progress toward achieving the Center for Disease Control’s (CDC) Health Protection Goals. Increasingly, this information is being demanded in the form of small area statistics to monitor health trends and support policy decisions in small geographic areas, including those that are typically underrepresented in large-scale data collection projects. However, the CDC is often prevented from releasing small area identifiers in public-use datasets because the data do not satisfy certain disclosure restrictions. This research tests and evaluates a new method for generating public-use micro-level datasets that contain enough geographical detail to permit small area estimation without compromising the confidentiality of survey respondents. The method uses the observed survey data to fit a statistical imputation model that generates synthetic data records, which comprise the public-use data records. The synthetic data are generated to emulate the observed data and preserve important statistical properties of the observed data. Moreover, the synthetic data can account for the hierarchical clustering structure associated with multiple levels of geography; thus, permitting data users to perform various geographical analyses with a single dataset. Confidentiality protection is greatly enhanced because no actual data values are released to the public. The proposed methodology is tested and evaluated using confidential data from the National Health Interview Survey. Synthetic versions of this data source will be generated for key variables relevant to national health objectives. Various parametric and non-parametric imputation models capable of handling different variable types will be investigated."
Designing biological circuits: from principles to applications,"Genetic circuit design is a well-studied problem in synthetic biology. Ever
since the first genetic circuits -- the repressilator and the toggle switch --
were designed and implemented, many advances have been made in this area of
research. The current review systematically organizes a number of key works in
this domain by employing the versatile framework of generalized morphological
analysis. Literature in the area has been mapped based on (a) the design
methodologies used, ranging from brute-force searches to control-theoretic
approaches, (b) the modelling techniques employed, (c) various circuit
functionalities implemented, (d) key design characteristics, and (e) the
strategies used for the robust design of genetic circuits. We conclude our
review with an outlook on multiple exciting areas for future research, based on
the systematic assessment of key research gaps that have been readily
unravelled by our analysis framework.","['Debomita Chakraborty', 'Raghunathan Rengaswamy', 'Karthik Raman']",[],0,arXiv,http://arxiv.org/abs/2111.04508v1,False,True,False,False,False,755,Trivellore E Raghunathan,Michigan,Completed,2009,2012.0,"Sample surveys are a crucial source of information about the state of public health and people’s quality of life. Moreover, they provide an efficient way to identify and monitor illness and disability trends and track progress toward achieving the Center for Disease Control’s (CDC) Health Protection Goals. Increasingly, this information is being demanded in the form of small area statistics to monitor health trends and support policy decisions in small geographic areas, including those that are typically underrepresented in large-scale data collection projects. However, the CDC is often prevented from releasing small area identifiers in public-use datasets because the data do not satisfy certain disclosure restrictions. This research tests and evaluates a new method for generating public-use micro-level datasets that contain enough geographical detail to permit small area estimation without compromising the confidentiality of survey respondents. The method uses the observed survey data to fit a statistical imputation model that generates synthetic data records, which comprise the public-use data records. The synthetic data are generated to emulate the observed data and preserve important statistical properties of the observed data. Moreover, the synthetic data can account for the hierarchical clustering structure associated with multiple levels of geography; thus, permitting data users to perform various geographical analyses with a single dataset. Confidentiality protection is greatly enhanced because no actual data values are released to the public. The proposed methodology is tested and evaluated using confidential data from the National Health Interview Survey. Synthetic versions of this data source will be generated for key variables relevant to national health objectives. Various parametric and non-parametric imputation models capable of handling different variable types will be investigated."
Synthetic Area Weighting for Measuring Public Opinion in Small Areas,"The comparison of subnational areas is ubiquitous but survey samples of these
areas are often biased or prohibitively small. Researchers turn to methods such
as multilevel regression and poststratification (MRP) to improve the efficiency
of estimates by partially pooling data across areas via random effects.
However, the random effect approach can pool observations only through
area-level aggregates. We instead propose a weighting estimator, the synthetic
area estimator, which weights on variables measured only in the survey to
partially pool observations individually. The proposed method consists of
two-step weighting: first to adjust differences across areas and then to adjust
for differences between the sample and population. Unlike MRP, our estimator
can directly use the national weights that are often estimated from pollsters
using proprietary information. Our approach also clarifies the assumptions
needed for valid partial pooling, without imposing an outcome model. We apply
the proposed method to estimate the support for immigration policies at the
congressional district level in Florida. Our empirical results show that small
area estimation models with insufficient covariates can mask opinion
heterogeneities across districts.","['Shiro Kuriwaki', 'Soichiro Yamauchi']",[],0,arXiv,http://arxiv.org/abs/2105.05829v1,False,True,False,False,False,755,Trivellore E Raghunathan,Michigan,Completed,2009,2012.0,"Sample surveys are a crucial source of information about the state of public health and people’s quality of life. Moreover, they provide an efficient way to identify and monitor illness and disability trends and track progress toward achieving the Center for Disease Control’s (CDC) Health Protection Goals. Increasingly, this information is being demanded in the form of small area statistics to monitor health trends and support policy decisions in small geographic areas, including those that are typically underrepresented in large-scale data collection projects. However, the CDC is often prevented from releasing small area identifiers in public-use datasets because the data do not satisfy certain disclosure restrictions. This research tests and evaluates a new method for generating public-use micro-level datasets that contain enough geographical detail to permit small area estimation without compromising the confidentiality of survey respondents. The method uses the observed survey data to fit a statistical imputation model that generates synthetic data records, which comprise the public-use data records. The synthetic data are generated to emulate the observed data and preserve important statistical properties of the observed data. Moreover, the synthetic data can account for the hierarchical clustering structure associated with multiple levels of geography; thus, permitting data users to perform various geographical analyses with a single dataset. Confidentiality protection is greatly enhanced because no actual data values are released to the public. The proposed methodology is tested and evaluated using confidential data from the National Health Interview Survey. Synthetic versions of this data source will be generated for key variables relevant to national health objectives. Various parametric and non-parametric imputation models capable of handling different variable types will be investigated."
Synthetic Data for Deep Learning,"Synthetic data is an increasingly popular tool for training deep learning
models, especially in computer vision but also in other areas. In this work, we
attempt to provide a comprehensive survey of the various directions in the
development and application of synthetic data. First, we discuss synthetic
datasets for basic computer vision problems, both low-level (e.g., optical flow
estimation) and high-level (e.g., semantic segmentation), synthetic
environments and datasets for outdoor and urban scenes (autonomous driving),
indoor scenes (indoor navigation), aerial navigation, simulation environments
for robotics, applications of synthetic data outside computer vision (in neural
programming, bioinformatics, NLP, and more); we also survey the work on
improving synthetic data development and alternative ways to produce it such as
GANs. Second, we discuss in detail the synthetic-to-real domain adaptation
problem that inevitably arises in applications of synthetic data, including
synthetic-to-real refinement with GAN-based models and domain adaptation at the
feature/model level without explicit data transformations. Third, we turn to
privacy-related applications of synthetic data and review the work on
generating synthetic datasets with differential privacy guarantees. We conclude
by highlighting the most promising directions for further work in synthetic
data studies.",['Sergey I. Nikolenko'],[],0,arXiv,http://arxiv.org/abs/1909.11512v1,False,True,False,False,False,755,Trivellore E Raghunathan,Michigan,Completed,2009,2012.0,"Sample surveys are a crucial source of information about the state of public health and people’s quality of life. Moreover, they provide an efficient way to identify and monitor illness and disability trends and track progress toward achieving the Center for Disease Control’s (CDC) Health Protection Goals. Increasingly, this information is being demanded in the form of small area statistics to monitor health trends and support policy decisions in small geographic areas, including those that are typically underrepresented in large-scale data collection projects. However, the CDC is often prevented from releasing small area identifiers in public-use datasets because the data do not satisfy certain disclosure restrictions. This research tests and evaluates a new method for generating public-use micro-level datasets that contain enough geographical detail to permit small area estimation without compromising the confidentiality of survey respondents. The method uses the observed survey data to fit a statistical imputation model that generates synthetic data records, which comprise the public-use data records. The synthetic data are generated to emulate the observed data and preserve important statistical properties of the observed data. Moreover, the synthetic data can account for the hierarchical clustering structure associated with multiple levels of geography; thus, permitting data users to perform various geographical analyses with a single dataset. Confidentiality protection is greatly enhanced because no actual data values are released to the public. The proposed methodology is tested and evaluated using confidential data from the National Health Interview Survey. Synthetic versions of this data source will be generated for key variables relevant to national health objectives. Various parametric and non-parametric imputation models capable of handling different variable types will be investigated."
"Male Earnings Volatility in LEHD before, during, and after the Great
  Recession","This paper is part of a coordinated collection of papers on prime-age male
earnings volatility. Each paper produces a similar set of statistics for the
same reference population using a different primary data source. Our primary
data source is the Census Bureau's Longitudinal Employer-Household Dynamics
(LEHD) infrastructure files. Using LEHD data from 1998 to 2016, we create a
well-defined population frame to facilitate accurate estimation of temporal
changes comparable to designed longitudinal samples of people. We show that
earnings volatility, excluding increases during recessions, has declined over
the analysis period, a finding robust to various sensitivity analyses.","['Kevin L. McKinney', 'John M. Abowd']",[],0,arXiv,http://arxiv.org/abs/2008.00253v3,True,True,False,False,False,758,William R Walker,Baruch,Completed,2010,2012.0,"Recent studies have found a significant relationship between environmental regulation and job loss. However, little is known about those affected by regulation induced job separation, including the significant costs incurred by those displaced. Several authors, including the EPA, have stressed the importance of accounting for adjustment costs of environmental mandates. However, little work has been done in this area, mostly as a result of limitations involving data. I plan to use the very rich administrative data of the Longitudinal Employer-Household Dynamics file to explore transitional impacts of the Clean Air Act Amendments of the 1990's. With the data, I will be able to provide robust estimates as to unemployment duration and long term earnings losses associated with increased air quality mandates. The richness of the data also allows me to examine whether regulation induces job transfer within or across industries. This project will provide the first estimates as to the adjustment costs associated with transitioning into a more stringent environmental regime. "
"Spatially resolved properties for extremely metal-poor star-forming
  galaxies with Wolf-Rayet features and high-ionization lines","Extremely metal-poor, high-ionizing starbursts in the local Universe provide
unique laboratories for exploring in detail the physics of high-redshift
systems. Also, their ongoing star-formation and haphazard morphology make them
outstanding proxies for primordial galaxies. Using integral field spectroscopy,
we spatially resolved the ISM properties and massive stars of two first-class
low metallicity galaxies with Wolf-Rayet features and nebular HeII emission:
Mrk178 and IZw18. In this review, we summarize our main results for these two
objects.",['C. Kehrig'],[],0,arXiv,http://arxiv.org/abs/1701.05435v1,False,True,False,False,False,761,Matthias G Kehrig,Texas,Completed,2009,2014.0,"This project uses establishment-level data to estimate returns to scale in U.S. manufacturing and construction firms. Understanding how the degree of returns to scale shapes the production of investment goods will be instrumental for the U.S. Census Bureau to understand establishment-level data. Constructing a comprehensive panel of manufacturing firms and estimating production functions can identify the degree of returns to scale at the establishment level. Regressing output on factor inputs will deliver the degree of returns to scale. Prices for equipment and structure investment behave very differently which suggests differential technologies and returns to scale. To pay special attention to that difference, the Census of Construction Industries is used to contrast construction firms to manufacturing establishments. This project benefits the Census Bureau by correcting for measurement error, imputing capital stock for manufacturing establishments in the Annual Survey of Manufactures, seasonally adjusting older (annual) Plant Capacity Utilization data, and estimating capacity utilization for a large population of manufacturing plants that is not covered at present. This project will also deliver a precise estimate of the degree of returns to scale that is free from aggregation bias. This information is very relevant by helping to assess two competing theories of macroeconomic fluctuations. Understanding the source, nature, and transmission of fluctuations will not only advance the insight in the field of fluctuations but will also have neighboring fields in economics."
3D spectroscopy of Wolf-Rayet HII galaxies,"Wolf-Rayet (WR) HII galaxies are local metal-poor star-forming galaxies,
observed when the most massive stars are evolving from O stars to WR stars,
making them template systems to study distant starbursts. We have been
performing a program to investigate the interplay between massive stars and gas
in WR HII galaxies using IFS. Here, we highlight some results from the first 3D
spectroscopic study of Mrk 178, the closest metal-poor WR HII galaxy, focusing
on the origin of the nebular HeII emission and the aperture effects on the
detection of WR features.","['C. Kehrig', 'E. Perez-Montero', 'J. M. Vilchez', 'J. Brinchmann', 'D. Kunth', 'F. Durret', 'J. Iglesias-Paramo', 'J. Hernandez-Fernandez']",[],0,arXiv,http://arxiv.org/abs/1410.2635v1,False,True,False,False,False,761,Matthias G Kehrig,Texas,Completed,2009,2014.0,"This project uses establishment-level data to estimate returns to scale in U.S. manufacturing and construction firms. Understanding how the degree of returns to scale shapes the production of investment goods will be instrumental for the U.S. Census Bureau to understand establishment-level data. Constructing a comprehensive panel of manufacturing firms and estimating production functions can identify the degree of returns to scale at the establishment level. Regressing output on factor inputs will deliver the degree of returns to scale. Prices for equipment and structure investment behave very differently which suggests differential technologies and returns to scale. To pay special attention to that difference, the Census of Construction Industries is used to contrast construction firms to manufacturing establishments. This project benefits the Census Bureau by correcting for measurement error, imputing capital stock for manufacturing establishments in the Annual Survey of Manufactures, seasonally adjusting older (annual) Plant Capacity Utilization data, and estimating capacity utilization for a large population of manufacturing plants that is not covered at present. This project will also deliver a precise estimate of the degree of returns to scale that is free from aggregation bias. This information is very relevant by helping to assess two competing theories of macroeconomic fluctuations. Understanding the source, nature, and transmission of fluctuations will not only advance the insight in the field of fluctuations but will also have neighboring fields in economics."
"Variational and parquet-diagram calculations for neutron matter. II.
  Twisted Chain Diagrams","We develop a manifestly microscopic method to deal with strongly interacting
nuclear systems that have different interactions in spin-singlet and
spin-triplet states. In a first step we analyze variational wave functions that
have been suggested to describe such systems, and demonstrate that the
so-called commutator contributions can have important effects whenever the
interactions in the spin-singlet and the spin-triplet states are very
different. We then identify these contributions as terms that correspond, in
the language of perturbation theory, to non-parquet diagrams. We include these
diagrams in a way that is suggested by the Jastrow-Feenberg approach and show
that the corrections from non-parquet contributions are, at short distances,
larger than all other many-body effects.","['E. Krotscheck', 'J. Wang']",[],0,arXiv,http://arxiv.org/abs/2009.10849v1,False,True,False,False,False,763,Daniel Feenberg,Boston,Completed,2009,2010.0,This project uses the NBER TAXSIM program to calculate income tax liabilities for CPS households.
"Impact of the Three-Child Policy and Delayed Retirement on the Transfer
  of Surplus Rural Labor under Xi Jinping's New Population Vision: A
  Re-examination of China's Lewis Turning Point","Chinese-style modernization involves the modernization of a large population,
requiring top-level design in terms of scale and structure. The population
perspective in Xi Jinping's Thought on Socialism with Chinese Characteristics
for a New Era serves as the fundamental guide for population policies. The
three-child policy and delayed retirement will affect the supply of labor in
China and challenge the previous assessments of China's Lewis Turning Point.
This study examines the rural surplus labor transfer from 2013 to 2022 based on
urban and rural data. The results indicate that China's overall wage levels
have continuously increased, the urban-rural income gap has narrowed, and the
transfer of surplus rural labor has slowed. China has passed the first turning
point and entered a transitional phase. Factors such as the level of
agricultural mechanization, urbanization rate, and urban-rural income gap are
more significant in influencing the transfer of surplus labor than the normal
working-age population ratio. The delayed retirement policy has a more
immediate impact on the supply and transfer of rural surplus labor than the
three-child policy. Additionally, delayed retirement can offset the negative
impact of the reduced relative surplus labor supply caused by the three-child
policy, although the three-child policy could increase the future absolute
surplus labor supply.","['Jun Dai', 'Guanqing Shi', 'Xiaoke Xie', 'Aitong Xie']",[],0,arXiv,http://arxiv.org/abs/2409.14914v2,False,True,False,False,False,774,Amy B Martin,Triangle,Completed,2010,2013.0,"This project will ascertain oral health among rural and urban populations by estimating the rate of dental sealant use among urban and rural children, and in particular rural minority children, using two separate waves of the same Survey of Income and Program Participation (SIPP) topical module.  In so doing, the project will benefit the SIPP by furthering a primary purpose of the survey - measuring the effectiveness of federal, state, and local programs. Comparison of results from this project to concurrent results using National Health and Nutrition Examination Survey (NHANES) data will specifically benefit the SIPP data collection program as described in detail below. Findings will further benefit the Bureau by providing an outline for future use of the SIPP to assess questions related to health care, and by reporting population estimates of the level of dental sealant use in rural and urban populations."
"Attrition and Non-Response in Panel Data: The Case of the Canadian
  Survey of Labor and Income Dynamics","This paper provides an analysis of the effects of attrition and non-response
on employment and wages using the Canadian Survey of Labour and Income
Dynamics. We consider a structural model composed of three freely correlated
equations for nonattrition/response, employment and wages. The model is
estimated using microdata from 22,990 individuals who provided sufficient
information in the first wave of the 1996-2001 panel. The main findings of the
paper are that attrition is not random. Attritors and non-respondents likely
are less attached to employment and come from low-income population. The
correlation between non-attrition and employment is positive and statistically
significant, though small. Also, wage estimates are biased upwards. Observed
wages are on average higher than wages that would be observed if all the
individuals initially selected in the panel remained in the sample.","['Brahim Boudarbat', 'Lee Grenon']",[],0,arXiv,http://arxiv.org/abs/0710.4404v1,False,True,False,False,False,776,Philippe . Wingender,Berkeley,Completed,2011,2014.0,"This research project uses timing of childbirth to measure the income effect of taxes on parents' labor supply. The IRS Residency Test states that families can claim a dependent for the entire fiscal year if the child was born at any time during the year, and therefore provides an exogenous source of variation in tax liabilities for births that occur late in the year versus those that occur early the following year. By measuring the difference in earnings in the subsequent year for parents of December and January births, we can identify the impact of a one-time non-labor income shock on parents' labor supply since both groups face on average the same future stream of tax rates after birth. Preliminary results using public-use panel data from the Survey of Income and Program Participation (SIPP) and cross-sectional data from the American Community Survey (ACS) suggest that a temporary increase in after-tax income leads to a significant decrease in mothers' earnings with an estimated income effect of -0.9. This calls for a better understanding of the income effect of taxes of earnings, an important parameter that has not been studied carefully in previous work.  Restricted data from the 2000 Census Long Form and the ACS can alleviate the shortcomings of the current public-use datasets: coarse information on date of birth and small samples. This research will produce a new estimate of the income effect, an important characteristic of the US population that has been overlooked in previous work. The few previous studies that have incorporated measures of non-labor income in earnings elasticity estimations have all done so in the context of tax reform. This research project is the first one to look directly at changes in non-labor income's impact on earnings arising from taxes, resulting in a more transparent identification strategy and greater statistical power."
"Using Deep Learning and Google Street View to Estimate the Demographic
  Makeup of the US","The United States spends more than $1B each year on initiatives such as the
American Community Survey (ACS), a labor-intensive door-to-door study that
measures statistics relating to race, gender, education, occupation,
unemployment, and other demographic factors. Although a comprehensive source of
data, the lag between demographic changes and their appearance in the ACS can
exceed half a decade. As digital imagery becomes ubiquitous and machine vision
techniques improve, automated data analysis may provide a cheaper and faster
alternative. Here, we present a method that determines socioeconomic trends
from 50 million images of street scenes, gathered in 200 American cities by
Google Street View cars. Using deep learning-based computer vision techniques,
we determined the make, model, and year of all motor vehicles encountered in
particular neighborhoods. Data from this census of motor vehicles, which
enumerated 22M automobiles in total (8% of all automobiles in the US), was used
to accurately estimate income, race, education, and voting patterns, with
single-precinct resolution. (The average US precinct contains approximately
1000 people.) The resulting associations are surprisingly simple and powerful.
For instance, if the number of sedans encountered during a 15-minute drive
through a city is higher than the number of pickup trucks, the city is likely
to vote for a Democrat during the next Presidential election (88% chance);
otherwise, it is likely to vote Republican (82%). Our results suggest that
automated systems for monitoring demographic trends may effectively complement
labor-intensive approaches, with the potential to detect trends with fine
spatial resolution, in close to real time.","['Timnit Gebru', 'Jonathan Krause', 'Yilun Wang', 'Duyun Chen', 'Jia Deng', 'Erez Lieberman Aiden', 'Li Fei-Fei']",[],0,arXiv,http://arxiv.org/abs/1702.06683v2,False,True,False,False,True,776,Philippe . Wingender,Berkeley,Completed,2011,2014.0,"This research project uses timing of childbirth to measure the income effect of taxes on parents' labor supply. The IRS Residency Test states that families can claim a dependent for the entire fiscal year if the child was born at any time during the year, and therefore provides an exogenous source of variation in tax liabilities for births that occur late in the year versus those that occur early the following year. By measuring the difference in earnings in the subsequent year for parents of December and January births, we can identify the impact of a one-time non-labor income shock on parents' labor supply since both groups face on average the same future stream of tax rates after birth. Preliminary results using public-use panel data from the Survey of Income and Program Participation (SIPP) and cross-sectional data from the American Community Survey (ACS) suggest that a temporary increase in after-tax income leads to a significant decrease in mothers' earnings with an estimated income effect of -0.9. This calls for a better understanding of the income effect of taxes of earnings, an important parameter that has not been studied carefully in previous work.  Restricted data from the 2000 Census Long Form and the ACS can alleviate the shortcomings of the current public-use datasets: coarse information on date of birth and small samples. This research will produce a new estimate of the income effect, an important characteristic of the US population that has been overlooked in previous work. The few previous studies that have incorporated measures of non-labor income in earnings elasticity estimations have all done so in the context of tax reform. This research project is the first one to look directly at changes in non-labor income's impact on earnings arising from taxes, resulting in a more transparent identification strategy and greater statistical power."
Fine-Grained Car Detection for Visual Census Estimation,"Targeted socioeconomic policies require an accurate understanding of a
country's demographic makeup. To that end, the United States spends more than 1
billion dollars a year gathering census data such as race, gender, education,
occupation and unemployment rates. Compared to the traditional method of
collecting surveys across many years which is costly and labor intensive,
data-driven, machine learning driven approaches are cheaper and faster--with
the potential ability to detect trends in close to real time. In this work, we
leverage the ubiquity of Google Street View images and develop a computer
vision pipeline to predict income, per capita carbon emission, crime rates and
other city attributes from a single source of publicly available visual data.
We first detect cars in 50 million images across 200 of the largest US cities
and train a model to predict demographic attributes using the detected cars. To
facilitate our work, we have collected the largest and most challenging
fine-grained dataset reported to date consisting of over 2600 classes of cars
comprised of images from Google Street View and other web sources, classified
by car experts to account for even the most subtle of visual differences. We
use this data to construct the largest scale fine-grained detection system
reported to date. Our prediction results correlate well with ground truth
income data (r=0.82), Massachusetts department of vehicle registration, and
sources investigating crime rates, income segregation, per capita carbon
emission, and other market research. Finally, we learn interesting
relationships between cars and neighborhoods allowing us to perform the first
large scale sociological analysis of cities using computer vision techniques.","['Timnit Gebru', 'Jonathan Krause', 'Yilun Wang', 'Duyun Chen', 'Jia Deng', 'Li Fei-Fei']",[],0,arXiv,http://arxiv.org/abs/1709.02480v1,False,True,False,False,False,776,Philippe . Wingender,Berkeley,Completed,2011,2014.0,"This research project uses timing of childbirth to measure the income effect of taxes on parents' labor supply. The IRS Residency Test states that families can claim a dependent for the entire fiscal year if the child was born at any time during the year, and therefore provides an exogenous source of variation in tax liabilities for births that occur late in the year versus those that occur early the following year. By measuring the difference in earnings in the subsequent year for parents of December and January births, we can identify the impact of a one-time non-labor income shock on parents' labor supply since both groups face on average the same future stream of tax rates after birth. Preliminary results using public-use panel data from the Survey of Income and Program Participation (SIPP) and cross-sectional data from the American Community Survey (ACS) suggest that a temporary increase in after-tax income leads to a significant decrease in mothers' earnings with an estimated income effect of -0.9. This calls for a better understanding of the income effect of taxes of earnings, an important parameter that has not been studied carefully in previous work.  Restricted data from the 2000 Census Long Form and the ACS can alleviate the shortcomings of the current public-use datasets: coarse information on date of birth and small samples. This research will produce a new estimate of the income effect, an important characteristic of the US population that has been overlooked in previous work. The few previous studies that have incorporated measures of non-labor income in earnings elasticity estimations have all done so in the context of tax reform. This research project is the first one to look directly at changes in non-labor income's impact on earnings arising from taxes, resulting in a more transparent identification strategy and greater statistical power."
Visualizing Income Distribution in the United States,"The distribution of household income is a central concern of modern economic
policy due to its strong influence on life quality. Yet, non-expert audiences
are unaware of the relationship between these two factors. To effectively
communicate the effect of income inequality on the quality of life and among
the strata, we have designed a novel technique for visualizing income
distribution and inequality over time by using the U.S. household income
microdata from the Current Population Survey. The result is a striking dynamic
animation of income distribution over time, drawing public attention and
further investigating economic inequality. Detailed implementation of this
project is available at https://github.com/sangttruong/incomevis.","['Sang Truong', 'Humberto Barreto']",[],0,arXiv,http://arxiv.org/abs/2108.03733v1,False,True,False,False,False,776,Philippe . Wingender,Berkeley,Completed,2011,2014.0,"This research project uses timing of childbirth to measure the income effect of taxes on parents' labor supply. The IRS Residency Test states that families can claim a dependent for the entire fiscal year if the child was born at any time during the year, and therefore provides an exogenous source of variation in tax liabilities for births that occur late in the year versus those that occur early the following year. By measuring the difference in earnings in the subsequent year for parents of December and January births, we can identify the impact of a one-time non-labor income shock on parents' labor supply since both groups face on average the same future stream of tax rates after birth. Preliminary results using public-use panel data from the Survey of Income and Program Participation (SIPP) and cross-sectional data from the American Community Survey (ACS) suggest that a temporary increase in after-tax income leads to a significant decrease in mothers' earnings with an estimated income effect of -0.9. This calls for a better understanding of the income effect of taxes of earnings, an important parameter that has not been studied carefully in previous work.  Restricted data from the 2000 Census Long Form and the ACS can alleviate the shortcomings of the current public-use datasets: coarse information on date of birth and small samples. This research will produce a new estimate of the income effect, an important characteristic of the US population that has been overlooked in previous work. The few previous studies that have incorporated measures of non-labor income in earnings elasticity estimations have all done so in the context of tax reform. This research project is the first one to look directly at changes in non-labor income's impact on earnings arising from taxes, resulting in a more transparent identification strategy and greater statistical power."
Endogenous Labour Flow Networks,"In the last decade, the study of labour dynamics has led to the introduction
of labour flow networks (LFNs) as a way to conceptualise job-to-job
transitions, and to the development of mathematical models to explore the
dynamics of these networked flows. To date, LFN models have relied upon an
assumption of static network structure. However, as recent events (increasing
automation in the workplace, the COVID-19 pandemic, a surge in the demand for
programming skills, etc.) have shown, we are experiencing drastic shifts to the
job landscape that are altering the ways individuals navigate the labour
market. Here we develop a novel model that emerges LFNs from agent-level
behaviour, removing the necessity of assuming that future job-to-job flows will
be along the same paths where they have been historically observed. This model,
informed by microdata for the United Kingdom, generates empirical LFNs with a
high level of accuracy. We use the model to explore how shocks impacting the
underlying distributions of jobs and wages alter the topology of the LFN. This
framework represents a crucial step towards the development of models that can
answer questions about the future of work in an ever-changing world.","['Kathyrn R. Fair', 'Omar A. Guerrero']",[],0,arXiv,http://arxiv.org/abs/2301.07979v2,False,True,False,False,False,786,Erling Barth,Boston,Completed,2009,2014.0,"The purpose of this research project is to improve understanding of the quality of Title 13, Chapter 5 data on employment and wages, and to prepare estimates of the dynamics of workplace wages and their relation to overall changes in the wage distribution, the economic return to Research and Development (R&D) (both private and social), and the role of science and engineering workers on innovation, knowledge transmission, and economic growth. The project compares employment and wages across the data files of the Longitudinal Business Database (LBD), the economic census, and the Longitudinal Employer Household Dynamics (LEHD) Employer Characteristics File. Cross comparison of employment and wages from the three separate sources will establish whether reporting variance can be duplicated across sources, methods for the allocation of firm based R&D expenditure to establishments, and investigate recall bias for employment measures in the economic census. Recall bias will be quantified for economic census respondents that report March 12 employment that has a closer match to fourth quarter LEHD employment or first quarter employment for the following year, in comparison to first quarter LEHD employment for the March 12 period of the economic census year. Fourth quarter or first quarter of the following year employment is closer to the timing of when economic census survey forms are mailed out. The extent of recall bias will be related to firm characteristics, for example as captured by worker turnover statistics in the Quarterly Workforce Indicators. The project will produce population estimates of establishment and individual wage distributions and wage dynamics. The project will also produce population estimates of the effect of R&D activity on firm productivity. As a complement to the firm based analyses, the research will investigate wage distributions over time, coming from the perspective of the worker and using the Current Population Survey. The data will allow the follow­ing of workers from firm to firm and measure the R&D content of their working experience. Finally, the project will provide population estimates of the impact of unions, innovation, firm volatil­ity, and financial distress on employment and wages."
EUCLID : Dark Universe Probe and Microlensing planet Hunter,"There is a remarkable synergy between requirements for Dark Energy probes by
cosmic shear measurements and planet hunting by microlensing. Employing weak
and strong gravitational lensing to trace and detect the distribution of matter
on cosmic and Galactic scales, but as well as to the very small scales of
exoplanets is a unique meeting point from cosmology to exoplanets. It will use
gravity as the tool to explore the full range of masses not accessible by any
other means. EUCLID is a 1.2m telescope with optical and IR wide field imagers
and slitless spectroscopy, proposed to ESA Cosmic Vision to probe for Dark
Energy, Baryonic acoustic oscillation, galaxy evolution, and an exoplanet hunt
via microlensing. A 3 months microlensing program will already efficiently
probe for planets down to the mass of Mars at the snow line, for free floating
terrestrial or gaseous planets and habitable super Earth. A 12+ months survey
would give a census on habitable Earth planets around solar like stars. This is
the perfect complement to the statistics that will be provided by the KEPLER
satellite, and these missions combined will provide a full census of extrasolar
planets from hot, warm, habitable, frozen to free floating.","['J. P. Beaulieu', 'D. P. Bennett', 'V Batista', 'A Cassan', 'D. Kubas', 'P. Fouque', 'E. Kerrins', 'S. Mao', 'J. Miralda-Escude', 'J. Wambsganss', 'B. S. Gaudi', 'A. Gould', 'S. Dong']",[],0,arXiv,http://arxiv.org/abs/1001.3349v1,False,True,False,False,False,787,Hunt V Allcott,Baruch,Completed,2013,2017.0,"This research will analyze the energy intensity of individual manufacturing establishments in order to understand how dynamics such as entry, exit, and within-plant changes contribute to trends in the energy intensity of the overall economy. This project will benefit the Census Bureau by examining the quality of energy expenditure data in the Census of Manufacturers and Annual Survey of Manufacturers, and the implications of different imputation procedures – regression imputation, hot-decking, and single vs. multiple imputation – on statistics derived from the data."
"Low-Metallicity Blue Compact Dwarfs as Templates for Primordial Star
  Formation","Understanding how galaxies formed their first stars is a vital cosmological
question, but the study of high-redshift objects, caught in the act of forming
their first stars, is difficult. Here we argue that two extremely
low-metallicity Blue Compact Dwarf galaxies (BCDs), IZw18 and SBS0335-052,
could be local templates for primordial star formation, since both lack evolved
($> $1 Gyr) stellar populations; but they form stars differently.","['L. K. Hunt', 'H. Hirashita', 'T. X. Thuan', 'Y. I. Izotov', 'L. Vanzi']",[],0,arXiv,http://arxiv.org/abs/astro-ph/0310865v1,False,True,False,False,False,787,Hunt V Allcott,Baruch,Completed,2013,2017.0,"This research will analyze the energy intensity of individual manufacturing establishments in order to understand how dynamics such as entry, exit, and within-plant changes contribute to trends in the energy intensity of the overall economy. This project will benefit the Census Bureau by examining the quality of energy expenditure data in the Census of Manufacturers and Annual Survey of Manufacturers, and the implications of different imputation procedures – regression imputation, hot-decking, and single vs. multiple imputation – on statistics derived from the data."
"Local Labor Market Effects of Mergers and Acquisitions in Developing
  Countries: Evidence from Brazil","I use matched employer-employee records merged with corporate tax information
from 2003 to 2017 to estimate labor market-wide effects of mergers and
acquisitions in Brazil. Labor markets are defined by pairs of commuting zone
and industry sector. In the following year of a merger, market size falls by
10.8%. The employment adjustment is concentrated in merging firms. For the
firms not involved in M&As, I estimate a 1.07% decline in workers earnings and
a positive, although not significant, increase in their size. Most mergers have
a predicted impact of zero points in concentration, measured by the
Herfindahl-Hirschman Index (HHI). I spillover firms, earnings decline similarly
for mergers with high and low predicted changes in HHI. Contrary to the recent
literature on market concentration in developed economies, I find no evidence
of oligopsonistic behavior in Brazilian labor markets.",['Vitor Costa'],[],0,arXiv,http://arxiv.org/abs/2306.08797v1,False,True,False,False,False,792,Ted D Mouw,Triangle,Completed,2010,2015.0,"This project will increase the understanding of how local labor markets adjust to an influx of immigrants.  With quarterly data on the earnings and geographic location of workers from the Longitudinal Employer Household Dynamics (LEHD) data, the proposed research will model the process of labor market adjustment by following individual workers over time.  This unique data will allow us to both model the short term effects that are missed by other studies using Census data as well as document the longer-term effects on workers affected by immigration in specific industries.  The study will carefully analyze the impact of skill complementarities between native and immigrant workers by occupation, within detailed industries, and at the firm level. 
The research proposes to link the Current Population Survey March Supplement to the LEHD using the PIK-CPS crosswalk.  In addition to the LEHD Employer Characteristic File (ECF), this project needs the LEHD Employment History Files (EHF), the LEHD Individual Characteristics File (ICF), the LEHD Quarterly Workforce Indicators (QWI), the LEHD Geocoded Address List (GAL), and the LEHD Unit-to-Worker (U2W) Impute files.
This project will model the process of labor market adjustment to immigration, and in the process increase the Census Bureau's understanding of the quality of the data produced through the Longitudinal Employer Household Dynamics (LEHD) Program by comparing Unemployment Insurance (UI) earnings records in the LEHD Employment History Files (EHF) to the earnings reported for the same individuals in the March Supplement of the Current Population Survey (CPS).  "
Hedonic Prices and Quality Adjusted Price Indices Powered by AI,"Accurate, real-time measurements of price index changes using electronic
records are essential for tracking inflation and productivity in today's
economic environment. We develop empirical hedonic models that can process
large amounts of unstructured product data (text, images, prices, quantities)
and output accurate hedonic price estimates and derived indices. To accomplish
this, we generate abstract product attributes, or ``features,'' from text
descriptions and images using deep neural networks, and then use these
attributes to estimate the hedonic price function. Specifically, we convert
textual information about the product to numeric features using large language
models based on transformers, trained or fine-tuned using product descriptions,
and convert the product image to numeric features using a residual network
model. To produce the estimated hedonic price function, we again use a
multi-task neural network trained to predict a product's price in all time
periods simultaneously. To demonstrate the performance of this approach, we
apply the models to Amazon's data for first-party apparel sales and estimate
hedonic prices. The resulting models have high predictive accuracy, with $R^2$
ranging from $80\%$ to $90\%$. Finally, we construct the AI-based hedonic
Fisher price index, chained at the year-over-year frequency. We contrast the
index with the CPI and other electronic indices.","['Patrick Bajari', 'Zhihao Cen', 'Victor Chernozhukov', 'Manoj Manukonda', 'Suhas Vijaykumar', 'Jin Wang', 'Ramon Huerta', 'Junbo Li', 'Ling Leng', 'George Monokroussos', 'Shan Wan']",[],0,arXiv,http://arxiv.org/abs/2305.00044v1,False,True,False,False,False,795,Ralph Bradley,Washington,Completed,2014,2016.0,"This project uses data from the Insurance Component of the Medical Expenditure Panel Survey (MEPS-IC) in conjunction with the Longitudinal Business Database to develop a methodology to construct a quality-and-risk-adjusted hedonic price index for health insurance premiums. The hedonic price index will be an estimate of the premium level in a local geographic market in a particular year, holding quality and risk constant. Since this project will also examine the role that geography plays in setting these premiums, the estimated premiums will be used to test whether insurance prices differ across local geographic markets. In so doing, this project will also examine the factors that affect health insurance premiums and will develop a method to impute for non-response based on these factors."
Cluster detection and risk estimation for spatio-temporal health data,"In epidemiological disease mapping one aims to estimate the spatio-temporal
pattern in disease risk and identify high-risk clusters, allowing health
interventions to be appropriately targeted. Bayesian spatio-temporal models are
used to estimate smoothed risk surfaces, but this is contrary to the aim of
identifying groups of areal units that exhibit elevated risks compared with
their neighbours. Therefore, in this paper we propose a new Bayesian
hierarchical modelling approach for simultaneously estimating disease risk and
identifying high-risk clusters in space and time. Inference for this model is
based on Markov chain Monte Carlo simulation, using the freely available R
package CARBayesST that has been developed in conjunction with this paper. Our
methodology is motivated by two case studies, the first of which assesses if
there is a relationship between Public health Districts and colon cancer
clusters in Georgia, while the second looks at the impact of the smoking ban in
public places in England on cardiovascular disease clusters.","['Duncan Lee', 'Andrew Lawson']",[],0,arXiv,http://arxiv.org/abs/1408.1191v2,False,True,False,False,False,796,Lee R Mobley,Triangle,Completed,2011,2016.0,"This project will develop methods that use information from restricted-access Census Bureau data to characterize risk across populations. Work will demonstrate that using restricted-access Census data to develop and test risk assessment methods in conjunction with public data provide superior measures than could be accomplished with public data alone. Research will use the American Community Survey and the American Community Survey Multiyear Estimates Study data in conjunction with other public-use geospatial data. With these combined data sources, the researchers will create several geospatial risk-scapes, each measuring a different dimension of population risk to health hazards, economic hazards, or natural disasters. The researchers will then use these risk-scape measures to demonstrate the utility of the Census microdata in timely assessment of social vulnerability."
"On two non-existence results for Cameron-Liebler $k$-sets in
  $\mathrm{PG}(n,q)$","This paper focuses on non-existence results for Cameron-Liebler $k$-sets. A
Cameron-Liebler $k$-set is a collection of $k$-spaces in $\mathrm{PG}(n,q)$ or
$\mathrm{AG}(n,q)$ admitting a certain parameter $x$, which is dependent on the
size of this collection. One of the main research questions remains the
(non-)existence of Cameron-Liebler $k$-sets with parameter $x$. This paper
improves two non-existence results. First we show that the parameter of a
non-trivial Cameron-Liebler $k$-set in $\mathrm{PG}(n,q)$ should be larger than
$q^{n-\frac{5k}{2}-1}$, which is an improvement of an earlier known lower
bound. Secondly, we prove a modular equality on the parameter $x$ of
Cameron-Liebler $k$-sets in $\mathrm{PG}(n,q)$ with
$x<\frac{q^{n-k}-1}{q^{k+1}-1}$, $n\geq 2k+1$, $n-k+1\geq 7$ and $n-k$ even. In
the affine case we show a similar result for $n-k+1\geq 3$ and $n-k$ even. This
is a generalization of earlier known modular equalities in the projective and
affine case.","['Jan De Beule', 'Jonathan Mannaert', 'Leo Storme']",[],0,arXiv,http://arxiv.org/abs/2403.00519v1,False,True,False,False,False,797,Carolyn A Liebler,Minnesota,Completed,2010,2015.0,"This project aims to understand respondents' answers to Census Bureau questions about race using a three-pronged approach. First, we will apply demographic life table techniques to restricted data to learn about the characteristics of people who have changed their answers to race questions between censuses. Second, we will investigate the characteristics of American Indians/Alaska Natives who neglected to report their tribes when asked as part of the race question. Third, we will study why some children of interracially married people are reported to be multiracial on the post-2000 race question, but many are reported to be single race. "
Cameron-Liebler Line Classes with parameter $x=\frac{(q+1)^2}{3}$,"Cameron-Liebler line classes were introduced in \cite{CL}, and motivated by a
question about orbits of collineation groups of $\PG(3,q)$. These line classes
have appeared in different contexts under disguised names such as Boolean
degree one functions, regular codes of covering radius one, and tight sets. In
this paper we construct an infinite family of Cameron-Liebler line classes in
$\PG(3,q)$ with new parameter $x=(q+1)^2/3$ for all prime powers $q$ congruent
to 2 modulo 3. The examples obtained when $q$ is an odd power of two represent
the first infinite family of Cameron-Liebler line classes in $\PG(3,q)$, $q$
even.","['Tao Feng', 'Koji Momihara', 'Morgan Rodgers', 'Qing Xiang', 'Hanlin Zou']",[],0,arXiv,http://arxiv.org/abs/2006.14206v1,False,True,False,False,False,797,Carolyn A Liebler,Minnesota,Completed,2010,2015.0,"This project aims to understand respondents' answers to Census Bureau questions about race using a three-pronged approach. First, we will apply demographic life table techniques to restricted data to learn about the characteristics of people who have changed their answers to race questions between censuses. Second, we will investigate the characteristics of American Indians/Alaska Natives who neglected to report their tribes when asked as part of the race question. Third, we will study why some children of interracially married people are reported to be multiracial on the post-2000 race question, but many are reported to be single race. "
Large-Scale Analysis of New Employee Network Dynamics,"The COVID-19 pandemic has accelerated digital transformations across
industries, but also introduced new challenges into workplaces, including the
difficulties of effectively socializing with colleagues when working remotely.
This challenge is exacerbated for new employees who need to develop workplace
networks from the outset. In this paper, by analyzing a large-scale telemetry
dataset of more than 10,000 Microsoft employees who joined the company in the
first three months of 2022, we describe how new employees interact and
telecommute with their colleagues during their ``onboarding'' period. Our
results reveal that although new hires are gradually expanding networks over
time, there still exists significant gaps between their network statistics and
those of tenured employees even after the six-month onboarding phase. We also
observe that heterogeneity exists among new employees in how their networks
change over time, where employees whose job tasks do not necessarily require
extensive and diverse connections could be at a disadvantaged position in this
onboarding process. By investigating how web-based people recommendations in
organizational knowledge base facilitate new employees naturally expand their
networks, we also demonstrate the potential of web-based applications for
addressing the aforementioned socialization challenges. Altogether, our
findings provide insights on new employee network dynamics in remote and hybrid
work environments, which may help guide organizational leaders and web
application developers on quantifying and improving the socialization
experiences of new employees in digital workplaces.","['Yulin Yu', 'Longqi Yang', 'Siân Lindley', 'Mengting Wan']",[],0,arXiv,http://arxiv.org/abs/2304.03441v1,False,True,False,False,False,798,Charles M Tolbert,Chicago,Completed,2010,2014.0,"We focus on new employers, their first employees, and the potential of micro-firms for rural development. Linking internal datasets ILBD, LEHD, and SBO, we will analyze the transition from nonemployer to employer status in rural communities and how nascent employers fare (revenue, longevity). Our models of new employer performance will include attributes of the rural communities. For those new employers who can be matched to SBO, we will analyze the personal characteristics of business owners and how they relate to firm performance. We will also study the first employees hired on by these new employers with special attention given to the wages, tenure, mobility of the newly employed. "
Personalized Subsidy Rules,"Subsidies are commonly used to encourage behaviors that can lead to short- or
long-term benefits. Typical examples include subsidized job training programs
and provisions of preventive health products, in which both behavioral
responses and associated gains can exhibit heterogeneity. This study uses the
marginal treatment effect (MTE) framework to study personalized assignments of
subsidies based on individual characteristics. First, we derive the optimality
condition for a welfare-maximizing subsidy rule by showing that the welfare can
be represented as a function of the MTE. Next, we show that subsidies generally
result in better welfare than directly mandating the encouraged behavior
because subsidy rules implicitly target individuals through unobserved
heterogeneity in the behavioral response. When there is positive selection,
that is, when individuals with higher returns are more likely to select the
encouraged behavior, the optimal subsidy rule achieves the first-best welfare,
which is the optimal welfare if a policy-maker can observe individuals' private
information. We then provide methods to (partially) identify the optimal
subsidy rule when the MTE is identified and unidentified. Particularly,
positive selection allows for the point identification of the optimal subsidy
rule even when the MTE curve is not. As an empirical application, we study the
optimal wage subsidy using the experimental data from the Jordan New
Opportunities for Women pilot study.","['Yu-Chang Chen', 'Haitian Xie']",[],0,arXiv,http://arxiv.org/abs/2202.13545v2,False,True,False,False,False,800,Isaac N McFarlin,Michigan,Completed,2010,2015.0,"This project describes and estimates models of student geographic mobility. Its objective is to support ongoing research on college-going behavior among high school graduates. This work focuses on the college-going behavior of youth residing near college taxing districts. It exploits the abrupt change in tuition costs at college taxing district boundaries to estimate price elasticities of demand for higher education. For this proposal, we use household data from the restricted-version of the Decennial Census and the American Community Survey to test a key identifying assumption: college-age youth on opposite, yet adjacent, sides of community college taxing district boundaries are similar along observable dimensions. This work will enhance the utility Census Bureau data in two ways. First, it contributes to developing means of increasing the utility of Census Bureau data for analyzing public programs and policies. Accordingly, the project will link census blocks for a large state with detailed information on college taxing district attributes such as tuition levels and property tax rates. Furthermore, the proposal contributes to preparing estimates of population and characteristics of populations by examining the extent to which college tuition subsidies affect geographic mobility and whether such subsidies are capitalized into housing values."
Electrocardiography Separation of Mother and Baby,"Extraction of Electrocardiography (ECG or EKG) signals of mother and baby is
a challenging task, because one single device is used and it receives a mixture
of multiple heart beats. In this paper, we would like to design a filter to
separate the signals from each other.",['Wei Wang'],[],0,arXiv,http://arxiv.org/abs/1411.1446v1,False,True,False,False,False,812,Andrea Hetling,Baruch,Completed,2010,2012.0,The proportion of single mothers in poverty not receiving public assistance or participating in the formal employment sector has approximately doubled over the past decade.  Recent research indicates that personal barriers are common and likely hinder entry into the workforce and navigation of welfare bureaucracies. The proposed project will examine the ecological circumstances of disconnected single mothers with a focus on the influence of welfare rules and community circumstances on the likelihood of being disconnected.
"Accounting for survey design in Bayesian disaggregation of survey-based
  areal estimates of proportions: an application to the American Community
  Survey","Understanding the effects of social determinants of health on health outcomes
requires data on characteristics of the neighborhoods in which subjects live.
However, estimates of these characteristics are often aggregated over space and
time in a fashion that diminishes their utility. Take, for example, estimates
from the American Community Survey (ACS), a multi-year nationwide survey
administered by the U.S. Census Bureau: estimates for small municipal areas are
aggregated over 5-year periods, whereas 1-year estimates are only available for
municipal areas with populations $>$65,000. Researchers may wish to use ACS
estimates in studies of population health to characterize neighborhood-level
exposures. However, 5-year estimates may not properly characterize temporal
changes or align temporally with other data in the study, while the coarse
spatial resolution of the 1-year estimates diminishes their utility in
characterizing neighborhood exposure. To circumvent this issue, in this paper
we propose a modeling framework to disaggregate estimates of proportions
derived from sampling surveys which explicitly accounts for the survey design
effect. We illustrate the utility of our model by applying it to the ACS data,
generating estimates of poverty for the state of Michigan at fine
spatio-temporal resolution.","['Marco H. Benedetti', 'Veronica J. Berrocal', 'Roderick J. Little']",[],0,arXiv,http://arxiv.org/abs/2112.06802v2,True,True,False,False,True,813,Gary J Gates,UCLA,Completed,2011,2014.0,"A significant amount of the research on same-sex couples in the United States uses the Decennial Census and the American Community Survey (ACS) as primary data sources. With the advent of legal marriage and other forms of recognition for these couples, interest in this group has intensified. This project will help determine if new procedures used in the 2008 ACS have improved the reliability and accuracy of data on same-sex couples, especially those where one partner is designated as a spouse. Beginning with the 2008 ACS, the Census Bureau now formally releases estimates of same-sex spouses (prior to this change, all same-sex partners designated as ""husband"" or ""wife"" were reclassified as ""unmarried partners""). This only increases the urgency of assessing the reliability of the same-sex couple data, especially same-sex spouses.
Research suggests that a potentially large fraction of same-sex spouses may actually be comprised of different-sex spouses who miscode their sex. This project compares data from the 2007 and 2008 ACS to assess whether the new 2008 ACS data collection and editing procedures yield greater accuracy of responses and improve the reliability of the same-sex spousal data.  The primary research goal is to verify the extent of the measurement error using explicit identification of same-sex spouses. The use of data that includes original unedited responses to the household roster and variables associated with marital status and sex will allow a determination of whether the changes in the 2008 ACS data result in a more accurate enumeration of same-sex spouses. A second goal is to consider how state-level differences in responses to household roster and marital status questions may be associated with variation in the legal and social climate regarding recognition of same-sex relationships."
"Bayesian Nonparametric Multivariate Spatial Mixture Mixed Effects Models
  with Application to American Community Survey Special Tabulations","Leveraging multivariate spatial dependence to improve the precision of
estimates using American Community Survey data and other sample survey data has
been a topic of recent interest among data-users and federal statistical
agencies. One strategy is to use a multivariate spatial mixed effects model
with a Gaussian observation model and latent Gaussian process model. In
practice, this works well for a wide range of tabulations. Nevertheless, in
situations that exhibit heterogeneity among geographies and/or sparsity in the
data, the Gaussian assumptions may be problematic and lead to underperformance.
To remedy these situations, we propose a multivariate hierarchical Bayesian
nonparametric mixed effects spatial mixture model to increase model
flexibility. The number of clusters is chosen automatically in a data-driven
manner. The effectiveness of our approach is demonstrated through a simulation
study and motivating application of special tabulations for American Community
Survey data.","['Ryan Janicki', 'Andrew M. Raim', 'Scott H. Holan', 'Jerry Maples']",[],0,arXiv,http://arxiv.org/abs/2009.12351v1,False,False,False,False,True,813,Gary J Gates,UCLA,Completed,2011,2014.0,"A significant amount of the research on same-sex couples in the United States uses the Decennial Census and the American Community Survey (ACS) as primary data sources. With the advent of legal marriage and other forms of recognition for these couples, interest in this group has intensified. This project will help determine if new procedures used in the 2008 ACS have improved the reliability and accuracy of data on same-sex couples, especially those where one partner is designated as a spouse. Beginning with the 2008 ACS, the Census Bureau now formally releases estimates of same-sex spouses (prior to this change, all same-sex partners designated as ""husband"" or ""wife"" were reclassified as ""unmarried partners""). This only increases the urgency of assessing the reliability of the same-sex couple data, especially same-sex spouses.
Research suggests that a potentially large fraction of same-sex spouses may actually be comprised of different-sex spouses who miscode their sex. This project compares data from the 2007 and 2008 ACS to assess whether the new 2008 ACS data collection and editing procedures yield greater accuracy of responses and improve the reliability of the same-sex spousal data.  The primary research goal is to verify the extent of the measurement error using explicit identification of same-sex spouses. The use of data that includes original unedited responses to the household roster and variables associated with marital status and sex will allow a determination of whether the changes in the 2008 ACS data result in a more accurate enumeration of same-sex spouses. A second goal is to consider how state-level differences in responses to household roster and marital status questions may be associated with variation in the legal and social climate regarding recognition of same-sex relationships."
"Capitalizing on a Crisis: A Computational Analysis of all Five Million
  British Firms During the Covid-19 Pandemic","The Covid-19 pandemic brought unprecedented changes to business ownership in
the UK which affects a generation of entrepreneurs and their employees.
Nonetheless, the impact remains poorly understood. This is because research on
capital accumulation has typically lacked high-quality, individualized,
population-level data. We overcome these barriers to examine who benefits from
economic crises through a computationally orientated lens of firm creation.
Leveraging a comprehensive cache of administrative data on every UK firm and
all nine million people running them, combined with probabilistic algorithms,
we conduct individual-level analyses to understand who became Covid
entrepreneurs. Using these techniques, we explore characteristics of
entrepreneurs--such as age, gender, region, business experience, and
industry--which potentially predict Covid entrepreneurship. By employing an
automated time series model selection procedure to generate counterfactuals, we
show that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),
and had previously held roles in existing firms (59.4%). For most industries,
growth was disproportionately concentrated around London. It was therefore
existing corporate elites who were most able to capitalize on the Covid crisis
and not, as some hypothesized, young entrepreneurs who were setting up their
first businesses. In this respect, the pandemic will likely impact future
wealth inequalities. Our work offers methodological guidance for future
policymakers during economic crises and highlights the long-term consequences
for capital and wealth inequality.","['Naomi Muggleton', 'Charles Rahal', 'Aaron Reeves']",[],0,arXiv,http://arxiv.org/abs/2502.09383v2,False,True,False,False,False,815,Mariko Sakakibara,UCLA,Completed,2010,2021.0,"This project will analyze the process of new venture formation in the US economy with particular emphasis on employee entrepreneurship. The project will create a new-ventures dataset, and will use that dataset to examine new venture formation in three closely related investigations. The first will compare the performance of new ventures with different pre-entry histories, and analyze how these performance variations are related to the characteristics of the new venture's founders and its parent firm. The second will shift the focus to spinoffs, i.e., new firms started by clusters of employees from an existing firm and will examine why some firms and industries seem to spawn more spinoffs than others. The last investigation will narrow the focus to a single institutional factor -- non-compete covenants -- and will seek to understand if inter-state differences in the enforceability of these covenants affect new firm creation. "
Structure-Preserving 3D Garment Modeling with Neural Sewing Machines,"3D Garment modeling is a critical and challenging topic in the area of
computer vision and graphics, with increasing attention focused on garment
representation learning, garment reconstruction, and controllable garment
manipulation, whereas existing methods were constrained to model garments under
specific categories or with relatively simple topologies. In this paper, we
propose a novel Neural Sewing Machine (NSM), a learning-based framework for
structure-preserving 3D garment modeling, which is capable of learning
representations for garments with diverse shapes and topologies and is
successfully applied to 3D garment reconstruction and controllable
manipulation. To model generic garments, we first obtain sewing pattern
embedding via a unified sewing pattern encoding module, as the sewing pattern
can accurately describe the intrinsic structure and the topology of the 3D
garment. Then we use a 3D garment decoder to decode the sewing pattern
embedding into a 3D garment using the UV-position maps with masks. To preserve
the intrinsic structure of the predicted 3D garment, we introduce an
inner-panel structure-preserving loss, an inter-panel structure-preserving
loss, and a surface-normal loss in the learning process of our framework. We
evaluate NSM on the public 3D garment dataset with sewing patterns with diverse
garment shapes and categories. Extensive experiments demonstrate that the
proposed NSM is capable of representing 3D garments under diverse garment
shapes and topologies, realistically reconstructing 3D garments from 2D images
with the preserved structure, and accurately manipulating the 3D garment
categories, shapes, and topologies, outperforming the state-of-the-art methods
by a clear margin.","['Xipeng Chen', 'Guangrun Wang', 'Dizhong Zhu', 'Xiaodan Liang', 'Philip H. S. Torr', 'Liang Lin']",[],0,arXiv,http://arxiv.org/abs/2211.06701v1,False,True,False,False,False,816,Bryan K Bollinger,Berkeley,Completed,2010,2014.0,"The purpose of this research is to structurally model the dry cleaning industry in its evolution to green cleaning technologies and to perform reduced form analyses on the industry at a national level as made possible with the use of the Census data. Several states have implemented different incentive programs and fee structures designed to phase out the use of perchoroethylene (perc), and these different incentive programs and ultimate transition of the industry will have direct impact on the industry structure. The results from this analysis will yield profit estimates in the industry and create a sample selection methodology for specified Census data sets. "
Bollinger Bands Thirty Years Later,"The goal of this study is to explain and examine the statistical
underpinnings of the Bollinger Band methodology. We start off by elucidating
the rolling regression time series model and deriving its explicit relationship
to Bollinger Bands. Next we illustrate the use of Bollinger Bands in pairs
trading and prove the existence of a specific return duration relationship in
Bollinger Band pairs trading.Then by viewing the Bollinger Band moving average
as an approximation to the random walk plus noise (RWPN) time series model, we
develop a pairs trading variant that we call ""Fixed Forecast Maximum Duration'
Bands"" (FFMDPT). Lastly, we conduct pairs trading simulations using SAP and
Nikkei index data in order to compare the performance of the variant with
Bollinger Bands.",['Mark Leeds'],[],0,arXiv,http://arxiv.org/abs/1212.4890v2,False,True,False,False,False,816,Bryan K Bollinger,Berkeley,Completed,2010,2014.0,"The purpose of this research is to structurally model the dry cleaning industry in its evolution to green cleaning technologies and to perform reduced form analyses on the industry at a national level as made possible with the use of the Census data. Several states have implemented different incentive programs and fee structures designed to phase out the use of perchoroethylene (perc), and these different incentive programs and ultimate transition of the industry will have direct impact on the industry structure. The results from this analysis will yield profit estimates in the industry and create a sample selection methodology for specified Census data sets. "
"FloodGenome: Interpretable Machine Learning for Decoding Features
  Shaping Property Flood Risk Predisposition in Cities","Understanding the fundamental characteristics that shape the inherent flood
risk disposition of urban areas is critical for integrated urban design
strategies for flood risk reduction. Flood risk disposition specifies an
inherent and event-independent magnitude of property flood risk and measures
the extent to which urban areas are susceptible to property damage if exposed
to a weather hazard. This study presents FloodGenome as an interpretable
machine learning model for evaluation of the extent to which various
hydrological, topographic, and built-environment features and their
interactions shape flood risk disposition in urban areas. Using flood damage
claims data from the U.S. National Flood Insurance Program covering the period
2003 through 2023 across four metropolitan statistical areas (MSAs), the
analysis computes building damage ratios and flood claim counts by employing
k-means clustering for classifying census block groups (CBGs) into distinct
property flood risk disposition levels. Then a random forest model is created
to specify property flood risk levels of CBGs based on various intertwined
hydrological, topographic, and built-environment features. The model
transferability analysis results show consistent performance across MSAs,
revealing the universality of underlying features that shape city property
flood risks. The FloodGenome model is then used to:(1) evaluate the extent to
which future urban development would exacerbate flood risk disposition of urban
areas; and (2) specify property flood risk levels at finer spatial resolution
providing critical insights for flood risk management processes. The
FloodGenome model and the findings provide novel tools and insights for
improving the characterization and understanding of intertwined features that
shape flood risk profiles of cities.","['Chenyue Liu', 'Ali Mostafavi']",[],0,arXiv,http://arxiv.org/abs/2403.10625v2,False,True,False,False,False,829,Jingyuan Li,Triangle,Completed,2011,2012.0,"Little empirical evidence exists to shed light on what factors influence the establishment of local hazard mitigation projects. One objective of this study is to provide such evidence through an examination of patterns in Community Rating System (CRS) scores across a panel of National Flood Insurance Program (NFIP) communities. In the process, this work will benefit the Census Bureau by developing means for increasing the utility of Bureau-collected data, linking relevant external data, and producing population estimates. The researchers will test a number of hypotheses previously offered to explain why some local governments adopt hazard mitigation but others do not. Research will focus on flood hazard mitigation projects in 1104 NFIP communities in North Carolina, South Carolina, and Georgia between 2005 and 2009, but the results will generalize across other flood-prone communities around the nation. By examining the influence of physical, risk, and socioeconomic factors on community hazard mitigation decisions as reflected in CRS scores for these areas, the results will forge a better understanding of community decision making under natural hazard risk on a national scale."
"Collision of Environmental Injustice and Sea Level Rise: Assessment of
  Risk Inequality in Flood-induced Pollutant Dispersion from Toxic Sites in
  Texas","Global sea-level rise causes increasing threats of coastal flood and
subsequent pollutant dispersion. However, there are still few studies on the
disparity arising from such threats and the extent to which different
communities could be exposed to flood-induced pollution dispersion from toxic
sites under future sea level rise. To address this gap, this study selects
Texas (a U.S. state with a large number of toxic sites and significant flood
hazards) as the study area and investigates impacts of flood-induced pollutant
dispersion on different communities under current (2018) and future (2050)
flood hazard scenarios.The results show, currently, north coastline in Texas
bears higher threats and vulnerable communities (i.e., low income, minorities
and unemployed) are disproportionally exposed to these threats. In addition,
the future sea-level rise and the exacerbated flood hazards will put additional
threats on more (about 10%) Texas residents, among which vulnerable communities
will still be disproportionately exposed to the increased threats. Our study
reveals the facts that potential coastal pollutant dispersion will further
aggravate the environmental injustice issues at the intersection of toxic sites
and flood hazards for vulnerable populations and exacerbate risk inequalities.
Given the dire impacts of flood-induced pollution dispersion on public health,
the findings have important implications for specific actions from the policy
makers to mitigate the inequitable risks.","['Zhewei Liu', 'Ali Mostafavi']",[],0,arXiv,http://arxiv.org/abs/2301.00312v2,False,True,False,False,False,829,Jingyuan Li,Triangle,Completed,2011,2012.0,"Little empirical evidence exists to shed light on what factors influence the establishment of local hazard mitigation projects. One objective of this study is to provide such evidence through an examination of patterns in Community Rating System (CRS) scores across a panel of National Flood Insurance Program (NFIP) communities. In the process, this work will benefit the Census Bureau by developing means for increasing the utility of Bureau-collected data, linking relevant external data, and producing population estimates. The researchers will test a number of hypotheses previously offered to explain why some local governments adopt hazard mitigation but others do not. Research will focus on flood hazard mitigation projects in 1104 NFIP communities in North Carolina, South Carolina, and Georgia between 2005 and 2009, but the results will generalize across other flood-prone communities around the nation. By examining the influence of physical, risk, and socioeconomic factors on community hazard mitigation decisions as reflected in CRS scores for these areas, the results will forge a better understanding of community decision making under natural hazard risk on a national scale."
"Effect of State and Local Sexual Orientation Anti-Discrimination Laws on
  Labor Market Differentials","This paper presents the first quasi-experimental research examining the
effect of both local and state anti-discrimination laws on sexual orientation
on the labor supply and wages of lesbian, gay, and bisexual (LGB) workers. To
do so, we use the American Community Survey data on household composition to
infer sexual orientation and combine this with a unique panel dataset on local
anti-discrimination laws. Using variation in law implementation across
localities over time and between same-sex and different-sex couples, we find
that anti-discrimination laws significantly reduce gaps in labor force
participation rate, employment, and the wage gap for gay men relative to
straight men. These laws also significantly reduce the labor force
participation rate, employment, and wage premium for lesbian women relative to
straight women. One explanation for the reduced labor supply and wage premium
is that lesbian couples begin to have more children in response to the laws.
Finally, we present evidence that state anti-discrimination laws significantly
and persistently increased support for same-sex marriage. This research shows
that anti-discrimination laws can be an effective policy tool for reducing
labor market inequalities across sexual orientation and improving sentiment
toward LGB Americans.","['Scott Delhommer', 'Domonkos F. Vamossy']",[],0,arXiv,http://arxiv.org/abs/2404.03794v1,False,True,False,False,True,832,Joyce Burnette,Chicago,Completed,2010,2012.0,"This project will link firms in the Census of Manufactures to workers in the LEHD files in order to estimate production functions with labor disaggregated by gender and age.  The production functions will provide estimates of relative female productivity, which can be compared to relative female earnings, producing a better test of wage discrimination than is currently available.  We will determine whether disaggregating labor by age and gender improves the imputation of output. "
"Institutionalization of Software Product Line: An Empirical
  Investigation of Key Organizational Factors","A good fit between the person and the organization is essential in a better
organizational performance. This is even more crucial in case of
institutionalization of a software product line practice within an
organization. Employees participation, organizational behavior and management
contemplation play a vital role in successfully institutionalizing software
product lines in a firm. Organizational dimension has been weighted as one of
the critical dimensions in software product line theory and practice. A
comprehensive empirical investigation to study the impact of some
organizational factors on the performance of software product line practice is
presented in this work. This is the first study to empirically investigate and
demonstrate the relationships between some of the key organizational factors
and software product line performance of an organization. The results of this
investigation provide empirical evidence and further support the theoretical
foundations that in order to institutionalize software product lines within an
organization, organizational factors play an important role.","['Faheem Ahmed', 'Luiz Fernando Capretz', 'Shahbaz Ali Sheikh']",[],0,arXiv,http://arxiv.org/abs/2203.14873v1,False,True,False,False,False,834,Yue M Zhou,Michigan,Completed,2010,2015.0,This project focuses on internal organization as an explanation for firm performance heterogeneity. It explores two broad categories of organization factors: (1) the cost of coordinating the various activities that firms carry out in house and (2) the organization structures firms adopt to facilitate coordination. 
Open Collaboration for Innovation: Principles and Performance,"The principles of open collaboration for innovation (and production), once
distinctive to open source software, are now found in many other ventures. Some
of these ventures are internet-based: Wikipedia, online forums and communities.
Others are off-line: in medicine, science, and everyday life. Such ventures
have been affecting traditional firms, and may represent a new organizational
form. Despite the impact of such ventures, questions remain about their
operating principles and performance. Here we define open collaboration (OC),
the underlying set of principles, and propose that it is a robust engine for
innovation and production. First, we review multiple OC ventures and identify
four defining principles. In all instances, participants create goods and
services of economic value, they exchange and reuse each other's work, they
labor purposefully with just loose coordination, and they permit anyone to
contribute and consume. These principles distinguish OC from other
organizational forms, such as firms or cooperatives. Next, we turn to
performance. To understand the performance of OC, we develop a computational
model, combining innovation theory with recent evidence on human cooperation.
We identify and investigate three elements that affect performance: the
cooperativeness of participants, the diversity of their needs, and the degree
to which the goods are rival (subtractable). Through computational experiments,
we find that OC performs well even in seemingly harsh environments: when
cooperators are a minority, free riders are present, diversity is lacking, or
goods are rival. We conclude that OC is viable and likely to expand into new
domains. The findings also inform the discussion on new organizational forms,
collaborative and communal.","['Sheen Levine', 'Michael Prietula']",[],0,arXiv,http://arxiv.org/abs/1406.7541v1,False,True,False,False,False,834,Yue M Zhou,Michigan,Completed,2010,2015.0,This project focuses on internal organization as an explanation for firm performance heterogeneity. It explores two broad categories of organization factors: (1) the cost of coordinating the various activities that firms carry out in house and (2) the organization structures firms adopt to facilitate coordination. 
"The Link Between Health Insurance Coverage and Citizenship Among
  Immigrants: Bayesian Unit-Level Regression Modeling of Categorical Survey
  Data Observed with Measurement Error","Social scientists are interested in studying the impact that citizenship
status has on health insurance coverage among immigrants in the United States.
This can be done using data from the Survey of Income and Program Participation
(SIPP); however, two primary challenges emerge. First, statistical models must
account for the survey design in some fashion to reduce the risk of bias due to
informative sampling. Second, it has been observed that survey respondents
misreport citizenship status at nontrivial rates. This too can induce bias
within a statistical model. Thus, we propose the use of a weighted
pseudo-likelihood mixture of categorical distributions, where the mixture
component is determined by the latent true response variable, in order to model
the misreported data. We illustrate through an empirical simulation study that
this approach can mitigate the two sources of bias attributable to the sample
design and misreporting. Importantly, our misreporting model can be further
used as a component in a deeper hierarchical model. With this in mind, we
conduct an analysis of the relationship between health insurance coverage and
citizenship status using data from the SIPP.","['Paul A. Parker', 'Scott H. Holan', 'James D. Bachmeier', 'Claire Altman']",[],0,arXiv,http://arxiv.org/abs/2311.07524v1,False,True,False,False,False,842,Deborah R Graefe,Washington,Completed,2010,2011.0,"This study aims to document the patterns of health insurance coverage among children in Mexican-immigrant families, comparing Mexican children of immigrants (i.e., children having at least one Mexican immigrant parent) and children with U.S.-born parents, both inter- and intra-ethnically, with particular attention to child and parent documentation status. The project uses the 1996, 2001, 2004, and 2008 SIPP panels, including core and topical module files. The restricted-use internal SIPP files provide information regarding immigration status at the time the immigrant respondent arrived in the United States to develop an algorithm for estimating immigrant parent and child documentation in public use files.  Descriptive results, including cumulative proportions experiencing transitions to and from coverage, based on life table analyses, will demonstrate the patterns and trajectories for each comparison group. "
"WCLD: Curated Large Dataset of Criminal Cases from Wisconsin Circuit
  Courts","Machine learning based decision-support tools in criminal justice systems are
subjects of intense discussions and academic research. There are important open
questions about the utility and fairness of such tools. Academic researchers
often rely on a few small datasets that are not sufficient to empirically study
various real-world aspects of these questions. In this paper, we contribute
WCLD, a curated large dataset of 1.5 million criminal cases from circuit courts
in the U.S. state of Wisconsin. We used reliable public data from 1970 to 2020
to curate attributes like prior criminal counts and recidivism outcomes. The
dataset contains large number of samples from five racial groups, in addition
to information like sex and age (at judgment and first offense). Other
attributes in this dataset include neighborhood characteristics obtained from
census data, detailed types of offense, charge severity, case decisions,
sentence lengths, year of filing etc. We also provide pseudo-identifiers for
judge, county and zipcode. The dataset will not only enable researchers to more
rigorously study algorithmic fairness in the context of criminal justice, but
also relate algorithmic challenges with various systemic issues. We also
discuss in detail the process of constructing the dataset and provide a
datasheet. The WCLD dataset is available at
\url{https://clezdata.github.io/wcld/}.","['Elliott Ash', 'Naman Goel', 'Nianyun Li', 'Claudia Marangon', 'Peiyao Sun']",[],0,arXiv,http://arxiv.org/abs/2310.18724v1,False,True,False,False,False,851,Eirik Evenhouse,Berkeley,Completed,2010,2013.0,"We will compute estimates of the prevalence of multiple-father fertility from 1985-2008. We will examine, at the MSA level, the correlation between multiple-father fertility and rates of male involvement with the criminal justice system.  We will build on these estimates to further investigate the  impact of TANF policy and child support enforcement policy on multipartnered fertility.  Security clearance is necessary because, starting with SIPP 2004, a respondent's MSA is no longer reported, and we propose to conduct an MSA-level analysis. "
"Colocation of skill related suppliers -- Revisiting coagglomeration
  using firm-to-firm network data","Strong local clusters help firms compete on global markets. One explanation
for this is that firms benefit from locating close to their suppliers and
customers. However, the emergence of global supply chains shows that physical
proximity is not necessarily a prerequisite to successfully manage
customer-supplier relations anymore. This raises the question when firms need
to colocate in value chains and when they can coordinate over longer distances.
We hypothesize that one important aspect is the extent to which supply chain
partners exchange not just goods but also know-how. To test this, we build on
an expanding literature that studies the drivers of industrial coagglomeration
to analyze when supply chain connections lead firms to colocation. We exploit
detailed micro-data for the Hungarian economy between 2015 and 2017, linking
firm registries, employer-employee matched data and firm-to-firm transaction
data from value-added tax records. This allows us to observe colocation, labor
flows and value chain connections at the level of firms, as well as construct
aggregated coagglomeration patterns, skill relatedness and input-output
connections between pairs of industries. We show that supply chains are more
likely to support coagglomeration when the industries involved are also skill
related. That is, input-output and labor market channels reinforce each other,
but supplier connections only matter for colocation when industries have
similar labor requirements, suggesting that they employ similar types of
know-how. We corroborate this finding by analyzing the interactions between
firms, showing that supplier relations are more geographically constrained
between companies that operate in skill related industries.","['Sándor Juhász', 'Zoltán Elekes', 'Virág Ilyés', 'Frank Neffke']",[],0,arXiv,http://arxiv.org/abs/2405.07071v1,False,True,False,False,False,859,Pinar Celikkol Geylani,Penn State,Completed,2016,2019.0,"This project analyzes food manufacturing firms’ adjustment to globalization by investigating the linkages among productivity, exports and imports, and the role of mergers and acquisitions on firms’ decisions to conduct arm’s length transactions as opposed to intra-firm trade. This project will shed light on the impact of trade in the following areas of the food manufacturing industry: (1) changes in product mix and product proliferation as well as changes in employment due to import competition and trade reforms; (2) the relationship between mergers and acquisitions and firms’ decisions to engage in arm’s length vs. intra-firm trade; (3) productivity differences between firms engaged in trade relative to those that are not; (4) innovation and product differentiation as engines for growth which promote competitiveness (measured as returns to factors of production). For the productivity estimation, this project applies a methodology to adjust the measurement of productivity by taking into consideration both simultaneity and omitted price variable biases. The industries where firms have differentiated products, as in the food manufacturing industry, have biased coefficient estimates using the general approach."
"Financing Costs, Per-Shipment Costs and Shipping Frequency: Firm-Level
  Evidence from Bangladesh","In international trade, firms face lengthy ordering-producing-delivery times
and make shipping frequency decisions based on the per-shipment costs and
financing costs. In this paper, I develop a model of importer-exporter
procurement where the importer procures international inputs from exporting
firms in developing countries. The exporters are credit constrained for working
capital, incur the per-shipment fixed costs, and get paid after goods are
delivered to the importer. The model shows that the shipping frequency
increases for high financing costs in origin and destination. Furthermore,
longer delivery times increase shipping frequency as well as procurement costs.
The model also shows that the higher per-shipment fixed costs reduce the
shipping frequency, in line with previous literature. Reduced transaction costs
lower the exporter's demand for financial services through shipping frequency
adjustment, mitigating the financial frictions of the firm. Then, I empirically
investigate whether the conclusions regarding the effect of per-shipment fixed
costs on shipping frequency from the theoretical model and in the existing
literature extend to developing countries. My estimation method addresses
several biases. First, I deal with aggregation bias with the firm, product, and
country-level analysis. Second, I consider the Poisson Pseudo Maximum
Likelihood (PPML) estimation method to deal with heteroscedasticity bias from
the OLS estimation of log-linear models. Third, I fix the distance
non-linearity of Bangladeshi exports. Finally, I consider the effect of
financing cost on shipping frequency to address omitted variable bias. Using
transaction-level export data from Bangladesh, I find that 10% higher
per-shipment costs reduce the shipping frequency by 3.45%. The findings are
robust to different specifications and subsamples.",['Md Deluair Hossen'],[],0,arXiv,http://arxiv.org/abs/2303.04223v1,False,True,False,False,False,860,Jonathan I Dingel,Chicago,Completed,2012,2016.0,"This project investigates the relationship between product quality and shipping costs. Using the Commodity Flow Surveys and the Census of Manufactures of 1997, 2002, and 2007, establishment-level regressions of output unit values on shipment distance, and input unit values on average shipment distance, will test whether establishments sell higher-quality goods to more distant destinations. Using these domestic data is a significant improvement relative to existing studies (which use national export data) due to the precision of the distance measures and the data on shipments originating from many locations."
Fertility and its Meaning: Evidence from Search Behavior,"Fertility choices are linked to the different preferences and constraints of
individuals and couples, and vary importantly by socio-economic status, as well
by cultural and institutional context. The meaning of childbearing and
child-rearing, therefore, differs between individuals and across groups. In
this paper, we combine data from Google Correlate and Google Trends for the
U.S. with ground truth data from the American Community Survey to derive new
insights into fertility and its meaning. First, we show that Google Correlate
can be used to illustrate socio-economic differences on the circumstances
around pregnancy and birth: e.g., searches for ""flying while pregnant"" are
linked to high income fertility, and ""paternity test"" are linked to non-marital
fertility. Second, we combine several search queries to build predictive models
of regional variation in fertility, explaining about 75% of the variance.
Third, we explore if aggregated web search data can also be used to model
fertility trends.","['Jussi Ojala', 'Emilio Zagheni', 'Francesco C. Billari', 'Ingmar Weber']",[],0,arXiv,http://arxiv.org/abs/1703.03935v1,False,True,False,False,True,869,Emily G Collins,Michigan,Completed,2011,2016.0,"Despite celebrating the 50th anniversary of the FDA approval of the oral contraceptive pill, significant scholarly debate remains about the role that the Pill played in the dramatic demographic shifts of the 1960s. Estimating the causal impact of the Pill has been difficult because of the coincidence of its release with the peak of the baby boom, the rise of the women's movement, and many other social changes that render standard inter-temporal comparisons invalid. Bailey (2010) developed a quasi-experimental empirical strategy to address these problems. Specifically, she uses state-level variation in anti-obscenity ""Comstock laws"" which made the Pill illegal in 24 states, in conjunction with the timing of the introduction of the Pill in 1957 and the Supreme Court's decision to strike down Connecticut's Comstock statute in 1965 with Griswold v. Connecticut. This project proposes to use data from both the publicly available IPUMS and the restricted-access microdata from the decennial censuses to pursue three specific scientific aims:  (1) To use individual county-identifiers to develop and test a distance-based regression discontinuity methodology for estimating the impact of the birth control pill on completed fertility; (2) To use individual county-identifiers and the methodology in (1) to quantify the impact of the birth control pill on completed fertility, marital outcomes, child quality, and female labor force participation; and (3) To use this methodology to examine the impact of the birth control pill on disparities in these outcomes by race and education level."
"Mater certa est, pater numquam: What can Facebook Advertising Data Tell
  Us about Male Fertility Rates?","In many developing countries, timely and accurate information about birth
rates and other demographic indicators is still lacking, especially for male
fertility rates. Using anonymous and aggregate data from Facebook's Advertising
Platform, we produce global estimates of the Mean Age at Childbearing (MAC), a
key indicator of fertility postponement. Our analysis indicates that fertility
measures based on Facebook data are highly correlated with conventional
indicators based on traditional data, for those countries for which we have
statistics. For instance, the correlation of the MAC computed using Facebook
and United Nations data is 0.47 (p = 4.02e-08) and 0.79 (p = 2.2e-15) for
female and male respectively. Out of sample validation for a simple regression
model indicates that the mean absolute percentage error is 2.3%. We use the
linear model and Facebook data to produce estimates of the male MAC for
countries for which we do not have data.","['Francesco Rampazzo', 'Emilio Zagheni', 'Ingmar Weber', 'Maria Rita Testa', 'Francesco Billari']",[],0,arXiv,http://arxiv.org/abs/1804.04632v1,False,True,False,False,False,869,Emily G Collins,Michigan,Completed,2011,2016.0,"Despite celebrating the 50th anniversary of the FDA approval of the oral contraceptive pill, significant scholarly debate remains about the role that the Pill played in the dramatic demographic shifts of the 1960s. Estimating the causal impact of the Pill has been difficult because of the coincidence of its release with the peak of the baby boom, the rise of the women's movement, and many other social changes that render standard inter-temporal comparisons invalid. Bailey (2010) developed a quasi-experimental empirical strategy to address these problems. Specifically, she uses state-level variation in anti-obscenity ""Comstock laws"" which made the Pill illegal in 24 states, in conjunction with the timing of the introduction of the Pill in 1957 and the Supreme Court's decision to strike down Connecticut's Comstock statute in 1965 with Griswold v. Connecticut. This project proposes to use data from both the publicly available IPUMS and the restricted-access microdata from the decennial censuses to pursue three specific scientific aims:  (1) To use individual county-identifiers to develop and test a distance-based regression discontinuity methodology for estimating the impact of the birth control pill on completed fertility; (2) To use individual county-identifiers and the methodology in (1) to quantify the impact of the birth control pill on completed fertility, marital outcomes, child quality, and female labor force participation; and (3) To use this methodology to examine the impact of the birth control pill on disparities in these outcomes by race and education level."
"High Accurate and Explainable Multi-Pill Detection Framework with Graph
  Neural Network-Assisted Multimodal Data Fusion","Due to the significant resemblance in visual appearance, pill misuse is
prevalent and has become a critical issue, responsible for one-third of all
deaths worldwide. Pill identification, thus, is a crucial concern needed to be
investigated thoroughly. Recently, several attempts have been made to exploit
deep learning to tackle the pill identification problem. However, most
published works consider only single-pill identification and fail to
distinguish hard samples with identical appearances. Also, most existing pill
image datasets only feature single pill images captured in carefully controlled
environments under ideal lighting conditions and clean backgrounds. In this
work, we are the first to tackle the multi-pill detection problem in real-world
settings, aiming at localizing and identifying pills captured by users in a
pill intake. Moreover, we also introduce a multi-pill image dataset taken in
unconstrained conditions. To handle hard samples, we propose a novel method for
constructing heterogeneous a priori graphs incorporating three forms of
inter-pill relationships, including co-occurrence likelihood, relative size,
and visual semantic correlation. We then offer a framework for integrating a
priori with pills' visual features to enhance detection accuracy. Our
experimental results have proved the robustness, reliability, and
explainability of the proposed framework. Experimentally, it outperforms all
detection benchmarks in terms of all evaluation metrics. Specifically, our
proposed framework improves COCO mAP metrics by 9.4% over Faster R-CNN and
12.0% compared to vanilla YOLOv5. Our study opens up new opportunities for
protecting patients from medication errors using an AI-based pill
identification solution.","['Anh Duy Nguyen', 'Huy Hieu Pham', 'Huynh Thanh Trung', 'Quoc Viet Hung Nguyen', 'Thao Nguyen Truong', 'Phi Le Nguyen']",[],0,arXiv,http://arxiv.org/abs/2303.09782v1,False,True,False,False,False,869,Emily G Collins,Michigan,Completed,2011,2016.0,"Despite celebrating the 50th anniversary of the FDA approval of the oral contraceptive pill, significant scholarly debate remains about the role that the Pill played in the dramatic demographic shifts of the 1960s. Estimating the causal impact of the Pill has been difficult because of the coincidence of its release with the peak of the baby boom, the rise of the women's movement, and many other social changes that render standard inter-temporal comparisons invalid. Bailey (2010) developed a quasi-experimental empirical strategy to address these problems. Specifically, she uses state-level variation in anti-obscenity ""Comstock laws"" which made the Pill illegal in 24 states, in conjunction with the timing of the introduction of the Pill in 1957 and the Supreme Court's decision to strike down Connecticut's Comstock statute in 1965 with Griswold v. Connecticut. This project proposes to use data from both the publicly available IPUMS and the restricted-access microdata from the decennial censuses to pursue three specific scientific aims:  (1) To use individual county-identifiers to develop and test a distance-based regression discontinuity methodology for estimating the impact of the birth control pill on completed fertility; (2) To use individual county-identifiers and the methodology in (1) to quantify the impact of the birth control pill on completed fertility, marital outcomes, child quality, and female labor force participation; and (3) To use this methodology to examine the impact of the birth control pill on disparities in these outcomes by race and education level."
"Image-based Contextual Pill Recognition with Medical Knowledge Graph
  Assistance","Identifying pills given their captured images under various conditions and
backgrounds has been becoming more and more essential. Several efforts have
been devoted to utilizing the deep learning-based approach to tackle the pill
recognition problem in the literature. However, due to the high similarity
between pills' appearance, misrecognition often occurs, leaving pill
recognition a challenge. To this end, in this paper, we introduce a novel
approach named PIKA that leverages external knowledge to enhance pill
recognition accuracy. Specifically, we address a practical scenario (which we
call contextual pill recognition), aiming to identify pills in a picture of a
patient's pill intake. Firstly, we propose a novel method for modeling the
implicit association between pills in the presence of an external data source,
in this case, prescriptions. Secondly, we present a walk-based graph embedding
model that transforms from the graph space to vector space and extracts
condensed relational features of the pills. Thirdly, a final framework is
provided that leverages both image-based visual and graph-based relational
features to accomplish the pill identification task. Within this framework, the
visual representation of each pill is mapped to the graph embedding space,
which is then used to execute attention over the graph representation,
resulting in a semantically-rich context vector that aids in the final
classification. To our knowledge, this is the first study to use external
prescription data to establish associations between medicines and to classify
them using this aiding information. The architecture of PIKA is lightweight and
has the flexibility to incorporate into any recognition backbones. The
experimental results show that by leveraging the external knowledge graph, PIKA
can improve the recognition accuracy from 4.8% to 34.1% in terms of F1-score,
compared to baselines.","['Anh Duy Nguyen', 'Thuy Dung Nguyen', 'Huy Hieu Pham', 'Thanh Hung Nguyen', 'Phi Le Nguyen']",[],0,arXiv,http://arxiv.org/abs/2208.02432v2,False,True,False,False,False,869,Emily G Collins,Michigan,Completed,2011,2016.0,"Despite celebrating the 50th anniversary of the FDA approval of the oral contraceptive pill, significant scholarly debate remains about the role that the Pill played in the dramatic demographic shifts of the 1960s. Estimating the causal impact of the Pill has been difficult because of the coincidence of its release with the peak of the baby boom, the rise of the women's movement, and many other social changes that render standard inter-temporal comparisons invalid. Bailey (2010) developed a quasi-experimental empirical strategy to address these problems. Specifically, she uses state-level variation in anti-obscenity ""Comstock laws"" which made the Pill illegal in 24 states, in conjunction with the timing of the introduction of the Pill in 1957 and the Supreme Court's decision to strike down Connecticut's Comstock statute in 1965 with Griswold v. Connecticut. This project proposes to use data from both the publicly available IPUMS and the restricted-access microdata from the decennial censuses to pursue three specific scientific aims:  (1) To use individual county-identifiers to develop and test a distance-based regression discontinuity methodology for estimating the impact of the birth control pill on completed fertility; (2) To use individual county-identifiers and the methodology in (1) to quantify the impact of the birth control pill on completed fertility, marital outcomes, child quality, and female labor force participation; and (3) To use this methodology to examine the impact of the birth control pill on disparities in these outcomes by race and education level."
Synthetic Census Data Generation via Multidimensional Multiset Sum,"The US Decennial Census provides valuable data for both research and policy
purposes. Census data are subject to a variety of disclosure avoidance
techniques prior to release in order to preserve respondent confidentiality.
While many are interested in studying the impacts of disclosure avoidance
methods on downstream analyses, particularly with the introduction of
differential privacy in the 2020 Decennial Census, these efforts are limited by
a critical lack of data: The underlying ""microdata,"" which serve as necessary
input to disclosure avoidance methods, are kept confidential.
  In this work, we aim to address this limitation by providing tools to
generate synthetic microdata solely from published Census statistics, which can
then be used as input to any number of disclosure avoidance algorithms for the
sake of evaluation and carrying out comparisons. We define a principled
distribution over microdata given published Census statistics and design
algorithms to sample from this distribution. We formulate synthetic data
generation in this context as a knapsack-style combinatorial optimization
problem and develop novel algorithms for this setting. While the problem we
study is provably hard, we show empirically that our methods work well in
practice, and we offer theoretical arguments to explain our performance.
Finally, we verify that the data we produce are ""close"" to the desired ground
truth.","['Cynthia Dwork', 'Kristjan Greenewald', 'Manish Raghavan']",[],0,arXiv,http://arxiv.org/abs/2404.10095v1,False,True,True,False,True,874,Nicholas Nagle,Atlanta,Completed,2013,2017.0,"This research uses restricted-use microdata from the 1970-2000 Decennial Censuses and 2005-2009 American Community Survey (ACS) to validate methods for estimating small area totals from public Census data and to evaluate the nature of disclosure protection present in the existing public data. The methodology developed and to be implemented by the research team generates sampling weights for allocating household records in the Public Use Microdata Samples (PUMS) to a given census tract. This methodology will allow users to combine public data in order to create population estimates with increased spatial resolution at the expense of increased statistical uncertainty. Such a tradeoff is desirable in many situations, and allowing researchers to choose between precision and uncertainty will increase the utility of publicly available Census data. In the process of the analysis, the researchers will also create a synthetic dataset from exclusively public sources, allowing them to assess the degree of disclosure protection present in existing public use samples."
"The 2010 Census Confidentiality Protections Failed, Here's How and Why","Using only 34 published tables, we reconstruct five variables (census block,
sex, age, race, and ethnicity) in the confidential 2010 Census person records.
Using the 38-bin age variable tabulated at the census block level, at most
20.1% of reconstructed records can differ from their confidential source on
even a single value for these five variables. Using only published data, an
attacker can verify that all records in 70% of all census blocks (97 million
people) are perfectly reconstructed. The tabular publications in Summary File 1
thus have prohibited disclosure risk similar to the unreleased confidential
microdata. Reidentification studies confirm that an attacker can, within blocks
with perfect reconstruction accuracy, correctly infer the actual census
response on race and ethnicity for 3.4 million vulnerable population uniques
(persons with nonmodal characteristics) with 95% accuracy, the same precision
as the confidential data achieve and far greater than statistical baselines.
The flaw in the 2010 Census framework was the assumption that aggregation
prevented accurate microdata reconstruction, justifying weaker disclosure
limitation methods than were applied to 2010 Census public microdata. The
framework used for 2020 Census publications defends against attacks that are
based on reconstruction, as we also demonstrate here. Finally, we show that
alternatives to the 2020 Census Disclosure Avoidance System with similar
accuracy (enhanced swapping) also fail to protect confidentiality, and those
that partially defend against reconstruction attacks (incomplete suppression
implementations) destroy the primary statutory use case: data for redistricting
all legislatures in the country in compliance with the 1965 Voting Rights Act.","['John M. Abowd', 'Tamara Adams', 'Robert Ashmead', 'David Darais', 'Sourya Dey', 'Simson L. Garfinkel', 'Nathan Goldschlag', 'Daniel Kifer', 'Philip Leclerc', 'Ethan Lew', 'Scott Moore', 'Rolando A. Rodríguez', 'Ramy N. Tadros', 'Lars Vilhuber']",[],0,arXiv,http://arxiv.org/abs/2312.11283v1,False,True,True,False,False,874,Nicholas Nagle,Atlanta,Completed,2013,2017.0,"This research uses restricted-use microdata from the 1970-2000 Decennial Censuses and 2005-2009 American Community Survey (ACS) to validate methods for estimating small area totals from public Census data and to evaluate the nature of disclosure protection present in the existing public data. The methodology developed and to be implemented by the research team generates sampling weights for allocating household records in the Public Use Microdata Samples (PUMS) to a given census tract. This methodology will allow users to combine public data in order to create population estimates with increased spatial resolution at the expense of increased statistical uncertainty. Such a tradeoff is desirable in many situations, and allowing researchers to choose between precision and uncertainty will increase the utility of publicly available Census data. In the process of the analysis, the researchers will also create a synthetic dataset from exclusively public sources, allowing them to assess the degree of disclosure protection present in existing public use samples."
"An Approximate Monte Carlo Simulation Method for Estimating Uncertainty
  and Constructing Confidence Intervals for 2020 Census Statistics","To protect the confidentiality of the 2020 Census, the U.S. Census Bureau
adopted a statistical disclosure limitation framework based on the principles
of differential privacy. A key component was the TopDown Algorithm, which
applied differentially-private noise to an extensive series of counts from the
confidential 2020 Census data and transformed the resulting noisy measurements
into privacy-protected microdata which were then tabulated to produce official
data products. Though the codebase is publicly available, currently there is no
way to estimate the uncertainty for statistics included in the published data
products that accounts for both the noisy measurements and the post-processing
performed within the TopDown Algorithm. We propose an approximate Monte Carlo
Simulation method that allows for the estimation of statistical quantities like
mean squared error, bias, and variance, as well as the construction of
confidence intervals. The method uses the output of the production iteration of
the TopDown Algorithm as the input to additional iterations, allowing for
statistical quantities to be estimated without impacting the formal privacy
protections of the 2020 Census. The results show that, in general, the
quantities estimated by this approximate method closely match their intended
targets and that the resulting confidence intervals are statistically valid.","['Robert Ashmead', 'Michael B. Hawes', 'Mary Pritts', 'Pavel Zhuravlev', 'Sallie Ann Keller']",[],0,arXiv,http://arxiv.org/abs/2503.19714v1,True,True,False,False,False,874,Nicholas Nagle,Atlanta,Completed,2013,2017.0,"This research uses restricted-use microdata from the 1970-2000 Decennial Censuses and 2005-2009 American Community Survey (ACS) to validate methods for estimating small area totals from public Census data and to evaluate the nature of disclosure protection present in the existing public data. The methodology developed and to be implemented by the research team generates sampling weights for allocating household records in the Public Use Microdata Samples (PUMS) to a given census tract. This methodology will allow users to combine public data in order to create population estimates with increased spatial resolution at the expense of increased statistical uncertainty. Such a tradeoff is desirable in many situations, and allowing researchers to choose between precision and uncertainty will increase the utility of publicly available Census data. In the process of the analysis, the researchers will also create a synthetic dataset from exclusively public sources, allowing them to assess the degree of disclosure protection present in existing public use samples."
"Releasing survey microdata with exact cluster locations and additional
  privacy safeguards","Household survey programs around the world publish fine-granular
georeferenced microdata to support research on the interdependence of human
livelihoods and their surrounding environment. To safeguard the respondents'
privacy, micro-level survey data is usually (pseudo)-anonymized through
deletion or perturbation procedures such as obfuscating the true location of
data collection. This, however, poses a challenge to emerging approaches that
augment survey data with auxiliary information on a local level. Here, we
propose an alternative microdata dissemination strategy that leverages the
utility of the original microdata with additional privacy safeguards through
synthetically generated data using generative models. We back our proposal with
experiments using data from the 2011 Costa Rican census and satellite-derived
auxiliary information. Our strategy reduces the respondents' re-identification
risk for any number of disclosed attributes by 60-80\% even under
re-identification attempts.","['Till Koebe', 'Alejandra Arias-Salazar']",[],0,arXiv,http://arxiv.org/abs/2205.12260v1,False,True,False,False,False,874,Nicholas Nagle,Atlanta,Completed,2013,2017.0,"This research uses restricted-use microdata from the 1970-2000 Decennial Censuses and 2005-2009 American Community Survey (ACS) to validate methods for estimating small area totals from public Census data and to evaluate the nature of disclosure protection present in the existing public data. The methodology developed and to be implemented by the research team generates sampling weights for allocating household records in the Public Use Microdata Samples (PUMS) to a given census tract. This methodology will allow users to combine public data in order to create population estimates with increased spatial resolution at the expense of increased statistical uncertainty. Such a tradeoff is desirable in many situations, and allowing researchers to choose between precision and uncertainty will increase the utility of publicly available Census data. In the process of the analysis, the researchers will also create a synthetic dataset from exclusively public sources, allowing them to assess the degree of disclosure protection present in existing public use samples."
"When the System does not Fit: Coping Strategies of Employment
  Consultants","Case and knowledge management systems are spread at the frontline across
public agencies. However, such systems are dedicated for the collaboration
within the agency rather than for the face-to-face interaction with the
clients. If used as a collaborative resource at the frontline, case and
knowledge management systems might disturb the service provision by displaying
unfiltered internal information, disclosing private data of other clients, or
revealing the limits of frontline employees' competence (if they cannot explain
something) or their authority (if they cannot override something). Observation
in the German Public Employment Agency shows that employment consultants make
use of various coping strategies during face-to-face consultations to extend
existing boundaries set by the case and knowledge management systems and by the
rules considering their usage. The analysis of these coping strategies unveils
the forces that shape the conduct of employment consultants during their
contacts with clients: the consultants' own understanding of work, the actual
and the perceived needs of the clients, and the political mission as well as
the internal rules of the employment agency. The findings form a twofold
contribution: First, they contribute to the discourse on work in employment
agencies by illustrating how the complexities of social welfare apparatus
demonstrate themselves in singular behavioural patterns. Second, they
contribute to the discourse on screen-level bureaucracy by depicting the
consultants as active and conscious mediators rather than passive interfaces
between the system and the client.","['Mateusz Dolata', 'Birgit Schenk', 'Jara Fuhrer', 'Alina Marti', 'Gerhard Schwabe']",[],0,arXiv,http://arxiv.org/abs/2409.09457v1,False,True,False,False,False,878,Jean M Abraham,Minnesota,Completed,2011,2015.0,"Employers and employees face economic incentives that encourage health insurance provision through the workplace. This project will use the Medical Expenditure Panel Survey-Insurance Component (MEPS-IC) augmented with other federal and non-federal data sources to analyze employer behavior regarding the provision of health insurance. Specifically, we will estimate the net advantage or disadvantage to private sector employers of keeping or dropping health insurance under any changing economic incentives created by reforms to health care. The “net advantage” of dropping health insurance reflects an establishment's assessment of the potential value of exchange-based premium assistance credits (subsidies) that its workers could get if the employer dropped coverage; the value of the tax subsidy associated with offering health insurance; and the cost of the employer-shared responsibility requirement that an employer would incur if it dropped coverage. In addition, we will quantify the relationship between an employer's propensity to offer insurance and the tax price of insurance among workers in the establishment, characteristics of the establishment and its workforce, labor market conditions, and competition in the market for health insurance by modeling an employer's decision to offer health insurance. Finally, we will predict how economic incentives facing employers will alter their incentives to provide health insurance. 
The MEPS-IC are critical for analyzing the proposed issues as no other nationally representative data set exists that contains detailed information on health benefit offerings, premiums, and workforce composition of U.S. establishments. The resulting analyses will inform the Census Bureau about the relation between health insurance provision by employers and the economic incentives that businesses face, which are driven in large part by the characteristics of their workers and their families. We will develop methods to enhance the information contained in the MEPS-IC with respect to measuring an establishment's workforce composition, including estimating the wage distribution of full-time workers within establishments who are most likely to be eligible for health insurance. We will also develop methods to facilitate a comparison of the distribution of wage income reported for workers relative to the distribution of household income by workers in establishments. These analyses can facilitate a more complete assessment of employers' changing incentives to offer health insurance and they can test the sensitivity of how particular assumptions about employer behavior affect the offering decision. The proposed research also will benefit the Census Bureau by providing population estimates of establishment offers of health insurance under existing economic incentives and offers a flexible model for understanding how employer behavior may change in light of new economic incentives (e.g., differences by state or under the Patient Protection and Affordable Care Act of 2010)."
"A multiplicative masking method for preserving the skewness of the
  original micro-records","Masking methods for the safe dissemination of microdata consist of distorting
the original data while preserving a pre-defined set of statistical properties
in the microdata. For continuous variables, available methodologies rely
essentially on matrix masking and in particular on adding noise to the original
values, using more or less refined procedures depending on the extent of
information that one seeks to preserve. Almost all of these methods make use of
the critical assumption that the original datasets follow a normal distribution
and/or that the noise has such a distribution. This assumption is, however,
restrictive in the sense that few variables follow empirically a Gaussian
pattern: the distribution of household income, for example, is positively
skewed, and this skewness is essential information that has to be considered
and preserved. This paper addresses these issues by presenting a simple
multiplicative masking method that preserves skewness of the original data
while offering a sufficient level of disclosure risk control. Numerical
examples are provided, leading to the suggestion that this method could be
well-suited for the dissemination of a broad range of microdata, including
those based on administrative and business records.",['Nicolas Ruiz'],[],0,arXiv,http://arxiv.org/abs/1712.02549v1,False,True,False,False,False,893,Marc Rubin,Washington,Completed,2010,2012.0,"This project is concerned specifically with evaluating internal microdata for island areas that are U.S. possessions (Northern Marianas, Guam, U.S. Virgin Islands, American Samoa) to determine whether the published tabulations accurately reflect the underlying relationships in the microdata.  Any discrepancies will be reported to the program area as well as the Bureau of Economic Analysis for input into BEA's GDP estimates for the affected sectors and geographical units."
"COVID-19's Unequal Toll: An assessment of small business impact
  disparities with respect to ethnorace in metropolitan areas in the US using
  mobility data","Early in the pandemic, counties and states implemented a variety of
non-pharmacological interventions (NPIs) focused on mobility, such as national
lockdowns or work-from-home strategies, as it became clear that restricting
movement was essential to containing the epidemic. Due to these restrictions,
businesses were severely affected and in particular, small, urban restaurant
businesses. In addition to that, COVID-19 has also amplified many of the
socioeconomic disparities and systemic racial inequities that exist in our
society. The overarching objective of this study was to examine the changes in
small urban restaurant visitation patterns following the COVID-19 pandemic and
associated mobility restrictions, as well as to uncover potential disparities
across different racial/ethnic groups in order to understand inequities in the
impact and recovery. Specifically, the two key objectives were: 1) to analyze
the overall changes in restaurant visitation patterns in US metropolitan areas
during the pandemic compared to a pre-pandemic baseline, and 2) to investigate
differences in visitation pattern changes across Census Block Groups with
majority Asian, Black, Hispanic, White, and American Indian populations,
identifying any disproportionate effects. Using aggregated geolocated cell
phone data from SafeGraph, we document the overall changes in small urban
restaurant businesses' visitation patterns with respect to racial composition
at a granularity of Census Block Groups. Our results show clear indications of
reduced visitation patterns after the pandemic, with slow recoveries. Via
visualizations and statistical analyses, we show that reductions in visitation
patterns were the highest for small urban restaurant businesses in majority
Asian neighborhoods.","['Saad Mohammad Abrar', 'Kazi Tasnim Zinat', 'Naman Awasthi', 'Vanessa Frias-Martinez']",[],0,arXiv,http://arxiv.org/abs/2405.11121v1,False,True,False,False,False,893,Marc Rubin,Washington,Completed,2010,2012.0,"This project is concerned specifically with evaluating internal microdata for island areas that are U.S. possessions (Northern Marianas, Guam, U.S. Virgin Islands, American Samoa) to determine whether the published tabulations accurately reflect the underlying relationships in the microdata.  Any discrepancies will be reported to the program area as well as the Bureau of Economic Analysis for input into BEA's GDP estimates for the affected sectors and geographical units."
"Early life exposure to measles and later-life outcomes: Evidence from
  the introduction of a vaccine","Until the mid 1960s, the UK experienced regular measles epidemics, with the
vast majority of children being infected in early childhood. The introduction
of a measles vaccine substantially reduced its incidence. The first part of
this paper examines the long-term human capital and health effects of this
change in the early childhood disease environment. The second part investigates
interactions between the vaccination campaign and individuals' endowments as
captured using molecular genetic data, shedding light on complementarities
between public health investments and individual endowments. We use two
identification approaches, based on the nationwide introduction of the vaccine
in 1968 and local vaccination trials in 1966. Our results show that exposure to
the vaccination in early childhood positively affects adult height, but only
among those with high genetic endowments for height. We find no effects on
years of education; neither a direct effect, nor evidence of complementarities.","['Gerard J. van den Berg', 'Stephanie von Hinke', 'Nicolai Vitt']",[],0,arXiv,http://arxiv.org/abs/2301.10558v1,False,True,False,False,False,898,Jason Fletcher,Wisconsin,Completed,2011,2018.0,"This project examines the potential causal effects of in utero exposure to the 1918 flu pandemic on later life mortality and economic and social outcomes. The project first replicates previous findings indicating substantial evidence that exposure to the flu reduces years of schooling and income and increases several measures of poor health. The research will extend these findings to include measures of health insurance, occupation, mobility, marital status, and spousal characteristics. Although some of these intermediate and long-term effects have been documented in several papers, much less is known about the links with mortality. The research will help to fill in this gap by estimating overall and cause-specific mortality outcomes using the restricted National Longitudinal Mortality Study (NLMS) data. "
Sustainable Venture Capital,"Sustainability initiatives are set to benefit greatly from the growing
involvement of venture capital, in the same way that other technological
endeavours have been enabled and accelerated in the post-war period. With the
spoils increasingly being shared between shareholders and other stakeholders,
this requires a more nuanced view than the finance-first methodologies deployed
to date. Indeed, it is possible for a venture-backed sustainability startup to
deliver outstanding results to society in general without returning a cent to
investors, though the most promising outcomes deliver profit with purpose,
satisfying all stakeholders in ways that make existing 'extractive' venture
capital seem hollow.
  To explore this nascent area, a review of related research was conducted and
social entrepreneurs & investors interviewed to construct a questionnaire
assessing the interests and intentions of current & future ecosystem
participants. Analysis of 114 responses received via several sampling methods
revealed statistically significant relationships between investing preferences
and genders, generations, sophistication, and other variables, all the way down
to the level of individual UN Sustainable Development Goals (SDGs).",['Sam Johnston'],[],0,arXiv,http://arxiv.org/abs/2209.10518v1,False,True,False,False,False,900,Timothy Dore,Washington,Completed,2011,2015.0,"This project examines the importance of venture capital in the performance of local economies and individual firms. In particular, it examines employment, wages, firm entry, and firm exit at the local level, as well as employment, wages, patenting activity, and expenditure patterns at the firm level, and estimates the effect of venture capital on these performance measures.  In addition to detailing the characteristics of local economies and firms as a function of venture capital involvement, the project will generate findings aimed at improving the sampling methodologies of the Survey of Industrial Research and Development and the Business R&D and Innovation Survey. The project will extend existing bridges and build new ones between Census data and external data on patenting and venture capital activity. It will then rely on this external data to generate concrete suggestions on improving the sampling methodology of Census surveys. Finally, the project will compare estimates of aggregate and firm level performance from Census data with estimates from external data sources to identify any quality issues in several Census data sources. "
"A Quantitative Framework for Network Resilience Evaluation using Dynamic
  Bayesian Network","Measuring and evaluating network resilience has become an important aspect
since the network is vulnerable to both uncertain disturbances and malicious
attacks. Networked systems are often composed of many dynamic components and
change over time, which makes it difficult for existing methods to access the
changeable situation of network resilience. This paper establishes a novel
quantitative framework for evaluating network resilience using the Dynamic
Bayesian Network. The proposed framework can be used to evaluate the network's
multi-stage resilience processes when suffering various attacks and recoveries.
First, we define the dynamic capacities of network components and establish the
network's five core resilience capabilities to describe the resilient
networking stages including preparation, resistance, adaptation, recovery, and
evolution; the five core resilience capabilities consist of rapid response
capability, sustained resistance capability, continuous running capability,
rapid convergence capability, and dynamic evolution capability. Then, we employ
a two-time slices approach based on the Dynamic Bayesian Network to quantify
five crucial performances of network resilience based on core capabilities
proposed above. The proposed approach can ensure the time continuity of
resilience evaluation in time-varying networks. Finally, our proposed
evaluation framework is applied to different attacks and recovery conditions in
typical simulations and real-world network topology. Results and comparisons
with extant studies indicate that the proposed method can achieve a more
accurate and comprehensive evaluation and can be applied to network scenarios
under various attack and recovery intensities.","['Shanqing Jiang', 'Lin Yang', 'Guang Cheng', 'Xianming Gao', 'Tao Feng', 'Yuyang Zhou']",[],0,arXiv,http://arxiv.org/abs/2108.09040v1,False,True,False,False,False,907,Lisa K Bates,Chicago,Completed,2011,2012.0,"Hurricane Katrina wrought major damage to housing across the New Orleans area.  Five years later, recovery remained spotty.  Over 100,000 residents had not returned to the city and in some neighborhoods physical reconstruction remained incomplete despite significant resources having been dedicated to recovery.  The 2009 American Housing Survey’s special post-Katrina sample for metropolitan New Orleans allows researchers to understand better the critical factors in recovery for housing and households.  This project uses American Housing Survey (AHS) data to address questions of vulnerability to and resilience after a major natural disaster event.  The 2009 AHS special examination of post-Katrina New Orleans provides a significant opportunity to analyze vulnerability and recovery, providing new information to policy makers about how better to prepare for and respond to such events.  This study analyzes pre-Hurricane Katrina conditions, disaster damage, and post-Katrina recovery.  It focuses on repair and re-occupancy of housing units by their original inhabitants to address the multiple dimensions of vulnerability, considering how household, housing unit, and neighborhood characteristics affect recovery.  The analysis employs multi-level modeling to distinguish effects of different facets of vulnerability, and estimates the contribution of neighborhood status to housing recovery over and above household factors."
"Resilience-based performance modeling and decision optimization for
  transportation network","This research presented a novel resilience-based framework to support
resilience planning regarding pre-disaster mitigation and post-disaster
recovery.
  First, the author proposes a new performance metric for transportation
network, weighted number of independent pathways (WIPW), integrating the
network topology, redundancy level, traffic patterns, structural reliability of
network components, and functionality of the network during community's
post-disaster recovery in a systematical way. To the best of our knowledge,
WIPW is the only performance metric that permits risk mitigation alternatives
for improving transportation network resilience to be compared on a common
basis. Based on the WIPW, a decision methodology of prioritizing transportation
network retrofit projects is developed.
  Second, our studies extend from pre-disaster mitigation to post-hazard
recovery, in which this research presents two metrics to evaluate the
restoration over the horizon after disasters . That is, total recovery time and
the skew of the recovery trajectory. Both metrics are involved in the
multi-objective stochastic optimization problem of restoration scheduling. The
metrics provided a new dimension to evaluate the relative efficiency of
alternative network recovery strategies. The author then develops a restoration
scheduling methodology for network post-disaster recovery that minimizes the
overall network recovery time and optimizes the recovery trajectory, which
ultimately will reduce economic losses due to network service disruption.",['Weili Zhang'],[],0,arXiv,http://arxiv.org/abs/1808.03761v1,False,True,False,False,False,907,Lisa K Bates,Chicago,Completed,2011,2012.0,"Hurricane Katrina wrought major damage to housing across the New Orleans area.  Five years later, recovery remained spotty.  Over 100,000 residents had not returned to the city and in some neighborhoods physical reconstruction remained incomplete despite significant resources having been dedicated to recovery.  The 2009 American Housing Survey’s special post-Katrina sample for metropolitan New Orleans allows researchers to understand better the critical factors in recovery for housing and households.  This project uses American Housing Survey (AHS) data to address questions of vulnerability to and resilience after a major natural disaster event.  The 2009 AHS special examination of post-Katrina New Orleans provides a significant opportunity to analyze vulnerability and recovery, providing new information to policy makers about how better to prepare for and respond to such events.  This study analyzes pre-Hurricane Katrina conditions, disaster damage, and post-Katrina recovery.  It focuses on repair and re-occupancy of housing units by their original inhabitants to address the multiple dimensions of vulnerability, considering how household, housing unit, and neighborhood characteristics affect recovery.  The analysis employs multi-level modeling to distinguish effects of different facets of vulnerability, and estimates the contribution of neighborhood status to housing recovery over and above household factors."
"Twin Estimates of the Effects of Prenatal Environment, Child Biology,
  and Parental Bias on Sex Differences in Early Age Mortality","Sex differences in early age mortality have been explained in prior
literature by differences in biological make-up and gender discrimination in
the allocation of household resources. Studies estimating the effects of these
factors have generally assumed that offspring sex ratio is random, which is
implausible in view of recent evidence that the sex of a child is partly
determined by prenatal environmental factors. These factors may also affect
child health and survival in utero or after birth, which implies that
conventional approaches to explaining sex differences in mortality are likely
to yield biased estimates. We propose a methodology for decomposing these
differences into the effects of prenatal environment, child biology, and
parental preferences. Using a large sample of twins, we compare mortality rates
in male-female twin pairs in India, a region known for discriminating against
daughters, and sub-Saharan Africa, a region where sons and daughters are
thought to be valued by their parents about equally. We find that: (1) prenatal
environment positively affects the mortality of male children; (2) biological
make-up of the latter contributes to their excess mortality, but its effect has
been previously overestimated; and (3) parental discrimination against female
children in India negatively affects their survival; but failure to control for
the effects of prenatal and biological factors leads conventional approaches to
underestimating its effect by 237 percent during infancy, and 44 percent during
childhood.",['Roland Pongou'],[],0,arXiv,http://arxiv.org/abs/2010.05712v1,False,True,False,False,False,932,William R Walker,Berkeley,Completed,2012,2017.0,"This research examines the long-run effects of positive shocks to prenatal health on adult outcomes for cohorts born in the 1960s and 1970s in the United States. This research uses data from the Longitudinal Employer-Household Dynamics (LEHD) program linked to the Survey of Income and Program Participation (SIPP) and Current Population Survey (CPS) to analyze how the implementation of the Supplemental Nutritional Program for Women, Infants, and Children (WIC) in the 1970's, the reduction of maternal smoking during pregnancy following the publication of the 1964 Surgeon General Report, and the reduction of prenatal exposure to air pollution following the Clean Air Act of 1970 have impacted numerous later-life measures of adult well-being such as health, income, educational attainment, and characteristics of the neighborhood of residence. The findings will shed light on whether the ""fetal origins hypothesis"", which postulates an important link between prenatal health and adult well-being, holds for the current adult population in the United States."
"Mapping Informal Settlements in Developing Countries with
  Multi-resolution, Multi-spectral Data","Detecting and mapping informal settlements encompasses several of the United
Nations sustainable development goals. This is because informal settlements are
home to the most socially and economically vulnerable people on the planet.
Thus, understanding where these settlements are is of paramount importance to
both government and non-government organizations (NGOs), such as the United
Nations Children's Fund (UNICEF), who can use this information to deliver
effective social and economic aid. We propose two effective methods for
detecting and mapping the locations of informal settlements. One uses only
low-resolution (LR), freely available, Sentinel-2 multispectral satellite
imagery with noisy annotations, whilst the other is a deep learning approach
that uses only costly very-high-resolution (VHR) satellite imagery. To our
knowledge, we are the first to map informal settlements successfully with
low-resolution satellite imagery. We extensively evaluate and compare the
proposed methods. Please find additional material at
https://frontierdevelopmentlab.github.io/informal-settlements/.","['Patrick Helber', 'Bradley Gram-Hansen', 'Indhu Varatharajan', 'Faiza Azam', 'Alejandro Coca-Castro', 'Veronika Kopackova', 'Piotr Bilinski']",[],0,arXiv,http://arxiv.org/abs/1812.00812v1,False,True,False,False,False,937,Donovan A Anderson,Triangle,Completed,2012,2017.0,"This study will use confidential Decennial Census and American Community Survey (ACS) data to examine trends in residential settlement patterns in the United States since 1970.  Specifically, the research will compare the relative importance of socioeconomic status (SES), race, and ethnicity as predictors of observed settlement patterns for households that move into southern metropolitan areas during recent decades.  In the process of making these comparisons, the researchers will examine differential trends in locational attainment between races for inter-urban migrants, intra-urban migrants, and non-migrants in southern U.S. metropolitan areas.  Results will provide a test of how well certain models predict the observed settlement and distribution patterns of movers during the time period under study, and in the process will establish the basis for better understanding the predictive role of migration when studying the locational attainment of African Americans.
Locational attainment refers to neighborhood quality as measured by variables such as poverty, crime rates, housing values, pollution levels, unemployment, and school quality.  This study will determine if locational attainment differences exist between particular groups of inter-urban migrants, and if so, what factors underlie such differences.  Similar analyses will compare locational attainment for inter-urban migrants with that of  intra-urban and non-migrants, based on questions in the aforementioned Decennial Censuses and the ACS about having moved in the past year (ACS) or five years (Decennial).  The assimilation model predicts that SES is the only important predictor of the settlement and distribution patterns of movers.  Alternatively, other models posit that racial and ethnic distributions underlie these patterns.  This study will determine which model best describes the patterns observed in the microdata from 1970 to 2008 and beyond.  This time period is of particular importance to the researchers, whose interest lies in studying the trend in migration of African-Americans out of the South and into non-southern metropolitan areas in the decades preceding 1970, coupled with the more recent decadal trends reversing this process as Blacks return to the South and disproportionately settle in growing Southern metropolitan areas.  The researchers will combine data from questions on recent movement and location of birth to determine which individuals meet the specific migration criteria of interest."
"Code Red: The Business Impact of Code Quality -- A Quantitative Study of
  39 Proprietary Production Codebases","Code quality remains an abstract concept that fails to get traction at the
business level. Consequently, software companies keep trading code quality for
time-to-market and new features. The resulting technical debt is estimated to
waste up to 42% of developers' time. At the same time, there is a global
shortage of software developers, meaning that developer productivity is key to
software businesses. Our overall mission is to make code quality a business
concern, not just a technical aspect. Our first goal is to understand how code
quality impacts 1) the number of reported defects, 2) the time to resolve
issues, and 3) the predictability of resolving issues on time. We analyze 39
proprietary production codebases from a variety of domains using the CodeScene
tool based on a combination of source code analysis, version-control mining,
and issue information from Jira. By analyzing activity in 30,737 files, we find
that low quality code contains 15 times more defects than high quality code.
Furthermore, resolving issues in low quality code takes on average 124% more
time in development. Finally, we report that issue resolutions in low quality
code involve higher uncertainty manifested as 9 times longer maximum cycle
times. This study provides evidence that code quality cannot be dismissed as a
technical concern. With 15 times fewer defects, twice the development speed,
and substantially more predictable issue resolution times, the business
advantage of high quality code should be unmistakably clear.","['Adam Tornhill', 'Markus Borg']",[],0,arXiv,http://arxiv.org/abs/2203.04374v1,False,True,False,False,False,987,Kyle L Handley,Michigan,Completed,2012,2018.0,"This project will match trade policy data from internal and external data to firm-level measures of trade participation and trade intensity.  We will investigate the impact of policy uncertainty on firm-level decisions and evaluate how government policy affects uncertainty, investment and trade."
"The Quality of the 2020 Census: An Independent Assessment of Census
  Bureau Activities Critical to Data Quality","This report summarizes major findings from an independent evaluation of 2020
census operations. The American Statistical Association 2020 Census Quality
Indicators Task Force selected the authors to conduct the evaluation using
nonpublic operations data provided by the Census Bureau.
  The evaluation focused on the quality of state-level population counts
released by the Census Bureau for congressional apportionment. The authors
first partitioned the census enumeration process into five operation phases.
Within each phase, one or more activities considered to be critical to census
data quality were identified. Operational data from each activity were then
analyzed to assess the risk of error, particularly as they related to similar
activities in the 2010 census.
  Overall, the evaluation found that census operations relied on higher risk
activities at a higher rate in 2020 than in 2010, suggesting that the risk of
error may be higher in 2020 than in 2010. However, the available data were
insufficient to determine whether the apportionment counts are of lower quality
in 2020 than in 2010.","['Paul Biemer', 'Joseph Salvo', 'Jonathan Auerbach']",[],0,arXiv,http://arxiv.org/abs/2110.02135v1,True,True,False,False,False,997,Merle Ederhof,Michigan,Completed,2014,2019.0,"This project investigates the degree to which production schedules and capacity utilization decisions are driven by companies' financial accounting goals, such as meeting the consensus analyst earnings forecast. It also examines the role that capacity utilization plays in the time series of financial accounting information and how analysts and investors react to them. Of particular interest in addressing this question is whether the recent change in the financial accounting treatment of capacity costs has improved the quality of the data, as perceived by analysts and investors. Thirdly, the project analyzes how product costs vary with the level of capacity utilization."
"Zero-Shot ECG Classification with Multimodal Learning and Test-time
  Clinical Knowledge Enhancement","Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for
detecting cardiac arrhythmic diseases in clinical practice. While ECG
Self-supervised Learning (eSSL) methods show promise in representation learning
from unannotated ECG data, they often overlook the clinical knowledge that can
be found in reports. This oversight and the requirement for annotated samples
for downstream tasks limit eSSL's versatility. In this work, we address these
issues with the Multimodal ECG Representation Learning (MERL}) framework.
Through multimodal learning on ECG records and associated reports, MERL is
capable of performing zero-shot ECG classification with text prompts,
eliminating the need for training data in downstream tasks. At test time, we
propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach,
which uses Large Language Models (LLMs) to exploit external expert-verified
clinical knowledge databases, generating more descriptive prompts and reducing
hallucinations in LLM-generated content to boost zero-shot classification.
Based on MERL, we perform the first benchmark across six public ECG datasets,
showing the superior performance of MERL compared against eSSL methods.
Notably, MERL achieves an average AUC score of 75.2% in zero-shot
classification (without training data), 3.2% higher than linear probed eSSL
methods with 10\% annotated training data, averaged across all six datasets.
Code and models are available at https://github.com/cheliu-computation/MERL","['Che Liu', 'Zhongwei Wan', 'Cheng Ouyang', 'Anand Shah', 'Wenjia Bai', 'Rossella Arcucci']",[],0,arXiv,http://arxiv.org/abs/2403.06659v3,False,True,False,False,False,997,Merle Ederhof,Michigan,Completed,2014,2019.0,"This project investigates the degree to which production schedules and capacity utilization decisions are driven by companies' financial accounting goals, such as meeting the consensus analyst earnings forecast. It also examines the role that capacity utilization plays in the time series of financial accounting information and how analysts and investors react to them. Of particular interest in addressing this question is whether the recent change in the financial accounting treatment of capacity costs has improved the quality of the data, as perceived by analysts and investors. Thirdly, the project analyzes how product costs vary with the level of capacity utilization."
An efficient counting method for the colored triad census,"The triad census is an important approach to understand local structure in
network science, providing comprehensive assessments of the observed relational
configurations between triples of actors in a network. However, researchers are
often interested in combinations of relational and categorical nodal
attributes. In this case, it is desirable to account for the label, or color,
of the nodes in the triad census. In this paper, we describe an efficient
algorithm for constructing the colored triad census, based, in part, on
existing methods for the classic triad census. We evaluate the performance of
the algorithm using empirical and simulated data for both undirected and
directed graphs. The results of the simulation demonstrate that the proposed
algorithm reduces computational time many-fold over the naive approach. We also
apply the colored triad census to the Zachary karate club network dataset. We
simultaneously show the efficiency of the algorithm, and a way to conduct a
statistical test on the census by forming a null distribution from 1,000
realizations of a mixing-matrix conditioned graph and comparing the observed
colored triad counts to the expected. From this, we demonstrate the method's
utility in our discussion of results about homophily, heterophily, and
bridging, simultaneously gained via the colored triad census. In sum, the
proposed algorithm for the colored triad census brings novel utility to social
network analysis in an efficient package.","['Jeffrey Lienert', 'Laura Koehly', 'Felix Reed-Tsochas', 'Christopher Steven Marcum']",[],0,arXiv,http://arxiv.org/abs/1802.01481v2,False,True,False,False,False,997,Merle Ederhof,Michigan,Completed,2014,2019.0,"This project investigates the degree to which production schedules and capacity utilization decisions are driven by companies' financial accounting goals, such as meeting the consensus analyst earnings forecast. It also examines the role that capacity utilization plays in the time series of financial accounting information and how analysts and investors react to them. Of particular interest in addressing this question is whether the recent change in the financial accounting treatment of capacity costs has improved the quality of the data, as perceived by analysts and investors. Thirdly, the project analyzes how product costs vary with the level of capacity utilization."
House Price Modeling with Digital Census,"Urban house prices are strongly associated with local socioeconomic factors.
In literature, house price modeling is based on socioeconomic variables from
traditional census, which is not real-time, dynamic and comprehensive. Inspired
by the emerging concept of ""digital census"" - using large-scale digital records
of human activities to measure urban population dynamics and socioeconomic
conditions, we introduce three typical datasets, namely 311 complaints, crime
complaints and taxi trips, into house price modeling. Based on the individual
housing sales data in New York City, we provide comprehensive evidence that
these digital census datasets can substantially improve the modeling
performances on both house price levels and changes, regardless whether
traditional census is included or not. Hence, digital census can serve as both
effective alternatives and complements to traditional census for house price
modeling.","['Enwei Zhu', 'Stanislav Sobolevsky']",[],0,arXiv,http://arxiv.org/abs/1809.03834v1,False,True,False,False,False,997,Merle Ederhof,Michigan,Completed,2014,2019.0,"This project investigates the degree to which production schedules and capacity utilization decisions are driven by companies' financial accounting goals, such as meeting the consensus analyst earnings forecast. It also examines the role that capacity utilization plays in the time series of financial accounting information and how analysts and investors react to them. Of particular interest in addressing this question is whether the recent change in the financial accounting treatment of capacity costs has improved the quality of the data, as perceived by analysts and investors. Thirdly, the project analyzes how product costs vary with the level of capacity utilization."
"Intervention scenarios to enhance knowledge transfer in a network of
  firm","We investigate a multi-agent model of firms in an R\&D network. Each firm is
characterized by its knowledge stock $x_{i}(t)$, which follows a non-linear
dynamics. It can grow with the input from other firms, i.e., by knowledge
transfer, and decays otherwise. Maintaining interactions is costly. Firms can
leave the network if their expected knowledge growth is not realized, which may
cause other firms to also leave the network. The paper discusses two bottom-up
intervention scenarios to prevent, reduce, or delay cascades of firms leaving.
The first one is based on the formalism of network controllability, in which
driver nodes are identified and subsequently incentivized, by reducing their
costs. The second one combines node interventions and network interventions. It
proposes the controlled removal of a single firm and the random replacement of
firms leaving. This allows to generate small cascades, which prevents the
occurrence of large cascades. We find that both approaches successfully
mitigate cascades and thus improve the resilience of the R\&D network.","['Frank Schweitzer', 'Yan Zhang', 'Giona Casiraghi']",[],0,arXiv,http://arxiv.org/abs/2006.14249v1,False,True,False,False,False,1000,Douglas V Almond,Baruch,Completed,2017,2023.0,"This project primarily uses Longitudinal Employer-Household Dynamics (LEHD) data to analyze the transitional dynamics of workers and adjustment of firms to the Family and Medical Leave Act (FMLA) of 1993. FMLA mandated large employers to provide job-protected unpaid leave for specified family and medical reasons. Little is known about how firms and workers have responded to this mandate. This project investigates whether the firms that qualify for FMLA have changed the employment composition of their workforce, if earnings and promotions of workers in those firms have adjusted to reflect the cost of the FMLA mandate, the impacts of the law on leave taking, hours of work, fertility, and employer based health insurance of the workforce, and if firms themselves have changed their size in response to FMLA, since only firms with 50 or more employees are subject to the law. This project also employs data from the Longitudinal Business Database, American Community Survey (for information on fertility and health insurance), and Current Population Surveys (for information on leave taking and hours of work)."
"Relatedness, Knowledge Diffusion, and the Evolution of Bilateral Trade","During the last decades two important contributions have reshaped our
understanding of international trade. First, countries trade more with those
with whom they share history, language, and culture, suggesting that trade is
limited by information frictions. Second, countries are more likely to start
exporting products that are similar to their current exports, suggesting that
knowledge diffusion among related industries is a key constrain shaping the
diversification of exports. But does knowledge about how to export to a
destination also diffuses among related products and geographic neighbors? Do
countries need to learn how to trade each product to each destination? Here, we
use bilateral trade data from 2000 to 2015 to show that countries are more
likely to increase their exports of a product to a destination when: (i) they
export related products to it, (ii) they export the same product to the
neighbor of a destination, (iii) they have neighbors who export the same
product to that destination. Then, we explore the magnitude of these effects
for new, nascent, and experienced exporters, (exporters with and without
comparative advantage in a product) and also for groups of products with
different level of technological sophistication. We find that the effects of
product and geographic relatedness are stronger for new exporters, and also,
that the effect of product relatedness is stronger for more technologically
sophisticated products. These findings support the idea that international
trade is shaped by information frictions that are reduced in the presence of
related products and experienced geographic neighbors.","['Bogang Jun', 'Aamena Alshamsi', 'Jian Gao', 'Cesar A Hidalgo']",[],0,arXiv,http://arxiv.org/abs/1709.05392v1,False,True,False,False,False,1027,Andrew J Cassey,Seattle,Completed,2014,2017.0,"The purpose of this research is to increase the knowledge base of the Census Bureau with respect to the impact of informational barriers on firms involved in international trade.  The objectives are to compile a measure of origin of movement state export scope, create a measure of state export scope based on the location of the exporting agent relative to the state production scope, and conduct regression analysis relating export promotion, the information required to make an informed purchase, and both state export scopes relative to production scope.  This will improve the understanding of the export patterns of the United States."
Predicting Exporters with Machine Learning,"In this contribution, we exploit machine learning techniques to evaluate
whether and how close firms are to becoming successful exporters. First, we
train and test various algorithms using financial information on both exporters
and non-exporters in France in 2010-2018. Thus, we show that we are able to
predict the distance of non-exporters from export status. In particular, we
find that a Bayesian Additive Regression Tree with Missingness In Attributes
(BART-MIA) performs better than other techniques with an accuracy of up to
0.90. Predictions are robust to changes in definitions of exporters and in the
presence of discontinuous exporting activity. Eventually, we discuss how our
exporting scores can be helpful for trade promotion, trade credit, and
assessing aggregate trade potential. For example, back-of-the-envelope
estimates show that a representative firm with just below-average exporting
scores needs up to 44% more cash resources and up to 2.5 times more capital to
get to foreign markets.","['Francesca Micocci', 'Armando Rungi']",[],0,arXiv,http://arxiv.org/abs/2107.02512v2,False,True,False,False,False,1027,Andrew J Cassey,Seattle,Completed,2014,2017.0,"The purpose of this research is to increase the knowledge base of the Census Bureau with respect to the impact of informational barriers on firms involved in international trade.  The objectives are to compile a measure of origin of movement state export scope, create a measure of state export scope based on the location of the exporting agent relative to the state production scope, and conduct regression analysis relating export promotion, the information required to make an informed purchase, and both state export scopes relative to production scope.  This will improve the understanding of the export patterns of the United States."
"The role of FDI along transitional dynamics of the host country in an
  endogenous growth model","We investigate the role of foreign direct investment (FDI) in the
transitional dynamics of host countries by using an optimal growth model. FDI
may be beneficial for the host country because local people can work for
multinational firms to get a favorable salary. However, if the host country
only focuses on FDI, it may face a middle-income trap. We show that if the host
country invests in research and development, its economy may have sustained
growth. Moreover, in this case, FDI helps the host country only at the first
stages of its development process.","['Ngoc-Sang Pham', 'Thanh Tam Nguyen-Huu']",[],0,arXiv,http://arxiv.org/abs/2501.12010v1,False,True,False,False,False,1057,Kyle L Handley,Michigan,Completed,2014,2019.0,"This research focuses on the measurement of multinational activity, comparing it to domestic activity, and using measures to benchmark Census Bureau data. Using measures of domestic and multinational activity, two broad questions are addressed. First, what are the causes and consequences of multinational growth within the U.S. and abroad? The research design assesses the impact of multinationals across a range of economic variables including productivity, employment, and trade patterns. Second, what are the main drivers for technological change and reorganization at the firm level? How are these different for multinationals in terms of aggregation and behavior over the business cycle?"
Bayesian Semiparametric Hierarchical Empirical Likelihood Spatial Models,"We introduce a general hierarchical Bayesian framework that incorporates a
flexible nonparametric data model specification through the use of empirical
likelihood methodology, which we term semiparametric hierarchical empirical
likelihood (SHEL) models. Although general dependence structures can be readily
accommodated, we focus on spatial modeling, a relatively underdeveloped area in
the empirical likelihood literature. Importantly, the models we develop
naturally accommodate spatial association on irregular lattices and irregularly
spaced point-referenced data. We illustrate our proposed framework by means of
a simulation study and through three real data examples. First, we develop a
spatial Fay-Herriot model in the SHEL framework and apply it to the problem of
small area estimation in the American Community Survey. Next, we illustrate the
SHEL model in the context of areal data (on an irregular lattice) through the
North Carolina sudden infant death syndrome (SIDS) dataset. Finally, we analyze
a point-referenced dataset from the North American Breeding Bird survey that
considers dove counts for the state of Missouri. In all cases, we demonstrate
superior performance of our model, in terms of mean squared prediction error,
over standard parametric analyses.","['Aaron T. Porter', 'Scott H. Holan', 'Christopher K. Wikle']",[],0,arXiv,http://arxiv.org/abs/1405.3880v1,False,True,False,False,True,1071,Scott H Holan,Missouri,Active,2015,,"This project will use confidential microdata from the American Community Survey (ACS) to provide an efficient framework for carrying out small area estimation (SAE), while preserving geographical and temporal constraints that arise from the survey’s aggregate structure. Further, by borrowing strength across multiple scales in space and time, and multiple outcomes, the proposed approach will reduce the variance in the ACS small area estimates and its derivatives. Additionally, from a data users perspective, methodology will be developed that will simultaneously provide coherent estimates on several temporal scales, rather than being hampered by the published multiyear estimates. Importantly, this will allow researchers to compare trends across different geographies. 
The proposed methods are of independent interest and can be used in many of the other surveys administered by US Census Bureau (and other Federal Statistics Agencies). For example, straightforward modification of the proposed methods can be used in conjunction with the Current Population Survey, Quarterly Census of Employment and Wages, Agricultural Resource Management Survey, among others. Furthermore, several of the proposed methods (SAE) will directly carry over to the area of disease mapping and thus, provide important tools for public health. In short, the proposed methods provide novel solutions across a wide-range of applied problems, contributing to the statistics literature, Federal, State and Local governments and many subject matter disciplines. "
"Bayesian binomial mixture models for estimating abundance in ecological
  monitoring studies","Investigation of species abundance has become a vital component of many
ecological monitoring studies. The primary objective of these studies is to
understand how specific species are distributed across the study domain, as
well as quantification of the sampling efficiency for detecting these species.
To achieve these goals, preselected locations are sampled during scheduled
visits, in which the number of species observed at each location is recorded.
This results in spatially referenced replicated count data that are often
unbalanced in structure and exhibit overdispersion. Motivated by the Baltimore
Ecosystem Study, we propose Bayesian hierarchical binomial mixture models,
including Binomial Conway-Maxwell Poisson (Bin-CMP) mixture models, that
formally account for varying levels of spatial dispersion. Our proposed models
also allow for variable selection of model covariates and grouping of
dispersion parameters through the implementation of reversible jump Markov
chain Monte Carlo methodology. Finally, using demographic covariates from the
American Community Survey, we demonstrate the effectiveness of our approach
through estimation of abundance for the American Robin (Turdus migratorius) in
the Baltimore Ecosystem Study.","['Guohui Wu', 'Scott H. Holan', 'Charles H. Nilon', 'Christopher K. Wikle']",[],0,arXiv,http://arxiv.org/abs/1505.02590v1,False,False,False,False,True,1071,Scott H Holan,Missouri,Active,2015,,"This project will use confidential microdata from the American Community Survey (ACS) to provide an efficient framework for carrying out small area estimation (SAE), while preserving geographical and temporal constraints that arise from the survey’s aggregate structure. Further, by borrowing strength across multiple scales in space and time, and multiple outcomes, the proposed approach will reduce the variance in the ACS small area estimates and its derivatives. Additionally, from a data users perspective, methodology will be developed that will simultaneously provide coherent estimates on several temporal scales, rather than being hampered by the published multiyear estimates. Importantly, this will allow researchers to compare trends across different geographies. 
The proposed methods are of independent interest and can be used in many of the other surveys administered by US Census Bureau (and other Federal Statistics Agencies). For example, straightforward modification of the proposed methods can be used in conjunction with the Current Population Survey, Quarterly Census of Employment and Wages, Agricultural Resource Management Survey, among others. Furthermore, several of the proposed methods (SAE) will directly carry over to the area of disease mapping and thus, provide important tools for public health. In short, the proposed methods provide novel solutions across a wide-range of applied problems, contributing to the statistics literature, Federal, State and Local governments and many subject matter disciplines. "
"Small Area Estimation via Multivariate Fay-Herriot Models with Latent
  Spatial Dependence","The Fay-Herriot model is a standard model for direct survey estimators in
which the true quantity of interest, the superpopulation mean, is latent and
its estimation is improved through the use of auxiliary covariates. In the
context of small area estimation, these estimates can be further improved by
borrowing strength across spatial region or by considering multiple outcomes
simultaneously. We provide here two formulations to perform small area
estimation with Fay-Herriot models that include both multivariate outcomes and
latent spatial dependence. We consider two model formulations, one in which the
outcome-by-space dependence structure is separable and one that accounts for
the cross dependence through the use of a generalized multivariate conditional
autoregressive (GMCAR) structure. The GMCAR model is shown in a state-level
example to produce smaller mean square prediction errors, relative to
equivalent census variables, than the separable model and the state-of-the-art
multivariate model with unstructured dependence between outcomes and no spatial
dependence. In addition, both the GMCAR and the separable models give smaller
mean squared prediction error than the state-of-the-art model when conducting
small area estimation on county level data from the American Community Survey.","['Aaron T. Porter', 'Christopher K. Wikle', 'Scott H. Holan']",[],0,arXiv,http://arxiv.org/abs/1310.7211v1,False,True,False,False,True,1071,Scott H Holan,Missouri,Active,2015,,"This project will use confidential microdata from the American Community Survey (ACS) to provide an efficient framework for carrying out small area estimation (SAE), while preserving geographical and temporal constraints that arise from the survey’s aggregate structure. Further, by borrowing strength across multiple scales in space and time, and multiple outcomes, the proposed approach will reduce the variance in the ACS small area estimates and its derivatives. Additionally, from a data users perspective, methodology will be developed that will simultaneously provide coherent estimates on several temporal scales, rather than being hampered by the published multiyear estimates. Importantly, this will allow researchers to compare trends across different geographies. 
The proposed methods are of independent interest and can be used in many of the other surveys administered by US Census Bureau (and other Federal Statistics Agencies). For example, straightforward modification of the proposed methods can be used in conjunction with the Current Population Survey, Quarterly Census of Employment and Wages, Agricultural Resource Management Survey, among others. Furthermore, several of the proposed methods (SAE) will directly carry over to the area of disease mapping and thus, provide important tools for public health. In short, the proposed methods provide novel solutions across a wide-range of applied problems, contributing to the statistics literature, Federal, State and Local governments and many subject matter disciplines. "
"Conjugate Modeling Approaches for Small Area Estimation with
  Heteroscedastic Structure","Small area estimation has become an important tool in official statistics,
used to construct estimates of population quantities for domains with small
sample sizes. Typical area-level models function as a type of heteroscedastic
regression, where the variance for each domain is assumed to be known and
plugged in following a design-based estimate. Recent work has considered
hierarchical models for the variance, where the design-based estimates are used
as an additional data point to model the latent true variance in each domain.
These hierarchical models may incorporate covariate information, but can be
difficult to sample from in high-dimensional settings. Utilizing recent
distribution theory, we explore a class of Bayesian hierarchical models for
small area estimation that smooth both the design-based estimate of the mean
and the variance. In addition, we develop a class of unit-level models for
heteroscedastic Gaussian response data. Importantly, we incorporate both
covariate information as well as spatial dependence, while retaining a
conjugate model structure that allows for efficient sampling. We illustrate our
methodology through an empirical simulation study as well as an application
using data from the American Community Survey.","['Paul A. Parker', 'Scott H. Holan', 'Ryan Janicki']",[],0,arXiv,http://arxiv.org/abs/2209.01670v1,False,False,False,False,True,1071,Scott H Holan,Missouri,Active,2015,,"This project will use confidential microdata from the American Community Survey (ACS) to provide an efficient framework for carrying out small area estimation (SAE), while preserving geographical and temporal constraints that arise from the survey’s aggregate structure. Further, by borrowing strength across multiple scales in space and time, and multiple outcomes, the proposed approach will reduce the variance in the ACS small area estimates and its derivatives. Additionally, from a data users perspective, methodology will be developed that will simultaneously provide coherent estimates on several temporal scales, rather than being hampered by the published multiyear estimates. Importantly, this will allow researchers to compare trends across different geographies. 
The proposed methods are of independent interest and can be used in many of the other surveys administered by US Census Bureau (and other Federal Statistics Agencies). For example, straightforward modification of the proposed methods can be used in conjunction with the Current Population Survey, Quarterly Census of Employment and Wages, Agricultural Resource Management Survey, among others. Furthermore, several of the proposed methods (SAE) will directly carry over to the area of disease mapping and thus, provide important tools for public health. In short, the proposed methods provide novel solutions across a wide-range of applied problems, contributing to the statistics literature, Federal, State and Local governments and many subject matter disciplines. "
Economic decision making: application of the theory of complex systems,"In this chapter the complex systems are discussed in the context of economic
and business policy and decision making. It will be showed and motivated that
social systems are typically chaotic, non-linear and/or non-equilibrium and
therefore complex systems. It is discussed that the rapid change in global
consumer behaviour is underway, that further increases the complexity in
business and management. For policy making under complexity, following
principles are offered: openness and international competition, tolerance and
variety of ideas, self-reliability and low dependence on external help. The
chapter contains four applications that build on the theoretical motivation of
complexity in social systems. The first application demonstrates that small
economies have good prospects to gain from the global processes underway, if
they can demonstrate production flexibility, reliable business ethics and good
risk management. The second application elaborates on and discusses the
opportunities and challenges in decision making under complexity from macro and
micro economic perspective. In this environment, the challenges for corporate
management are being also permanently changed: the balance between short term
noise and long term chaos whose attractor includes customers, shareholders and
employees must be found. The emergence of chaos in economic relationships is
demonstrated by a simple system of differential equations that relate the
stakeholders described above. The chapter concludes with two financial
applications: about debt and risk management. The non-equilibrium economic
establishment leads to additional problems by using excessive borrowing;
unexpected downturns in economy can more easily kill companies. Finally, the
demand for quantitative improvements in risk management is postulated.",['Robert Kitt'],[],0,arXiv,http://arxiv.org/abs/1208.1277v2,False,True,False,False,False,1091,Jie He,Atlanta,Active,2014,,"This research examines how employees’ tolerance for risk affects corporate decisions and firm performance, including firm debt, capital expenditures, patents, acquisitions, returns on assets and equity, firm age, and public/private status. Proxies for employee risk-tolerance include firm-level measures of employee age and gender, percentage of employees with earnings sources from other companies, percentage of employees with dual wage earners in their household, and county-level measures of religiosity. This project also examines a firm's ownership status, i.e., public or private, and the demographic characteristics of the firm's employees. "
"The interplay between migrants and natives as a determinant of migrants'
  assimilation: A coevolutionary approach","We study the migrants' assimilation, which we conceptualize as forming human
capital productive on the labor market of a developed host country, and we link
the observed frequent lack of assimilation with the relative deprivation that
the migrants start to feel when they move in social space towards the natives.
In turn, we presume that the native population is heterogenous and consists of
high-skill and low-skill workers. The presence of assimilated migrants might
shape the comparison group of the natives, influencing the relative deprivation
of the low-skill workers and, in consequence, the choice to form human capital
and become highly skilled. To analyse this interrelation between assimilation
choices of migrants and skill formation of natives, we construct a
coevolutionary model of the open-to-migration economy. Showing that the economy
might end up in a non-assimilation equilibrium, we discuss welfare consequences
of an assimilation policy funded from tax levied on the native population. We
identify conditions under which such costly policy can bring the migrants to
assimilation and at the same time increase the welfare of the natives, even
though the incomes of the former take a beating.","['Jakub Bielawski', 'Marcin Jakubek']",[],0,arXiv,http://arxiv.org/abs/1906.02657v1,False,True,False,False,False,1092,Christian Dippel,UCLA,Completed,2014,2017.0,"This project studies changes in Native American economic development as indicated by average incomes and measures of income inequality over the past several decades. The broad aim is to understand today's large differences in economic development between different tribes and between different reservations, rather than between different Native American individuals. First, this project estimates the effect of local governance on differences in average incomes among reservations. Second, it estimates the dynamics of income inequality and income growth across reservations and tribes. This requires building aggregate tribal and reservation characteristics from individual records in the Decennial Census and American Community Survey data."
"Sparse Models and Methods for Optimal Instruments with an Application to
  Eminent Domain","We develop results for the use of Lasso and Post-Lasso methods to form
first-stage predictions and estimate optimal instruments in linear instrumental
variables (IV) models with many instruments, $p$. Our results apply even when
$p$ is much larger than the sample size, $n$. We show that the IV estimator
based on using Lasso or Post-Lasso in the first stage is root-n consistent and
asymptotically normal when the first-stage is approximately sparse; i.e. when
the conditional expectation of the endogenous variables given the instruments
can be well-approximated by a relatively small set of variables whose
identities may be unknown. We also show the estimator is semi-parametrically
efficient when the structural error is homoscedastic. Notably our results allow
for imperfect model selection, and do not rely upon the unrealistic ""beta-min""
conditions that are widely used to establish validity of inference following
model selection. In simulation experiments, the Lasso-based IV estimator with a
data-driven penalty performs well compared to recently advocated
many-instrument-robust procedures. In an empirical example dealing with the
effect of judicial eminent domain decisions on economic outcomes, the
Lasso-based IV estimator outperforms an intuitive benchmark.
  In developing the IV results, we establish a series of new results for Lasso
and Post-Lasso estimators of nonparametric conditional expectation functions
which are of independent theoretical and practical interest. We construct a
modification of Lasso designed to deal with non-Gaussian, heteroscedastic
disturbances which uses a data-weighted $\ell_1$-penalty function. Using
moderate deviation theory for self-normalized sums, we provide convergence
rates for the resulting Lasso and Post-Lasso estimators that are as sharp as
the corresponding rates in the homoscedastic Gaussian case under the condition
that $\log p = o(n^{1/3})$.","['Alexandre Belloni', 'Daniel Chen', 'Victor Chernozhukov', 'Christian Hansen']",[],0,arXiv,http://arxiv.org/abs/1010.4345v5,False,True,False,False,False,1099,Daniel L Chen,Triangle,Completed,2013,2016.0,"This study will use data from the American Community Survey (ACS), American Housing Survey (AHS), and Decennial Censuses to examine the impact of eminent domain takings decisions on housing values in geographic areas affected by those decisions. Most of the empirical work in this field focuses on the relationship between property rights and investment in developing countries. Theoretical arguments exist as to how takings decisions might, on net, either increase or decrease housing values, yet relevant empirical work using U.S. data focuses on producing area-level estimates of housing value changes. In contrast, this research will take advantage of restricted-use household-level microdata to examine changes in individual housing values, producing more accurate estimates of the effects of takings decisions on housing values. This research will also assess alternative methods of imputing missing data in the AHS and assess the impact eminent domain law has on ACS and AHS data collection.  "
"Artificial Intelligence in the Service of Entrepreneurial Finance:
  Knowledge Structure and the Foundational Algorithmic Paradigm","While the application of Artificial Intelligence in Finance has a long
tradition, its potential in Entrepreneurship has been intensively explored only
recently. In this context, Entrepreneurial Finance is a particularly fertile
ground for future Artificial Intelligence proliferation. To support the latter,
the study provides a bibliometric review of Artificial Intelligence
applications in (1) entrepreneurial finance literature, and (2) corporate
finance literature with implications for Entrepreneurship. Rigorous search and
screening procedures of the scientific database Web of Science Core Collection
resulted in the identification of 1890 relevant journal articles subjected to
analysis. The bibliometric analysis gives a rich insight into the knowledge
field's conceptual, intellectual, and social structure, indicating nascent and
underdeveloped research directions. As far as we were able to identify, this is
the first study to map and bibliometrically analyze the academic field
concerning the relationship between Artificial Intelligence, Entrepreneurship,
and Finance, and the first review that deals with Artificial Intelligence
methods in Entrepreneurship. According to the results, Artificial Neural
Network, Deep Neural Network and Support Vector Machine are highly represented
in almost all identified topic niches. At the same time, applying Topic
Modeling, Fuzzy Neural Network and Growing Hierarchical Self-organizing Map is
quite rare. As an element of the research, and before final remarks, the
article deals as well with a discussion of certain gaps in the relationship
between Computer Science and Economics. These gaps do represent problems in the
application of Artificial Intelligence in Economic Science. As a way to at
least in part remedy this situation, the foundational paradigm and the bespoke
demonstration of the Monte Carlo randomized algorithm are presented.","['Robert Kudelić', 'Tamara Šmaguc', 'Sherry Robinson']",[],0,arXiv,http://arxiv.org/abs/2311.13213v1,False,True,False,False,False,1101,Robert C Seamans,Baruch,Completed,2014,2017.0,"This research investigates how statewide changes in debtor protection provided by U.S. personal bankruptcy law affect firm entry and exit dynamics. The project assesses the effects of personal bankruptcy law on entrepreneurship rates, the size and industry distribution of incumbent firms, and on business closures, as well as the extent to which firm entry and exit varies with bankruptcy exemption laws and local demographic and economic conditions. "
"Bankruptcy Shocks and Legal Labor Markets: Evidence from the Court
  Competition Era","We study how Chapter 11 bankruptcies affect local legal labor markets. We
document that bankruptcy shocks increase county legal employment and
corroborate this finding by exploiting a stipulation of the law known as Forum
Shopping during the Court Competition Era (1991-1996). We quantify losses to
local communities from firms forum shopping away from their local area as
follows. First, we calculate the unrealized potential employment gains implied
by our reduced-form results. Second, we structurally estimate a model of legal
labor markets and quantify welfare losses. We uncover meaningful costs to local
communities from lax bankruptcy venue laws.","['Chad Brown', 'Jeronimo Carballo', 'Alessandro Peri']",[],0,arXiv,http://arxiv.org/abs/2202.00044v1,False,True,False,False,False,1105,William R Walker,Berkeley,Completed,2013,2017.0,"The ways in which external economic forces influence firm-level input and output decisions have important implications for economic welfare and social well-being. Firms and plant-owners often make decisions to maximize profits or shareholder value, and they may not fully internalize the effects of their production decisions on the local economy in which they are located. Similarly, firm owners or managers do not necessarily internalize the potential long run costs to workers when making plant layoff decisions. In some cases, these externalities create market failures. Addressing these externalities requires a proper understanding both of how external economic forces influence firm and worker decisions as well as how these decisions influence long run costs and economic incidence. This project will characterize how external market and non-market forces influence firm and worker behavior, while also evaluating the costs and incidence of sectoral reallocation of the labor market more generally. The researchers will evaluate the costs and incidence of sectoral reallocation in the labor market using the longitudinal earnings and employment records from the LEHD infrastructure file. This research will produce estimates quantifying how changes in demand drive employment responses and worker outcomes in the automobile industry and how demand shocks in a particular industry may propagate to nearby industries. The researchers will also evaluate how environmental regulation influences the type of worker hired at newly regulated firms. In addition, this research will evaluate how environmental regulation influences technology choice within a firm, and how these technology choices influence the skill mix and wage structure within that particular firm."
Bounds for Approximate Regret-Matching Algorithms,"A dominant approach to solving large imperfect-information games is
Counterfactural Regret Minimization (CFR). In CFR, many regret minimization
problems are combined to solve the game. For very large games, abstraction is
typically needed to render CFR tractable. Abstractions are often manually
tuned, possibly removing important strategic differences in the full game and
harming performance. Function approximation provides a natural solution to
finding good abstractions to approximate the full game. A common approach to
incorporating function approximation is to learn the inputs needed for a regret
minimizing algorithm, allowing for generalization across many regret
minimization problems. This paper gives regret bounds when a regret minimizing
algorithm uses estimates instead of true values. This form of analysis is the
first to generalize to a larger class of $(\Phi, f)$-regret matching
algorithms, and includes different forms of regret such as swap, internal, and
external regret. We demonstrate how these results give a slightly tighter bound
for Regression Regret-Matching (RRM), and present a novel bound for combining
regression with Hedge.","[""Ryan D'Orazio"", 'Dustin Morrill', 'James R. Wright']",[],0,arXiv,http://arxiv.org/abs/1910.01706v2,False,True,False,False,False,1109,James R Tybout,Penn State,Active,2014,,"This project attempts to increase the usefulness of the Census Bureau's international trade statistics by assessing the quality and possible biases of the shipment-level data that lie behind them. A second goal is to develop descriptive statistics and structural models that characterize the formation and maturation of cross-border business relationships, again using shipment-level data. Both dimensions of the analysis will improve an understanding of trade flow dynamics between the United States and its trading partners. The first part of this project will document international discrepancies in bilateral trade statistics at the level of individual shipments, looking in particular for evidence that might indicate a reporting/collection problem on the U.S. side. The second part of the project will augment the trade shipments records with information on the characteristics of the exporting firms and importing firms, which will allow study of the characteristics of buyer-seller matches. One exercise will involve the estimation of a dynamic model of international trade in which firms' exporting (importing) behavior reflects a search and learning process in their foreign markets. A second exercise will develop descriptive statistics that allow characterization of the evolution of international buyer-seller networks, and will contrast the characteristics of rapidly expanding networks (China-U.S.) with slower-growing networks (Colombia-U.S.). A third type of exercise will involve the development of structural models of international buyer-seller networks."
A Novel Microdata Privacy Disclosure Risk Measure,"A tremendous amount of individual-level data is generated each day, of use to
marketing, decision makers, and machine learning applications. This data often
contain private and sensitive information about individuals, which can be
disclosed by adversaries. An adversary can recognize the underlying
individual's identity for a data record by looking at the values of
quasi-identifier attributes, known as identity disclosure, or can uncover
sensitive information about an individual through attribute disclosure. In
Statistical Disclosure Control, multiple disclosure risk measures have been
proposed. These share two drawbacks: they do not consider identity and
attribute disclosure concurrently in the risk measure, and they make
restrictive assumptions on an adversary's knowledge by assuming certain
attributes are quasi-identifiers and there is a clear boundary between
quasi-identifiers and sensitive information. In this paper, we present a novel
disclosure risk measure that addresses these limitations, by presenting a
single combined metric of identity and attribute disclosure risk, and providing
flexibility in modeling adversary's knowledge. We have developed an efficient
algorithm for computing the proposed risk measure and evaluated the feasibility
and performance of our approach on a real-world data set from the domain of
social work.","['Marmar Orooji', 'Gerald M. Knapp']",[],0,arXiv,http://arxiv.org/abs/1901.07311v1,False,True,False,False,False,1157,Kristine M Witkowski,Michigan,Completed,2013,2018.0,"Demand for contextualized microdata has increased dramatically over the last decade and is expected to increase even more in years to come. The Census Bureau collects data to produce timely population estimates on demographic, social, housing, and economic characteristics for a broad spectrum of geographic areas in the United States and is therefore in a unique position to meet some of this demand. However, releasing geographic details in public-use microdata may increase the risk of disclosure. Current disclosure limitation practices involve suppressing geographic details for spatial units that fail to meet a predefined population threshold (e.g., 100,000). Users interested in studying persons nested within less-populated geographies (e.g., rural counties, tracts, block groups) must access restricted-use data through data enclaves. Neither of these approaches fully satisfies the growing demand for geographically-rich microdata that is being fueled by a variety of researchers, analysts, decision-makers, and administrators. To help alleviate barriers to use, this project will develop methodology that supports the efficient identification of sampling and database designs and associated masking techniques that allow more geographic detail to be released on microdata products, without increased disclosure risk or unnecessary survey costs. This research will use the 2005-2009 American Community Survey (ACS) data as the empirical foundation, with the aim of producing statistics that summarize findings from the evaluation of the methodology and broad guidelines derived from disclosure simulations, all of which are to be carefully constructed so as to ensure the confidentiality of the underlying data and disclosure practices."
"Comparing the Utility and Disclosure Risk of Synthetic Data with Samples
  of Microdata","Most statistical agencies release randomly selected samples of Census
microdata, usually with sample fractions under 10% and with other forms of
statistical disclosure control (SDC) applied. An alternative to SDC is data
synthesis, which has been attracting growing interest, yet there is no clear
consensus on how to measure the associated utility and disclosure risk of the
data. The ability to produce synthetic Census microdata, where the utility and
associated risks are clearly understood, could mean that more timely and
wider-ranging access to microdata would be possible.
  This paper follows on from previous work by the authors which mapped
synthetic Census data on a risk-utility (R-U) map. The paper presents a
framework to measure the utility and disclosure risk of synthetic data by
comparing it to samples of the original data of varying sample fractions,
thereby identifying the sample fraction which has equivalent utility and risk
to the synthetic data. Three commonly used data synthesis packages are compared
with some interesting results. Further work is needed in several directions but
the methodology looks very promising.","['Claire Little', 'Mark Elliot', 'Richard Allmendinger']",[],0,arXiv,http://arxiv.org/abs/2207.03339v1,False,True,False,False,False,1157,Kristine M Witkowski,Michigan,Completed,2013,2018.0,"Demand for contextualized microdata has increased dramatically over the last decade and is expected to increase even more in years to come. The Census Bureau collects data to produce timely population estimates on demographic, social, housing, and economic characteristics for a broad spectrum of geographic areas in the United States and is therefore in a unique position to meet some of this demand. However, releasing geographic details in public-use microdata may increase the risk of disclosure. Current disclosure limitation practices involve suppressing geographic details for spatial units that fail to meet a predefined population threshold (e.g., 100,000). Users interested in studying persons nested within less-populated geographies (e.g., rural counties, tracts, block groups) must access restricted-use data through data enclaves. Neither of these approaches fully satisfies the growing demand for geographically-rich microdata that is being fueled by a variety of researchers, analysts, decision-makers, and administrators. To help alleviate barriers to use, this project will develop methodology that supports the efficient identification of sampling and database designs and associated masking techniques that allow more geographic detail to be released on microdata products, without increased disclosure risk or unnecessary survey costs. This research will use the 2005-2009 American Community Survey (ACS) data as the empirical foundation, with the aim of producing statistics that summarize findings from the evaluation of the methodology and broad guidelines derived from disclosure simulations, all of which are to be carefully constructed so as to ensure the confidentiality of the underlying data and disclosure practices."
"Studying Preferences and Concerns about Information Disclosure in Email
  Notifications","The proliferation of network-connected devices and applications has resulted
in people receiving dozens, or hundreds, of notifications per day. When people
are in the presence of others, each notification poses some risk of accidental
information disclosure; onlookers may see notifications appear above the lock
screen of a mobile phone, on the periphery of a desktop or laptop display, or
projected onscreen during a presentation. In this paper, we quantify the
prevalence of these accidental disclosures in the context of email
notifications, and we study people's relevant preferences and concerns. Our
results are compiled from an exploratory retrospective survey of 131
respondents, and a separate contextual-labeling study in which 169 participants
labeled 1,040 meeting-email pairs. We find that, for 53% of people, at least 1
in 10 email notifications poses an information disclosure risk. We also find
that the real or perceived severity of these risks depend both on user
characteristics and attributes of the meeting or email (e.g. the number of
recipients or attendees). We conclude by exploring machine learning algorithms
to predict people's comfort levels given an email notification and a context,
then we present implications for the design of future contextually-relevant
notification systems.","['Yongsung Kim', 'Adam Fourney', 'Ece Kamar']",[],0,arXiv,http://arxiv.org/abs/1808.00356v1,False,True,False,False,False,1157,Kristine M Witkowski,Michigan,Completed,2013,2018.0,"Demand for contextualized microdata has increased dramatically over the last decade and is expected to increase even more in years to come. The Census Bureau collects data to produce timely population estimates on demographic, social, housing, and economic characteristics for a broad spectrum of geographic areas in the United States and is therefore in a unique position to meet some of this demand. However, releasing geographic details in public-use microdata may increase the risk of disclosure. Current disclosure limitation practices involve suppressing geographic details for spatial units that fail to meet a predefined population threshold (e.g., 100,000). Users interested in studying persons nested within less-populated geographies (e.g., rural counties, tracts, block groups) must access restricted-use data through data enclaves. Neither of these approaches fully satisfies the growing demand for geographically-rich microdata that is being fueled by a variety of researchers, analysts, decision-makers, and administrators. To help alleviate barriers to use, this project will develop methodology that supports the efficient identification of sampling and database designs and associated masking techniques that allow more geographic detail to be released on microdata products, without increased disclosure risk or unnecessary survey costs. This research will use the 2005-2009 American Community Survey (ACS) data as the empirical foundation, with the aim of producing statistics that summarize findings from the evaluation of the methodology and broad guidelines derived from disclosure simulations, all of which are to be carefully constructed so as to ensure the confidentiality of the underlying data and disclosure practices."
Survey on Privacy-Preserving Techniques for Data Publishing,"The exponential growth of collected, processed, and shared microdata has
given rise to concerns about individuals' privacy. As a result, laws and
regulations have emerged to control what organisations do with microdata and
how they protect it. Statistical Disclosure Control seeks to reduce the risk of
confidential information disclosure by de-identifying them. Such
de-identification is guaranteed through privacy-preserving techniques. However,
de-identified data usually results in loss of information, with a possible
impact on data analysis precision and model predictive performance. The main
goal is to protect the individuals' privacy while maintaining the
interpretability of the data, i.e. its usefulness. Statistical Disclosure
Control is an area that is expanding and needs to be explored since there is
still no solution that guarantees optimal privacy and utility. This survey
focuses on all steps of the de-identification process. We present existing
privacy-preserving techniques used in microdata de-identification, privacy
measures suitable for several disclosure types and, information loss and
predictive performance measures. In this survey, we discuss the main challenges
raised by privacy constraints, describe the main approaches to handle these
obstacles, review taxonomies of privacy-preserving techniques, provide a
theoretical analysis of existing comparative studies, and raise multiple open
issues.","['Tânia Carvalho', 'Nuno Moniz', 'Pedro Faria', 'Luís Antunes']",[],0,arXiv,http://arxiv.org/abs/2201.08120v1,False,True,False,False,False,1157,Kristine M Witkowski,Michigan,Completed,2013,2018.0,"Demand for contextualized microdata has increased dramatically over the last decade and is expected to increase even more in years to come. The Census Bureau collects data to produce timely population estimates on demographic, social, housing, and economic characteristics for a broad spectrum of geographic areas in the United States and is therefore in a unique position to meet some of this demand. However, releasing geographic details in public-use microdata may increase the risk of disclosure. Current disclosure limitation practices involve suppressing geographic details for spatial units that fail to meet a predefined population threshold (e.g., 100,000). Users interested in studying persons nested within less-populated geographies (e.g., rural counties, tracts, block groups) must access restricted-use data through data enclaves. Neither of these approaches fully satisfies the growing demand for geographically-rich microdata that is being fueled by a variety of researchers, analysts, decision-makers, and administrators. To help alleviate barriers to use, this project will develop methodology that supports the efficient identification of sampling and database designs and associated masking techniques that allow more geographic detail to be released on microdata products, without increased disclosure risk or unnecessary survey costs. This research will use the 2005-2009 American Community Survey (ACS) data as the empirical foundation, with the aim of producing statistics that summarize findings from the evaluation of the methodology and broad guidelines derived from disclosure simulations, all of which are to be carefully constructed so as to ensure the confidentiality of the underlying data and disclosure practices."
"Assessing the protection provided by misclassification-based disclosure
  limitation methods for survey microdata","Government statistical agencies often apply statistical disclosure limitation
techniques to survey microdata to protect the confidentiality of respondents.
There is a need for valid and practical ways to assess the protection provided.
This paper develops some simple methods for disclosure limitation techniques
which perturb the values of categorical identifying variables. The methods are
applied in numerical experiments based upon census data from the United Kingdom
which are subject to two perturbation techniques: data swapping (random and
targeted) and the post randomization method. Some simplifying approximations to
the measure of risk are found to work well in capturing the impacts of these
techniques. These approximations provide simple extensions of existing risk
assessment methods based upon Poisson log-linear models. A numerical experiment
is also undertaken to assess the impact of multivariate misclassification with
an increasing number of identifying variables. It is found that the
misclassification dominates the usual monotone increasing relationship between
this number and risk so that the risk eventually declines, implying less
sensitivity of risk to choice of identifying variables. The methods developed
in this paper may also be used to obtain more realistic assessments of risk
which take account of the kinds of measurement and other nonsampling errors
commonly arising in surveys.","['Natalie Shlomo', 'Chris Skinner']",[],0,arXiv,http://arxiv.org/abs/1011.2905v1,False,True,False,False,False,1157,Kristine M Witkowski,Michigan,Completed,2013,2018.0,"Demand for contextualized microdata has increased dramatically over the last decade and is expected to increase even more in years to come. The Census Bureau collects data to produce timely population estimates on demographic, social, housing, and economic characteristics for a broad spectrum of geographic areas in the United States and is therefore in a unique position to meet some of this demand. However, releasing geographic details in public-use microdata may increase the risk of disclosure. Current disclosure limitation practices involve suppressing geographic details for spatial units that fail to meet a predefined population threshold (e.g., 100,000). Users interested in studying persons nested within less-populated geographies (e.g., rural counties, tracts, block groups) must access restricted-use data through data enclaves. Neither of these approaches fully satisfies the growing demand for geographically-rich microdata that is being fueled by a variety of researchers, analysts, decision-makers, and administrators. To help alleviate barriers to use, this project will develop methodology that supports the efficient identification of sampling and database designs and associated masking techniques that allow more geographic detail to be released on microdata products, without increased disclosure risk or unnecessary survey costs. This research will use the 2005-2009 American Community Survey (ACS) data as the empirical foundation, with the aim of producing statistics that summarize findings from the evaluation of the methodology and broad guidelines derived from disclosure simulations, all of which are to be carefully constructed so as to ensure the confidentiality of the underlying data and disclosure practices."
"Implication of Natal Care and Maternity Leave on Child Morbidity:
  Evidence from Ghana","Failure to receive post-natal care within first week of delivery causes a 3%
increase in the possibility of Acute Respiratory Infection in children under
five. Mothers with unpaid maternity leave put their children at a risk of 3.9%
increase in the possibility of ARI compared to those with paid maternity leave.","['Danny Turkson', 'Joy Kafui Ahiabor']",[],0,arXiv,http://arxiv.org/abs/2008.12910v1,False,True,False,False,False,1169,Robert Kaestner,Chicago,Completed,2013,2015.0,"This research will estimate the effect of Head Start on maternal labor supply and the effect of Head Start on child school progression. This study will use data from decennial censuses to estimate changes in aggregate maternal labor force participation rates and hours worked per week in response to the enrollment of 3-5 year old children in the Head Start program. First, the researchers will examine the non-response rates on maternal labor force participation and hours worked for low-income mothers by mothers’ characteristics such as education, race, and marital status, and document how this has changed from 1970 to 2000, to evaluate the comparability between different waves of the survey data. This will permit evaluation of whether current weights sufficiently adjust for the non-response patterns for low-income families and potentially identify the sources and the magnitude of this measurement problem. Second, this research will provide aggregate measures for the employment of low-income mothers with young children (e.g., aged 3-8) by education and race, thereby providing measures of the social conditions or well-being of children in low-income families. "
"A Members First Approach to Enabling LinkedIn's Labor Market Insights at
  Scale","We describe the privatization method used in reporting labor market insights
from LinkedIn's Economic Graph, including the differentially private algorithms
used to protect member's privacy. The reports show who are the top employers,
as well as what are the top jobs and skills in a given country/region and
industry. We hope this data will help governments and citizens track labor
market trends during the COVID-19 pandemic while also protecting the privacy of
our members.","['Ryan Rogers', 'Adrian Rivera Cardoso', 'Koray Mancuhan', 'Akash Kaura', 'Nikhil Gahlawat', 'Neha Jain', 'Paul Ko', 'Parvez Ahammad']",[],0,arXiv,http://arxiv.org/abs/2010.13981v1,False,True,False,False,False,1175,Matthew L Freedman,Irvine,Completed,2018,2023.0,"Substantial debate surrounds the effects of government-determined factors such as minimum wages, unemployment insurance (UI), trade protections, and place-based economic development programs. Moreover, what researchers observe in observational data frequently runs counter to theoretical predictions from standard economic models. Recent work suggests that publicly available data from surveys may mask important micro-heterogeneity and obscure differential impacts across local labor markets. We match employee-employer data from the Census Bureau’s Longitudinal Employer-Household Dynamics program to uncover such micro-heterogeneity as we examine how minimum wages, UI, trade protections, and place-based economic development programs affect the functioning of the labor market."
"A first unbiased global determination of polarized PDFs and their
  uncertainties","We present a first global determination of spin-dependent parton distribution
functions (PDFs) and their uncertainties using the NNPDF methodology:
NNPDFpol1.1. Longitudinally polarized deep-inelastic scattering data, already
used for the previous NNPDFpol1.0 PDF set, are supplemented with the most
recent polarized hadron collider data for inclusive jet and $W$ boson
production from the STAR and PHENIX experiments at RHIC, and with open-charm
production data from the COMPASS experiment, thereby allowing for a separate
determination of the polarized quark and anti-quark PDFs, and an improved
determination of the medium- and large-$x$ polarized gluon PDF. We study the
phenomenological implications of the NNPDFpol1.1 set, and we provide
predictions for the longitudinal double-spin asymmetry for semi-inclusive pion
production at RHIC.","['Emanuele R. Nocera', 'Richard D. Ball', 'Stefano Forte', 'Giovanni Ridolfi', 'Juan Rojo']",[],0,arXiv,http://arxiv.org/abs/1406.5539v2,False,True,False,False,False,1179,Teresa C Fort,Boston,Completed,2013,2018.0,"The world economy is becoming more integrated as different stages of a single production process are performed in multiple countries. Measuring this type of production fragmentation is essential to ensure accurate statistics about current U.S. economic activity and its future growth prospects. This project will combine multiple data sources to provide new measures of production fragmentation, with a particular emphasis on activities that U.S. firms choose to locate in foreign countries. These new measures will address: (1) how fragmentation differs across U.S. firms and geography; (2) the determinants of firms' decisions to fragment production; and (3) the domestic employment and R&D ramifications of firms' fragmentation decisions. To develop new data that measure firms’ global production choices, this project will use items collected in the 2007 Economic Census to identify individual establishments that report fragmenting and offshoring production. To assess firms’ globalization decisions over time, the research will improve the existing linkages currently in the Longitudinal Foreign Trade Transaction Database by developing new methodologies to link the trade transactions data to the Longitudinal Business Database, as well as by linking the trade data directly to the economic censuses, the Company Organization Survey, and to the SDC Thomson merger and acquisition data. The new datasets developed under this project will expand the scope and range of the economic activity that can be studied at the plant and firm levels, leading to improved measures of the import, export, and offshoring activity in the U.S. economy. "
"Global attractivity for a nonautonomous Nicholson's equation with mixed
  monotonicities","We consider a Nicholson's equation with multiple pairs of time-varying delays
and nonlinear terms given by mixed monotone functions. Sufficient conditions
for the permanence, local stability and global attractivity of its positive
equilibrium are established. Our criteria depend on the size of some delays,
improve results in recent literature and provide answers to some open problems.","['Teresa Faria', 'Henrique C. Prates']",[],0,arXiv,http://arxiv.org/abs/2106.11194v1,False,True,False,False,False,1179,Teresa C Fort,Boston,Completed,2013,2018.0,"The world economy is becoming more integrated as different stages of a single production process are performed in multiple countries. Measuring this type of production fragmentation is essential to ensure accurate statistics about current U.S. economic activity and its future growth prospects. This project will combine multiple data sources to provide new measures of production fragmentation, with a particular emphasis on activities that U.S. firms choose to locate in foreign countries. These new measures will address: (1) how fragmentation differs across U.S. firms and geography; (2) the determinants of firms' decisions to fragment production; and (3) the domestic employment and R&D ramifications of firms' fragmentation decisions. To develop new data that measure firms’ global production choices, this project will use items collected in the 2007 Economic Census to identify individual establishments that report fragmenting and offshoring production. To assess firms’ globalization decisions over time, the research will improve the existing linkages currently in the Longitudinal Foreign Trade Transaction Database by developing new methodologies to link the trade transactions data to the Longitudinal Business Database, as well as by linking the trade data directly to the economic censuses, the Company Organization Survey, and to the SDC Thomson merger and acquisition data. The new datasets developed under this project will expand the scope and range of the economic activity that can be studied at the plant and firm levels, leading to improved measures of the import, export, and offshoring activity in the U.S. economy. "
"More Robust Estimators for Instrumental-Variable Panel Designs, With An
  Application to the Effect of Imports from China on US Employment","We show that first-difference two-stages-least-squares regressions identify
non-convex combinations of location-and-period-specific treatment effects.
Thus, those regressions could be biased if effects are heterogeneous. We
propose an alternative instrumental-variable correlated-random-coefficient
(IV-CRC) estimator, that is more robust to heterogeneous effects. We revisit
Autor et al. (2013), who use a first-difference two-stages-least-squares
regression to estimate the effect of imports from China on US manufacturing
employment. Their regression estimates a highly non-convex combination of
effects. Our more robust IV-CRC estimator is small and insignificant. Though
its confidence interval is wide, it significantly differs from the
first-difference two-stages-least-squares estimator.","['Clément de Chaisemartin', 'Ziteng Lei']",[],0,arXiv,http://arxiv.org/abs/2103.06437v10,False,True,False,False,False,1181,David H Autor,Boston,Completed,2013,2016.0,"This study will exploit trade-induced shocks to U.S. manufacturing to study productivity spillovers from de-agglomeration. Using restricted-use microdata from the Economic Census, Annual Survey of Manufactures, and Longitudinal Business Database, this project will investigate the response of productivity to local de-agglomeration shocks at a highly disaggregated level. This study intends to uncover the key features of firms' responses to these shocks, including the extent to which these responses vary with geographic and economic distance. This project provides a natural setting to evaluate how nonresponse changes following shocks to the local economy, an issue highlighted by recent Census research."
"Populist Discourse and Entrepreneurship: The Role of Political Ideology
  and Institutions","Using institutional economic theory as our guiding framework, we develop a
model to describe how populist discourse by a nation's political leader
influences entrepreneurship. We hypothesize that populist discourse reduces
entrepreneurship by creating regime uncertainty concerning the future stability
of the institutional environment, resulting in entrepreneurs anticipating
higher future transaction costs. Our model highlights two important factors
that moderate the relationship. First, is the strength of political checks and
balances, which we hypothesize weakens the negative relationship between
populist discourse and entrepreneurship by providing entrepreneurs with greater
confidence that the actions of a populist will be constrained. Second, the
political ideology of the leader moderates the relationship between populist
discourse and entrepreneurship. The anti-capitalistic rhetoric of left-wing
populism will create greater regime uncertainty than right-wing populism, which
is often accompanied by rhetoric critical of free trade and foreigners but also
supportive of business interests. The effect of centrist populism, which is
accompanied by a mix of contradictory and often moderate ideas that make it
difficult to discern future transaction costs, will have a weaker negative
effect on entrepreneurship than either left-wing or right-wing populism. We
empirically test our model using a multi-level design and a dataset comprised
of more than 780,000 individuals in 33 countries over the period 2002-2016. Our
analysis largely supports our theory regarding the moderating role of ideology.
Still, surprisingly, our findings suggest that the negative effect of populism
on entrepreneurship is greater in nations with stronger checks and balances.","['Daniel L. Bennett', 'Christopher J. Boudreaux', 'Boris N. Nikolaev']",[],0,arXiv,http://arxiv.org/abs/2203.04101v1,False,True,False,False,False,1182,Jason Greenberg,Baruch,Completed,2015,2020.0,"This project will use various Census Bureau datasets to examine how social and technological change in geographic space have a bearing on the performance of businesses – particularly startups. For example, how have changes in neighborhood characteristics impacted distributional outcomes for minority and female owned businesses, and how have these businesses impact minority and female employment. This project will also investigate the quality, accuracy, and comprehensiveness of Census Bureau data on firm age and minority and female firm ownership of U.S. companies by making statistical comparisons to Dunn and Bradstreet (D&B) data."
Suicide ideation of individuals in online social networks,"Suicide explains the largest number of death tolls among Japanese adolescents
in their twenties and thirties. Suicide is also a major cause of death for
adolescents in many other countries. Although social isolation has been
implicated to influence the tendency to suicidal behavior, the impact of social
isolation on suicide in the context of explicit social networks of individuals
is scarcely explored. To address this question, we examined a large data set
obtained from a social networking service dominant in Japan. The social network
is composed of a set of friendship ties between pairs of users created by
mutual endorsement. We carried out the logistic regression to identify users'
characteristics, both related and unrelated to social networks, which
contribute to suicide ideation. We defined suicide ideation of a user as the
membership to at least one active user-defined community related to suicide. We
found that the number of communities to which a user belongs to, the
intransitivity (i.e., paucity of triangles including the user), and the
fraction of suicidal neighbors in the social network, contributed the most to
suicide ideation in this order. Other characteristics including the age and
gender contributed little to suicide ideation. We also found qualitatively the
same results for depressive symptoms.","['Naoki Masuda', 'Issei Kurahashi', 'Hiroko Onari']",[],0,arXiv,http://arxiv.org/abs/1207.0561v3,False,True,False,False,False,1188,David A Boulifard,Chicago,Completed,2016,2019.0,"This project estimates individual-level suicide risk within the general population for sixteen states during the years 2005-2011 using person-level data from the American Community Survey (ACS) and the National Violent Death Reporting System (NVDRS), which compiles follow-back information on nearly all suicides occurring in roughly one third of U.S. states. Appropriate combination of these datasets generates a cross-sectional sample that provides adequate power for statistical hypothesis tests and permits joint examination of individual- and community-level risk factors. This project aims to refit models on a dataset constructed using restricted-access ACS records containing county of residence. This increased geographic specificity may enhance previous findings, which include several individual-community interaction effects."
"Enhanced Suicidal Ideation Detection from Social Media Using a
  CNN-BiLSTM Hybrid Model","Suicidal ideation detection is crucial for preventing suicides, a leading
cause of death worldwide. Many individuals express suicidal thoughts on social
media, offering a vital opportunity for early detection through advanced
machine learning techniques. The identification of suicidal ideation in social
media text is improved by utilising a hybrid framework that integrates
Convolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory
(BiLSTM), enhanced with an attention mechanism. To enhance the interpretability
of the model's predictions, Explainable AI (XAI) methods are applied, with a
particular focus on SHapley Additive exPlanations (SHAP), are incorporated. At
first, the model managed to reach an accuracy of 92.81%. By applying
fine-tuning and early stopping techniques, the accuracy improved to 94.29%. The
SHAP analysis revealed key features influencing the model's predictions, such
as terms related to mental health struggles. This level of transparency boosts
the model's credibility while helping mental health professionals understand
and trust the predictions. This work highlights the potential for improving the
accuracy and interpretability of detecting suicidal tendencies, making a
valuable contribution to the progress of mental health monitoring systems. It
emphasizes the significance of blending powerful machine learning methods with
explainability to develop reliable and impactful mental health solutions.","['Mohaiminul Islam Bhuiyan', 'Nur Shazwani Kamarudin', 'Nur Hafieza Ismail']",[],0,arXiv,http://arxiv.org/abs/2501.11094v1,False,True,False,False,False,1188,David A Boulifard,Chicago,Completed,2016,2019.0,"This project estimates individual-level suicide risk within the general population for sixteen states during the years 2005-2011 using person-level data from the American Community Survey (ACS) and the National Violent Death Reporting System (NVDRS), which compiles follow-back information on nearly all suicides occurring in roughly one third of U.S. states. Appropriate combination of these datasets generates a cross-sectional sample that provides adequate power for statistical hypothesis tests and permits joint examination of individual- and community-level risk factors. This project aims to refit models on a dataset constructed using restricted-access ACS records containing county of residence. This increased geographic specificity may enhance previous findings, which include several individual-community interaction effects."
Network Pruning via Resource Reallocation,"Channel pruning is broadly recognized as an effective approach to obtain a
small compact model through eliminating unimportant channels from a large
cumbersome network. Contemporary methods typically perform iterative pruning
procedure from the original over-parameterized model, which is both tedious and
expensive especially when the pruning is aggressive. In this paper, we propose
a simple yet effective channel pruning technique, termed network Pruning via
rEsource rEalLocation (PEEL), to quickly produce a desired slim model with
negligible cost. Specifically, PEEL first constructs a predefined backbone and
then conducts resource reallocation on it to shift parameters from less
informative layers to more important layers in one round, thus amplifying the
positive effect of these informative layers. To demonstrate the effectiveness
of PEEL , we perform extensive experiments on ImageNet with ResNet-18,
ResNet-50, MobileNetV2, MobileNetV3-small and EfficientNet-B0. Experimental
results show that structures uncovered by PEEL exhibit competitive performance
with state-of-the-art pruning algorithms under various pruning settings. Our
code is available at https://github.com/cardwing/Codes-for-PEEL.","['Yuenan Hou', 'Zheng Ma', 'Chunxiao Liu', 'Zhe Wang', 'Chen Change Loy']",[],0,arXiv,http://arxiv.org/abs/2103.01847v1,False,True,False,False,False,1195,Xavier A Giroud,Boston,Completed,2014,2018.0,"This project examines how firms internally reallocate resources (e.g., labor, capital) over the business cycle, with emphasis on the recent financial and economic crisis of 2007-2009 (the “Great Recession”). This research assesses capital stock imputation from Census Bureau data, assesses the geographical classification of establishments, and builds a bridge among several establishment- and employee-level datasets."
Contrastive Pre-training for Imbalanced Corporate Credit Ratings,"Corporate credit rating reflects the level of corporate credit and plays a
crucial role in modern financial risk control. But real-world credit rating
data usually shows long-tail distributions, which means heavy class imbalanced
problem challenging the corporate credit rating system greatly. To tackle that,
inspried by the recent advances of pre-train techniques in self-supervised
representation learning, we propose a novel framework named Contrastive
Pre-training for Corporate Credit Rating (CP4CCR), which utilizes the
self-surpervision for getting over class imbalance. Specifically, we propose
to, in the first phase, exert constrastive self-superivised pre-training
without label information, which want to learn a better class-agnostic
initialization. During this phase, two self-supervised task are developed
within CP4CCR: (i) Feature Masking (FM) and (ii) Feature Swapping(FS). In the
second phase, we can train any standard corporate redit rating model
initialized by the pre-trained network. Extensive experiments conducted on the
Chinese public-listed corporate rating dataset, prove that CP4CCR can improve
the performance of standard corporate credit rating models, especially for
class with few samples.","['Bojing Feng', 'Wenfang Xue']",[],0,arXiv,http://arxiv.org/abs/2102.12580v2,False,True,False,False,False,1212,Rustom D Manouchehri Irani,Chicago,Completed,2014,2018.0,"This project investigates the impact of a reliance on credit markets on real corporate behavior—patterns of investment and employment—by conducting a detailed microeconomic analysis using plant-level data. Two topics will be considered: first, real estate asset collateral values and corporate debt capacity; second, the transfer of control rights to creditors (“creditor intervention”) following contractual default in private credit agreements. The project will build new bridge files between Census Bureau data and external sources, such as data on financial contracts associated with bank lending in the U.S. syndicated loan market (Thomson Reuters’ Loan Pricing Corporation Deal Scan dataset), as well as data on real estate price indices and local housing supply elasticities. By producing estimates of various firm characteristics, this project will enhance the Census Bureau’s understanding of economy-wide establishment dynamics (formation, closure, growth, contraction, and performance) and their responsiveness to changes in credit market conditions. "
The puzzle of Carbon Allowance spread,"A growing number of contributions in the literature have identified a puzzle
in the European carbon allowance (EUA) market. Specifically, a persistent
cost-of-carry spread (C-spread) over the risk-free rate has been observed. We
are the first to explain the anomalous C-spread with the credit spread of the
corporates involved in the emission trading scheme. We obtain statistical
evidence that the C-spread is cointegrated with both this credit spread and the
risk-free interest rate. This finding has a relevant policy implication: the
most effective solution to solve the market anomaly is including the EUA in the
list of European Central Bank eligible collateral for refinancing operations.
This change in the ECB monetary policy operations would greatly benefit the
carbon market and the EU green transition.","['Michele Azzone', 'Roberto Baviera', 'Pietro Manzoni']",[],0,arXiv,http://arxiv.org/abs/2405.12982v1,False,True,False,False,False,1212,Rustom D Manouchehri Irani,Chicago,Completed,2014,2018.0,"This project investigates the impact of a reliance on credit markets on real corporate behavior—patterns of investment and employment—by conducting a detailed microeconomic analysis using plant-level data. Two topics will be considered: first, real estate asset collateral values and corporate debt capacity; second, the transfer of control rights to creditors (“creditor intervention”) following contractual default in private credit agreements. The project will build new bridge files between Census Bureau data and external sources, such as data on financial contracts associated with bank lending in the U.S. syndicated loan market (Thomson Reuters’ Loan Pricing Corporation Deal Scan dataset), as well as data on real estate price indices and local housing supply elasticities. By producing estimates of various firm characteristics, this project will enhance the Census Bureau’s understanding of economy-wide establishment dynamics (formation, closure, growth, contraction, and performance) and their responsiveness to changes in credit market conditions. "
"Drell-Yan and J/psi Production in High Energy Proton-Nucleus and
  Nucleus-Nucleus Collisions","The distributions of outgoing protons and charged hadrons in high energy
proton-nucleus collisions are described rather well by a linear extrapolation
from proton-proton collisions. This linear extrapolation is applied to
precisely measured Drell-Yan cross sections for 800 GeV protons incident on a
variety of nuclear targets. The deviation from linear scaling in the atomic
number A can be accounted for by energy degradation of the proton as it passes
through the nucleus if account is taken of the time delay of particle
production due to quantum coherence. We infer an average proper coherence time
of 0.4 +/- 0.1 fm/c. Then we apply the linear extrapolation to measured J/psi
production cross sections for 200 and 450 GeV/c protons incident on a variety
of nuclear targets. Our analysis takes into account energy loss of the beam
proton, the time delay of particle production due to quantum coherence, and
absorption of the J/psi on nucleons. The best representation is obtained for a
coherence time of 0.5 fm/c, which is consistent with Drell-Yan production, and
an absorption cross section of 3.6 mb, which is consistent with the value
deduced from photoproduction of the J/psi on nuclear targets. Finally, we
compare to recent J/psi data from S+U and Pb+Pb collisions at the SPS. The
former are reproduced reasonably well with no new parameters, but not the
latter.","['Charles Gale', 'Sangyong Jeon', 'Joseph Kapusta']",[],0,arXiv,http://arxiv.org/abs/hep-ph/9908259v1,False,True,False,False,False,1213,Gale A Boyd,Triangle,Completed,2014,2019.0,"This research continues work conducted under prior projects, conducting both cross-sectional and time series analyses of the underlying causes of changes in the distributions of production and energy efficiency. The principal analytic approach will be the application of frontier production functions and related procedures. Prior projects have successfully implemented these methods for selected industrial sectors."
J/psi Production and Absorption in High Energy Proton-Nucleus Collisions,"Measured J/Psi production cross sections for 200 and 450 GeV/c protons
incident on a variety of nuclear targets are analyzed within a Glauber
framework which takes into account energy loss of the beam proton, the time
delay of particle production due to quantum coherence, and absorption of the
J/Psi on nucleons. The best representation is obtained for a coherence time of
0.5 fm/c, previously determined by Drell-Yan production in proton-nucleus
collisions, and an absorption cross section of 3.6 mb, which is consistent with
the value deduced from photoproduction of the J/Psi on nuclear targets.","['Charles Gale', 'Sangyong Jeon', 'Joseph Kapusta']",[],0,arXiv,http://arxiv.org/abs/nucl-th/9812056v2,False,True,False,False,False,1213,Gale A Boyd,Triangle,Completed,2014,2019.0,"This research continues work conducted under prior projects, conducting both cross-sectional and time series analyses of the underlying causes of changes in the distributions of production and energy efficiency. The principal analytic approach will be the application of frontier production functions and related procedures. Prior projects have successfully implemented these methods for selected industrial sectors."
"Baryon, Charged Hadron, Drell-Yan and J/Psi Production in High Energy
  Proton-Nucleus Collisions","We show that the distributions of outgoing protons and charged hadrons in
high energy proton-nucleus collisions are described rather well by a linear
extrapolation from proton-proton collisions. The only adjustable parameter
required is the shift in rapidity of a produced charged meson when it
encounters a target nucleon. Its fitted value is 0.16. Next, we apply this
linear extrapolation to precisely measured Drell-Yan cross sections for 800 GeV
protons incident on a variety of nuclear targets which exhibit a deviation from
linear scaling in the atomic number A. We show that this deviation can be
accounted for by energy degradation of the proton as it passes through the
nucleus if account is taken of the time delay of particle production due to
quantum coherence. We infer an average proper coherence time of 0.4 +/- 0.1
fm/c, corresponding to a coherence path length of 8 +/- 2 fm in the rest frame
of the nucleus. Finally, we apply the linear extrapolation to measured J/Psi
production cross sections for 200 and 450 GeV/c protons incident on a variety
of nuclear targets. Our analysis takes into account energy loss of the beam
proton, the time delay of particle production due to quantum coherence, and
absorption of the J/Psi on nucleons. The best representation is obtained for a
coherence time of 0.5 fm/c, which is consistent with Drell-Yan production, and
an absorption cross section of 3.6 mb, which is consistent with the value
deduced from photoproduction of the J/Psi on nuclear targets.","['Charles Gale', 'Sangyong Jeon', 'Joseph Kapusta']",[],0,arXiv,http://arxiv.org/abs/nucl-th/9903073v1,False,True,False,False,False,1213,Gale A Boyd,Triangle,Completed,2014,2019.0,"This research continues work conducted under prior projects, conducting both cross-sectional and time series analyses of the underlying causes of changes in the distributions of production and energy efficiency. The principal analytic approach will be the application of frontier production functions and related procedures. Prior projects have successfully implemented these methods for selected industrial sectors."
"Addressing the Recruitment and Retention of Female Students in Computer
  Science at Third Level","In the School of Computing at the Dublin Institute of Technology (DIT),
Ireland, we undertook our Computer Science for All (CS4All) initiative, a five
year strategy to implement structural reforms at Faculty level, to address
recruitment and retention issues of female undergraduate computer science (CS)
students. Since 2012, under CS4All we implemented a variety of reforms to
improve student retention, set up a new CS program to attract more female
students, and delivered changes to promote a sense of community amongst our
female students. We have made significant improvements. For example, we have
achieved a dramatic improvement in retention rising from 45% to 89% in first
year progression rates. Our new hybrid CS International program has more than
double the percentage of females first year enrolments in comparison to our
other undergraduate programs. As at 2018, we continue to roll out the remaining
parts of CS4All within our School.","['Susan McKeever', 'Deirdre Lillis']",[],0,arXiv,http://arxiv.org/abs/2110.06090v1,False,True,False,False,False,1220,Caroline P Danielson,Berkeley,Completed,2018,2024.0,"This research will quantify short-run earnings volatility among low- and moderate-income families in California to estimate the incidence of family income changes and gauge implications for means-tested public insurance program eligibility. Moreover, this project will examine uptake of insurance programs and assess causes of disenrollment. To identify low- and moderate-income families in California, quantify earnings volatility, and assess public insurance eligibility, we will use quarterly earnings records contained in the Employment History File from the Longitudinal Employer-Household Dynamics (LEHD) program linked to household records in the American Community Survey and Current Population Survey, and person-level characteristics in the LEHD Individual Characteristics File. To this, we will link Medicaid Statistics Information Systems records, in order to capture spells of enrollment to Medicaid and SCHIP. With this we will generate measures of inappropriate disenrollment (program drop-out despite continuing eligibility) and legitimate disenrollment where return to these programs occurs in a short period of time (churning). In 2014, millions of Americans became eligible for government-subsidized health insurance programs where eligibility is determined by income falling within precisely defined ranges. We find it important to know whether eligible individuals have taken up benefits and, for families with more variable incomes, whether costs of maintaining enrollment results in an increased likelihood of churning—moving off and then back on a program within a short period of time—which is costly both for households and programs."
Understanding Retail Productivity by Simulating Management Practise,"Intelligent agents offer a new and exciting way of understanding the world of
work. In this paper we apply agent-based modeling and simulation to investigate
a set of problems in a retail context. Specifically, we are working to
understand the relationship between human resource management practices and
retail productivity. Despite the fact we are working within a relatively novel
and complex domain, it is clear that intelligent agents could offer potential
for fostering sustainable organizational capabilities in the future. Our
research so far has led us to conduct case study work with a top ten UK
retailer, collecting data in four departments in two stores. Based on our case
study data we have built and tested a first version of a department store
simulator. In this paper we will report on the current development of our
simulator which includes new features concerning more realistic data on the
pattern of footfall during the day and the week, a more differentiated view of
customers, and the evolution of customers over time. This allows us to
investigate more complex scenarios and to analyze the impact of various
management practices.","['Peer-Olaf Siebers', 'Uwe Aickelin', 'Helen Celia', 'Christopher Clegg']",[],0,arXiv,http://arxiv.org/abs/0803.1600v1,False,True,False,False,False,1227,Matthew Sveum,Missouri,Completed,2014,2018.0,"This project uses data from the Survey of Business Owners and the Census of Retail Trade (CRT), augmented with other federal and enterprise data, to analyze the relationship between franchising and establishment productivity. Focusing on establishments that indicated a franchise connection on the CRT, this study compares franchisee-run establishments with franchisor-run establishments and investigates the productivity effects of franchising. "
A determinant on birational maps of Severi-Brauer surfaces,"We define a determinant on the group of automorphisms of non-trivial
Severi-Brauer surfaces over a perfect field. Using the generators and
relations, we extend this determinant to birational maps between Severi-Brauer
surfaces. Using this determinant and a group homomorphism found in
arXiv:2211.17123 we can determine the abelianization of the group of birational
transformations of a non-trivial Severi-Brauer surface. This is the first
example of an abelianization of the group of birational transformations of a
geometrically rational surface where the automorphisms are non trivial. Using
the abelianization we find maximal subgroups of the group of birational
transformations of a non-trivial Severi-Brauer surface over a perfect field.",['Elias Kurz'],[],0,arXiv,http://arxiv.org/abs/2502.02981v1,False,True,False,False,False,1237,Christopher J Kurz,Fed Board,Completed,2014,2023.0,"This project documents the recent employment and productivity dynamics within the manufacturing sector and analyzes the factors driving these dynamics. This research establishes basic facts about changing dynamics, empirically tests explanations for the change in manufacturing dynamics, and analyzes the factors behind the changing manufacturing landscape with a focus on production fragmentation and innovation."
A Binary Wyner-Ziv Code Design Based on Compound LDGM-LDPC Structures,"In this paper, a practical coding scheme is designed for the binary Wyner-Ziv
(WZ) problem by using nested low-density generator-matrix (LDGM) and
low-density parity-check (LDPC) codes. This scheme contains two steps in the
encoding procedure. The first step involves applying the binary quantization by
employing LDGM codes and the second one is using the syndrome-coding technique
by utilizing LDPC codes. The decoding algorithm of the proposed scheme is based
on the Sum-Product (SP) algorithm with the help of a side information available
at the decoder side. It is theoretically shown that the compound structure has
the capability of achieving the WZ bound. The proposed method approaches this
bound by utilizing the iterative message-passing algorithms in both encoding
and decoding, although theoretical results show that it is asymptotically
achievable.","['Mahdi Nangir', 'Mahmoud Ahmadian-Attari', 'Reza Asvadi']",[],0,arXiv,http://arxiv.org/abs/1710.07985v1,False,True,False,False,False,1255,Oren Ziv,Boston,Completed,2014,2017.0,"Firms in cities are larger, more productive, and more profitable. At the same time, rents in cities are higher. This relationship between density, rent, and profits holds true in comparisons between cities in terms of size, average population density, and average firm density. This project explores intra-city relationships between firm and location characteristics to understand how firm location decisions affect the relationship between density, market access, and firm productivity at the intra-city level, and to test and estimate a novel model accounting for these relationships. "
"Will Southeast Asia be the next global manufacturing hub? A multiway
  cointegration, causality, and dynamic connectedness analyses on factors
  influencing offshore decisions","The COVID-19 pandemic has compelled multinational corporations to diversify
their global supply chain risk and to relocate their factories to Southeast
Asian countries beyond China. Such recent phenomena provide a good opportunity
to understand the factors that influenced offshore decisions in the last two
decades. We propose a new conceptual framework based on econometric approaches
to examine the relationships between these factors. Firstly, the Vector Auto
Regression (VAR) for multi-way cointegration analysis by a Johansen test as
well as the embedding Granger causality analysis to examine offshore
decisions--innovation, technology readiness, infrastructure, foreign direct
investment (FDI), and intermediate imports. Secondly, a Quantile Vector
Autoregressive (QVAR) model is used to assess the dynamic connectedness among
Southeast Asian countries based on the offshore factors. This study explores a
system-wide experiment to evaluate the spillover effects of offshore decisions.
It reports a comprehensive analysis using time-series data collected from the
World Bank. The results of the cointegration, causality, and dynamic
connectedness analyses show that a subset of Southeast Asian countries have
spillover effects on each other. These countries present a multi-way
cointegration and dynamic connectedness relationship. The study contributes to
policymaking by providing a data-driven innovative approach through a new
conceptual framework.","['Haibo Wang', 'Lutfu S. Sua', 'Jun Huang', 'Jaime Ortiz', 'Bahram Alidaee']",[],0,arXiv,http://arxiv.org/abs/2406.07525v1,False,True,False,False,False,1257,Wolfgang Keller,Colorado,Completed,2015,2019.0,"This project will quantify the relationship between offshoring activities and the rate of innovation of U.S. firms. To analyze this, the researchers will compare the innovation rates of firms that offshore with those that do not, using China’s accession to the World Trade Organization (WTO) in 2001 as an external shock that generates a quasi-random sample of firms. The key challenge is to ensure that firms who do offshore are not too different from firms that do not offshore in terms of their determinants of innovation, which will require an appropriate comparison group that only Census Bureau microdata can provide. Building upon two previous studies that point to evidence of R&D spillovers to domestic firms from foreign-owned production and the potential knowledge costs from separating production facilities and firm headquarters, this empirical study will attempt to disentangle these opposing effects and quantify the influence of offshoring on different measures of innovation, including R&D expenditures, patenting, and trademarks. This project will provide a better understanding of how plant characteristics relate to offshoring and of the international scope of R&D for many firms and improve accuracy by comparing locational outcomes and activities of innovation."
"A Game-Theoretic View of the Interference Channel: Impact of
  Coordination and Bargaining","This work considers coordination and bargaining between two selfish users
over a Gaussian interference channel. The usual information theoretic approach
assumes full cooperation among users for codebook and rate selection. In the
scenario investigated here, each user is willing to coordinate its actions only
when an incentive exists and benefits of cooperation are fairly allocated. The
users are first allowed to negotiate for the use of a simple Han-Kobayashi type
scheme with fixed power split. Conditions for which users have incentives to
cooperate are identified. Then, two different approaches are used to solve the
associated bargaining problem. First, the Nash Bargaining Solution (NBS) is
used as a tool to get fair information rates and the operating point is
obtained as a result of an optimization problem. Next, a dynamic
alternating-offer bargaining game (AOBG) from bargaining theory is introduced
to model the bargaining process and the rates resulting from negotiation are
characterized. The relationship between the NBS and the equilibrium outcome of
the AOBG is studied and factors that may affect the bargaining outcome are
discussed. Finally, under certain high signal-to-noise ratio regimes, the
bargaining problem for the generalized degrees of freedom is studied.","['Xi Liu', 'Elza Erkip']",[],0,arXiv,http://arxiv.org/abs/1101.3574v1,False,True,False,False,False,1273,Sebastian Heise,Baruch,Completed,2014,2017.0,"This project investigates the effects of bargaining power and long-term relationships on price setting in producer markets. One set of questions centers on the effect of bargaining power on the average price level and the size and frequency of price changes. For example, are sellers with more bargaining power able to charge higher prices for the same product? Another set of research questions concerns the connection between the average length of a relationship and bargaining power. For example, do relationships characterized by asymmetric bargaining power become more stable over time? Using transaction-level trade data involving U.S. firms, this research identifies both the buyer and the seller firm for import transactions. This feature makes it possible to determine in the import data whether firms are in an ongoing relationship with each other and to assess their relative bargaining power. Bargaining power is estimated using proxies for firms’ size, the ease with which they can find an alternative trading partner for the same product, and the uniqueness of the product traded. "
"Impacts of Differential Privacy on Fostering more Racially and
  Ethnically Diverse Elementary Schools","In the face of increasingly severe privacy threats in the era of data and AI,
the US Census Bureau has recently adopted differential privacy, the de facto
standard of privacy protection for the 2020 Census release. Enforcing
differential privacy involves adding carefully calibrated random noise to
sensitive demographic information prior to its release. This change has the
potential to impact policy decisions like political redistricting and other
high-stakes practices, partly because tremendous federal funds and resources
are allocated according to datasets (like Census data) released by the US
government. One under-explored yet important application of such data is the
redrawing of school attendance boundaries to foster less demographically
segregated schools. In this study, we ask: how differential privacy might
impact diversity-promoting boundaries in terms of resulting levels of
segregation, student travel times, and school switching requirements?
Simulating alternative boundaries using differentially-private student counts
across 67 Georgia districts, we find that increasing data privacy requirements
decreases the extent to which alternative boundaries might reduce segregation
and foster more diverse and integrated schools, largely by reducing the number
of students who would switch schools under boundary changes. Impacts on travel
times are minimal. These findings point to a privacy-diversity tradeoff local
educational policymakers may face in forthcoming years, particularly as
computational methods are increasingly poised to facilitate attendance boundary
redrawings in the pursuit of less segregated schools.","['Keyu Zhu', 'Nabeel Gillani', 'Pascal Van Hentenryck']",[],0,arXiv,http://arxiv.org/abs/2305.07762v1,True,True,False,False,False,1310,Salvatore Saporito,Cornell,Completed,2016,2021.0,"Public schools are more racially segregated than the school attendance areas to which they supply services. Individual choice for private schools may play a substantial role in contributing to public school segregation, beyond what it would be if all students enrolled in the public school that served their residential area. One of three basic factors likely drives private school enrollment rates. First, children from one racial group, particularly white children, may be more likely to enroll in a private school as shares of non-white students in their school catchment areas increases. Second, members of all racial groups are less likely to enroll in private schools as shares of children in their catchment area who are of their same race increases. A third view is that race is inconsequential in driving private school enrollment. These competing models will be assessed by integrating three data sources: restricted-access American Community Survey (ACS) 5-year period estimates for 2005-2009, the School Attendance Boundary Information System (SABINS), which contains geography delineating school catchment areas for thousands of such areas for the 2009-2010 school year, and the Common Core of Data, which describes the characteristics of children who are enrolled in all public schools throughout the United States. Analyses of these three datasets will result in models estimating the probability that a child is enrolled in private school based on their race and the racial composition of the school catchment area in which they live, while holding constant a battery of family, attendance area, and school characteristics."
Federated Learning and Free-riding in a Competitive Market,"Federated learning (FL) is a collaborative technique for training large-scale
models while protecting user data privacy. Despite its substantial benefits,
the free-riding behavior raises a major challenge for the formation of FL,
especially in competitive markets. Our paper explores this under-explored issue
on how the free-riding behavior in a competitive market affects firms'
incentives to form FL. Competing firms can improve technologies through forming
FL to increase the performance of their products, which in turn, affects
consumers' product selection and market size. The key complication is whether
the free-riding behavior discourages information contribution by participating
firms and results in the decomposition of FL, and even free riding does not
discourage information contribution, this does not necessarily mean that a firm
wants to form FL in a competitive market because free riding may reshape the
competition positions of each participating firm and thus forming FL may not be
profitable. We build a parsimonious game theoretical model that captures these
interactions and our analyses show several new findings. First, even in the
presence of the free-riding behavior, competing firms under FL find it optimal
to contribute all its available information. Second, the firm with less amount
of information always finds it profitable to free ride; whether its rival (with
more amount of information) have an incentive to form FL only when the level of
competition or when the gap in information volume is not high. Third, when FL
is formed, there exists an ""All-Win"" situation in which all stakeholders
(participating firms, consumers, and social planner) benefit. Last, subsidizing
by the free-riding firm can align its rival's incentive to form FL only when
the level of competition is intermediate.","['Jiajun Meng', 'Jing Chen', 'Dongfang Zhao', 'Lin Liu']",[],0,arXiv,http://arxiv.org/abs/2410.12723v1,False,True,False,False,False,1314,Ildiko Magyari,Baruch,Completed,2014,2017.0,"This project investigates how local competition among suppliers within geographically segmented markets drives manufacturing firms' decisions of whether to integrate or outsource the production of their intermediate inputs, and if they outsource, whether to buy the input from domestic producers or import it from abroad. Using Census of Manufactures, Commodity Flow Survey, and foreign trade data, this research examines market outcomes such as mark-ups, prices, and quantities supplied, as well as consumers' welfare. "
Fair Compensation,"We introduce a novel framework that considers how a firm could fairly
compensate its workers. A firm has a group of workers, each of whom has varying
productivities over a set of tasks. After assigning workers to tasks, the firm
must then decide how to distribute its output to the workers. We first consider
three compensation rules and various fairness properties they may satisfy. We
show that among efficient and symmetric rules: the Egalitarian rule is the only
rule that does not decrease a worker's compensation when every worker becomes
weakly more productive (Group Productivity Monotonicity); the Shapley Value
rule is the only rule that, for any two workers, equalizes the impact one
worker has on the other worker's compensation (Balanced Impact); and the
Individual Contribution rule is the only rule that is invariant to the removal
of workers and their assigned tasks (Consistency). We introduce other rules and
axioms, and relate each rule to each axiom.",['John E. Stovall'],[],0,arXiv,http://arxiv.org/abs/2109.04583v3,False,True,False,False,False,1315,Gerald R Marschke,Cornell,Active,2019,,"This project will investigate the use of big data machine learning methods to  generate estimates of the population of Research and Development (R&D) performing firms and establishments, and of Science, Technology, Engineering and Math (STEM) workers. The project includes analyses to improve R&D survey data, identification of postdoc and graduate student survey respondents, analyses of the labor market for STEM PhDs and postdocs, estimates of the regional economic impacts of federally funded science, the economic impact of science and engineering workers and their role in mediating R&D spillovers among firms, and the impact of technology on older workers."
Using Deep Learning to Generate Complete Log Statements,"Logging is a practice widely adopted in several phases of the software
lifecycle. For example, during software development log statements allow
engineers to verify and debug the system by exposing fine-grained information
of the running software. While the benefits of logging are undisputed, taking
proper decisions about where to inject log statements, what information to log,
and at which log level (e.g., error, warning) is crucial for the logging
effectiveness. In this paper, we present LANCE (Log stAtemeNt reCommEnder), the
first approach supporting developers in all these decisions. LANCE features a
Text-To-Text-Transfer-Transformer (T5) model that has been trained on 6,894,456
Java methods. LANCE takes as input a Java method and injects in it a full log
statement, including a human-comprehensible logging message and properly
choosing the needed log level and the statement location. Our results show that
LANCE is able to (i) properly identify the location in the code where to inject
the statement in 65.9% of Java methods requiring it; (ii) selecting the proper
log level in 66.2% of cases; and (iii) generate a completely correct log
statement including a meaningful logging message in 15.2% of cases.","['Antonio Mastropaolo', 'Luca Pascarella', 'Gabriele Bavota']",[],0,arXiv,http://arxiv.org/abs/2201.04837v2,False,True,False,False,False,1329,Lance Freeman,Baruch,Completed,2017,2020.0,"This research examines locational attainment, and in particular, the changing ability of households to translate individual traits into access to high-quality neighborhoods and whether existing rental and mortgage subsidies facilitate or hinder access to such neighborhoods for their recipients. With few exceptions the extant literature on locational attainment has not considered temporal trends nor how housing assistance might facilitate or retard access to different types of neighborhoods. For poorer individuals, housing assistance is likely to be an important determinant of the type of neighborhood they are able to reside in. Housing assistance allows the recipients to live in housing they could otherwise not afford. Conversely, project based housing assistance and especially public housing has a long history of confining its clientele to the poorest and least desirable neighborhoods. This project uses decades of restricted-use American Housing Survey (AHS) data, which allows for ideal neighborhood definitions at the census tract level. This research yields important insights into socioeconomic conditions and the effects of housing programs and subsidies."
"FreeMan: Towards Benchmarking 3D Human Pose Estimation under Real-World
  Conditions","Estimating the 3D structure of the human body from natural scenes is a
fundamental aspect of visual perception. 3D human pose estimation is a vital
step in advancing fields like AIGC and human-robot interaction, serving as a
crucial technique for understanding and interacting with human actions in
real-world settings. However, the current datasets, often collected under
single laboratory conditions using complex motion capture equipment and
unvarying backgrounds, are insufficient. The absence of datasets on variable
conditions is stalling the progress of this crucial task. To facilitate the
development of 3D pose estimation, we present FreeMan, the first large-scale,
multi-view dataset collected under the real-world conditions. FreeMan was
captured by synchronizing 8 smartphones across diverse scenarios. It comprises
11M frames from 8000 sequences, viewed from different perspectives. These
sequences cover 40 subjects across 10 different scenarios, each with varying
lighting conditions. We have also established an semi-automated pipeline
containing error detection to reduce the workload of manual check and ensure
precise annotation. We provide comprehensive evaluation baselines for a range
of tasks, underlining the significant challenges posed by FreeMan. Further
evaluations of standard indoor/outdoor human sensing datasets reveal that
FreeMan offers robust representation transferability in real and complex
scenes. Code and data are available at https://wangjiongw.github.io/freeman.","['Jiong Wang', 'Fengyu Yang', 'Wenbo Gou', 'Bingliang Li', 'Danqi Yan', 'Ailing Zeng', 'Yijun Gao', 'Junle Wang', 'Yanqing Jing', 'Ruimao Zhang']",[],0,arXiv,http://arxiv.org/abs/2309.05073v4,False,True,False,False,False,1329,Lance Freeman,Baruch,Completed,2017,2020.0,"This research examines locational attainment, and in particular, the changing ability of households to translate individual traits into access to high-quality neighborhoods and whether existing rental and mortgage subsidies facilitate or hinder access to such neighborhoods for their recipients. With few exceptions the extant literature on locational attainment has not considered temporal trends nor how housing assistance might facilitate or retard access to different types of neighborhoods. For poorer individuals, housing assistance is likely to be an important determinant of the type of neighborhood they are able to reside in. Housing assistance allows the recipients to live in housing they could otherwise not afford. Conversely, project based housing assistance and especially public housing has a long history of confining its clientele to the poorest and least desirable neighborhoods. This project uses decades of restricted-use American Housing Survey (AHS) data, which allows for ideal neighborhood definitions at the census tract level. This research yields important insights into socioeconomic conditions and the effects of housing programs and subsidies."
"A note on an integration by parts formula for the generators of uniform
  translations on configuration space","An integration by parts formula is derived for the first order differential
operator corresponding to the action of translations on the space of locally
finite simple configurations of infinitely many points on R^d. As reference
measures, tempered grand canonical Gibbs measures are considered corresponding
to a non-constant non-smooth intensity (one-body potential) and translation
invariant potentials fulfilling the usual conditions. It is proven that such
Gibbs measures fulfill the intuitive integration by parts formula if and only
if the action of the translation is not broken for this particular measure. The
latter is automatically fulfilled in the high temperature and low intensity
regime.","['Florian Conrad', 'Tobias Kuna']",[],0,arXiv,http://arxiv.org/abs/1103.6001v1,False,True,False,False,False,1338,Conrad C Miller,Berkeley,Completed,2015,2018.0,"This project will estimate the effects of affirmative action regulation and equal employment opportunity law – what are referred to as anti-discrimination laws – have on establishment productivity. To measure these effects, the researcher will measure total factor productivity and labor productivity using Economic Census data, and exploit variation across establishments and over time in exposure to anti-discrimination law to identify its causal effect. In addition, the researcher will estimate how these effects vary with the demographic background of the establishment's ownership. Data from the EEO-1 form include self-reported employment totals at the establishment level from 1971 to 2011 and are unique in that they include employment breakdowns by race, ethnicity, sex, and occupation. The researcher will benchmark these data with establishment employment totals by race and sex in the 1987 and 1992 Characteristics of Business Owners data."
"A new characterization of Conrad's property for group orderings, with
  applications","We provide a pure algebraic version of the dynamical characterization of
Conrad's property. This approach allows dealing with general group actions on
totally ordered spaces. As an application, we give a new and somehow
constructive proof of a theorem first established by Linnell: an orderable
group having infinitely many orderings has uncountably many. This proof is
achieved by extending to uncountable orderable groups a result about orderings
which may be approximated by their conjugates. This last result is illustrated
by an example of an exotic ordering on the free group given in the Appendix.","['Adam Clay', 'Andrés Navas', 'Cristóbal Rivas']",[],0,arXiv,http://arxiv.org/abs/0901.0880v2,False,True,False,False,False,1338,Conrad C Miller,Berkeley,Completed,2015,2018.0,"This project will estimate the effects of affirmative action regulation and equal employment opportunity law – what are referred to as anti-discrimination laws – have on establishment productivity. To measure these effects, the researcher will measure total factor productivity and labor productivity using Economic Census data, and exploit variation across establishments and over time in exposure to anti-discrimination law to identify its causal effect. In addition, the researcher will estimate how these effects vary with the demographic background of the establishment's ownership. Data from the EEO-1 form include self-reported employment totals at the establishment level from 1971 to 2011 and are unique in that they include employment breakdowns by race, ethnicity, sex, and occupation. The researcher will benchmark these data with establishment employment totals by race and sex in the 1987 and 1992 Characteristics of Business Owners data."
"Advancing Ischemic Stroke Diagnosis: A Novel Two-Stage Approach for
  Blood Clot Origin Identification","An innovative two-stage methodology for categorizing blood clot origins is
presented in this paper, which is important for the diagnosis and treatment of
ischemic stroke. First, a background classifier based on MobileNetV3 segments
big whole-slide digital pathology images into numerous tiles to detect the
presence of cellular material. After that, different pre-trained image
classification algorithms are fine-tuned to determine the origin of blood
clots. Due to complex blood flow dynamics and limitations in conventional
imaging methods such as computed tomography (CT), magnetic resonance imaging
(MRI), and ultrasound, identifying the sources of blood clots is a challenging
task. Although these techniques are useful for identifying blood clots, they
are not very good at determining how they originated. To address these
challenges, our method makes use of robust computer vision models that have
been refined using information from whole-slide digital pathology images. Out
of all the models tested, the PoolFormer \cite{yu2022metaformer} performs
better than the others, with 93.4\% accuracy, 93.4\% precision, 93.4\% recall,
and 93.4\% F1-score. Moreover, it achieves the good weighted multi-class
logarithmic loss (WMCLL) of 0.4361, which emphasizes how effective it is in
this particular application. These encouraging findings suggest that our
approach can successfully identify the origin of blood clots in a variety of
vascular locations, potentially advancing ischemic stroke diagnosis and
treatment approaches.","['Koushik Sivarama Krishnan', 'P. J. Joe Nikesh', 'Swathi Gnanasekar', 'Karthik Sivarama Krishnan']",[],0,arXiv,http://arxiv.org/abs/2304.13775v2,False,True,False,False,False,1356,Karthik Krishnan,Boston,Active,2020,,"The project will explore how access to various sources of financing can affect entrepreneurial start up activity. We will also analyze the relationship between access to financing and start up firm performance, productivity, employment growth, and sales, and the extent to which these performance measures relate to successful exit outcomes for private firms through acquisitions and IPOs. Further, prior literature has indicated that innovative startups are the ones that foster the greatest growth in employment and productivity. We will thus also analyze the relationship between access to financing and startup innovative activity. Since greater availability of financing will enhance opportunities for entrepreneurs to invest in their projects, we expect a positive relationship between access to financing and these outcome variables. We will also try to understand the differential effect of the various types of financing options available to startup firms and their impact on the above outcome variables. Moreover, we will test whether these different sources of funding are substitutes or complements (i.e., getting one type of financing allows a firm to obtain another type of financing later on). For instance, venture capitalists screen the investments through a detailed due-diligence process (e.g., Chemmanur, Krishnan, and Nandy (2011)). They may thus choose firms that are successful in obtaining seed financing from angels, which may potentially serve as another screening mechanism. Broadly, we expect to analyze the impact from three different sources of financing:  debt (e.g., bank loans); equity or stakeholder funded (e.g., angel, venture capital, and crowdfunding); and government awards (e.g., SBIR). 

The project will further investigate the extent to which non-employer businesses transition to employer firms, and if an exogenous shift in financing has an enhanced effect on these transitions. Further, an important milestone in the life of an entrepreneurial venture is the entrepreneur's choice of exit and the particular channel of exit, i.e. IPO or acquisition of the entrepreneurial firm. This is important from various perspectives. First, the evolution and the eventual success of a firm will require the ability to raise larger amounts of capital, potentially from public markets. Second, early stage investors such as angel financiers and venture capitalists typically provide capital and support to entrepreneurial ventures with the explicit or implicit understanding that they will exit the investment at a certain stage. Moreover, becoming a public firm or selling off one's venture is considered by many to be the final goal of entrepreneurship. Thus, at least from a financial perspective, in both the early and the later stages of a firm's life, the exit strategy is important. We will analyze if there are significant interactions between the exit strategy of firms and the extent and type of financing of the entrepreneurial firm. A related issue for an entrepreneurial firm is how to keep financing itself as it grows and how to choose between various debt and equity financing sources (which would shape the capital structure of the entrepreneurial firm)."
User Engagement and the Toxicity of Tweets,"Twitter is one of the most popular online micro-blogging and social
networking platforms. This platform allows individuals to freely express
opinions and interact with others regardless of geographic barriers. However,
with the good that online platforms offer, also comes the bad. Twitter and
other social networking platforms have created new spaces for incivility. With
the growing interest on the consequences of uncivil behavior online,
understanding how a toxic comment impacts online interactions is imperative. We
analyze a random sample of more than 85,300 Twitter conversations to examine
differences between toxic and non-toxic conversations and the relationship
between toxicity and user engagement. We find that toxic conversations, those
with at least one toxic tweet, are longer but have fewer individual users
contributing to the dialogue compared to the non-toxic conversations. However,
within toxic conversations, toxicity is positively associated with more
individual Twitter users participating in conversations. This suggests that
overall, more visible conversations are more likely to include toxic replies.
  Additionally, we examine the sequencing of toxic tweets and its impact on
conversations. Toxic tweets often occur as the main tweet or as the first
reply, and lead to greater overall conversation toxicity. We also find a
relationship between the toxicity of the first reply to a toxic tweet and the
toxicity of the conversation, such that whether the first reply is toxic or
non-toxic sets the stage for the overall toxicity of the conversation,
following the idea that hate can beget hate.","['Nazanin Salehabadi', 'Anne Groggel', 'Mohit Singhal', 'Sayak Saha Roy', 'Shirin Nilizadeh']",[],0,arXiv,http://arxiv.org/abs/2211.03856v1,False,True,False,False,False,1359,Alesha Istvan,Texas,Completed,2015,2018.0,"This project will investigate population characteristics surrounding food manufacturing establishments in the United States; comparing populations located around different establishments as well as populations over time surrounding the same food manufacturing establishment. The researcher will develop social vulnerability scores for populations at relatively small geographical levels. These scores will be used in multilevel analyses of the relationship between reported toxic emissions from food manufacturing establishments and the characteristics of the adjacent populations. Results from this project will be used in the completion of a doctoral dissertation as well as for academic articles prepared for publication and academic conference presentation. This project will benefit the census as authorized under Title 13, Chapter 5 using criterion 11 which refers to preparing population estimates as well as criterion 3 which refers to increasing the utility of Census Bureau Data. This research will greatly increase the utility of the American Community Survey, which is currently the best source of geographically specific, national-level information on the social, demographic, and economic characteristics of individuals and households. Furthermore, social vulnerability scores that will be developed for this project can be used to further understand and examine census hard to count populations."
Understanding the Bystander Effect on Toxic Twitter Conversations,"In this study, we explore the power of group dynamics to shape the toxicity
of Twitter conversations. First, we examine how the presence of others in a
conversation can potentially diffuse Twitter users' responsibility to address a
toxic direct reply. Second, we examine whether the toxicity of the first direct
reply to a toxic tweet in conversations establishes the group norms for
subsequent replies. By doing so, we outline how bystanders and the tone of
initial responses to a toxic reply are explanatory factors which affect whether
others feel uninhibited to post their own abusive or derogatory replies. We
test this premise by analyzing a random sample of more than 156k tweets
belonging to ~9k conversations. Central to this work is the social
psychological research on the ""bystander effect"" documenting that the presence
of bystanders has the power to alter the dynamics of a social situation. If the
first direct reply reaffirms the divisive tone, other replies may follow suit.
We find evidence of a bystander effect, with our results showing that an
increased number of users participating in the conversation before receiving a
toxic tweet is negatively associated with the number of Twitter users who
responded to the toxic reply in a non-toxic way. We also find that the initial
responses to toxic tweets within conversations is of great importance. Posting
a toxic reply immediately after a toxic comment is negatively associated with
users posting non-toxic replies and Twitter conversations becoming increasingly
toxic.","['Ana Aleksandric', 'Mohit Singhal', 'Anne Groggel', 'Shirin Nilizadeh']",[],0,arXiv,http://arxiv.org/abs/2211.10764v1,False,True,False,False,False,1359,Alesha Istvan,Texas,Completed,2015,2018.0,"This project will investigate population characteristics surrounding food manufacturing establishments in the United States; comparing populations located around different establishments as well as populations over time surrounding the same food manufacturing establishment. The researcher will develop social vulnerability scores for populations at relatively small geographical levels. These scores will be used in multilevel analyses of the relationship between reported toxic emissions from food manufacturing establishments and the characteristics of the adjacent populations. Results from this project will be used in the completion of a doctoral dissertation as well as for academic articles prepared for publication and academic conference presentation. This project will benefit the census as authorized under Title 13, Chapter 5 using criterion 11 which refers to preparing population estimates as well as criterion 3 which refers to increasing the utility of Census Bureau Data. This research will greatly increase the utility of the American Community Survey, which is currently the best source of geographically specific, national-level information on the social, demographic, and economic characteristics of individuals and households. Furthermore, social vulnerability scores that will be developed for this project can be used to further understand and examine census hard to count populations."
Equity in the Distribution of Regulatory PM2.5 Monitors,"Unequal exposure to air pollution by race and socioeconomic status is
well-documented in the U.S. However, there has been relatively little research
on inequities in the collection of PM2.5 data, creating a critical gap in
understanding which neighborhood exposures are represented in these datasets.
In this study we use multilevel models with random intercepts by county and
state, stratified by urbanicity to investigate the association between six key
environmental justice (EJ) attributes (%AIAN, %Asian %Black, %Hispanic, %White,
%Poverty) and proximity to the nearest regulatory monitor at the census
tract-level across the contiguous 48 states. We also separately stratify our
models by EPA region. Our results show that most EJ attributes exhibit weak or
statistically insignificant associations with monitor proximity, except in
rural areas where higher poverty levels are significantly linked to greater
monitor distances ($\beta$ = 0.6, 95%CI = [0.49, 0.71]). While the US EPA's
siting criteria may be effective in ensuring equitable monitor distribution in
some contexts, the low density of monitors in rural areas may impact the
accuracy of national-level air pollution monitoring.","['Zoé Haskell-Craig', 'Kevin P. Josey', 'Patrick L. Kinney', 'Priyanka deSouza']",[],0,arXiv,http://arxiv.org/abs/2410.18692v1,False,True,False,False,False,1360,Ann L Owens,USC,Completed,2015,2019.0,"Hispanics comprise an increasing proportion of the U.S. population, particularly in urban areas, and thus profoundly shape urban neighborhoods. One recent phenomenon occurring in Hispanic neighborhoods is socioeconomic ascent--neighborhoods experiencing increases in residents' household income, rents, house values, educational and occupational attainment. This project seeks to understand the demographic processes underlying these changes using quantitative analyses of restricted-use Census and American Community Survey microdata on tracts in metropolitan areas in the U.S. Specifically, I will explore the demographic causes of Hispanic neighborhood socioeconomic ascent--whether ascent is due to long-time residents' fortunes improving, new residents moving in, or exit of longtime residents--and how these processes vary across metropolitan areas. Documenting these changes in Hispanic neighborhoods will provide a new perspective on inequality in metropolitan areas as the U.S. population becomes increasingly diverse, and it will generate hypotheses for further research about how living in these neighborhoods is shaping the lives of Hispanics."
"Global Impact of COVID-19 Restrictions on the Atmospheric Concentrations
  of Nitrogen Dioxide and Ozone","Social-distancing to combat the COVID-19 pandemic has led to widespread
reductions in air pollutant emissions. Quantifying these changes requires a
business as usual counterfactual that accounts for the synoptic and seasonal
variability of air pollutants. We use a machine learning algorithm driven by
information from the NASA GEOS-CF model to assess changes in nitrogen dioxide
(NO$_{2}$) and ozone (O$_{3}$) at 5,756 observation sites in 46 countries from
January through June 2020. Reductions in NO$_{2}$ correlate with timing and
intensity of COVID-19 restrictions, ranging from 60% in severely affected
cities (e.g., Wuhan, Milan) to little change (e.g., Rio de Janeiro, Taipei). On
average, NO$_{2}$ concentrations were 18% lower than business as usual from
February 2020 onward. China experienced the earliest and steepest decline, but
concentrations since April have mostly recovered and remained within 5% to the
business as usual estimate. NO$_{2}$ reductions in Europe and the US have been
more gradual with a halting recovery starting in late March. We estimate that
the global NO$_{x}$ (NO+NO$_{2}$) emission reduction during the first 6 months
of 2020 amounted to 2.9 TgN, equivalent to 5.1% of the annual anthropogenic
total. The response of surface O$_{3}$ is complicated by competing influences
of non-linear atmospheric chemistry. While surface O$_{3}$ increased by up to
50% in some locations, we find the overall net impact on daily average O$_{3}$
between February - June 2020 to be small. However, our analysis indicates a
flattening of the O$_{3}$ diurnal cycle with an increase in night time ozone
due to reduced titration and a decrease in daytime ozone, reflecting a
reduction in photochemical production. The O$_{3}$ response is dependent on
season, time scale, and environment, with declines in surface O$_{3}$
forecasted if NO$_{x}$ emission reductions continue.","['Christoph A. Keller', 'Mat. J. Evans', 'K. Emma Knowland', 'Christa A. Hasenkopf', 'Sruti Modekurty', 'Robert A. Lucchesi', 'Tomohiro Oda', 'Bruno B. Franca', 'Felipe C. Mandarino', 'M. Valeria Díaz Suárez', 'Robert G. Ryan', 'Luke H. Fakes', 'Steven Pawson']",[],0,arXiv,http://arxiv.org/abs/2008.01127v1,False,True,False,False,False,1381,Robert J Kurtzman,Fed Board,Completed,2017,2022.0,"In this research, we present accounting decompositions of changes in aggregate labor and capital productivity. Such decompositions are a useful tool for researchers looking to assess the role of distortions to the distribution of labor or capital across firms in driving the dynamics of productivity and other aggregates over the business cycle. These decompositions can be used to test whether firm-level behavior in models with frictions is consistent with firm-level behavior in data, or to help guide model selection. Our simplest decomposition breaks changes in an aggregate factor productivity ratio into two components: a mean component, which captures common changes to firm factor productivity ratios, and a dispersion component, which captures changes in the higher order moments of the distribution of firm factor productivity ratios. We demonstrate analytically, in a model of frictions to firm labor and capital choices, that the dispersion component reflects changes in the extent of distortions to firm factor input allocations across firms. We then present results on our decomposition using data on non-financial public firms from the United States and Japan. For aggregate labor productivity, we find the dispersion component is relatively constant over the business cycle, but the mean component moves closely with movements in aggregate labor productivity."
A Generator for Generalized Inverse Gaussian Distributions,"We propose a new generator for the generalized inverse Gaussian (GIG)
distribution by decomposing the density of GIG into two components. The first
component is a truncated inverse Gamma density, in order to sample from which
we improve the traditional inverse CDF method. The second component is the
product of an exponential pdf and an inverse Gamma CDF. In order to sample from
this quasi-density, we develop a rejection sampling procedure that adaptively
adjusts the piecewise proposal density according to the user-specified
rejection rate or the desired number of cutoff points. The resulting complete
algorithm enjoys controllable rejection rate and moderate setup time. It
preserves efficiency for both parameter varying case and large sample case.","['Xiaozhu Zhang', 'Jerome P. Reiter']",[],0,arXiv,http://arxiv.org/abs/2211.13049v1,False,True,False,False,False,1385,Jerome P Reiter,Triangle,Completed,2015,2019.0,"Nicole Dalzell will carry out project-specific Ph.D. research supervised by RDC researcher Jerry Reiter and coordinated with Project PI Gale Boyd.  One goal of this work is to create either a linked file or programming code that will assist other researchers on ARTS-934 (CES project 1213) link internal Census Bureau data files to external datasets.  Dr. Reiter will supervise Nicole's research and not perform any active research on the project.  His addition to the project will allow him to see Nicole's work without it having to be requested for release, minimizing disclosure risk.  This research requires access to ASM, CMF, MECS, and SSEL data, as well as external datasets as described in the original ARTS-934 proposal.  Any record linkages between internal Census datasets or between internal and external data will occur as described in the project proposal at the establishment and/or firm level for businesses, or by geography for certain external datasets.  As described above, the main research task will be probabilistic record linkages between external datasets described in the project proposal, especially the IAC database, and the internal datasets listed.  All external data to be used during the course of this research are described in the original ARTS-934 proposal."
"Surveying Residential Burglaries: A Case Study of Local Crime
  Measurement","We consider the problem of estimating the incidence of residential burglaries
that occur over a well-defined period of time within the 10 most populous
cities in North Carolina. Our analysis typifies some of the general issues that
arise in estimating and comparing local crime rates over time and for different
cities. Typically, the only information we have about crime incidence within
any particular city is what that city's police department tells us, and the
police can only count and describe the crimes that come to their attention. To
address this, our study combines information from police-based residential
burglary counts and the National Crime Victimization Survey to obtain interval
estimates of residential burglary incidence at the local level. We use those
estimates as a basis for commenting on the fragility of between-city and
over-time comparisons that often appear in both public discourse about crime
patterns.","['Robert Brame', 'Michael G. Turner', 'Raymond Paternoster']",[],0,arXiv,http://arxiv.org/abs/1204.6735v4,False,False,False,False,True,1392,Megan Bears Augustyn,Texas,Completed,2016,2019.0,"There is a rich literature examining the factors associated with help-seeking behaviors among victims of crime. However, the ability to draw conclusions from this body of research is hampered by small, highly selective (biased)  samples of victims and/or a limited focus on incident and individual-level factors. One implication is a dearth of information on the general and specific effects of socio-cultural factors that likely influence reporting behaviors among victims of violent crimes. The National Crime Victimization Survey (NCVS) contains a nationally representative sample of persons ages 12 and over and has the potential to overcome prior limitations by examining whether victims of intimate partner and sexual violence engage in two types of formal help-seeking behavior: reporting and accessing victim services. In addition to detailed information on incident and victim characteristics, the NCVS also contains geographic identifiers that enable a researcher to link incident-level and individual-level information with contextual factors such as proactive justice policies, resource availability, and community attitudes, in order to estimate how socio-cultural factors influence the help seeking pathways of victims of intimate partner and sexual assault."
"SOLBP: Second-Order Loopy Belief Propagation for Inference in Uncertain
  Bayesian Networks","In second-order uncertain Bayesian networks, the conditional probabilities
are only known within distributions, i.e., probabilities over probabilities.
The delta-method has been applied to extend exact first-order inference methods
to propagate both means and variances through sum-product networks derived from
Bayesian networks, thereby characterizing epistemic uncertainty, or the
uncertainty in the model itself. Alternatively, second-order belief propagation
has been demonstrated for polytrees but not for general directed acyclic graph
structures. In this work, we extend Loopy Belief Propagation to the setting of
second-order Bayesian networks, giving rise to Second-Order Loopy Belief
Propagation (SOLBP). For second-order Bayesian networks, SOLBP generates
inferences consistent with those generated by sum-product networks, while being
more computationally efficient and scalable.","['Conrad D. Hougen', 'Lance M. Kaplan', 'Magdalena Ivanovska', 'Federico Cerutti', 'Kumar Vijay Mishra', 'Alfred O. Hero III']",[],0,arXiv,http://arxiv.org/abs/2208.07368v1,False,True,False,False,False,1400,Lawrence D Schmidt,Boston,Completed,2017,2023.0,"This project aims at (i) measuring the reaction of production networks to various firm-specific or sector-specific shocks, and (ii) understanding how firms adjust their network position in anticipation of these shocks. This research relates to a growing body of work assessing whether significant aggregate fluctuations may originate from microeconomic shocks. While earlier work has focused on the linkages across sectors, with mixed results, the objective here is to estimate linkages within networks of firms. The Commodity Flow Survey, which is the main source of supply chain information produced by the Census Bureau, will be merged and compared with two publicly available sources of information on supply chain relationships: Compustat and the Federal Procurement Data System. A variety of sources of shocks will be considered, including natural disasters, power outages, trade shocks, government spending shocks, and credit-supply shocks. This project also builds on earlier work that considers the importance of switching costs for the propagation of firm-level shocks. The study of the degree of interdependencies between firms in production network is a key parameter to assess the vulnerability of the real economy to microeconomic shocks."
"The transmission of uncertainty shocks on income inequality: State-level
  evidence from the United States","In this paper, we explore the relationship between state-level household
income inequality and macroeconomic uncertainty in the United States. Using a
novel large-scale macroeconometric model, we shed light on regional disparities
of inequality responses to a national uncertainty shock. The results suggest
that income inequality decreases in most states, with a pronounced degree of
heterogeneity in terms of shapes and magnitudes of the dynamic responses. By
contrast, some few states, mostly located in the West and South census region,
display increasing levels of income inequality over time. We find that this
directional pattern in responses is mainly driven by the income composition and
labor market fundamentals. In addition, forecast error variance decompositions
allow for a quantitative assessment of the importance of uncertainty shocks in
explaining income inequality. The findings highlight that volatility shocks
account for a considerable fraction of forecast error variance for most states
considered. Finally, a regression-based analysis sheds light on the driving
forces behind differences in state-specific inequality responses.","['Manfred M. Fischer', 'Florian Huber', 'Michael Pfarrhofer']",[],0,arXiv,http://arxiv.org/abs/1806.08278v1,False,True,False,False,False,1403,Marina Gindelsky,Washington,Completed,2016,2019.0,"This study chooses models to best forecast several inequality measures, provide short-term forecasts, and examine the changing nature of the income distribution over the course of a year, as affected by survey design and business cycles. This research builds off a preliminary exercise with public-use data from the March Supplement of the Current Population Survey (CPS) to model and forecast income inequality. Extending that analysis with the use of restricted-access CPS data will improve earlier income inequality measures. Internally topcoded data will enable the researcher to accurately model the income distribution and provide better forecasts, as the majority of the changes in income inequality have occurred in the top income percentiles, i.e., precisely those data which are topcoded. This research also examines how the income distribution changes over the course of the year. The researcher will compare results obtained from the CPS March supplement to the rolling data available from the American Community Survey, administered monthly. Given the timing of the CPS survey (shortly before the tax filing deadline) and the number of questions on various income sources for the past calendar year, estimates obtained using the CPS are judged to most accurately represent the true income distribution of the United States."
"Lady and the Tramp Nextdoor: Online Manifestations of Economic
  Inequalities in the Nextdoor Social Network","From health to education, income impacts a huge range of life choices.
Earlier research has leveraged data from online social networks to study
precisely this impact. In this paper, we ask the opposite question: do
different levels of income result in different online behaviors? We demonstrate
it does. We present the first large-scale study of Nextdoor, a popular
location-based social network. We collect 2.6 Million posts from 64,283
neighborhoods in the United States and 3,325 neighborhoods in the United
Kingdom, to examine whether online discourse reflects the income and income
inequality of a neighborhood. We show that posts from neighborhoods with
different incomes indeed differ, e.g. richer neighborhoods have a more positive
sentiment and discuss crimes more, even though their actual crime rates are
much lower. We then show that user-generated content can predict both income
and inequality. We train multiple machine learning models and predict both
income (R-squared=0.841) and inequality (R-squared=0.77).","['Waleed Iqbal', 'Vahid Ghafouri', 'Gareth Tyson', 'Guillermo Suarez-Tangil', 'Ignacio Castro']",[],0,arXiv,http://arxiv.org/abs/2304.05232v2,False,True,False,False,False,1403,Marina Gindelsky,Washington,Completed,2016,2019.0,"This study chooses models to best forecast several inequality measures, provide short-term forecasts, and examine the changing nature of the income distribution over the course of a year, as affected by survey design and business cycles. This research builds off a preliminary exercise with public-use data from the March Supplement of the Current Population Survey (CPS) to model and forecast income inequality. Extending that analysis with the use of restricted-access CPS data will improve earlier income inequality measures. Internally topcoded data will enable the researcher to accurately model the income distribution and provide better forecasts, as the majority of the changes in income inequality have occurred in the top income percentiles, i.e., precisely those data which are topcoded. This research also examines how the income distribution changes over the course of the year. The researcher will compare results obtained from the CPS March supplement to the rolling data available from the American Community Survey, administered monthly. Given the timing of the CPS survey (shortly before the tax filing deadline) and the number of questions on various income sources for the past calendar year, estimates obtained using the CPS are judged to most accurately represent the true income distribution of the United States."
Mechanical Model of Personal Income Distribution,"A microeconomic model is developed, which accurately predicts the shape of
personal income distribution (PID) in the United States and the evolution of
the shape over time. The underlying concept is borrowed from geo-mechanics and
thus can be considered as mechanics of income distribution. The model allows
the resolution of empirical and definitional problems associated with personal
income measurements. It also serves as a firm fundament for definitions of
income inequality as secondary derivatives from personal income distribution.
It is found that in relative terms the PID in the US has not been changing
since 1947. Effectively, the Gini coefficient has been almost constant during
the last 60 years, as reported by the Census Bureau.",['Ivan O. Kitov'],[],0,arXiv,http://arxiv.org/abs/0903.0203v1,True,True,False,False,False,1403,Marina Gindelsky,Washington,Completed,2016,2019.0,"This study chooses models to best forecast several inequality measures, provide short-term forecasts, and examine the changing nature of the income distribution over the course of a year, as affected by survey design and business cycles. This research builds off a preliminary exercise with public-use data from the March Supplement of the Current Population Survey (CPS) to model and forecast income inequality. Extending that analysis with the use of restricted-access CPS data will improve earlier income inequality measures. Internally topcoded data will enable the researcher to accurately model the income distribution and provide better forecasts, as the majority of the changes in income inequality have occurred in the top income percentiles, i.e., precisely those data which are topcoded. This research also examines how the income distribution changes over the course of the year. The researcher will compare results obtained from the CPS March supplement to the rolling data available from the American Community Survey, administered monthly. Given the timing of the CPS survey (shortly before the tax filing deadline) and the number of questions on various income sources for the past calendar year, estimates obtained using the CPS are judged to most accurately represent the true income distribution of the United States."
Beating the House: Identifying Inefficiencies in Sports Betting Markets,"Inefficient markets allow investors to consistently outperform the market. To
demonstrate that inefficiencies exist in sports betting markets, we created a
betting algorithm that generates above market returns for the NFL, NBA, NCAAF,
NCAAB, and WNBA betting markets. To formulate our betting strategy, we
collected and examined a novel dataset of bets, and created a non-parametric
win probability model to find positive expected value situations. As the United
States Supreme Court has recently repealed the federal ban on sports betting,
research on sports betting markets is increasingly relevant for the growing
sports betting industry.","['Sathya Ramesh', 'Ragib Mostofa', 'Marco Bornstein', 'John Dobelman']",[],0,arXiv,http://arxiv.org/abs/1910.08858v2,False,True,False,False,False,1410,German Bet,Chicago,Completed,2015,2018.0,"This project will examine the effects of labor and capital adjustment costs on market competition and market structure. This question is of particular interest since capacity addition and withdrawal decisions are important strategic decisions that can have a significant impact on price and profitability in the short run. Moreover, given that investment is long-lived, it is a critical determinant of how the competitive environment evolves in the long-run. Although the empirical literature in industrial organization has widely explored the connection between market structure and the competitiveness of market outcomes, the literature connecting labor and capital adjustment costs and market competition is scarce. This project will attempt to fill this gap in the literature by conducting a detailed microeconomic analysis using plant-level data. The relationship between four main topics and adjustment costs will be studied: entry and exit, investment, market power, and technology adoption."
"Investigating the efficiency of the Asian handicap football betting
  market with ratings and Bayesian networks","Despite the massive popularity of the Asian Handicap (AH) football (soccer)
betting market, its efficiency has not been adequately studied by the relevant
literature. This paper combines rating systems with Bayesian networks and
presents the first published model specifically developed for prediction and
assessment of the efficiency of the AH betting market. The results are based on
13 English Premier League seasons and are compared to the traditional market,
where the bets are for win, lose or draw. Different betting situations have
been examined including a) both average and maximum (best available) market
odds, b) all possible betting decision thresholds between predicted and
published odds, c) optimisations for both return-on-investment and profit, and
d) simple stake adjustments to investigate how the variance of returns changes
when targeting equivalent profit in both traditional and AH markets. While the
AH market is found to share the inefficiencies of the traditional market, the
findings reveal both interesting differences as well as similarities between
the two.",['Anthony Constantinou'],[],0,arXiv,http://arxiv.org/abs/2003.09384v2,False,True,False,False,False,1410,German Bet,Chicago,Completed,2015,2018.0,"This project will examine the effects of labor and capital adjustment costs on market competition and market structure. This question is of particular interest since capacity addition and withdrawal decisions are important strategic decisions that can have a significant impact on price and profitability in the short run. Moreover, given that investment is long-lived, it is a critical determinant of how the competitive environment evolves in the long-run. Although the empirical literature in industrial organization has widely explored the connection between market structure and the competitiveness of market outcomes, the literature connecting labor and capital adjustment costs and market competition is scarce. This project will attempt to fill this gap in the literature by conducting a detailed microeconomic analysis using plant-level data. The relationship between four main topics and adjustment costs will be studied: entry and exit, investment, market power, and technology adoption."
"XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an
  Agent-Based Model of a Sports Betting Exchange","We present first results from the use of XGBoost, a highly effective machine
learning (ML) method, within the Bristol Betting Exchange (BBE), an open-source
agent-based model (ABM) designed to simulate a contemporary sports-betting
exchange with in-play betting during track-racing events such as horse races.
We use the BBE ABM and its array of minimally-simple bettor-agents as a
synthetic data generator which feeds into our XGBoost ML system, with the
intention that XGBoost discovers profitable dynamic betting strategies by
learning from the more profitable bets made by the BBE bettor-agents. After
this XGBoost training, which results in one or more decision trees, a
bettor-agent with a betting strategy determined by the XGBoost-learned decision
tree(s) is added to the BBE ABM and made to bet on a sequence of races under
various conditions and betting-market scenarios, with profitability serving as
the primary metric of comparison and evaluation. Our initial findings presented
here show that XGBoost trained in this way can indeed learn profitable betting
strategies, and can generalise to learn strategies that outperform each of the
set of strategies used for creation of the training data. To foster further
research and enhancements, the complete version of our extended BBE, including
the XGBoost integration, has been made freely available as an open-source
release on GitHub.","['Chawin Terawong', 'Dave Cliff']",[],0,arXiv,http://arxiv.org/abs/2401.06086v1,False,True,False,False,False,1410,German Bet,Chicago,Completed,2015,2018.0,"This project will examine the effects of labor and capital adjustment costs on market competition and market structure. This question is of particular interest since capacity addition and withdrawal decisions are important strategic decisions that can have a significant impact on price and profitability in the short run. Moreover, given that investment is long-lived, it is a critical determinant of how the competitive environment evolves in the long-run. Although the empirical literature in industrial organization has widely explored the connection between market structure and the competitiveness of market outcomes, the literature connecting labor and capital adjustment costs and market competition is scarce. This project will attempt to fill this gap in the literature by conducting a detailed microeconomic analysis using plant-level data. The relationship between four main topics and adjustment costs will be studied: entry and exit, investment, market power, and technology adoption."
"Firm dynamics in a closed, conserved economy: A model of size
  distribution of employment and related statistics","We address the issue of the distribution of firm size. To this end we propose
a model of firms in a closed, conserved economy populated with
zero-intelligence agents who continuously move from one firm to another. We
then analyze the size distribution and related statistics obtained from the
model. Our ultimate goal is to reproduce the well known statistical features
obtained from the panel study of the firms i.e., the power law in size (in
terms of income and/or employment), the Laplace distribution in the growth
rates and the slowly declining standard deviation of the growth rates
conditional on the firm size. First, we show that the model generalizes the
usual kinetic exchange models with binary interaction to interactions between
an arbitrary number of agents. When the number of interacting agents is in the
order of the system itself, it is possible to decouple the model. We provide
some exact results on the distributions. Our model easily reproduces the power
law. The fluctuations in the growth rate falls with increasing size following a
power law (with an exponent 1 whereas the data suggests that the exponent is
around 1/6). However, the distribution of the difference of the firm-size in
this model has Laplace distribution whereas the real data suggests that the
difference of the log sizes has the same distribution.",['Anindya S. Chakrabarti'],[],0,arXiv,http://arxiv.org/abs/1112.2168v1,False,True,False,False,False,1422,Lawrence F Warren,Minnesota,Completed,2015,2019.0,"This project will examine and compare the establishment-level responses of labor demand to productivity and business cycle fluctuations using the Annual Survey of Manufactures (ASM), Census of Manufactures (CM), and the Quarterly Survey of Plant Capacity Utilization (QPC). The growth rates of production hours per worker, revenue, and employment by establishment size and age classification will be calculated for both the QPC and ASM/CM. The research will document the correlation of these establishment-class growth rates in hours, productivity, and employment with aggregate and regional business conditions. Using multiple econometric techniques, the project will document the differences in volatility, correlation, and magnitudes of the hours and employment adjustments of establishments by age and size categories. The study will also provide estimates for the revenue productivity of establishments in the ASM/CM data, controlling for the endogeneity of productivity and intermediate input demand. In addition, this project will examine the quality of voluntary responses in the QPC relative to the mandatory ASM/CM in several ways. "
"Cognitive Endurance, Talent Selection, and the Labor Market Returns to
  Human Capital","Cognitive endurance -- the ability to sustain performance on a
cognitively-demanding task over time -- is thought to be a crucial productivity
determinant. However, a lack of data on this variable has limited researchers'
ability to understand its role for success in college and the labor market.
This paper uses college-admission-exam records from 15 million Brazilian high
school students to measure cognitive endurance based on changes in performance
throughout the exam. By exploiting exogenous variation in the order of exam
questions, I show that students are 7.1 percentage points more likely to
correctly answer a given question when it appears at the beginning of the day
versus the end (relative to a sample mean of 34.3%). I develop a method to
decompose test scores into fatigue-adjusted ability and cognitive endurance. I
then merge these measures into a higher-education census and the earnings
records of the universe of Brazilian formal-sector workers to quantify the
association between endurance and long-run outcomes. I find that cognitive
endurance has a statistically and economically significant wage return.
Controlling for fatigue-adjusted ability and other student characteristics, a
one-standard-deviation higher endurance predicts a 5.4% wage increase. This
wage return to endurance is sizable, equivalent to a third of the wage return
to ability. I also document positive associations between endurance and college
attendance, college quality, college graduation, firm quality, and other
outcomes. Finally, I show how systematic differences in endurance across
students interact with the exam design to determine the sorting of students to
colleges. I discuss the implications of these findings for the use of cognitive
assessments for talent selection and investments in interventions that build
cognitive endurance.",['Germán Reyes'],[],0,arXiv,http://arxiv.org/abs/2301.02575v1,False,True,False,False,False,1449,Erica Palmer,Texas,Completed,2016,2020.0,"This project seeks to provide estimates that establish the differences in union formation between the establishments of domestic firms and those of multinational ones, and the differences in labor compensation between unionized establishments and non-unionized ones of multinational and non-multinational firms, respectively. This research uses restricted establishment-level and firm-level data in order to determine geographic locations of plants and their workers’ compensation and benefits, as well as transactional trade data to determine whether firms engage in related-party trade and thus are multinational or not. The research also uses data on state right-to-work law and National Labor Relations Board election results."
"When invariants matter: The role of I1 and I2 in neural network models
  of incompressible hyperelasticity","For the formulation of machine learning-based material models, the usage of
invariants of deformation tensors is attractive, since this can a priori
guarantee objectivity and material symmetry. In this work, we consider
incompressible, isotropic hyperelasticity, where two invariants I1 and I2 are
required for depicting a deformation state. First, we aim at enhancing the
understanding of the invariants. We provide an explicit representation of the
set of invariants that are admissible, i.e. for which (I1, I2) a deformation
state does indeed exist. Furthermore, we prove that uniaxial and equi-biaxial
deformation correspond to the boundary of the set of admissible invariants.
Second, we study how the experimentally-observed behaviour of different
materials can be captured by means of neural network models of incompressible
hyperelasticity, depending on whether both I1 and I2 or solely one of the
invariants, i.e. either only I1 or only I2, are taken into account. To this
end, we investigate three different experimental data sets from the literature.
In particular, we demonstrate that considering only one invariant, either I1 or
I2, can allow for good agreement with experiments in case of small
deformations. In contrast, it is necessary to consider both invariants for
precise models at large strains, for instance when rubbery polymers are
deformed. Moreover, we show that multiaxial experiments are strictly required
for the parameterisation of models considering I2. Otherwise, if only data from
uniaxial deformation is available, significantly overly stiff responses could
be predicted for general deformation states. On the contrary, I1-only models
can make qualitatively correct predictions for multiaxial loadings even if
parameterised only from uniaxial data, whereas I2-only models are completely
incapable in even qualitatively capturing experimental stress data at large
deformations.","['Franz Dammaß', 'Karl A. Kalina', 'Markus Kästner']",[],0,arXiv,http://arxiv.org/abs/2503.20598v1,False,True,False,False,False,1450,Christopher M Clapp,Chicago,Active,2015,,"Communities across the country are implementing policies to address their increasing commuter congestion. These policies are relatively new and vary from city to city, so not much is known about their full effects. To evaluate different congestion reduction policies, this project will develop a discrete choice structural model of the joint decision of individual residence and commuting mode, given the characteristics of the housing market and commuting options. The model is estimated for the Washington, D.C. metropolitan area using individual-level, restricted-access data from the 1996-2013 American Community Surveys (ACS), which includes information on where individuals live and work, together with data on the structure of the transportation network, to map each individual’s optimal commute for each option in the individual's choice set. The mappings will create a dataset of commute options and characteristics that will be used to estimate the trade-offs that individuals make among consumption, housing amenities, and leisure when choosing a home and commuting mode pair. The model estimates will be used to simulate the effects of transportation policies that alter the financial and time costs of commuting. These policies include congestion pricing schemes, fuel or carbon taxes, and increased parking fees."
Sustained Online Amplification of COVID-19 Elites in the United States,"The ongoing, fluid nature of the COVID-19 pandemic requires individuals to
regularly seek information about best health practices, local community
spreading, and public health guidelines. In the absence of a unified response
to the pandemic in the United States and clear, consistent directives from
federal and local officials, people have used social media to collectively
crowdsource COVID-19 elites, a small set of trusted COVID-19 information
sources. We take a census of COVID-19 crowdsourced elites in the United States
who have received sustained attention on Twitter during the pandemic. Using a
mixed methods approach with a panel of Twitter users linked to public U.S.
voter registration records, we find that journalists, media outlets, and
political accounts have been consistently amplified around COVID-19, while
epidemiologists, public health officials, and medical professionals make up
only a small portion of all COVID-19 elites on Twitter. We show that COVID-19
elites vary considerably across demographic groups, and that there are notable
racial, geographic, and political similarities and disparities between various
groups and the demographics of their elites. With this variation in mind, we
discuss the potential for using the disproportionate online voice of
crowdsourced COVID-19 elites to equitably promote timely public health
information and mitigate rampant misinformation.","['Ryan J. Gallagher', 'Larissa Doroshenko', 'Sarah Shugars', 'David Lazer', 'Brooke Foucault Welles']",[],0,arXiv,http://arxiv.org/abs/2009.07255v1,False,True,False,False,False,1456,Sarah Gollust,Minnesota,Completed,2016,2020.0,This project examines the relationship between media messages about the Affordable Care Act (ACA) and health insurance enrollment. The researchers first examine the associations between media market-level characteristics of broadcast media and the market-level socio-demographics of the populations plausibly exposed to those media in late 2013. They then examine the associations between the volume and tone of media messages about the ACA with changes in insurance enrollment from 2013 to 2014/2015. They do so by estimating individual-level models of insurance coverage on indicator variables for the post-ACA period interacted with the market-level variables and a host of state- and county-level controls. The study will contribute new understanding of an important health issue: the influence of news and advertising media on insurance enrollment during the implementation of the ACA.
Regular ovoids and Cameron-Liebler sets of generators in polar spaces,"Cameron-Liebler sets of generators in polar spaces were introduced a few
years ago as natural generalisations of the Cameron-Liebler sets of subspaces
in projective spaces. In this article we present the first two constructions of
non-trivial Cameron-Liebler sets of generators in polar spaces. Also regular
m-ovoids of k-spaces are introduced as a generalization of m-ovoids of polar
spaces. They are used in one of the aforementioned constructions of
Cameron-Liebler sets.","['Maarten De Boeck', ""Jozefien D'haeseleer"", 'Morgan Rodgers']",[],0,arXiv,http://arxiv.org/abs/2310.14739v1,False,True,False,False,False,1459,Carolyn A Liebler,Minnesota,Completed,2017,2022.0,"This project seeks to improve our understanding of the demographic and social processes that may affect responses to Census Bureau surveys. In particular, this project will examine responses to the race and Hispanic origin questions in the decennial censuses of 1960-2020 and the American Community Survey (ACS) of 2000-2023. The researcher will investigate demographic and social processes leading to longer-term changes in race and Hispanic origin responses (using non-linked decennial census files from 1960 to 2020, with supplementary data from the ACS and public data sources). The results will include estimates of populations who have changed race and/or Hispanic responses over the period. This project will also examine social and demographic processes leading to the choice of a race/Hispanic response for a child of an interracial marriage over the same period, generating estimates of characteristics of mixed-heritage populations giving each particular race/Hispanic response. Finally, this project will examine the demographic and social processes related to non-response among American Indians and Alaska Natives to the tribal affiliation question (within the race question) on the 1970-2020 decennial censuses and the 2000-2023 ACS. This research will utilize multiple multivariate regression approaches, as well as life table techniques, to estimate expected population sizes."
"Inventories, Demand Shocks Propagation and Amplification in Supply
  Chains","I study the role of industries' position in supply chains in shaping the
transmission of final demand shocks. First, I use a shift-share design based on
destination-specific final demand shocks and destination exposure shares to
show that shocks amplify upstream. Quantitatively, upstream industries respond
to final demand shocks up to three times as much as final goods producers, in
line with the bullwhip effect. To organize the reduced form results, I develop
a tractable production network model with inventories and study how the
properties of the network and the cyclicality of inventories interact to
determine whether final demand shocks amplify or dissipate upstream. I test the
mechanism by directly estimating the model-implied relationship between output
growth and demand shocks, mediated by network position and inventories. I find
evidence of the role of inventories in explaining heterogeneous output
elasticities. Finally, I use the model to quantitatively study how inventories
and network properties shape the volatility of the economy.",['Alessandro Ferrari'],[],0,arXiv,http://arxiv.org/abs/2205.03862v6,False,True,False,False,False,1506,Parag Mahajan,Philadelphia,Completed,2016,2021.0,"This project aims to assess whether economic, political, and natural disaster shocks abroad promoted migration from developing countries to the United States (and its states and municipalities) from 1960 until the present. This work augments and develops preliminary work suggesting a link between hurricanes in Central America and the Caribbean and subsequent migration inflows to the U.S. from those regions. Establishing such a relationship requires reliable, year-by-year estimates of migration flows from foreign countries into the United States. Surveys that ask respondents for their country of birth and year of entry yield counts that are likely to be noisy and unreliable for use in empirical work. This project seeks to exploit the richness of the full-count Long Form Decennial Censuses from 1970, 1980, 1990, and 2000, along with the full-count ACS surveys from 1996-2013 (and 2014-2018, when available). Access to these data allows construction of precise, year-by-year counts of immigrants. Furthermore, since the Long Form Census responses will not be constrained by categories such as “Other Caribbean,” the data will also increase the country-by-year sample size, allowing for more precise regression estimates."
"Do Black and Indigenous Communities Receive their Fair Share of Vaccines
  Under the 2018 CDC Guidelines","A major focus of debate about rationing guidelines for COVID-19 vaccines is
whether and how to prioritize access for minority populations that have been
particularly affected by the pandemic, and been the subject of historical and
structural disadvantage, particularly Black and Indigenous individuals. We
simulate the 2018 CDC Vaccine Allocation guidelines using data from the
American Community Survey under different assumptions on total vaccine supply.
Black and Indigenous individuals combined receive a higher share of vaccines
compared to their population share for all assumptions on total vaccine supply.
However, their vaccine share under the 2018 CDC guidelines is considerably
lower than their share of COVID-19 deaths and age-adjusted deaths. We then
simulate one method to incorporate disadvantage in vaccine allocation via a
reserve system. In a reserve system, units are placed into categories and units
reserved for a category give preferential treatment to individuals from that
category. Using the Area Deprivation Index (ADI) as a proxy for disadvantage,
we show that a 40% high-ADI reserve increases the number of vaccines allocated
to Black or Indigenous individuals, with a share that approaches their COVID-19
death share when there are about 75 million units. Our findings illustrate that
whether an allocation is equitable depends crucially on the benchmark and
highlight the importance of considering the expected distribution of outcomes
from implementing vaccine allocation guidelines.","['Parag A. Pathak', 'Harald Schmidt', 'Adam Solomon', 'Edwin Song', 'Tayfun Sönmez', 'M. Utku Ünver']",[],0,arXiv,http://arxiv.org/abs/2009.02853v1,False,False,False,False,True,1506,Parag Mahajan,Philadelphia,Completed,2016,2021.0,"This project aims to assess whether economic, political, and natural disaster shocks abroad promoted migration from developing countries to the United States (and its states and municipalities) from 1960 until the present. This work augments and develops preliminary work suggesting a link between hurricanes in Central America and the Caribbean and subsequent migration inflows to the U.S. from those regions. Establishing such a relationship requires reliable, year-by-year estimates of migration flows from foreign countries into the United States. Surveys that ask respondents for their country of birth and year of entry yield counts that are likely to be noisy and unreliable for use in empirical work. This project seeks to exploit the richness of the full-count Long Form Decennial Censuses from 1970, 1980, 1990, and 2000, along with the full-count ACS surveys from 1996-2013 (and 2014-2018, when available). Access to these data allows construction of precise, year-by-year counts of immigrants. Furthermore, since the Long Form Census responses will not be constrained by categories such as “Other Caribbean,” the data will also increase the country-by-year sample size, allowing for more precise regression estimates."
"The price elasticity of Gleevec in patients with Chronic Myeloid
  Leukemia enrolled in Medicare Part D: Evidence from a regression
  discontinuity design","Objective To assess the price elasticity of branded imatinib in chronic
myeloid leukemia (CML) patients on Medicare Part D to determine if high
out-of-pocket payments (OOP) are driving the substantial levels of
non-adherence observed in this population.
  Data sources and study setting We use data from the TriNetX Diamond Network
(TDN) United States database for the period from first availability in 2011
through the end of patent exclusivity following the introduction of generic
imatinib in early 2016.
  Study design We implement a fuzzy regression discontinuity design to
separately estimate the effect of Medicare Part D enrollment at age 65 on
adherence and OOP in newly-diagnosed CML patients initiating branded imatinib.
The corresponding price elasticity of demand (PED) is estimated and results are
assessed across a variety of specifications and robustness checks.
  Data collection/extraction methods Data from eligible patients following the
application of inclusion and exclusion criteria were analyzed.
  Principal findings Our analysis suggests that there is a significant increase
in initial OOP of $232 (95% Confidence interval (CI): $102 to $362) for
individuals that enrolled in Part D due to expanded eligibility at age 65. The
relatively smaller and non-significant decrease in adherence of only 6
percentage points (95% CI: -0.21 to 0.08) led to a PED of -0.02 (95% CI:
-0.056, 0.015).
  Conclusion This study provides evidence regarding the financial impact of
coinsurance-based benefit designs on Medicare-age patients with CML initiating
branded imatinib. Results indicate that factors besides high OOP are driving
the substantial non-adherence observed in this population and add to the
growing literature on PED for specialty drugs.","['Samantha E. Clark', 'Ruth Etzioni', 'Jerry Radich', 'Zachary Marcum', 'Anirban Basu']",[],0,arXiv,http://arxiv.org/abs/2305.06076v1,False,True,False,False,False,1510,Kyle J Caswell,Washington,Completed,2016,2019.0,"Medicare Savings Programs (MSP) provide financial assistance to participants for Medicare premiums and, in some cases, required cost sharing for medical services covered by Medicare. However, the rate of participation in these programs, as well as the individual characteristics associated with participation, is not well understood. This is in part because household surveys administered by the Census Bureau and other entities do not collect information on MSP participation, and administrative sources that identify MSP enrollment clearly exclude those eligible but not enrolled. This research attempts to fill this gap in data collection by using the Survey of Income and Program Participation (SIPP) linked with administrative data from the Center for Medicare and Medicaid Services (CMS). The administrative data will serve as the means to identify MSP enrollees among SIPP respondents, while the SIPP will identify the MSP eligible population using survey data on income, assets, and state of residence. Using these linked data, this work will study participation in the MSP programs and factors that influence participation."
"Conclusions from a NAIVE Bayes Operator Predicting the Medicare 2011
  Transaction Data Set","Introduction: The United States Federal Government operates one of the worlds
largest medical insurance programs, Medicare, to ensure payment for clinical
services for the elderly, illegal aliens and those without the ability to pay
for their care directly. This paper evaluates the Medicare 2011 Transaction
Data Set which details the transfer of funds from Medicare to private and
public clinical care facilities for specific clinical services for the
operational year 2011. Methods: Data mining was conducted to establish the
relationships between reported and computed transaction values in the data set
to better understand the drivers of Medicare transactions at a programmatic
level. Results: The models averaged 88 for average model accuracy and 38 for
average Kappa during training. Some reported classes are highly independent
from the available data as their predictability remains stable regardless of
redaction of supporting and contradictory evidence. DRG or procedure type
appears to be unpredictable from the available financial transaction values.
Conclusions: Overlay hypotheses such as charges being driven by the volume
served or DRG being related to charges or payments is readily false in this
analysis despite 28 million Americans being billed through Medicare in 2011 and
the program distributing over 70 billion in this transaction set alone. It may
be impossible to predict the dependencies and data structures the payer of last
resort without data from payers of first and second resort. Political concerns
about Medicare would be better served focusing on these first and second order
payer systems as what Medicare costs is not dependent on Medicare itself.",['Nick Williams'],[],0,arXiv,http://arxiv.org/abs/1403.7087v1,False,True,False,False,False,1510,Kyle J Caswell,Washington,Completed,2016,2019.0,"Medicare Savings Programs (MSP) provide financial assistance to participants for Medicare premiums and, in some cases, required cost sharing for medical services covered by Medicare. However, the rate of participation in these programs, as well as the individual characteristics associated with participation, is not well understood. This is in part because household surveys administered by the Census Bureau and other entities do not collect information on MSP participation, and administrative sources that identify MSP enrollment clearly exclude those eligible but not enrolled. This research attempts to fill this gap in data collection by using the Survey of Income and Program Participation (SIPP) linked with administrative data from the Center for Medicare and Medicaid Services (CMS). The administrative data will serve as the means to identify MSP enrollees among SIPP respondents, while the SIPP will identify the MSP eligible population using survey data on income, assets, and state of residence. Using these linked data, this work will study participation in the MSP programs and factors that influence participation."
Russian Agricultural Industry under Sanction Wars,"The motivation for focusing on economic sanctions is the mixed evidence of
their effectiveness. We assess the role of sanctions on the Russian
international trade flow of agricultural products after 2014. We use a
differences-in-differences model of trade flows data for imported and exported
agricultural products from 2010 to 2020 in Russia. The main expectation was
that the Russian economy would take a hit since it had lost its importers. We
assess the economic impact of the Russian food embargo on agricultural
commodities, questioning whether it has achieved its objective and resulted in
a window of opportunity for the development of the domestic agricultural
sector. Our results confirm that the sanctions have significantly impacted
foodstuff imports; they have almost halved in the first two years since the
sanctions were imposed. However, Russia has embarked on a path to reduce
dependence on food imports and managed self-sufficient agricultural production.","['Alexandra Lukyanova', 'Ayaz Zeynalov']",[],0,arXiv,http://arxiv.org/abs/2211.09205v2,False,True,False,False,False,1518,Sharat Ganapati,Georgetown,Active,2017,,"This project aims to gain a better understanding of the market structure of supply chains and the impacts of globalization with a focus on the agricultural sector. Agriculture supply chains involve four stages of production: farming, wholesaling, manufacturing, and retailing. Using a combination of data from the Census Bureau and USDA, this project constructs and analyzes the economic outcomes – market concentration, exports, prices, entry and exit, output and revenue, and the extent of vertical integration – at each stage of the supply chain. The project also examines how these outcomes respond at each stage of production in response to international shocks."
"(Unintended) Consequences of export restrictions on medical goods during
  the Covid-19 pandemic","In the first half of 2020, several countries have responded to the challenges
posed by the Covid-19 pandemic by restricting their export of medical supplies.
Such measures are meant to increase the domestic availability of critical
goods, and are commonly used in times of crisis. Yet, not much is known about
their impact, especially on countries imposing them. Here we show that export
bans are, by and large, counterproductive. Using a model of shock diffusion
through the network of international trade, we simulate the impact of
restrictions under different scenarios. We observe that while they would be
beneficial to a country implementing them in isolation, their generalized use
makes most countries worse off relative to a no-ban scenario. As a corollary,
we estimate that prices increase in many countries imposing the restrictions.
We also find that the cost of restraining from export bans is small, even when
others continue to implement them. Finally, we document a change in countries'
position within the international trade network, suggesting that export bans
have geopolitical implications.","['Marco Grassia', 'Giuseppe Mangioni', 'Stefano Schiavo', 'Silvio Traverso']",[],0,arXiv,http://arxiv.org/abs/2007.11941v1,False,True,False,False,False,1518,Sharat Ganapati,Georgetown,Active,2017,,"This project aims to gain a better understanding of the market structure of supply chains and the impacts of globalization with a focus on the agricultural sector. Agriculture supply chains involve four stages of production: farming, wholesaling, manufacturing, and retailing. Using a combination of data from the Census Bureau and USDA, this project constructs and analyzes the economic outcomes – market concentration, exports, prices, entry and exit, output and revenue, and the extent of vertical integration – at each stage of the supply chain. The project also examines how these outcomes respond at each stage of production in response to international shocks."
Decoupling multistep schemes for elliptic-parabolic problems,"We study the construction and convergence of decoupling multistep schemes of
higher order using the backward differentiation formulae for an
elliptic-parabolic problem, which includes multiple-network poroelasticity as a
special case. These schemes were first introduced in [Altmann, Maier, Unger,
BIT Numer. Math., 64:20, 2024], where a convergence proof for the second-order
case is presented. Here, we present a slightly modified version of these
schemes using a different construction of related time delay systems. We
present a novel convergence proof relying on concepts from G-stability
applicable for any order and providing a sharper characterization of the
required weak coupling condition. The key tool for the convergence analysis is
the construction of a weighted norm enabling a telescoping argument for the sum
of the errors.","['Robert Altmann', 'Abdullah Mujahid', 'Benjamin Unger']",[],0,arXiv,http://arxiv.org/abs/2407.18594v1,False,True,False,False,False,1519,Gabriel Unger,Boston,Completed,2016,2022.0,"This project investigates the impact of structural transformation on the process of regional economic convergence. An earlier literature documented strong evidence of regional convergence of incomes per capita throughout the United States up until the 1980s. That is, workers in poorer states were catching up to workers in the richer states, just as simple neoclassical growth models would predict. But, over the past 30 years, regional convergence amongst the U.S. states has dramatically diminished, presenting a puzzle for macroeconomists. Structural transformation, defined as both employment shifts between sectors and as the transformation of the production technology of any given sector may be a major cause of this convergence slow-down. This project estimates speeds of convergence for different sectors at new, more precise levels of geographic and industrial disaggregation as well as estimates the potential determinants of convergence speed, such as education, capital-intensity, technology, trade exposure, legal organization, R&D, and so on. These new empirical results will reveal whether and how different kinds of structural transformation might slow down the convergence process."
Sub 20 nm Short Channel Carbon Nanotube Transistors,"Carbon nanotube field-effect transistors with sub 20 nm long channels and
on/off current ratios of > 1000000 are demonstrated. Individual single-walled
carbon nanotubes with diameters ranging from 0.7 nm to 1.1 nm grown from
structured catalytic islands using chemical vapor deposition at 700 degree
Celsius form the channels. Electron beam lithography and a combination of HSQ,
calix[6]arene and PMMA e-beam resists were used to structure the short channels
and source and drain regions. The nanotube transistors display on-currents in
excess of 15 microA for drain-source biases of only 0.4 Volt.","['R. V. Seidel', 'A. P. Graham', 'J. Kretz', 'B. Rajasekharan', 'G. S. Duesberg', 'M. Liebau', 'E. Unger', 'F. Kreupl', 'W. Hoenlein']",[],0,arXiv,http://arxiv.org/abs/cond-mat/0411177v1,False,True,False,False,False,1519,Gabriel Unger,Boston,Completed,2016,2022.0,"This project investigates the impact of structural transformation on the process of regional economic convergence. An earlier literature documented strong evidence of regional convergence of incomes per capita throughout the United States up until the 1980s. That is, workers in poorer states were catching up to workers in the richer states, just as simple neoclassical growth models would predict. But, over the past 30 years, regional convergence amongst the U.S. states has dramatically diminished, presenting a puzzle for macroeconomists. Structural transformation, defined as both employment shifts between sectors and as the transformation of the production technology of any given sector may be a major cause of this convergence slow-down. This project estimates speeds of convergence for different sectors at new, more precise levels of geographic and industrial disaggregation as well as estimates the potential determinants of convergence speed, such as education, capital-intensity, technology, trade exposure, legal organization, R&D, and so on. These new empirical results will reveal whether and how different kinds of structural transformation might slow down the convergence process."
"FlexLLM: A System for Co-Serving Large Language Model Inference and
  Parameter-Efficient Finetuning","Parameter-efficient finetuning (PEFT) is a widely used technique to adapt
large language models for different tasks. Service providers typically create
separate systems for users to perform PEFT model finetuning and inference
tasks. This is because existing systems cannot handle workloads that include a
mix of inference and PEFT finetuning requests. As a result, shared GPU
resources are underutilized, leading to inefficiencies. To address this
problem, we present FlexLLM, the first system that can serve inference and
parameter-efficient finetuning requests in the same iteration. Our system
leverages the complementary nature of these two tasks and utilizes shared GPU
resources to run them jointly, using a method called co-serving. To achieve
this, FlexLLM introduces a novel token-level finetuning mechanism, which breaks
down the finetuning computation of a sequence into smaller token-level
computations and uses dependent parallelization and graph pruning, two static
compilation optimizations, to minimize the memory overhead and latency for
co-serving. Compared to existing systems, FlexLLM's co-serving approach reduces
the activation GPU memory overhead by up to 8x, and the end-to-end GPU memory
requirement of finetuning by up to 36% while maintaining a low inference
latency and improving finetuning throughput. For example, under a heavy
inference workload, FlexLLM can still preserve more than 80% of the peak
finetuning throughput, whereas existing systems cannot make any progress with
finetuning. The source code of FlexLLM is publicly available at
https://github.com/flexflow/FlexFlow.","['Xupeng Miao', 'Gabriele Oliaro', 'Xinhao Cheng', 'Mengdi Wu', 'Colin Unger', 'Zhihao Jia']",[],0,arXiv,http://arxiv.org/abs/2402.18789v1,False,True,False,False,False,1519,Gabriel Unger,Boston,Completed,2016,2022.0,"This project investigates the impact of structural transformation on the process of regional economic convergence. An earlier literature documented strong evidence of regional convergence of incomes per capita throughout the United States up until the 1980s. That is, workers in poorer states were catching up to workers in the richer states, just as simple neoclassical growth models would predict. But, over the past 30 years, regional convergence amongst the U.S. states has dramatically diminished, presenting a puzzle for macroeconomists. Structural transformation, defined as both employment shifts between sectors and as the transformation of the production technology of any given sector may be a major cause of this convergence slow-down. This project estimates speeds of convergence for different sectors at new, more precise levels of geographic and industrial disaggregation as well as estimates the potential determinants of convergence speed, such as education, capital-intensity, technology, trade exposure, legal organization, R&D, and so on. These new empirical results will reveal whether and how different kinds of structural transformation might slow down the convergence process."
Transaction Cost Analytics for Corporate Bonds,"The electronic platform has been increasingly popular for executing large
corporate bond orders by asset managers, who in turn have to assess the quality
of their executions via Transaction Cost Analysis (TCA). One of the challenges
in TCA is to build a realistic benchmark for the expected transaction cost and
to characterize the price impact of each individual trade with given bond
characteristics and market conditions.
  Taking the viewpoint of retail investors, this paper presents an analytical
methodology for TCA of corporate bond trading. Our analysis is based on the
TRACE Enhanced dataset; and starts with estimating the initiator of a bond
transaction, followed by estimating the bid-ask spread and the mid-price
dynamics. With these estimations, the first part of our study is to identify
key features for corporate bonds and to compute the expected average trading
cost. This part is on the time scale of weekly transactions, and is by applying
and comparing several regularized regression models. The second part of our
study is using the estimated mid-price dynamics to investigate the amplitude of
its price impact and the decay pattern of individual bond transaction. This
part is on the time scale of each transaction of liquid corporate bonds, and is
by applying a transient impact model to estimate the price impact kernel using
a non-parametric method.
  Our benchmark model allows for identifying abnormal transactions and for
enhancing counter-party selections. A key discovery of our study is the price
impact asymmetry between customer-buy orders and consumer-sell orders.","['Xin Guo', 'Charles-Albert Lehalle', 'Renyuan Xu']",[],0,arXiv,http://arxiv.org/abs/1903.09140v4,False,True,False,False,False,1528,Youngsuk Yook,Fed Board,Completed,2016,2023.0,"This project investigates how firms manage their liquid assets. The Census Bureau’s Quarterly Financial Report (QFR) provides information on the types and amount of firms’ liquid assets, including cash, deposits, commercial paper, government securities, and other short-term financial investments. The project also uses Compustat, Federal Reserve Economic Data, and the Federal Reserve Board’s Commercial Paper Statistical Release, to examine whether and how much the allocation among different liquid assets is explained by various firm characteristics, such as firm size, leverage, and financial constraints. The project also investigates whether firms holding more excess cash are likely to invest more in relatively risky liquid assets such as commercial paper and government securities. Finally, the project investigates whether the liquid asset composition is affected by the riskiness and liquidity of individual liquid assets."
"Liquidity Stress Testing in Asset Management -- Part 2. Modeling the
  Asset Liquidity Risk","This article is part of a comprehensive research project on liquidity risk in
asset management, which can be divided into three dimensions. The first
dimension covers liability liquidity risk (or funding liquidity) modeling, the
second dimension focuses on asset liquidity risk (or market liquidity)
modeling, and the third dimension considers the asset-liability management of
the liquidity gap risk (or asset-liability matching). The purpose of this
research is to propose a methodological and practical framework in order to
perform liquidity stress testing programs, which comply with regulatory
guidelines (ESMA, 2019, 2020) and are useful for fund managers. The review of
the academic literature and professional research studies shows that there is a
lack of standardized and analytical models. The aim of this research project is
then to fill the gap with the goal of developing mathematical and statistical
approaches, and providing appropriate answers.
  In this second article focused on asset liquidity risk modeling, we propose a
market impact model to estimate transaction costs. After presenting a toy model
that helps to understand the main concepts of asset liquidity, we consider a
two-regime model, which is based on the power-law property of price impact.
Then, we define several asset liquidity measures such as liquidity cost,
liquidation ratio and shortfall or time to liquidation in order to assess the
different dimensions of asset liquidity. Finally, we apply this asset liquidity
framework to stocks and bonds and discuss the issues of calibrating the
transaction cost model.","['Thierry Roncalli', 'Amina Cherief', 'Fatma Karray-Meziou', 'Margaux Regnault']",[],0,arXiv,http://arxiv.org/abs/2105.08377v1,False,True,False,False,False,1528,Youngsuk Yook,Fed Board,Completed,2016,2023.0,"This project investigates how firms manage their liquid assets. The Census Bureau’s Quarterly Financial Report (QFR) provides information on the types and amount of firms’ liquid assets, including cash, deposits, commercial paper, government securities, and other short-term financial investments. The project also uses Compustat, Federal Reserve Economic Data, and the Federal Reserve Board’s Commercial Paper Statistical Release, to examine whether and how much the allocation among different liquid assets is explained by various firm characteristics, such as firm size, leverage, and financial constraints. The project also investigates whether firms holding more excess cash are likely to invest more in relatively risky liquid assets such as commercial paper and government securities. Finally, the project investigates whether the liquid asset composition is affected by the riskiness and liquidity of individual liquid assets."
"Liquidity Stress Testing in Asset Management -- Part 3. Managing the
  Asset-Liability Liquidity Risk","This article is part of a comprehensive research project on liquidity risk in
asset management, which can be divided into three dimensions. The first
dimension covers the modeling of the liability liquidity risk (or funding
liquidity), the second dimension is dedicated to the modeling of the asset
liquidity risk (or market liquidity), whereas the third dimension considers the
management of the asset-liability liquidity risk (or asset-liability matching).
The purpose of this research is to propose a methodological and practical
framework in order to perform liquidity stress testing programs, which comply
with regulatory guidelines (ESMA, 2019, 2020) and are useful for fund managers.
In this third and last research paper focused on managing the asset-liability
liquidity risk, we explore the ALM tools that can be put in place to control
the liquidity gap. These ALM tools can be split into three categories:
measurement tools, management tools and monitoring tools. In terms of
measurement tools, we focus on the computation of the redemption coverage ratio
(RCR), which is the central instrument of liquidity stress testing programs. We
also study the redemption liquidation policy and the different implementation
methodologies, and we show how reverse stress testing can be developed. In
terms of liquidity management tools, we study the calibration of liquidity
buffers, the pros and cons of special arrangements (redemption suspensions,
gates, side pockets and in-kind redemptions) and the effectiveness of swing
pricing. In terms of liquidity monitoring tools, we compare the macro- and
micro-approaches of liquidity monitoring in order to identify the transmission
channels of liquidity risk.",['Thierry Roncalli'],[],0,arXiv,http://arxiv.org/abs/2110.01302v1,False,True,False,False,False,1528,Youngsuk Yook,Fed Board,Completed,2016,2023.0,"This project investigates how firms manage their liquid assets. The Census Bureau’s Quarterly Financial Report (QFR) provides information on the types and amount of firms’ liquid assets, including cash, deposits, commercial paper, government securities, and other short-term financial investments. The project also uses Compustat, Federal Reserve Economic Data, and the Federal Reserve Board’s Commercial Paper Statistical Release, to examine whether and how much the allocation among different liquid assets is explained by various firm characteristics, such as firm size, leverage, and financial constraints. The project also investigates whether firms holding more excess cash are likely to invest more in relatively risky liquid assets such as commercial paper and government securities. Finally, the project investigates whether the liquid asset composition is affected by the riskiness and liquidity of individual liquid assets."
Bertrand oligopoly in insurance markets with Value at Risk Constraints,"Since 2016 the operation of insurance companies in the European Union is
regulated by the Solvency II directive. According to the EU directive the
capital requirement should be calculated as a 99.5\% of Value at Risk. In this
study, we examine the impact of this capital requirement constraint on
equilibrium premiums and capitals. We discuss the case of the oligopoly
insurance market using Bertrand's model, assuming profit maximizing insurance
companies facing Value at Risk constraints. First we analyze companies'
decision on premium level. The companies strategic behavior can result positive
as well as negative expected profit for companies. The desired situation where
competition eliminate positive profit and lead the market to zero-profit state
is rare. Later we examine ex post and ax ante capital adjustments. Capital
adjustment does not rule out market anomalies, although somehow changes them.
Possibility of capital adjustment can lead the market to a situation where all
of the companies suffer loss. Allowing capital adjustment results monopolistic
premium level or market failure with positive probabilities.","['Kolos Csaba Ágoston', 'Veronika Varga']",[],0,arXiv,http://arxiv.org/abs/2404.17915v1,False,True,False,False,False,1532,Eva Labro,Triangle,Active,2017,,"This study examines how capital market forces affect incentive design and influence managerial decisions within firms. This project investigates differences in information provision and related incentives between public and private firms, examines the management practices and incentive structures within companies, and details the degree to which firm-level incentives translate into establishment-level outcomes. To that end, this project employs data from several Census Bureau surveys, as well as financial accounting information publicly disclosed by firms on U.S. Securities and Exchange Commission forms 10-K and 10-Q."
"Crime in Proportions: Applying Compositional Data Analysis to European
  Crime Trends for 2022","This article investigates crime patterns across European countries in 2022
using Compositional Data Analysis (CoDA) to address limitations of traditional
statistical approaches in dealing with the relative nature of crime data.
Recognizing crime types as components of a whole, we employ CoDA to explore
relationships between different crime categories while respecting their
inherent interdependencies. The study utilizes k-means clustering to group
countries based on their crime profiles, identifying three distinct clusters
largely aligning with geographical locations. This clustering is visualized
through t-SNE and geographic mapping, revealing regional similarities. Further
analysis using Robust Principal Component Analysis on identified crime clusters
reveals insightful relationships between specific crime types, such as
homicide, smuggling, and financial crimes, and how their prevalence varies
across countries. The findings reveals distinct crime patterns across Europe,
highlighting regional commonalities while also highlighting divergences like
Norway and Latvia that deviate from their expected geographical
classifications. Moreover, the study identifies specific crime groups; for
example, it pairs countries high in corruption and smuggling, such as Austria,
with those countries that exhibit a higher relevance to homicide and smuggling,
such as Luxembourg. It also points to the presence of financial crimes like
fraud in countries such as Romania and Estonia.","['Onur Batın Doğan', 'Fatma Sevinç Kurnaz']",[],0,arXiv,http://arxiv.org/abs/2502.12099v1,False,True,False,False,False,1549,Eric P Baumer,Penn State,Completed,2016,2024.0,"This project uses internal National Crime Victimization Survey (NCVS) data from 1996-2014 that include geographic codes (i.e., zip codes, census tracts, and counties), along with NCVS language files from 2007-2014 that include the interview language for respondents. While the publicly-available NCVS data provide a rich set of indicators, including the racial and ethnic identity of respondents in some detail, no additional items are available that might further illuminate potentially important nuances in experiences with crime or the police among ethnic minorities who differ on levels of acculturation to American society. Linking these data to the core internal NCVS files would yield valuable insights, both for the nature and quality of data collected in non-English interviews, and for scientific understanding of how language proficiency may shape victimization risk and decisions to notify the police. "
A Benchmark for Crime Surveillance Video Analysis with Large Models,"Anomaly analysis in surveillance videos is a crucial topic in computer
vision. In recent years, multimodal large language models (MLLMs) have
outperformed task-specific models in various domains. Although MLLMs are
particularly versatile, their abilities to understand anomalous concepts and
details are insufficiently studied because of the outdated benchmarks of this
field not providing MLLM-style QAs and efficient algorithms to assess the
model's open-ended text responses. To fill this gap, we propose a benchmark for
crime surveillance video analysis with large models denoted as UCVL, including
1,829 videos and reorganized annotations from the UCF-Crime and UCF-Crime
Annotation datasets. We design six types of questions and generate diverse QA
pairs. Then we develop detailed instructions and use OpenAI's GPT-4o for
accurate assessment. We benchmark eight prevailing MLLMs ranging from 0.5B to
40B parameters, and the results demonstrate the reliability of this bench.
Moreover, we finetune LLaVA-OneVision on UCVL's training set. The improvement
validates our data's high quality for video anomaly analysis.","['Haoran Chen', 'Dong Yi', 'Moyan Cao', 'Chensen Huang', 'Guibo Zhu', 'Jinqiao Wang']",[],0,arXiv,http://arxiv.org/abs/2502.09325v1,False,True,False,False,False,1549,Eric P Baumer,Penn State,Completed,2016,2024.0,"This project uses internal National Crime Victimization Survey (NCVS) data from 1996-2014 that include geographic codes (i.e., zip codes, census tracts, and counties), along with NCVS language files from 2007-2014 that include the interview language for respondents. While the publicly-available NCVS data provide a rich set of indicators, including the racial and ethnic identity of respondents in some detail, no additional items are available that might further illuminate potentially important nuances in experiences with crime or the police among ethnic minorities who differ on levels of acculturation to American society. Linking these data to the core internal NCVS files would yield valuable insights, both for the nature and quality of data collected in non-English interviews, and for scientific understanding of how language proficiency may shape victimization risk and decisions to notify the police. "
"Extinction dynamics from meta-stable coexistences in an evolutionary
  game","Deterministic evolutionary game dynamics can lead to stable coexistences of
different types. Stochasticity, however, drives the loss of such coexistences.
This extinction is usually accompanied by population size fluctuations. We
investigate the most probable extinction trajectory under such fluctuations by
mapping a stochastic evolutionary model to a problem of classical mechanics
using the Wentzel-Kramers-Brillouin (WKB) approximation. Our results show that
more abundant types in a coexistence can be more likely to go extinct first
well agreed with previous results, and also the distance between the
coexistence and extinction point is not a good predictor of extinction.
Instead, the WKB method correctly predicts the type going extinct first.","['Hye Jin Park', 'Arne Traulsen']",[],0,arXiv,http://arxiv.org/abs/1710.01339v1,False,True,False,False,False,1561,Hye Jin Rho,Michigan,Active,2018,,"This research seeks to investigate the relationship between longitudinal establishment-level characteristics and plant-level skill demands in the U.S. manufacturing industry as well as two additional occupations in the IT and healthcare industries—computer support specialists and laboratory technologists. What establishment characteristics predict high-skill demands and/or skill-related hiring difficulties? Although the issue of skills receives considerable attention in debates related to economic growth, unemployment, and income inequality, the existing literature rarely measures skills directly, instead relying on rough proxies for skill. Even in cases where more precise measures of skill are available, these measures are rarely linked to firm- or establishment-level data. We seek to address these limitations by linking manufacturing datasets from the Census Bureau with an external data that contains detailed measurements of manufacturing establishment-level skill demands. More specifically, we will link the 2012 MIT Production in the Innovation Economy (PIE) Manufacturing Survey to the Longitudinal Business Database, Annual Survey of Manufactures, including the Management and Organizational Practices Survey, Census of Manufactures, and National Employer Survey. The link will occur both at the establishment- and firm-level. In addition, we will link the 2015 Computer Support Specialist/IT Helpdesk National Skills Survey and Laboratory Technologist National Skills Survey (IT and healthcare industry equivalent survey to the PIE survey) to the Census of Services and Services Annual Survey, among others."
"Uncovering the Skillsets Required in Computer Science Jobs Using Social
  Network Analysis","The rapid growth of technology and computer science, which has led to a surge
in demand for skilled professionals in this field. The skill set required for
computer science jobs has evolved rapidly, creating challenges for those
already in the workforce who need to adapt their skills quickly to meet
industry demands. To stay ahead of the curve, it is essential to understand the
hottest skills needed in the field. The article introduces a new method for
analyzing job advertisements using social network analysis to identify the most
critical skills required by employers in the market. In this research, to form
the communication network of skills, first 5763 skills were collected from the
LinkedIn social network, then the relationship between skills was collected and
searched in 7777 computer science job advertisements, and finally, the balanced
communication network of skills was formed. The study analyzes the formed
communication network of skills in the computer science job market and
identifies four distinct communities of skills: Generalists, Infrastructure and
Security, Software Development, and Embedded Systems. The findings reveal that
employers value both hard and soft skills, such as programming languages and
teamwork. Communication skills were found to be the most important skill in the
labor market. Additionally, certain skills were highlighted based on their
centrality indices, including communication, English, SQL, Git, and business
skills, among others. The study provides valuable insights into the current
state of the computer science job market and can help guide individuals and
organizations in making informed decisions about skills acquisition and hiring
practices.",['Mehrdad Maghsoudi'],[],0,arXiv,http://arxiv.org/abs/2308.08582v1,False,True,False,False,False,1561,Hye Jin Rho,Michigan,Active,2018,,"This research seeks to investigate the relationship between longitudinal establishment-level characteristics and plant-level skill demands in the U.S. manufacturing industry as well as two additional occupations in the IT and healthcare industries—computer support specialists and laboratory technologists. What establishment characteristics predict high-skill demands and/or skill-related hiring difficulties? Although the issue of skills receives considerable attention in debates related to economic growth, unemployment, and income inequality, the existing literature rarely measures skills directly, instead relying on rough proxies for skill. Even in cases where more precise measures of skill are available, these measures are rarely linked to firm- or establishment-level data. We seek to address these limitations by linking manufacturing datasets from the Census Bureau with an external data that contains detailed measurements of manufacturing establishment-level skill demands. More specifically, we will link the 2012 MIT Production in the Innovation Economy (PIE) Manufacturing Survey to the Longitudinal Business Database, Annual Survey of Manufactures, including the Management and Organizational Practices Survey, Census of Manufactures, and National Employer Survey. The link will occur both at the establishment- and firm-level. In addition, we will link the 2015 Computer Support Specialist/IT Helpdesk National Skills Survey and Laboratory Technologist National Skills Survey (IT and healthcare industry equivalent survey to the PIE survey) to the Census of Services and Services Annual Survey, among others."
"Machine-learning classifiers for logographic name matching in public
  health applications: approaches for incorporating phonetic, visual, and
  keystroke similarity in large-scale probabilistic record linkage","Approximate string-matching methods to account for complex variation in
highly discriminatory text fields, such as personal names, can enhance
probabilistic record linkage. However, discriminating between matching and
non-matching strings is challenging for logographic scripts, where similarities
in pronunciation, appearance, or keystroke sequence are not directly encoded in
the string data. We leverage a large Chinese administrative dataset with known
match status to develop logistic regression and Xgboost classifiers integrating
measures of visual, phonetic, and keystroke similarity to enhance
identification of potentially-matching name pairs. We evaluate three methods of
leveraging name similarity scores in large-scale probabilistic record linkage,
which can adapt to varying match prevalence and information in supporting
fields: (1) setting a threshold score based on predicted quality of
name-matching across all record pairs; (2) setting a threshold score based on
predicted discriminatory power of the linkage model; and (3) using empirical
score distributions among matches and nonmatches to perform Bayesian adjustment
of matching probabilities estimated from exact-agreement linkage. In
experiments on holdout data, as well as data simulated with varying name error
rates and supporting fields, a logistic regression classifier incorporated via
the Bayesian method demonstrated marked improvements over exact-agreement
linkage with respect to discriminatory power, match probability estimation, and
accuracy, reducing the total number of misclassified record pairs by 21% in
test data and up to an average of 93% in simulated datasets. Our results
demonstrate the value of incorporating visual, phonetic, and keystroke
similarity for logographic name matching, as well as the promise of our
Bayesian approach to leverage name-matching within large-scale record linkage.","['Philip A. Collender', 'Zhiyue Tom Hu', 'Charles Li', 'Qu Cheng', 'Xintong Li', 'Yue You', 'Song Liang', 'Changhong Yang', 'Justin V. Remais']",[],0,arXiv,http://arxiv.org/abs/2001.01895v1,False,True,False,False,False,1583,Robert Belli,NCRN,Active,2016,,"The reengineering of the Survey of Income and Program Participation (SIPP) yielded a variety of innovations in the data collection, including the introduction of the Event History Calendar (EHC) method, the collection of paradata/auxiliary data, and dependent interviewing. Using the SIPP‐EHC field tests, this research project involves two research objectives. First, the research project will delve into the respondent retrieval processes by parsing the audit trails generated during EHC. Second, the research project will make significant use of paradata/auxiliary data (e.g., audit trails, contact history, sampling frame information) in order to identify data quality indicators and imputation variables."
Hadron Production at Fixed Target Energies and Extensive Air Showers,"NA61/SHINE is a fixed-target experiment to study hadron production in
hadron-nucleus and nucleus-nucleus collisions at the CERN SPS. Due to the very
good acceptance and particle identification in forward direction, NA61/SHINE is
well suited for measuring particle production to improve the reliability of air
shower simulations. Data with proton and pion beams have been taken in 2007 and
2009. First analysis results for the pion yield in proton-carbon interactions
at 31 GeV will be shown and compared to predictions from models used in air
shower simulations.",['M. Unger'],[],0,arXiv,http://arxiv.org/abs/1012.2604v1,False,True,False,False,False,1591,Gabriel Unger,Boston,Completed,2016,2022.0,"This project studies the decomposition of productivity growth into the respective contributions of entering firms, exiting firms, surviving firms as a whole, and market share shifts between survivors, using a new decomposition method (the Dynamic Olley-Pakes Decomposition). The study will document the extent to which this productivity decomposition differs across different sectors of the economy (e.g., whether the contribution of net entry to productivity growth in the manufacturing sector is roughly the same as the contribution of net entry to productivity growth in the construction sector) and whether other aspects of the reallocation process (like the role of financial constraint on firm exit) are different across different sectors. "
"Cosmic Strings in an Open Universe: Quantitative Evolution and
  Observational Consequences","The cosmic string scenario in an open universe is developed -- including the
equations of motion, a model of network evolution, the large angular scale CMB
temperature anisotropy, and the power spectrum of density fluctuations produced
by cosmic strings with dark matter. First we derive the equations of motion for
cosmic string in an open FRW space-time and construct a quantitative model of
the evolution of the gross features of a cosmic string network. Second, we
apply this model of network evolution to estimate the rms CMB temperature
anisotropy induced by cosmic strings, obtaining the normalization for the mass
per unit length $\mu$ as a function of $\Omega$. Third, we consider the effects
of the network evolution and normalization in an open universe on the large
scale structure formation scenarios with either cold or hot dark matter.","['P. P. Avelino', 'R. R. Caldwell', 'C. J. A. P. Martins']",[],0,arXiv,http://arxiv.org/abs/astro-ph/9708057v1,False,True,False,False,False,1594,Sydnee Caldwell,Berkeley,Active,2021,,"In the past three decades, the number of robots has increased by more than 400 percent in both Europe and the US (Acemoglu and Restrepo, 2017) . Automation has rapidly become one of the most debated political and economic issues in many countries. But why do firms decide to automate and what are the consequences of these decisions for workers and local communities?

This project will add to our understanding of the causes and consequences of automation by constructing firm-level measures of automation, which we will use to isolate the effects of automation on workers' employment trajectories and on their political behavior. We will leverage the fact that, because most industrial robots are imported from abroad, we can observe firms' purchases of robots in product-level trade data. 

The first part of our analysis will look at why firms decided to automate, focusing on two potential channels: (1) changes in the cost of labor, and (2) changes in the composition of the labor force. We will then analyze its impact on workers' employment and earnings trajectories, by comparing workers in firms that choose to automate with a matched sample of control workers. The final part of the project will link these effects to political outcomes. Are communities (or groups of individuals) that are more affected by automation more likely to start supporting policies that would shield them against labor market competition (including from robots)?  Are they more politically engaged?  We will answer these questions by comparing the evolution of vote shares and turnout in Congressional and Presidential elections in areas that were more or less exposed to automation."
Coarse Wage-Setting and Behavioral Firms,"This paper shows that the bunching of wages at round numbers is partly driven
by firm coarse wage-setting. Using data from over 200 million new hires in
Brazil, I first establish that contracted salaries tend to cluster at round
numbers. Then, I show that firms that tend to hire workers at round-numbered
salaries have worse market outcomes. Next, I develop a wage-posting model in
which optimization costs lead to the adoption of coarse rounded wages and
provide evidence supporting two model predictions using two research designs.
Finally, I examine some consequences of coarse wage-setting for relevant
economic outcomes.",['Germán Reyes'],[],0,arXiv,http://arxiv.org/abs/2206.01114v5,False,True,False,False,False,1616,Christina Patterson,Chicago,Active,2017,,"Using the LEHD and ACS data, this project explores firm-level wage compression, a practice in which lower productivity workers are paid more while higher productivity workers are paid less. This research will document how prevalent wage compression is in the U.S. economy, as well as the relationship between wage compression and the cyclical properties of nominal wages and the recent trends in earnings inequality. For example, wage compression within the firm may be related to the cyclical properties of the labor market if firms with more wage compression are more attune to fairness concerns and, therefore, are less likely to drop their wages in response to negative economic shocks. Furthermore, if there are differences in wage compression across firms, high ability workers will likely want to sort into the firms with less wage compression, and lower ability individuals will want to sort into higher wage compression firms where they are paid more. These incentives will lead to an increase in the sorting of workers across firms over time. Additionally, it may be that firms share their profits with workers, and that, because of fairness concerns, the firm shares the economic profits with all of its workers. Wage compression within the firm, therefore, affects how much differences in profits across firms can explain the rise in between-firm wage variability, even for people at the bottom of the skill distribution."
"Modeling the Evolutionary Trends in Corporate ESG Reporting: A Study
  based on Knowledge Management Model","Environmental, social, and governance (ESG) reports are globally recognized
as a keystone in sustainable enterprise development. However, current
literature has not concluded the development of topics and trends in ESG
contexts in the twenty-first century. Therefore, We selected 1114 ESG reports
from firms in the technology industry to analyze the evolutionary trends of ESG
topics by text mining. We discovered the homogenization effect towards low
environmental, medium governance, and high social features in the evolution. We
also designed a strategic framework to look closer into the dynamic changes of
firms' within-industry scores and across-domain importances. We found that
companies are gradually converging towards the third quadrant, which indicates
that firms contribute less to industrial outstanding and professional
distinctiveness in ESG reporting. Firms choose to imitate ESG reports from each
other to mitigate uncertainty and enhance behavioral legitimacy.","['Ziyuan Xia', 'Anchen Sun', 'Xiaodong Cai', 'Saixing Zeng']",[],0,arXiv,http://arxiv.org/abs/2309.07001v2,False,True,False,False,False,1632,Audrey M Guo,Stanford,Active,2017,,"The U.S. unemployment insurance (UI) program provides temporary monetary benefits to laid off workers, a program that millions of Americans utilize annually. Each state independently administers and finances their own UI program, determining funding schemes, rates, and program generosity within general federal guidelines. This project uses state-level variation in the financing of UI and other social insurance programs, such as workers’ compensation, to analyze the effect of social insurance financing on firm behavior and outcomes. How do firms respond to the funding mechanisms in their state and how does that feeds back into the economy as a whole?"
"Harnessing the Power of the Crowd to Increase Capacity for Data Science
  in the Social Sector","We present three case studies of organizations using a data science
competition to answer a pressing question. The first is in education where a
nonprofit that creates smart school budgets wanted to automatically tag budget
line items. The second is in public health, where a low-cost, nonprofit women's
health care provider wanted to understand the effect of demographic and
behavioral questions on predicting which services a woman would need. The third
and final example is in government innovation: using online restaurant reviews
from Yelp, competitors built models to forecast which restaurants were most
likely to have hygiene violations when visited by health inspectors. Finally,
we reflect on the unique benefits of the open, public competition model.","['Peter Bull', 'Isaac Slavitt', 'Greg Lipstein']",[],0,arXiv,http://arxiv.org/abs/1606.07781v1,False,True,False,False,False,1633,Benjamin M Marx,UIUC,Active,2017,,"The Urban Institute has estimated that nonprofit workers account for more than 8 percent of all income in the United States. This labor share has grown over time, as have estimates of the income and expenses of nonprofits as a share of national income. This project merges Census Bureau data on nearly all firms and establishments in participating states with IRS data on those firms that are nonprofits to describe the scope and growth of the nonprofit sector in the United States. This research examines the reasons for the growth of the nonprofit sector, the ways in which taxes and other policies affect the nonprofit sector, the nature of competition between for-profits and nonprofits, and the effects of nonprofits on their communities. "
Resource Allocation for 5G-UAV Based Emergency Wireless Communications,"For unforeseen natural disasters, such as earthquakes, hurricanes, and
floods, etc., the traditional communication infrastructure is unavailable or
seriously disrupted along with persistent secondary disasters. Under such
circumstances, it is highly demanded to deploy emergency wireless communication
(EWC) networks to restore connectivity in accident/incident areas. The emerging
fifth-generation (5G)/beyond-5G (B5G) wireless communication system, like
unmanned aerial vehicle (UAV) assisted networks and intelligent reflecting
surface (IRS) based communication systems, are expected to be designed or
re-farmed for supporting temporary high quality communications in post-disaster
areas. However, the channel characteristics of post-disaster areas quickly
change as the secondary disaster resulted topographical changes, imposing new
but critical challenges for EWC networks. In this paper, we propose a novel
heterogeneous $\mathcal{F}$ composite fading channel model for EWC networks
which accurately models and characterizes the composite fading channel with
reflectors, path-loss exponent, fading, and shadowing parameters in 5G-UAV
based EWC networks. Based on the model, we develop the optimal power allocation
scheme with the simple closed-form expression and the numerical results based
optimal joint bandwidth-power allocation scheme. We derive the corresponding
capacities and compare the energy efficiency between IRS and traditional relay
based 5G-UAVs. Numerical results show that the new heterogeneous
Fisher-Snedecor $\mathcal{F}$ composite fading channel adapted resource
allocation schemes can achieve higher capacity and energy efficiency than those
of traditional channel model adapted resource allocation schemes, thus
providing better communications service for post-disaster areas.","['Zhuohui Yao', 'Wenchi Cheng', 'Wei Zhang', 'Hailin Zhang']",[],0,arXiv,http://arxiv.org/abs/2407.17094v1,False,True,False,False,False,1652,Ishuwar Seetharam,Stanford,Completed,2017,2021.0,"What factors determine the adaptive capacity of organizations or the economy, when faced with unanticipated disruptive events? This research examines how the performance of establishments, firms, and the economy is affected by unanticipated shocks, including natural disasters and business cycle movements. This research aims to discern the characteristics of plants, firms, and local economies that determine their capacity to effectively respond – through adjustments in behavior, resource utilization, and technologies – to disruptions. Furthermore, this research will examine how responses to disruptions differ depending on the frequency and intensity of the disruption. For example, economic disasters such as recessions are infrequent, affect everyone, and are costly to the economy. There have only been five since the 1970s, but they have been widely studied and documented to be highly damaging. In contrast, natural disasters are more frequent in occurrence, local to certain geographies, and exhibit large variation in the extent of damage. While the economy-wide impact of natural disasters has also been studied, there exists limited plant- and firm-level evidence on the consequences of such frequent, unpredictable disasters; on the extent to which their impact differs from the repercussions of economic disasters; and the various characteristics that may determine the capacity to respond and adapt to uncertain and disruptive events. "
Multi-Hazard Bayesian Hierarchical Model for Damage Prediction,"A fundamental theoretical limitation undermines current disaster risk models:
existing approaches suffer from two critical constraints. First, conventional
damage prediction models remain predominantly deterministic, relying on fixed
parameters established through expert judgment rather than learned from data.
Second, probabilistic frameworks are fundamentally restricted by their
underlying assumption of hazard independence, which directly contradicts the
observed reality of cascading and compound disasters. By relying on fixed
expert parameters and treating hazards as independent phenomena, these models
dangerously misrepresent the true risk landscape. This work addresses this
challenge by developing the Multi-Hazard Bayesian Hierarchical Model (MH-BHM),
which reconceptualizes the classical risk equation beyond its deterministic
origins. The model's core theoretical contribution lies in reformulating a
classical risk formula as a fully probabilistic model that naturally
accommodates hazard interactions through its hierarchical structure while
preserving the traditional hazard-exposure-vulnerability framework. Using
tropical cyclone damage data (1952-2020) from the Philippines as a test case,
with out-of-sample validation on recent events (2020-2022), the model
demonstrates significant empirical advantages. Key findings include a reduction
in damage prediction error by 61% compared to a single-hazard model, and 80%
compared to a benchmark deterministic model. This corresponds to an improvement
in damage estimation accuracy of USD 0.8 billion and USD 2 billion,
respectively. The improved accuracy enables more effective disaster risk
management across multiple domains, from optimized insurance pricing and
national resource allocation to local adaptation strategies, fundamentally
improving society's capacity to prepare for and respond to disasters.",['Mary Lai O. Salvaña'],[],0,arXiv,http://arxiv.org/abs/2502.00658v1,False,True,False,False,False,1652,Ishuwar Seetharam,Stanford,Completed,2017,2021.0,"What factors determine the adaptive capacity of organizations or the economy, when faced with unanticipated disruptive events? This research examines how the performance of establishments, firms, and the economy is affected by unanticipated shocks, including natural disasters and business cycle movements. This research aims to discern the characteristics of plants, firms, and local economies that determine their capacity to effectively respond – through adjustments in behavior, resource utilization, and technologies – to disruptions. Furthermore, this research will examine how responses to disruptions differ depending on the frequency and intensity of the disruption. For example, economic disasters such as recessions are infrequent, affect everyone, and are costly to the economy. There have only been five since the 1970s, but they have been widely studied and documented to be highly damaging. In contrast, natural disasters are more frequent in occurrence, local to certain geographies, and exhibit large variation in the extent of damage. While the economy-wide impact of natural disasters has also been studied, there exists limited plant- and firm-level evidence on the consequences of such frequent, unpredictable disasters; on the extent to which their impact differs from the repercussions of economic disasters; and the various characteristics that may determine the capacity to respond and adapt to uncertain and disruptive events. "
The evolution of the game of baccarat,"The game of baccarat has evolved from a parlor game played by French
aristocrats in the first half of the 19th century to a casino game that
generated over US\$41 billion in revenue for the casinos of Macau in 2013. The
parlor game was originally a three-person zero-sum game. Later in the 19th
century it was simplified to a two-person zero-sum game. Early in the 20th
century the parlor game became a casino game, no longer zero-sum. In the mid
20th century, the strategic casino game became a nonstrategic game, with
players competing against the house instead of against each other. We argue
that this evolution was motivated by both economic and game-theoretic
considerations.","['S. N. Ethier', 'Jiyeon Lee']",[],0,arXiv,http://arxiv.org/abs/1308.1481v2,False,True,False,False,False,1664,Seth G Sanders,Triangle,Completed,2016,2022.0,"This research will use the unique nature of American Indian reservations, which constitute clearly defined local labor markets, to produce estimates describing how labor markets, housing markets, and migration respond to labor demand shocks in the context of limited in-migration. Labor demand shocks in the context of this study will be the opening of various American Indian-owned casino gaming operations on reservations over the past 35 years across the United States. Restricted-access American Community Survey (ACS) and Decennial Census data are used to test a model of spatial equilibrium with one-sided migration. The estimates will provide evidence on whether place-based development interventions can be effective in economically lagging localities, as well as the extent to which such interventions impose unanticipated externalities (positive or negative) on the surrounding economy. This research will also examine the fluidity of racial identification among American Indian and Alaska Native (AIAN) populations."
"Game Design Inspired by Quantum Physics: A Case Study on The Quantum
  Photo Booth","In this paper, we explain the conceptual development of the STAGE Lab Quantum
Casino (a.k.a. the STAGE Lab Quantum Arcade), one of the Lab's most recent
artistic endeavors about quantum physics. This work consists of a series of
card and digital games and an interactive experience, exposing the public to
quantum physics and minimizing learning barriers. Furthermore, we will also
present a case study of the interactive experience, in the form of The Quantum
Photo Booth.
  The STAGE Lab Quantum Casino provides an entertaining and approachable
experience for people of all ages to become familiar with quantum physics. By
using core concepts of quantum physics as tools and strategies to overcome
challenges that arise in gameplay, players gain an intuitive understanding of
these concepts. These games provide players with a first-hand experience of the
following quantum physics concepts: measurement, superposition, encryption,
decoherence, and entanglement. Instead of teaching the concepts through a
traditional classroom pedagogy, these games aim to invoke curiosity, spark
moments of playfulness, and catalyze play-centric learning modalities. This
paper provides a general overview of the development of the STAGE Lab Quantum
Casino, focusing on The Quantum Photo Booth experience and how science is
integrated into the very nature of the game development process in addition to
its outcome.","['Sunanda Prabhu Gaunkar', 'Denise Fischer', 'Filip Rozpędek', 'Umang Bhatia', 'Shobhit Verma', 'Ahit Kaan Tarhan', 'Uri Zvi', 'Nancy Kawalek']",[],0,arXiv,http://arxiv.org/abs/2402.13431v2,False,True,False,False,False,1664,Seth G Sanders,Triangle,Completed,2016,2022.0,"This research will use the unique nature of American Indian reservations, which constitute clearly defined local labor markets, to produce estimates describing how labor markets, housing markets, and migration respond to labor demand shocks in the context of limited in-migration. Labor demand shocks in the context of this study will be the opening of various American Indian-owned casino gaming operations on reservations over the past 35 years across the United States. Restricted-access American Community Survey (ACS) and Decennial Census data are used to test a model of spatial equilibrium with one-sided migration. The estimates will provide evidence on whether place-based development interventions can be effective in economically lagging localities, as well as the extent to which such interventions impose unanticipated externalities (positive or negative) on the surrounding economy. This research will also examine the fluidity of racial identification among American Indian and Alaska Native (AIAN) populations."
"Dynamic Fraud Detection: Integrating Reinforcement Learning into Graph
  Neural Networks","Financial fraud refers to the act of obtaining financial benefits through
dishonest means. Such behavior not only disrupts the order of the financial
market but also harms economic and social development and breeds other illegal
and criminal activities. With the popularization of the internet and online
payment methods, many fraudulent activities and money laundering behaviors in
life have shifted from offline to online, posing a great challenge to
regulatory authorities. How to efficiently detect these financial fraud
activities has become an urgent issue that needs to be resolved. Graph neural
networks are a type of deep learning model that can utilize the interactive
relationships within graph structures, and they have been widely applied in the
field of fraud detection. However, there are still some issues. First,
fraudulent activities only account for a very small part of transaction
transfers, leading to an inevitable problem of label imbalance in fraud
detection. At the same time, fraudsters often disguise their behavior, which
can have a negative impact on the final prediction results. In addition,
existing research has overlooked the importance of balancing neighbor
information and central node information. For example, when the central node
has too many neighbors, the features of the central node itself are often
neglected. Finally, fraud activities and patterns are constantly changing over
time, so considering the dynamic evolution of graph edge relationships is also
very important.","['Yuxin Dong', 'Jianhua Yao', 'Jiajing Wang', 'Yingbin Liang', 'Shuhan Liao', 'Minheng Xiao']",[],0,arXiv,http://arxiv.org/abs/2409.09892v1,False,True,False,False,False,1668,Jung Ho Choi,Stanford,Active,2017,,"Prior studies have investigated the effect of accounting fraud on various parties, including investors, top managers, consumers, and peer firms. However, the impact of accounting fraud on labor markets has received little attention, likely because of data limitations. The question this study addresses is whether the labor market outcomes of employees of accounting fraud firms could have been different if those firms had not been involved in accounting fraud. Using data from the Census Bureau's Longitudinal Employer-Household Dynamics program, Longitudinal Business Database, Annual Survey of Manufactures, and Census of Manufactures, we examine employment effects, such as wages and employee turnover, before, during, and after periods of fraudulent financial reporting. These data sets allow us to track employee information, such as wages and job switches, over time. To analyze the employment effects, we combine Census Bureau data with SEC enforcement actions against firms with serious misreporting. We find that, compared to a matched sample, employee wages decline during and after fraud, and that employment growth at fraud firms is positive during fraud periods and negative after. During fraud, managers overinvest in labor. Frauds cause informational opacity, and fraudulent reports tend to indicate good prospects, encouraging employees to still join the firm. After the fraud is revealed and the overemployment is unwound, employee wages fall due to turnover, with related job-search challenges and losses of firm-specific investments, and the stigma associated with the fraud. We use various subsamples to provide evidence for these mechanisms, showing that labor market disruptions and stigma have meaningful and negative consequences for employees."
TradingAgents: Multi-Agents LLM Financial Trading Framework,"Significant progress has been made in automated problem-solving using
societies of agents powered by large language models (LLMs). In finance,
efforts have largely focused on single-agent systems handling specific tasks or
multi-agent frameworks independently gathering data. However, multi-agent
systems' potential to replicate real-world trading firms' collaborative
dynamics remains underexplored. TradingAgents proposes a novel stock trading
framework inspired by trading firms, featuring LLM-powered agents in
specialized roles such as fundamental analysts, sentiment analysts, technical
analysts, and traders with varied risk profiles. The framework includes Bull
and Bear researcher agents assessing market conditions, a risk management team
monitoring exposure, and traders synthesizing insights from debates and
historical data to make informed decisions. By simulating a dynamic,
collaborative trading environment, this framework aims to improve trading
performance. Detailed architecture and extensive experiments reveal its
superiority over baseline models, with notable improvements in cumulative
returns, Sharpe ratio, and maximum drawdown, highlighting the potential of
multi-agent LLM frameworks in financial trading. TradingAgents is available at
https://github.com/PioneerFintech.","['Yijia Xiao', 'Edward Sun', 'Di Luo', 'Wei Wang']",[],0,arXiv,http://arxiv.org/abs/2412.20138v5,False,True,False,False,False,1670,Andre Kurmann,Philadelphia,Completed,2017,2024.0,"This project examines the role of increased trade exposure in the decline of entrepreneurship and the consequences for aggregate employment and productivity growth. This research is motivated by the burgeoning recent literature documenting that the rise in import competition from China and other low wage countries in the early 2000s has exerted important negative effects on employment, while simultaneously leading to increased technical change within firms and reallocation of employment towards more productive firms. This project addresses a set of important questions that have so far been left unexplored: What are the effects of increased trade exposure on startup rates and the post-entry dynamics of firms in terms of survival and employment growth? What is the role of offshoring in explaining these employment effects? How do firms react to increased trade exposure in terms of capital intensity, technical change, organization, and management practices? To what extent can increased trade exposure, through its impact on firm dynamics, account for the slowdown in aggregate employment and productivity growth observed in U.S. data? "
"An empirical test for Eurozone contagion using an asset-pricing model
  with heavy-tailed stochastic volatility","This paper proposes an empirical test of financial contagion in European
equity markets during the tumultuous period of 2008-2011. Our analysis shows
that traditional GARCH and Gaussian stochastic-volatility models are unable to
explain two key stylized features of global markets during presumptive
contagion periods: shocks to aggregate market volatility can be sudden and
explosive, and they are associated with specific directional biases in the
cross-section of country-level returns. Our model repairs this deficit by
assuming that the random shocks to volatility are heavy-tailed and correlated
cross-sectionally, both with each other and with returns. The fundamental
conclusion of our analysis is that great care is needed in modeling volatility
if one wishes to characterize the relationship between volatility and contagion
that is predicted by economic theory.
  In analyzing daily data, we find evidence for significant contagion effects
during the major EU crisis periods of May 2010 and August 2011, where contagion
is defined as excess correlation in the residuals from a factor model
incorporating global and regional market risk factors. Some of this excess
correlation can be explained by quantifying the impact of shocks to aggregate
volatility in the cross-section of expected returns - but only, it turns out,
if one is extremely careful in accounting for the explosive nature of these
shocks. We show that global markets have time-varying cross-sectional
sensitivities to these shocks, and that high sensitivities strongly predict
periods of financial crisis. Moreover, the pattern of temporal changes in
correlation structure between volatility and returns is readily interpretable
in terms of the major events of the periods in question.","['Nicholas G. Polson', 'James G. Scott']",[],0,arXiv,http://arxiv.org/abs/1110.5789v2,False,True,False,False,False,1675,Nathaniel A Pancost,Austin,Completed,2017,2022.0,"This research examines whether financial factors can explain the allocation of employment and capital across firms, and how that allocation affects aggregate productivity growth. Recent research has shown that differences in the allocation of resources across firms can explain differences in aggregate productivity across countries. Comparatively little research has focused on changes in the allocation of resources within a country, over time, or on the forces that affect this re-allocation. This projects seeks to answer three main questions. First, what determines a firm’s debt-to-asset ratio (leverage)? Second, what is the relationship between leverage and growth in the size of the firm? Third, what is the role of firm financial structure in aggregate productivity growth? "
"Long-Term Employment Effects of the Minimum Wage in Germany: New Data
  and Estimators","We study the long-term effects of the 2015 German minimum wage introduction
and its subsequent increases on regional employment. Using data from two waves
of the Structure of Earnings Survey allows us to estimate models that account
for changes in the minimum wage bite over time. While the introduction mainly
affected the labour market in East Germany, the raises are also increasingly
affecting low-wage regions in West Germany, such that around one third of
regions have changed their (binary) treatment status over time. We apply
different specifications and extensions of the classic
difference-in-differences approach as well as a set of new estimators that
enables for unbiased effect estimation with a staggered treatment adoption and
heterogeneous treatment effects. Our results indicate a small negative effect
on dependent employment of 0.5 percent, no significant effect on employment
subject to social security contributions, and a significant negative effect of
about 2.4 percent on marginal employment until the first quarter of 2022. The
extended specifications suggest additional effects of the minimum wage
increases, as well as stronger negative effects on total dependent and marginal
employment for those regions that were strongly affected by the minimum wage in
2015 and 2019.","['Marco Caliendo', 'Nico Pestel', 'Rebecca Olthaus']",[],0,arXiv,http://arxiv.org/abs/2310.15964v1,False,True,False,False,False,1680,Xi He,Maryland,Active,2020,,"This project will investigate wage and employment effects of mergers and acquisitions, and use mergers to test the law of one price in labor market. SDC Platinum and Compustat data will be merged using the SSEL to LEHD data, and an analysis of the data quality of firm dynamics in Title 13, Chapter 5 data will be analyzed. Estimates of the population will include results from semi-parametric difference-in-difference statistical estimates of the effects of M&As on target firms' employment and wage outcomes, and for public firms on post-merger stock price gains. The empirical framework will support testing if results are consistent with a ""breach of trust"" view of the relationship of workers and employers, where firms with a higher wage premium would experience a larger wage decline after the merger. Alternatively, if firm pay premiums reflect productivity differences, and workers are paid more at productive firms due to potential rent sharing, then a target firm that is acquired by a firm with higher wage premiums should experience an increase in wages. Estimates will also explore the effect of M&A on employment and earnings trajectories of workers in target firms over time."
A 22 percent increase in the German minimum wage: nothing crazy!,"We present the first empirical evidence on the 22 percent increase in the
German minimum wage, implemented in 2022, raising it from Euro 9.82 to 10.45 in
July and to Euro 12 in October. Leveraging the German Earnings Survey, a large
and novel data source comprising around 8 million employee-level observations
reported by employers each month, we apply a
difference-in-difference-in-differences approach to analyze the policy's impact
on hourly wages, monthly earnings, employment, and working hours. Our findings
reveal significant positive effects on wages, affirming the policy's intended
benefits for low-wage workers. Interestingly, we identify a negative effect on
working hours, mainly driven by minijobbers. The hours effect results in an
implied labor demand elasticity in terms of the employment volume of -0.2 which
only partially offsets the monthly wage gains. We neither observe a negative
effect on the individual's employment retention nor the regional employment
levels.","['Mario Bossler', 'Lars Chittka', 'Thorsten Schank']",[],0,arXiv,http://arxiv.org/abs/2405.12608v3,False,True,False,False,False,1680,Xi He,Maryland,Active,2020,,"This project will investigate wage and employment effects of mergers and acquisitions, and use mergers to test the law of one price in labor market. SDC Platinum and Compustat data will be merged using the SSEL to LEHD data, and an analysis of the data quality of firm dynamics in Title 13, Chapter 5 data will be analyzed. Estimates of the population will include results from semi-parametric difference-in-difference statistical estimates of the effects of M&As on target firms' employment and wage outcomes, and for public firms on post-merger stock price gains. The empirical framework will support testing if results are consistent with a ""breach of trust"" view of the relationship of workers and employers, where firms with a higher wage premium would experience a larger wage decline after the merger. Alternatively, if firm pay premiums reflect productivity differences, and workers are paid more at productive firms due to potential rent sharing, then a target firm that is acquired by a firm with higher wage premiums should experience an increase in wages. Estimates will also explore the effect of M&A on employment and earnings trajectories of workers in target firms over time."
Can ChatGPT evaluate research quality?,"Purpose: Assess whether ChatGPT 4.0 is accurate enough to perform research
evaluations on journal articles to automate this time-consuming task.
Design/methodology/approach: Test the extent to which ChatGPT-4 can assess the
quality of journal articles using a case study of the published scoring
guidelines of the UK Research Excellence Framework (REF) 2021 to create a
research evaluation ChatGPT. This was applied to 51 of my own articles and
compared against my own quality judgements. Findings: ChatGPT-4 can produce
plausible document summaries and quality evaluation rationales that match the
REF criteria. Its overall scores have weak correlations with my self-evaluation
scores of the same documents (averaging r=0.281 over 15 iterations, with 8
being statistically significantly different from 0). In contrast, the average
scores from the 15 iterations produced a statistically significant positive
correlation of 0.509. Thus, averaging scores from multiple ChatGPT-4 rounds
seems more effective than individual scores. The positive correlation may be
due to ChatGPT being able to extract the author's significance, rigour, and
originality claims from inside each paper. If my weakest articles are removed,
then the correlation with average scores (r=0.200) falls below statistical
significance, suggesting that ChatGPT struggles to make fine-grained
evaluations. Research limitations: The data is self-evaluations of a
convenience sample of articles from one academic in one field. Practical
implications: Overall, ChatGPT does not yet seem to be accurate enough to be
trusted for any formal or informal research quality evaluation tasks. Research
evaluators, including journal editors, should therefore take steps to control
its use. Originality/value: This is the first published attempt at
post-publication expert review accuracy testing for ChatGPT.",['Mike Thelwall'],[],0,arXiv,http://arxiv.org/abs/2402.05519v1,False,True,False,False,False,1681,Andreana Able,CensusHQ,Completed,2016,2018.0,"This project will examine a range of probabilistic multiple file matching procedures using the data sources provided by Census. These methods consider sets of records from different files simultaneously, utilizing all available information in order to estimate the probability that the various subsets of these records correspond to the same person. The results from these file matching techniques will be compared to results from current Census Bureau file matching practices."
Rate-Memory Trade-Off for Caching and Delivery of Correlated Sources,"This paper studies the fundamental limits of content delivery in a
cache-aided broadcast network for correlated content generated by a discrete
memoryless source with arbitrary joint distribution. Each receiver is equipped
with a cache of equal capacity, and the requested files are delivered over a
shared error-free broadcast link. A class of achievable correlation-aware
schemes based on a two-step source coding approach is proposed. Library files
are first compressed, and then cached and delivered using a combination of
correlation-unaware multiple-request cache-aided coded multicast schemes. The
first step uses Gray-Wyner source coding to represent the library via private
descriptions and descriptions that are common to more than one file. The second
step then becomes a multiple-request caching problem, where the demand
structure is dictated by the configuration of the compressed library, and it is
interesting in its own right. The performance of the proposed two-step scheme
is evaluated by comparing its achievable rate with a lower bound on the optimal
peak and average rate-memory tradeoffs in a two-file multiple-receiver network,
and in a three-file two-receiver network. Specifically, in a network with two
files and two receivers, the achievable rate matches the lower bound for a
significant memory regime and it is within half of the conditional entropy of
files for all other memory values. In the three-file two-receiver network, the
two-step strategy achieves the lower bound for large cache capacities, and it
is within half of the joint entropy of two of the sources conditioned on the
third one for all other cache sizes.","['Parisa Hassanzadeh', 'Antonia M. Tulino', 'Jaime Llorca', 'Elza Erkip']",[],0,arXiv,http://arxiv.org/abs/1806.07333v1,False,True,False,False,False,1681,Andreana Able,CensusHQ,Completed,2016,2018.0,"This project will examine a range of probabilistic multiple file matching procedures using the data sources provided by Census. These methods consider sets of records from different files simultaneously, utilizing all available information in order to estimate the probability that the various subsets of these records correspond to the same person. The results from these file matching techniques will be compared to results from current Census Bureau file matching practices."
"A calculation of a half-integral weight multiplier system on SU(2,1)","In this thesis, we construct a half-integral weight multiplier system on the
group SU(2,1). In order to do so, we first find a formula for a 2-cocycle
representing the double cover of SU(2,1)(k), where k is a local field. For each
non-archimedean local field k, we describe how the cocycle splits on a compact
open subgroup. The multiplier system is then expressed in terms of the product
of the local splittings at each prime.",['Lina Jalal'],[],0,arXiv,http://arxiv.org/abs/1107.1939v1,False,True,False,False,False,1691,Yichen Su,Dallas,Completed,2018,2022.0,"We use the Longitudinal Business Database to evaluate the effect of local labor demand shocks on economic outcomes in neighborhoods surrounding the affected locations. This research uses a series of highly localized exogenous variations in labor demand to examine its impact on local economic outcomes such as firm entries and exits, employment, local wages, rents, etc. We examine how such effects differ by the characteristics of the locations in which the labor demand shocks take place. This study provides insights on how local jobs are created, how an initial labor demand shock would propagate through local economies through the “multiplier effects”, and how the propagation process depends on the types of neighborhoods in which it takes place. By estimating the cross-industry local multiplier effects, this research also helps in understanding the nature of local production agglomeration. In addition, by estimating the effect of labor demand shocks on entries of local service firms and rents, the research also sheds light on the mechanism of local consumption agglomeration. "
"Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize
  Configuration Errors via Logs","Configurable software systems are prone to configuration errors, resulting in
significant losses to companies. However, diagnosing these errors is
challenging due to the vast and complex configuration space. These errors pose
significant challenges for both experienced maintainers and new end-users,
particularly those without access to the source code of the software systems.
Given that logs are easily accessible to most end-users, we conduct a
preliminary study to outline the challenges and opportunities of utilizing logs
in localizing configuration errors. Based on the insights gained from the
preliminary study, we propose an LLM-based two-stage strategy for end-users to
localize the root-cause configuration properties based on logs. We further
implement a tool, LogConfigLocalizer, aligned with the design of the
aforementioned strategy, hoping to assist end-users in coping with
configuration errors through log analysis.
  To the best of our knowledge, this is the first work to localize the
root-cause configuration properties for end-users based on Large Language
Models~(LLMs) and logs. We evaluate the proposed strategy on Hadoop by
LogConfigLocalizer and prove its efficiency with an average accuracy as high as
99.91%. Additionally, we also demonstrate the effectiveness and necessity of
different phases of the methodology by comparing it with two other variants and
a baseline tool. Moreover, we validate the proposed methodology through a
practical case study to demonstrate its effectiveness and feasibility.","['Shiwen Shan', 'Yintong Huo', 'Yuxin Su', 'Yichen Li', 'Dan Li', 'Zibin Zheng']",[],0,arXiv,http://arxiv.org/abs/2404.00640v2,False,True,False,False,False,1691,Yichen Su,Dallas,Completed,2018,2022.0,"We use the Longitudinal Business Database to evaluate the effect of local labor demand shocks on economic outcomes in neighborhoods surrounding the affected locations. This research uses a series of highly localized exogenous variations in labor demand to examine its impact on local economic outcomes such as firm entries and exits, employment, local wages, rents, etc. We examine how such effects differ by the characteristics of the locations in which the labor demand shocks take place. This study provides insights on how local jobs are created, how an initial labor demand shock would propagate through local economies through the “multiplier effects”, and how the propagation process depends on the types of neighborhoods in which it takes place. By estimating the cross-industry local multiplier effects, this research also helps in understanding the nature of local production agglomeration. In addition, by estimating the effect of labor demand shocks on entries of local service firms and rents, the research also sheds light on the mechanism of local consumption agglomeration. "
Network-Aware Optimization of Distributed Learning for Fog Computing,"Fog computing promises to enable machine learning tasks to scale to large
amounts of data by distributing processing across connected devices. Two key
challenges to achieving this goal are heterogeneity in devices compute
resources and topology constraints on which devices can communicate with each
other. We address these challenges by developing the first network-aware
distributed learning optimization methodology where devices optimally share
local data processing and send their learnt parameters to a server for
aggregation at certain time intervals. Unlike traditional federated learning
frameworks, our method enables devices to offload their data processing tasks
to each other, with these decisions determined through a convex data transfer
optimization problem that trades off costs associated with devices processing,
offloading, and discarding data points. We analytically characterize the
optimal data transfer solution for different fog network topologies, showing
for example that the value of offloading is approximately linear in the range
of computing costs in the network. Our subsequent experiments on testbed
datasets we collect confirm that our algorithms are able to improve network
resource utilization substantially without sacrificing the accuracy of the
learned model. In these experiments, we also study the effect of network
dynamics, quantifying the impact of nodes entering or exiting the network on
model learning and resource costs.","['Su Wang', 'Yichen Ruan', 'Yuwei Tu', 'Satyavrat Wagle', 'Christopher G. Brinton', 'Carlee Joe-Wong']",[],0,arXiv,http://arxiv.org/abs/2004.08488v4,False,True,False,False,False,1691,Yichen Su,Dallas,Completed,2018,2022.0,"We use the Longitudinal Business Database to evaluate the effect of local labor demand shocks on economic outcomes in neighborhoods surrounding the affected locations. This research uses a series of highly localized exogenous variations in labor demand to examine its impact on local economic outcomes such as firm entries and exits, employment, local wages, rents, etc. We examine how such effects differ by the characteristics of the locations in which the labor demand shocks take place. This study provides insights on how local jobs are created, how an initial labor demand shock would propagate through local economies through the “multiplier effects”, and how the propagation process depends on the types of neighborhoods in which it takes place. By estimating the cross-industry local multiplier effects, this research also helps in understanding the nature of local production agglomeration. In addition, by estimating the effect of labor demand shocks on entries of local service firms and rents, the research also sheds light on the mechanism of local consumption agglomeration. "
The Distributed Bloom Filter,"The Distributed Bloom Filter is a space-efficient, probabilistic data
structure designed to perform more efficient set reconciliations in distributed
systems. It guarantees eventual consistency of states between nodes in a
system, while still keeping bloom filter sizes as compact as possible. The
eventuality can be tweaked as desired, by tweaking the distributed bloom
filter's parameters. The scalability, as well as accuracy of the data structure
is made possible by combining two novel ideas: The first idea introduces a new,
computationally inexpensive way for populating bloom filters, making it
possible to quickly compute new bloom filters when interacting with peers. The
second idea introduces the concept of unique bloom filter mappings between
peers. By applying these two simple ideas, one can achieve incredibly
bandwidth-efficient set reconciliation in networks. Instead of trying to
minimize the false positive rate of a single bloom filter, we use the unique
bloom filter mappings to increase the probability for an element to propagate
through a network. We compare the standard bloom filter with the distributed
bloom filter and show that even with a false positive rate of 50%, i.e. even
with a very small bloom filter size, the distributed bloom filter still manages
to reach complete set reconciliation across the network in a highly
space-efficient, as well as time-efficient way.","['Lum Ramabaja', 'Arber Avdullahu']",[],0,arXiv,http://arxiv.org/abs/1910.07782v2,False,True,False,False,False,1694,Nicholas A Bloom,Stanford,Completed,2018,2023.0,"There is substantial dispersion in productivity across establishments in the United States. Management is an important factor in explaining differences in productivity, as is the role of economic uncertainty and firm expectations, which drive micro and macro performance. This research will quantify the roles of these factors and evaluate their causes and effects, using data from the Management and Organizational Practices Survey, Annual Survey of Manufactures, Census of Manufactures, and Longitudinal Business Database, and other economic censuses and surveys. We will evaluate management practices and uncertainty across plants, firms, regions, and industries; examine channels through which management and uncertainty affect performance; and identify key factors driving uncertainty, as well as the adoption of better management practices in organizations. This analysis will allow us to evaluate whether establishments that have more structured management, more stable organizational environments, and better forecasting have superior performance, less short-termism, and less sensitivity to transitory shocks. "
"Deriving the number of jobs in proximity services from the number of
  inhabitants in French rural municipalities","We use a minimum requirement approach to derive the number of jobs in
proximity services per inhabitant in French rural municipalities. We first
classify the municipalities according to their time distance to the
municipality where the inhabitants go the most frequently to get services
(called MFM). For each set corresponding to a range of time distance to MFM, we
perform a quantile regression estimating the minimum number of service jobs per
inhabitant, that we interpret as an estimation of the number of proximity jobs
per inhabitant. We observe that the minimum number of service jobs per
inhabitant is smaller in small municipalities. Moreover, for municipalities of
similar sizes, when the distance to the MFM increases, we find that the number
of jobs of proximity services per inhabitant increases.","['Maxime Lenormand', 'Sylvie Huet', 'Guillaume Deffuant']",[],0,arXiv,http://arxiv.org/abs/1109.6760v3,False,True,False,False,False,1719,Arianna Ornaghi,Boston,Active,2017,,"This research examines the effects of civil service reforms on municipal bureaucracies and their performance. Historically, public administration in the United States was characterized by a spoils system in which elected politicians had the power to hire and fire bureaucrats. Provisions aimed at professionalizing the bureaucracy were first introduced at the federal level and slowly diffused to lower levels of government. These reforms were characterized by both meritocratic hiring and political protections for public employees. These reforms may select in better workers through competitive entrance requirements but reduce performance incentives through tenure. This projects examines those states that mandated cities to institute civil service boards for police and fire departments based on population thresholds. This research exploits these thresholds in a regression discontinuity design to estimate the causal effect of introducing the merit system. First, using data from the decennial censuses of 1960 to 2000, this projects looks at the effect on the demographic composition of police and fire departments and, in particular, the gender, age, and racial composition of these department, together with the educational level of policemen and firemen. Second, this research studies whether the reforms had effects on the performance of these departments, including crime rates. "
"Zoning in American Cities: Are Reforms Making a Difference? An AI-based
  Analysis","Cities are at the forefront of addressing global sustainability challenges,
particularly those exacerbated by climate change. Traditional zoning codes,
which often segregate land uses, have been linked to increased vehicular
dependence, urban sprawl, and social disconnection, undermining broader social
and environmental sustainability objectives. This study investigates the
adoption and impact of form-based codes (FBCs), which aim to promote
sustainable, compact, and mixed-use urban forms as a solution to these issues.
Using Natural Language Processing (NLP) techniques, we analyzed zoning
documents from over 2000 U.S. census-designated places to identify linguistic
patterns indicative of FBC principles. Our findings reveal widespread adoption
of FBCs across the country, with notable variations within regions. FBCs are
associated with higher floor-to-area ratios, narrower and more consistent
street setbacks, and smaller plots. We also find that places with FBCs have
improved walkability, shorter commutes, and a higher share of multi-family
housing. Our findings highlight the utility of NLP for evaluating zoning codes
and underscore the potential benefits of form-based zoning reforms for
enhancing urban sustainability.","['Arianna Salazar-Miranda', 'Emily Talen']",[],0,arXiv,http://arxiv.org/abs/2502.00008v1,False,True,False,False,False,1719,Arianna Ornaghi,Boston,Active,2017,,"This research examines the effects of civil service reforms on municipal bureaucracies and their performance. Historically, public administration in the United States was characterized by a spoils system in which elected politicians had the power to hire and fire bureaucrats. Provisions aimed at professionalizing the bureaucracy were first introduced at the federal level and slowly diffused to lower levels of government. These reforms were characterized by both meritocratic hiring and political protections for public employees. These reforms may select in better workers through competitive entrance requirements but reduce performance incentives through tenure. This projects examines those states that mandated cities to institute civil service boards for police and fire departments based on population thresholds. This research exploits these thresholds in a regression discontinuity design to estimate the causal effect of introducing the merit system. First, using data from the decennial censuses of 1960 to 2000, this projects looks at the effect on the demographic composition of police and fire departments and, in particular, the gender, age, and racial composition of these department, together with the educational level of policemen and firemen. Second, this research studies whether the reforms had effects on the performance of these departments, including crime rates. "
"An Empirical Study on Fertility Proposals Using Multi-Grained Topic
  Analysis Methods","Fertility issues are closely related to population security, in 60 years
China's population for the first time in a negative growth trend, the change of
fertility policy is of great concern to the community. 2023 ""two sessions""
proposal ""suggests that the country in the form of legislation, the birth of
the registration of the cancellation of the marriage restriction"" This topic
was once a hot topic on the Internet, and ""unbundling"" the relationship between
birth registration and marriage has become the focus of social debate. In this
paper, we adopt co-occurrence semantic analysis, topic analysis and sentiment
analysis to conduct multi-granularity semantic analysis of microblog comments.
It is found that the discussion on the proposal of ""removing marriage
restrictions from birth registration"" involves the individual, society and the
state at three dimensions, and is detailed into social issues such as personal
behaviour, social ethics and law, and national policy, with people's sentiment
inclined to be negative in most of the topics. Based on this, eight proposals
were made to provide a reference for governmental decision making and to form a
reference method for researching public opinion on political issues.",['Yulin Zhou'],[],0,arXiv,http://arxiv.org/abs/2307.10025v2,False,True,False,False,False,1727,Joelle H Abramowitz,Michigan,Completed,2016,2023.0,"Marriage, fertility, and migration behavior have been the subject of extensive research across many academic disciplines. Considerable work has been devoted to investigating why, whether, and when people decide to marry, have children, and change residences, whether particular programs or factors influence these decisions, and how these choices in turn affect other life decisions. This project considers determinants of these outcomes and evaluates the data used in such analyses. To these ends, this project considers the extent to which empirical analyses using the American Community Survey (ACS) marital history and marital status questions yield comparable or divergent results. The researcher considers determinants of marriage decisions, on their own and in conjunction with fertility and migration decisions, and examine the role of various legal changes and natural experiments occurring between 2008 and 2015. The project will also assess the benefit of the ACS marital history questions, which have been considered for removal in recent years and may again be considered for removal or revision in the future. "
"In the Beginning there were n Agents: Founding and Amending a
  Constitution","Consider n agents forming an egalitarian, self-governed community. Their
first task is to decide on a decision rule to make further decisions. We start
from a rather general initial agreement on the decision-making process based
upon a set of intuitive and self-evident axioms, as well as simplifying
assumptions about the preferences of the agents. From these humble beginnings
we derive a decision rule. Crucially, the decision rule also specifies how it
can be changed, or amended, and thus acts as a de facto constitution. Our main
contribution is in providing an example of an initial agreement that is simple
and intuitive, and a constitution that logically follows from it. The naive
agreement is on the basic process of decision making - that agents approve or
disapprove proposals; that their vote determines either the acceptance or
rejection of each proposal; and on the axioms, which are requirements regarding
a constitution that engenders a self-updating decision making process.","['Ben Abramowitz', 'Ehud Shapiro', 'Nimrod Talmon']",[],0,arXiv,http://arxiv.org/abs/2011.03111v4,False,True,False,False,False,1727,Joelle H Abramowitz,Michigan,Completed,2016,2023.0,"Marriage, fertility, and migration behavior have been the subject of extensive research across many academic disciplines. Considerable work has been devoted to investigating why, whether, and when people decide to marry, have children, and change residences, whether particular programs or factors influence these decisions, and how these choices in turn affect other life decisions. This project considers determinants of these outcomes and evaluates the data used in such analyses. To these ends, this project considers the extent to which empirical analyses using the American Community Survey (ACS) marital history and marital status questions yield comparable or divergent results. The researcher considers determinants of marriage decisions, on their own and in conjunction with fertility and migration decisions, and examine the role of various legal changes and natural experiments occurring between 2008 and 2015. The project will also assess the benefit of the ACS marital history questions, which have been considered for removal in recent years and may again be considered for removal or revision in the future. "
"The Effect of Financial Resources on Fertility: Evidence from
  Administrative Data on Lottery Winners","This paper utilizes wealth shocks from winning lottery prizes to examine the
causal effect of financial resources on fertility. We employ extensive panels
of administrative data encompassing over 0.4 million lottery winners in Taiwan
and implement a triple-differences design. Our analyses reveal that a
substantial lottery win can significantly increase fertility, the implied
wealth elasticity of which is around 0.06. Moreover, the primary channel
through which fertility increases is by prompting first births among previously
childless individuals. Finally, our analysis reveals that approximately 25% of
the total fertility effect stems from increased marriage rates following a
lottery win.","['Yung-Yu Tsai', 'Hsing-Wen Han', 'Kuang-Ta Lo', 'Tzu-Ting Yang']",[],0,arXiv,http://arxiv.org/abs/2212.06223v2,False,True,False,False,False,1727,Joelle H Abramowitz,Michigan,Completed,2016,2023.0,"Marriage, fertility, and migration behavior have been the subject of extensive research across many academic disciplines. Considerable work has been devoted to investigating why, whether, and when people decide to marry, have children, and change residences, whether particular programs or factors influence these decisions, and how these choices in turn affect other life decisions. This project considers determinants of these outcomes and evaluates the data used in such analyses. To these ends, this project considers the extent to which empirical analyses using the American Community Survey (ACS) marital history and marital status questions yield comparable or divergent results. The researcher considers determinants of marriage decisions, on their own and in conjunction with fertility and migration decisions, and examine the role of various legal changes and natural experiments occurring between 2008 and 2015. The project will also assess the benefit of the ACS marital history questions, which have been considered for removal in recent years and may again be considered for removal or revision in the future. "
"Three quantitative predictions based on past regularities about voter
  turnout at the French 2009 European election","The previous twelve turnout rates of French national elections by
municipality show regularities. These regularities do not depend on the
national turnout level, nor on the nature of the election. Based on past
statistical regularities we make three predictions. The first one deals with
the standard deviation of the turnout rate by municipality. The second one
refers to the continuity in time of the heterogeneity of turnout rates in the
vicinity of a municipality. The last one is about the correlation between the
heterogeneity of turnout rates in the vicinity of a municipality and the
population in its surroundings. Details, explanations and discussions will be
given in forthcoming papers.",['Christian Borghesi'],[],0,arXiv,http://arxiv.org/abs/0905.4578v1,False,True,False,False,False,1737,Jorg L Spenkuch,Chicago,Completed,2017,2022.0,"This research examines the causal effect of education on civic participation, as measured by voter turnout. In order to do so, this project implements a fuzzy regression discontinuity design that relies on exact date of birth relative to school entry cutoff dates. Data from the long form of the 2000 Decennial Census and the ACS (2002-2014) contain respondents’ exact date of birth, which allows the researchers to estimate whether individuals born just before the applicable school entry cutoff date in their state of residence are, on average, slightly more educated than those born just after the cutoff. This research will rely on voter registration and turnout data for all fifty states and the District of Columbia to estimate whether individuals born just before the cutoff date are more likely to vote; and by relating population estimates based on the 2010 Decennial Census to counts of registered voters in the user-supplied data, estimate whether there exists a discontinuity in the propensity to register to vote in the first place. Finding a discontinuity around school entry cutoff dates in educational attainment as well as voter turnout and/or registration would be evidence that education exerts a causal effect on civic participation. This research will also utilize the CPS Voting and Registration Supplement (2006-2014). "
Club Convergence of House Prices: Evidence from China's Ten Key Cities,"The latest global financial tsunami and its follow-up global economic
recession has uncovered the crucial impact of housing markets on financial and
economic systems. The Chinese stock market experienced a markedly fall during
the global financial tsunami and China's economy has also slowed down by about
2\%-3\% when measured in GDP. Nevertheless, the housing markets in diverse
Chinese cities seemed to continue the almost nonstop mania for more than ten
years. However, the structure and dynamics of the Chinese housing market are
less studied. Here we perform an extensive study of the Chinese housing market
by analyzing ten representative key cities based on both linear and nonlinear
econophysical and econometric methods. We identify a common collective driving
force which accounts for 96.5\% of the house price growth, indicating very high
systemic risk in the Chinese housing market. The ten key cities can be
categorized into clubs and the house prices of the cities in the same club
exhibit an evident convergence. These findings from different methods are
basically consistent with each other. The identified city clubs are also
consistent with the conventional classification of city tiers. The house prices
of the first-tier cities grow the fastest, and those of the third- and
fourth-tier cities rise the slowest, which illustrates the possible presence of
a ripple effect in the diffusion of house prices in different cities.","['Hao Meng', 'Wen-Jie Xie', 'Wei-Xing Zhou']",[],0,arXiv,http://arxiv.org/abs/1503.05550v1,False,True,False,False,False,1739,Ludovica A Gazze,Chicago,Completed,2017,2019.0,"Lead poisoning can have long-lasting consequences, especially on children's health and IQ; therefore, the Consumer Product Safety Commission effectively banned the use of lead in paint in 1978. State and federal laws regulate the disclosure of information concerning lead presence in homes built prior to 1978, as well as abatement. According to the Environmental Protection Agency, professional lead-based paint removal costs between $8 to $15 per square foot, with the average removal project costing about $10,000. This project assesses how the costs imposed by the regulations affect housing prices and home-ownership, while quantifying the health benefits of the regulation. Indeed, the adverse effects of lead-based paint on children's health are likely to be mediated by households' responses to the information regarding lead hazards. This research employs a triple differences approach to estimate the effects of state-level lead-safe housing regulations on the housing market and blood lead levels."
"BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised
  Learning","Data mixing methods play a crucial role in semi-supervised learning (SSL),
but their application is unexplored in long-tailed semi-supervised learning
(LTSSL). The primary reason is that the in-batch mixing manner fails to address
class imbalance. Furthermore, existing LTSSL methods mainly focus on
re-balancing data quantity but ignore class-wise uncertainty, which is also
vital for class balance. For instance, some classes with sufficient samples
might still exhibit high uncertainty due to indistinguishable features. To this
end, this paper introduces the Balanced and Entropy-based Mix (BEM), a
pioneering mixing approach to re-balance the class distribution of both data
quantity and uncertainty. Specifically, we first propose a class balanced mix
bank to store data of each class for mixing. This bank samples data based on
the estimated quantity distribution, thus re-balancing data quantity. Then, we
present an entropy-based learning approach to re-balance class-wise
uncertainty, including entropy-based sampling strategy, entropy-based selection
module, and entropy-based class balanced loss. Our BEM first leverages data
mixing for improving LTSSL, and it can also serve as a complement to the
existing re-balancing methods. Experimental results show that BEM significantly
enhances various LTSSL frameworks and achieves state-of-the-art performances
across multiple benchmarks.","['Hongwei Zheng', 'Linyuan Zhou', 'Han Li', 'Jinming Su', 'Xiaoming Wei', 'Xiaoming Xu']",[],0,arXiv,http://arxiv.org/abs/2404.01179v1,False,True,False,False,False,1745,Hongwei Xu,Baruch,Completed,2018,2022.0,"We examine how using the reduced sample sizes of the 2007–2016 American Community Survey data at the tract level, relative to the 2000 decennial long-form sample, can affect estimates of demographic composition and socioeconomic conditions. We find that measures of income segregation are biased upwards by smaller samples at the tract level, and seek to apply corrections to those measures. In addition, we examine how Bayesian models can be applied to small area estimation for point estimates of tract characteristics and how estimates of tract characteristics can be harmonized over time to adjust for changes in tract boundaries. "
Small Area Shrinkage Estimation,"The need for small area estimates is increasingly felt in both the public and
private sectors in order to formulate their strategic plans. It is now widely
recognized that direct small area survey estimates are highly unreliable owing
to large standard errors and coefficients of variation. The reason behind this
is that a survey is usually designed to achieve a specified level of accuracy
at a higher level of geography than that of small areas. Lack of additional
resources makes it almost imperative to use the same data to produce small area
estimates. For example, if a survey is designed to estimate per capita income
for a state, the same survey data need to be used to produce similar estimates
for counties, subcounties and census divisions within that state. Thus, by
necessity, small area estimation needs explicit, or at least implicit, use of
models to link these areas. Improved small area estimates are found by
""borrowing strength"" from similar neighboring areas.","['G. Datta', 'M. Ghosh']",[],0,arXiv,http://arxiv.org/abs/1203.5233v1,False,True,False,False,False,1745,Hongwei Xu,Baruch,Completed,2018,2022.0,"We examine how using the reduced sample sizes of the 2007–2016 American Community Survey data at the tract level, relative to the 2000 decennial long-form sample, can affect estimates of demographic composition and socioeconomic conditions. We find that measures of income segregation are biased upwards by smaller samples at the tract level, and seek to apply corrections to those measures. In addition, we examine how Bayesian models can be applied to small area estimation for point estimates of tract characteristics and how estimates of tract characteristics can be harmonized over time to adjust for changes in tract boundaries. "
Fractional Imputation in Survey Sampling: A Comparative Review,"Fractional imputation (FI) is a relatively new method of imputation for
handling item nonresponse in survey sampling. In FI, several imputed values
with their fractional weights are created for each missing item. Each
fractional weight represents the conditional probability of the imputed value
given the observed data, and the parameters in the conditional probabilities
are often computed by an iterative method such as EM algorithm. The underlying
model for FI can be fully parametric, semiparametric, or nonparametric,
depending on plausibility of assumptions and the data structure.
  In this paper, we give an overview of FI, introduce key ideas and methods to
readers who are new to the FI literature, and highlight some new development.
We also provide guidance on practical implementation of FI and valid
inferential tools after imputation. We demonstrate the empirical performance of
FI with respect to multiple imputation using a pseudo finite population
generated from a sample in Monthly Retail Trade Survey in US Census Bureau.","['Shu Yang', 'Jae Kwang Kim']",[],0,arXiv,http://arxiv.org/abs/1508.06945v1,True,True,False,False,True,1746,Satkartar Kinney,Triangle,Completed,2018,2022.0,"We will investigate methods for producing weighting factors using data from the Census Bureau’s Census of Retail Trade (CRT). This research will provide insight into the quality of North American Industrial Classification System codes and corresponding sales figures for food and beverage categories in the CRT by benchmarking CRT sales figures for these categories against industry classification and product line data from external Infoscan data provided by Information Resources Inc. We will generate weighting factors for the CRT data by industry, geographic area, and year, and investigate potential methodologies for generating such weighting factors in between Economic Census years."
"Trade-Offs Between Ranking Objectives: Reduced-Form Evidence and
  Structural Estimation","Online retailers and platforms typically present alternatives using ranked
product lists. By adjusting the ranking, these platforms influence consumers'
choices and, in turn, conversions, platform revenues, and consumer welfare. In
this paper, I study the trade-offs between ranking algorithms that target these
different objectives. First, I highlight and provide reduced-form evidence for
a key factor shaping these trade-offs: cross-product heterogeneity in position
effects. To quantify the effects of different rankings, I then develop an
empirical framework based on the search and discovery model of Greminger
(2022). For this framework, I show that the ranking that maximizes conversions
also maximizes consumer welfare, implying no trade-off between these two
objectives. Moreover, I develop and test a heuristic ranking algorithm to
maximize revenues. Finally, I estimate the model and compare the effects of the
rankings developed for the different objectives. The results highlight the
effectiveness of the different rankings and reveal that the proposed heuristic
to maximize revenues also increases consumer welfare, suggesting that the
trade-off between revenue maximization and consumer welfare also is limited.",['Rafael P. Greminger'],[],0,arXiv,http://arxiv.org/abs/2210.16408v5,False,True,False,False,False,1746,Satkartar Kinney,Triangle,Completed,2018,2022.0,"We will investigate methods for producing weighting factors using data from the Census Bureau’s Census of Retail Trade (CRT). This research will provide insight into the quality of North American Industrial Classification System codes and corresponding sales figures for food and beverage categories in the CRT by benchmarking CRT sales figures for these categories against industry classification and product line data from external Infoscan data provided by Information Resources Inc. We will generate weighting factors for the CRT data by industry, geographic area, and year, and investigate potential methodologies for generating such weighting factors in between Economic Census years."
"Using administrative data to improve the estimation of immigration to
  local areas in England","International migration is now a significant driver of population change
across Europe but the methods available to estimate its true impact upon
sub-national areas remain inconsistent, constrained by inadequate systems of
measurement and data capture. In the absence of a population register for
England, official statistics on immigration and emigration are derived from a
combination of survey and census sources. This paper demonstrates how
administrative data systems such as those which capture registrations of recent
migrants with a local doctor, National Insurance Number registrations by
workers from abroad and the registration of foreign students for higher
education, can provide data to better understand patterns and trends in
international migration. The paper proposes a model for the estimation of
immigration at a local level, integrating existing national estimates from the
Office for National Statistics with data from these administrative sources. The
model attempts to circumvent conceptual differences between datasets through
the use of proportional distributions rather than absolute migrant counts in
the estimation process. The model methodology and the results it produces
provide alternative estimates of immigration for consideration by the Office
for National Statistics as it develops its own programme of improvement to
sub-national migration statistics.","['Peter Boden', 'Phil Rees']",[],0,arXiv,http://arxiv.org/abs/0903.0507v2,False,True,False,False,False,1748,Annie E Ro,Irvine,Completed,2017,2020.0,"The overall goal of this research is to effectively categorize and describe the Asian immigration population by migration status in the Survey of Income and Program Participation (SIPP), to compare across their demographic characteristics, economic status, and employment patterns. No study to date has exclusively studied and described the detailed migration status of Asian immigrants in representative data sources. First, this project will compare imputation methods in the restricted-use SIPP dataset to identify an optimal approach to estimate migration status (legal permanent residents, legal non-immigrants, and remaining other non-LPRs within non-citizens). Second, this project will describe the demographic, economic, and health insurance characteristics of the Asian immigration population by migration status, both nationally and in California. "
"Credit Crunch: The Role of Household Lending Capacity in the Dutch
  Housing Boom and Bust 1995-2018","What causes house prices to rise and fall? Economists identify household
access to credit as a crucial factor. ""Loan-to-Value"" and ""Debt-to-GDP"" ratios
are the standard measures for credit access. However, these measures fail to
explain the depth of the Dutch housing bust after the 2009 Financial Crisis.
This work is the first to model household lending capacity based on the
formulas that Dutch banks use in the mortgage application process. We compare
the ability of regression models to forecast housing prices when different
measures of credit access are utilised. We show that our measure of household
lending capacity is a forward-looking, highly predictive variable that
outperforms `Loan-to-Value' and debt ratios in forecasting the Dutch crisis.
Sharp declines in lending capacity foreshadow the market deceleration.","['Menno Schellekens', 'Taha Yasseri']",[],0,arXiv,http://arxiv.org/abs/2101.00913v1,False,True,False,False,False,1751,John E Anderson,Nebraska,Active,2018,,"This research will examine the effects of the bursting of the housing bubble during the 2007–2009 period on the Tiebout-like sorting pattern of households within and across communities, using data from the American Community Survey. The areas of interest include three large metropolitan areas that experienced varying degrees of housing market impacts: Phoenix, Denver, and St. Louis. The testable hypothesis of this research project is that the housing crisis caused a significant resorting of individuals within and across communities in large metropolitan areas. We will conduct the analysis using two empirical strategies. The first is a descriptive approach in which homogeneity metrics will be computed and aggregated to the city, or borough, level in order to identify the varying degrees of homogeneity within and across communities. Such metrics will be used to gauge the intensity of change in these measures of homogeneity over time. The second approach will rely on regression analysis in order to identify which explanatory variables are driving these changes in homogeneity over time and across geography. This research will provide one of the most comprehensive pictures as to how households were forced to migrate during the Great Recession."
The Snowball Stratosphere,"According to the Snowball Earth hypothesis, Earth has experienced periods of
low-latitude glaciation in its deep past. Prior studies have used general
circulation models (GCMs) to examine the effects such an extreme climate state
might have on the structure and dynamics of Earth's troposphere, but the
behavior of the stratosphere has not been studied in detail. Understanding the
snowball stratosphere is important for developing an accurate account of the
Earth's radiative and chemical properties during these episodes. Here we
conduct the first analysis of the stratospheric circulation of the Snowball
Earth using ECHAM6 general circulation model simulations. In order to
understand the factors contributing to the stratospheric circulation, we extend
the Statistical Transformed Eulerian Mean framework. We find that the
stratosphere during a snowball with prescribed modern ozone levels exhibits a
weaker meridional overturning circulation, reduced wave activity, stronger
zonal jets, and is extremely cold relative to modern conditions. Notably, the
snowball stratosphere displays no sudden stratospheric warmings. Without ozone,
the stratosphere displays slightly weaker circulation, a complete lack of polar
vortex, and even colder temperatures. We also explicitly quantify for the first
time the cross-tropopause mass exchange rate and stratospheric mixing
efficiency during the snowball and show that our values do not change the
constraints on CO$_2$ inferred from geochemical proxies during the Marinoan
glaciation ($\sim$635 Ma), unless the O$_2$ concentration during the snowball
was orders of magnitude less than the CO$_2$ concentration.","['R. J. Graham', 'Tiffany Shaw', 'Dorian Abbot']",[],0,arXiv,http://arxiv.org/abs/1909.12717v1,False,True,False,False,False,1756,Joshua D Gottlieb,Chicago,Active,2017,,"This research project expands current statistics by developing series that describe inequality at the level of occupations and geographic areas. While the evolution of inequality is often measured and analyzed as a macroeconomic phenomenon, this project uses geographic and occupational variations to attempt to gain insight into its underlying causes. The theories examined emphasize economic channels through which inequality driven by “superstar” effects can spill over into occupations in which the superstar phenomenon is not directly applicable."
"Duration Dependence and Heterogeneity: Learning from Early Notice of
  Layoff","This paper presents a novel approach to distinguish the impact of
duration-dependent forces and adverse selection on the exit rate from
unemployment by leveraging variation in the length of layoff notices. I
formulate a Mixed Hazard model in discrete time and specify the conditions
under which variation in notice length enables the identification of structural
duration dependence while allowing for arbitrary heterogeneity across workers.
Utilizing data from the Displaced Worker Supplement (DWS), I employ the
Generalized Method of Moments (GMM) to estimate the model. According to the
estimates, the decline in the exit rate over the first 48 weeks of unemployment
is largely due to the worsening composition of surviving jobseekers.
Furthermore, I find that an individual's likelihood of exiting unemployment
decreases initially, then increases until unemployment benefits run out, and
remains steady thereafter. These findings are consistent with a standard search
model where returns to search decline early in the spell.",['Div Bhagia'],[],0,arXiv,http://arxiv.org/abs/2305.17344v2,False,True,False,False,False,1762,Ben Hyman,Baruch,Completed,2017,2024.0,"This research analyzes the effects of federal Trade Adjustment Assistance (TAA) benefits on worker-level outcomes, specifically educational attainment, cumulative earnings (wages), labor force participation, time-to-rehire, and sectoral reallocation (i.e., whether re-trained workers move to firms and industry of higher or lower relative productivity with respect to the firms from which they separated). TAA benefits – typically cash transfers for worker enrollment in (re)training programs – are awarded to workers that successfully demonstrate to the Department of Labor that their firm’s layoffs were caused by import competition with foreign competitors. This project also explores how the effects of TAA may differ from standard effects of unemployment insurance benefits by using workers laid off due to bankruptcy as a control group. Finally, the project assesses the costs and benefits of awarding TAA allowances, and also considers how regions with higher TAA “generosity” may vary in their support for trade measures with respect to lower generosity regions. "
"The heterogeneous impact of the EU-Canada agreement with causal machine
  learning","This paper introduces a causal machine learning approach to investigate the
impact of the EU-Canada Comprehensive Economic Trade Agreement (CETA). We
propose a matrix completion algorithm on French customs data to obtain
multidimensional counterfactuals at the firm, product and destination levels.
We find a small but significant positive impact on average at the product-level
intensive margin. On the other hand, the extensive margin shows product
churning due to the treaty beyond regular entry-exit dynamics: one product in
eight that was not previously exported substitutes almost as many that are no
longer exported. When we delve into the heterogeneity, we find that the effects
of the treaty are higher for products at a comparative advantage. Focusing on
multiproduct firms, we find that they adjust their portfolio in Canada by
reallocating towards their first and most exported product due to increasing
local market competition after trade liberalization. Finally, multidimensional
counterfactuals allow us to evaluate the general equilibrium effect of the
CETA. Specifically, we observe trade diversion, as exports to other
destinations are re-directed to Canada.","['Lionel Fontagné', 'Francesca Micocci', 'Armando Rungi']",[],0,arXiv,http://arxiv.org/abs/2407.07652v3,False,True,False,False,False,1785,Ishan Nath,Chicago,Active,2018,,"This research endeavors to estimate the causal impact of extreme weather on manufacturing productivity in the United States as part of a broader global analysis of the aggregate productivity impact of intensifying temperature extremes. We will use data from the Annual Survey of Manufactures, Census of Manufactures, and Longitudinal Business Database from 1972–2014 to construct a plant-level panel of manufacturing productivity that will be merged with county-level weather data to empirically estimate the causal effect of extreme temperatures. We will combine these results with separate estimates using microdata on manufacturing and agricultural production from many countries around the world to quantify the global heterogeneous impact of extreme temperatures on national comparative advantage between manufacturing and agriculture. These empirical estimates will then be embedded in a model of global trade to understand how the endogenous reallocation of production between broad sectors of the economy could reduce the aggregate costs of the shifting distribution of global temperatures. The trade model will also be used to quantify the degree to which trade barriers impede this mechanism of adaptation."
"Tracking Control foe Multi-Agent Systems Using Broadcast Signals Based
  on Positive Realness","Broadcast control is one of decentralized control methods for networked
multi-agent systems. In this method, each agent does not communicate with the
others, and autonomously determines its own action using only the same signal
sent from a central controller. Therefore, it is effective for systems with
numerous agents or no-communication between agents. However, it is difficult to
manage the stochastic action process of agents considering engineering
applications. This paper proposes a decentralized control such that agents
autonomously select the deterministic actions. Firstly, a non-linear controller
with a binary output of each agent including 0 is introduced in order to
express stop actions autonomously when the target is achieved. The asymptotic
stability to the target is proved. Secondly, the controller can adjust the
tendency of actions in order to make it easier to manage the actions. Thirdly,
the controller is extended to that with a continuous output in order to reduce
the tracking error to the target and the output vibration. Finally, the
effectiveness of the proposed control is verified by numerical experiments.","['Yasushi Amano', 'Tomohiko Jimbo', 'Kenji Fujimoto']",[],0,arXiv,http://arxiv.org/abs/2109.06372v1,False,True,False,False,False,1787,Noriko Amano,Yale,Active,2017,,"This research estimates (i) the effect of Affirmative Action regulation on the probability that a new hire in private sector firms meeting the size requirements is a minority, (ii) the effect of legal charges filed citing sex, race, color or national origin discrimination on the probability that a new hire is a minority, and (iii) the effect of working in an Affirmative Action regulated firm on current and future wages. This project will impute federal contractor status and racial composition reported in the Equal Employment Opportunity Employer Information Reports, to private sector firms in the LEHD database meeting the size requirements to fill these forms under the Civil Rights Act of 1964. In addition, it will assign a count of discrimination charges filed with the EEOC against federally contracted firms. "
"Multivariate spatio-temporal models for high-dimensional areal data with
  application to Longitudinal Employer-Household Dynamics","Many data sources report related variables of interest that are also
referenced over geographic regions and time; however, there are relatively few
general statistical methods that one can readily use that incorporate these
multivariate spatio-temporal dependencies. Additionally, many multivariate
spatio-temporal areal data sets are extremely high dimensional, which leads to
practical issues when formulating statistical models. For example, we analyze
Quarterly Workforce Indicators (QWI) published by the US Census Bureau's
Longitudinal Employer-Household Dynamics (LEHD) program. QWIs are available by
different variables, regions, and time points, resulting in millions of
tabulations. Despite their already expansive coverage, by adopting a fully
Bayesian framework, the scope of the QWIs can be extended to provide estimates
of missing values along with associated measures of uncertainty. Motivated by
the LEHD, and other applications in federal statistics, we introduce the
multivariate spatio-temporal mixed effects model (MSTM), which can be used to
efficiently model high-dimensional multivariate spatio-temporal areal data
sets. The proposed MSTM extends the notion of Moran's I basis functions to the
multivariate spatio-temporal setting. This extension leads to several
methodological contributions, including extremely effective dimension
reduction, a dynamic linear model for multivariate spatio-temporal areal
processes, and the reduction of a high-dimensional parameter space using a
novel parameter model.","['Jonathan R. Bradley', 'Scott H. Holan', 'Christopher K. Wikle']",[],0,arXiv,http://arxiv.org/abs/1503.00982v2,True,True,False,False,False,1802,Richard K Mansfield,Colorado,Active,2017,,"This research employs recently developed methodology for estimating two-sided assignment models for producing forecasts or simulations about a range of labor market phenomena. For such models to generate accurate and useful forecasts, one needs to be able to observe the key characteristics that capture the heterogeneity on both sides of the market that leads certain agents on one side to be disproportionately likely to match with certain agents or units on the opposite side. LEHD data provide a very rich set of characteristics that describe agents or units on both sides of the market (workers and firms). This research will produce forecasts about (i) which workers in which locations would be most affected by alternative forms of local labor demand shocks (plant relocations, stimulus packages, natural disasters), (ii) the degree to which differential access to jobs with strong career paths, differential promotions, and differential frequency and quality of outside offers (conditional on job type) contribute to gender and racial income disparities at different points in the life cycle, (iii) how the earnings distributions by race and gender are likely to evolve in the next decade, given the differences in the racial, gender, and educational attainment composition of entering vs. exiting cohorts in the U.S labor market, and (iv) how assortative matching patterns along various dimensions might change as the occupational and industry composition of the labor demand changes, given the degree to which occupation and industry affect search costs in the marriage market. "
"Strategic Responses to Technological Change: Evidence from Online Labor
  Markets","In this project, we examine how freelancers changed their behavior on an
online work platform following the launch of ChatGPT in November 2022. We first
document that, post-ChatGPT, freelancers bid on fewer jobs and reposition
themselves by differentiating their distribution of bids relative to their
prior behavior. We disentangle heterogeneity in repositioning across work
domains by exploring how exposure to changes in supply or demand underlie
repositioning. Decreases in the demand for labor post-ChatGPT lead workers to
reposition themselves by withdrawing from the focal market/exiting the platform
and changing their horizontal positioning (i.e., work domain), while increases
in the supply of labor post-ChatGPT are less likely to lead to changes in
volume of activity or horizontal positioning but more likely to result decrease
the proportion of bids to high-value jobs, perhaps in response to increased
competition. We further show that repositioning is less likely when adjustment
costs are higher due to greater skill. Our research contributes to our
understanding of how and why workers respond to technological change.","['Shun Yiu', 'Rob Seamans', 'Manav Raj', 'Ted Liu']",[],0,arXiv,http://arxiv.org/abs/2403.15262v4,False,True,False,False,False,1806,Ted D Mouw,Triangle,Active,2018,,"Existing research posits that changes in labor demand represent a source of labor market instability regardless of why the changes in demand arise. I will describe the impact of changes in labor demand on occupational and geographic mobility using data from the Survey of Income and Program Participation (SIPP) and American Community Survey. I will also evaluate the relative performance of public, confidential, and synthetic versions of the SIPP while conducting an analysis of geographic mobility via the process of worker reallocation due to labor demand shocks. Further, I will analyze the process of worker reallocation to different occupations and geographic regions in response to labor demand shocks, the speed with which the labor market adjusts to instability, and resulting impacts on worker well-being."
Cross-sectional Dependence in Idiosyncratic Volatility,"This paper introduces an econometric framework for analyzing cross-sectional
dependence in the idiosyncratic volatilities of assets using high frequency
data. We first consider the estimation of standard measures of dependence in
the idiosyncratic volatilities such as covariances and correlations. Naive
estimators of these measures are biased due to the use of the error-laden
estimates of idiosyncratic volatilities. We provide bias-corrected estimators
and the relevant asymptotic theory. Next, we introduce an idiosyncratic
volatility factor model, in which we decompose the variation in idiosyncratic
volatilities into two parts: the variation related to the systematic factors
such as the market volatility, and the residual variation. Again, naive
estimators of the decomposition are biased, and we provide bias-corrected
estimators. We also provide the asymptotic theory that allows us to test
whether the residual (non-systematic) components of the idiosyncratic
volatilities exhibit cross-sectional dependence. We apply our methodology to
the S&P 100 index constituents, and document strong cross-sectional dependence
in their idiosyncratic volatilities. We consider two different sets of
idiosyncratic volatility factors, and find that neither can fully account for
the cross-sectional dependence in idiosyncratic volatilities. For each model,
we map out the network of dependencies in residual (non-systematic)
idiosyncratic volatilities across all stocks.","['Ilze Kalnina', 'Kokouvi Tewou']",[],0,arXiv,http://arxiv.org/abs/2408.13437v1,False,True,False,False,False,1808,Chen Yeh,Fed Board,Completed,2018,2024.0,"This research seeks to comprehensively characterize how heterogeneity and idiosyncratic shocks at the firm and regional levels affect aggregate outcomes. Specifically, we are interested in describing how microeconomic forces shape aggregate outcomes through the dynamic behavior of firms. Recent contributions have shown that macroeconomic outcomes are disproportionately affected by specific categories of firms and/or regions. However, aggregate data masks this rich level of heterogeneity. Thus, we use microdata from the Economic Censuses, Longitudinal Business Database, Standard Statistical Establishment List, and Annual Survey of Manufactures to uncover the heterogeneity in aggregate outcomes at the firm and regional level, and to demonstrate its importance for the aggregate economy. In particular, we will document the extent of heterogeneity across plants and firms, and locations in the U.S. economy along several dimensions, including firm creation and growth, and produce several statistical estimates of the contribution of firm- and location-specific factors to the business cycle, labor market fluctuations, secular trends in market power, and business dynamism."
Economic Forces in Stock Returns,"When analyzing the components influencing the stock prices, it is commonly
believed that economic activities play an important role. More specifically,
asset prices are more sensitive to the systematic economic news that impose a
pervasive effect on the whole market. Moreover, the investors will not be
rewarded for bearing idiosyncratic risks as such risks are diversifiable. In
the paper Economic Forces and the Stock Market 1986, the authors introduced an
attribution model to identify the specific systematic economic forces
influencing the market. They first defined and examined five classic factors
from previous research papers: Industrial Production, Unanticipated Inflation,
Change in Expected Inflation, Risk Premia, and The Term Structure. By adding in
new factors, the Market Indices, Consumptions and Oil Prices, one by one, they
examined the significant contribution of each factor to the stock return. The
paper concluded that the stock returns are exposed to the systematic economic
news, and they are priced with respect to their risk exposure. Also, the
significant factors can be identified by simply adopting their model. Driven by
such motivation, we conduct an attribution analysis based on the general
framework of their model to further prove the importance of the economic
factors and identify the specific identity of significant factors.","['Yue Chen', 'Mohan Li']",[],0,arXiv,http://arxiv.org/abs/2401.04132v1,False,True,False,False,False,1808,Chen Yeh,Fed Board,Completed,2018,2024.0,"This research seeks to comprehensively characterize how heterogeneity and idiosyncratic shocks at the firm and regional levels affect aggregate outcomes. Specifically, we are interested in describing how microeconomic forces shape aggregate outcomes through the dynamic behavior of firms. Recent contributions have shown that macroeconomic outcomes are disproportionately affected by specific categories of firms and/or regions. However, aggregate data masks this rich level of heterogeneity. Thus, we use microdata from the Economic Censuses, Longitudinal Business Database, Standard Statistical Establishment List, and Annual Survey of Manufactures to uncover the heterogeneity in aggregate outcomes at the firm and regional level, and to demonstrate its importance for the aggregate economy. In particular, we will document the extent of heterogeneity across plants and firms, and locations in the U.S. economy along several dimensions, including firm creation and growth, and produce several statistical estimates of the contribution of firm- and location-specific factors to the business cycle, labor market fluctuations, secular trends in market power, and business dynamism."
"Paradigm Shift in Sustainability Disclosure Analysis: Empowering
  Stakeholders with CHATREPORT, a Language Model-Based Tool","This paper introduces a novel approach to enhance Large Language Models
(LLMs) with expert knowledge to automate the analysis of corporate
sustainability reports by benchmarking them against the Task Force for
Climate-Related Financial Disclosures (TCFD) recommendations. Corporate
sustainability reports are crucial in assessing organizations' environmental
and social risks and impacts. However, analyzing these reports' vast amounts of
information makes human analysis often too costly. As a result, only a few
entities worldwide have the resources to analyze these reports, which could
lead to a lack of transparency. While AI-powered tools can automatically
analyze the data, they are prone to inaccuracies as they lack domain-specific
expertise. This paper introduces a novel approach to enhance LLMs with expert
knowledge to automate the analysis of corporate sustainability reports. We
christen our tool CHATREPORT, and apply it in a first use case to assess
corporate climate risk disclosures following the TCFD recommendations.
CHATREPORT results from collaborating with experts in climate science, finance,
economic policy, and computer science, demonstrating how domain experts can be
involved in developing AI tools. We make our prompt templates, generated data,
and scores available to the public to encourage transparency.","['Jingwei Ni', 'Julia Bingler', 'Chiara Colesanti-Senni', 'Mathias Kraus', 'Glen Gostlow', 'Tobias Schimanski', 'Dominik Stammbach', 'Saeid Ashraf Vaghefi', 'Qian Wang', 'Nicolas Webersinke', 'Tobias Wekhof', 'Tingyu Yu', 'Markus Leippold']",[],0,arXiv,http://arxiv.org/abs/2306.15518v2,False,True,False,False,False,1814,MaryJane R Rabier,Missouri,Active,2018,,"This research examines which organizational policies contribute to efficient information collection and dissemination within manufacturing firms. Specifically, we will use the Census of Manufactures and the Management and Organizational Practices Survey to examine how (1) organizational structure, (2) managerial incentives along corporate hierarchy and corporate governance policies, and (3) the extent of use of information technologies, affect the quality of corporate disclosures. Examining how each dimension of organization policy affects corporate disclosures will help to understand the process of corporations’ internal communication, and build on and aim to contribute new insights to the determinants of high-quality corporate disclosures."
Economic Development and Inequality: a complex system analysis,"By borrowing methods from complex system analysis, in this paper we analyze
the features of the complex relationship that links the development and the
industrialization of a country to economic inequality. In order to do this, we
identify industrialization as a combination of a monetary index, the GDP per
capita, and a recently introduced measure of the complexity of an economy, the
Fitness. At first we explore these relations on a global scale over the time
period 1990--2008 focusing on two different dimensions of inequality: the
capital share of income and a Theil measure of wage inequality. In both cases,
the movement of inequality follows a pattern similar to the one theorized by
Kuznets in the fifties. We then narrow down the object of study ad we
concentrate on wage inequality within the United States. By employing data on
wages and employment on the approximately 3100 US counties for the time
interval 1990--2014, we generalize the Fitness-Complexity algorithm for
counties and NAICS sectors, and we investigate wage inequality between
industrial sectors within counties. At this scale, in the early nineties we
recover a behavior similar to the global one. While, in more recent years, we
uncover a trend reversal: wage inequality monotonically increases as
industrialization levels grow. Hence at a county level, at net of the social
and institutional factors that differ among countries, we not only observe an
upturn in inequality but also a change in the structure of the relation between
wage inequality and development.","['Angelica Sbardella', 'Emanuele Pugliese', 'Luciano Pietronero']",[],0,arXiv,http://arxiv.org/abs/1605.03133v1,False,True,False,False,False,1819,David G Wiczer,Atlanta,Active,2018,,"In this research, we study how firm-level worker flows affect the earnings growth that workers experience when they switch from one firm to another. While it is well-documented that earnings grow disproportionately during employer transitions, worker-side survey data are silent on the effect on this growth from shocks on the origin and the destination employers. Using matched employer-employee data from the Longitudinal Employer-Household Dynamics program, we estimate how the employers' net size changes and turn-over rate affect workers’ earnings in transition. Particularly, we estimate the contribution of job destruction at the origin to lower earnings growth and of job creation at the destination to higher growth. Turn-over also notably affects earnings growth as climbing the job wage-ladder is closely associated with job switches to employers with declining gross hires. We quantify how worker transitions with the flow of employment towards faster-growing employers, contribute more substantially to total earnings growth than simple reallocation."
"The Mortgage Cash-Flow Channel: How Rising Interest Rates Impact
  Household Consumption","This study investigates the impact of increased debt servicing costs on
household consumption resulting from monetary policy tightening. It utilizes
observational panel microdata on all mortgage holders in Israel and leverages
quasi-exogenous variation in exposure to adjustable-rate mortgages (ARMs) due
to a regulatory shift. Our analysis indicates that when monetary policy became
more restrictive, consumers with a higher ratio of ARMs experienced a more
marked reduction in their consumption patterns. This effect is predominantly
observed in mid- to lower-income households and those with a higher ratio of
mortgage payments to total spending. These findings highlight the substantial
role of the mortgage cash-flow channel in monetary policy transmission,
emphasizing its implications for economic stability and inequality.","['Itamar Caspi', 'Nadav Eshel', 'Nimrod Segev']",[],0,arXiv,http://arxiv.org/abs/2410.02445v1,False,True,False,False,False,1831,Xiao Cen,Texas,Active,2018,,"This research characterizes the two-way spillover effects between the labor and mortgage markets. An understudied facet of home mortgage is its impact on the liquidity profile of the borrowing households. When home equity is low, serving the debt with periodic payments can constitute a real burden to borrowers and may change their risk appetite in the labor market. We document how mortgage debt affects borrowers’ propensity to take riskier jobs, such as working in startups or high-turnover industries, as well as how employment status affects borrowers’ mortgage performance. Using data mainly from the Longitudinal Employer-Household Dynamics, Longitudinal Business Database, and Corelogic deeds, we identify effects utilizing a variety of quasi-exogenous shocks on employment, home equity, and mortgage payments. We expect that mortgage liability discourages borrowers, and especially if the heads of the households are already in debt, from taking a riskier career path and increases the likelihood of staying in a stable job with predictable incomes. "
Insurance valuation: a computable multi-period cost-of-capital approach,"We present an approach to market-consistent multi-period valuation of
insurance liability cash flows based on a two-stage valuation procedure. First,
a portfolio of traded financial instrument aimed at replicating the liability
cash flow is fixed. Then the residual cash flow is managed by repeated
one-period replication using only cash funds. The latter part takes capital
requirements and costs into account, as well as limited liability and risk
averseness of capital providers. The cost-of-capital margin is the value of the
residual cash flow. We set up a general framework for the cost-of-capital
margin and relate it to dynamic risk measurement. Moreover, we present explicit
formulas and properties of the cost-of-capital margin under further assumptions
on the model for the liability cash flow and on the conditional risk measures
and utility functions. Finally, we highlight computational aspects of the
cost-of-capital margin, and related quantities, in terms of an example from
life insurance.","['Hampus Engsner', 'Mathias Lindholm', 'Filip Lindskog']",[],0,arXiv,http://arxiv.org/abs/1607.04100v1,False,True,False,False,False,1831,Xiao Cen,Texas,Active,2018,,"This research characterizes the two-way spillover effects between the labor and mortgage markets. An understudied facet of home mortgage is its impact on the liquidity profile of the borrowing households. When home equity is low, serving the debt with periodic payments can constitute a real burden to borrowers and may change their risk appetite in the labor market. We document how mortgage debt affects borrowers’ propensity to take riskier jobs, such as working in startups or high-turnover industries, as well as how employment status affects borrowers’ mortgage performance. Using data mainly from the Longitudinal Employer-Household Dynamics, Longitudinal Business Database, and Corelogic deeds, we identify effects utilizing a variety of quasi-exogenous shocks on employment, home equity, and mortgage payments. We expect that mortgage liability discourages borrowers, and especially if the heads of the households are already in debt, from taking a riskier career path and increases the likelihood of staying in a stable job with predictable incomes. "
"Simulate and Optimise: A two-layer mortgage simulator for designing
  novel mortgage assistance products","We develop a novel two-layer approach for optimising mortgage relief products
through a simulated multi-agent mortgage environment. While the approach is
generic, here the environment is calibrated to the US mortgage market based on
publicly available census data and regulatory guidelines. Through the
simulation layer, we assess the resilience of households to exogenous income
shocks, while the optimisation layer explores strategies to improve the
robustness of households to these shocks by making novel mortgage assistance
products available to households. Households in the simulation are adaptive,
learning to make mortgage-related decisions (such as product enrolment or
strategic foreclosures) that maximize their utility, balancing their available
liquidity and equity. We show how this novel two-layer simulation approach can
successfully design novel mortgage assistance products to improve household
resilience to exogenous shocks, and balance the costs of providing such
products through post-hoc analysis. Previously, such analysis could only be
conducted through expensive pilot studies involving real participants,
demonstrating the benefit of the approach for designing and evaluating
financial products.","['Leo Ardon', 'Benjamin Patrick Evans', 'Deepeka Garg', 'Annapoorani Lakshmi Narayanan', 'Makada Henry-Nickie', 'Sumitra Ganesh']",[],0,arXiv,http://arxiv.org/abs/2411.00563v1,False,True,False,False,False,1831,Xiao Cen,Texas,Active,2018,,"This research characterizes the two-way spillover effects between the labor and mortgage markets. An understudied facet of home mortgage is its impact on the liquidity profile of the borrowing households. When home equity is low, serving the debt with periodic payments can constitute a real burden to borrowers and may change their risk appetite in the labor market. We document how mortgage debt affects borrowers’ propensity to take riskier jobs, such as working in startups or high-turnover industries, as well as how employment status affects borrowers’ mortgage performance. Using data mainly from the Longitudinal Employer-Household Dynamics, Longitudinal Business Database, and Corelogic deeds, we identify effects utilizing a variety of quasi-exogenous shocks on employment, home equity, and mortgage payments. We expect that mortgage liability discourages borrowers, and especially if the heads of the households are already in debt, from taking a riskier career path and increases the likelihood of staying in a stable job with predictable incomes. "
"Relevance distributions across Bradford Zones: Can Bradfordizing improve
  search?","The purpose of this paper is to describe the evaluation of the effectiveness
of the bibliometric technique Bradfordizing in an information retrieval (IR)
scenario. Bradfordizing is used to re-rank topical document sets from
conventional abstracting & indexing (A&I) databases into core and more
peripheral document zones. Bradfordized lists of journal articles and
monographs will be tested in a controlled scenario consisting of different A&I
databases from social and political sciences, economics, psychology and medical
science, 164 standardized IR topics and intellectual assessments of the listed
documents. Does Bradfordizing improve the ratio of relevant documents in the
first third (core) compared to the second and last third (zone 2 and zone 3,
respectively)? The IR tests show that relevance distributions after re-ranking
improve at a significant level if documents in the core are compared with
documents in the succeeding zones. After Bradfordizing of document pools, the
core has a significant better average precision than zone 2, zone 3 and
baseline. This paper should be seen as an argument in favour of alternative
non-textual (bibliometric) re-ranking methods which can be simply applied in
text-based retrieval systems and in particular in A&I databases.",['Philipp Mayr'],[],0,arXiv,http://arxiv.org/abs/1305.0357v1,False,True,False,False,False,1832,J Bradford Jensen,Georgetown,Active,2019,,The structure of the American economy has changed substantially over the past 40 years. The manufacturing sector accounted for more than 25 percent of the labor force in 1970; it accounts for less than 10 percent now. Technological innovation and international trade have contributed to the evolution of the American economy and firms continue to change and evolve through innovation and international trade and investment. A key question is whether the US statistical system has kept pace with the structural changes in the US economy and whether existing data products adequately measure and document these important changes.
Defacing the map: Cartographic vandalism in the digital commons,"This article addresses the emergent phenomenon of carto-vandalism, the
intentional defacement of collaborative cartographic digital artefacts in the
context of volunteered geographic information. Through a qualitative analysis
of reported incidents in WikiMapia and OpenStreetMap, a typology of this kind
of vandalism is outlined, including play, ideological, fantasy, artistic, and
industrial carto-vandalism, as well as carto-spam. Two families of
counter-strategies deployed in amateur mapping communities are discussed.
First, the contributors organise forms of policing, based on volunteered
community involvement, patrolling the maps and reporting incidents. Second, the
detection of carto-vandalism can be supported by automated tools, based either
on explicit rules or on machine learning.",['Andrea Ballatore'],[],0,arXiv,http://arxiv.org/abs/1404.3341v1,False,True,False,False,False,1833,Laurie Paarlberg,Atlanta,Active,2017,,"This research uses data from the Current Population Survey supplement on volunteering to explore how community cohesion affects volunteering behavior and how the effects of community cohesion (racial diversity, income inequality and mobility) differ across individual-level racial characteristics. A growing body of empirical research on civic engagement suggests that the effects of community cohesion differ across people of different racial backgrounds. However, there are limited studies specifically in the context of volunteering. The restricted-use data enables the researchers to merge in community characteristics at the smallest level of geography possible, which is important for individual volunteering behavior. It also enables researchers to include respondents from rural communities, reducing the bias in the results, and improving our understanding of volunteering in rural communities."
Automatic Estimation of Simultaneous Interpreter Performance,"Simultaneous interpretation, translation of the spoken word in real-time, is
both highly challenging and physically demanding. Methods to predict
interpreter confidence and the adequacy of the interpreted message have a
number of potential applications, such as in computer-assisted interpretation
interfaces or pedagogical tools. We propose the task of predicting simultaneous
interpreter performance by building on existing methodology for quality
estimation (QE) of machine translation output. In experiments over five
settings in three language pairs, we extend a QE pipeline to estimate
interpreter performance (as approximated by the METEOR evaluation metric) and
propose novel features reflecting interpretation strategy and evaluation
measures that further improve prediction accuracy.","['Craig Stewart', 'Nikolai Vogler', 'Junjie Hu', 'Jordan Boyd-Graber', 'Graham Neubig']",[],0,arXiv,http://arxiv.org/abs/1805.04016v2,False,True,False,False,False,1835,Craig Carpenter,Texas,Completed,2018,2023.0,"This study evaluates how local geographic, socioeconomic, and industrial factors impact the size of an industry in a county. We utilize data from the Business Register, the Longitudinal Business Database, the Annual Retail Trade Survey, and the Annual Survey of Manufactures, as well as other economic and demographic datasets to assess the size of an industry in a county. We model the unbiased minimum and total establishment counts due to the inclusion of nonemployers, and the increased accuracy in the measurement of industry size by using employment and payroll, rather than only the number of establishments, in a broader range of industries than previous research has addressed. We also examine how local geographic, socioeconomic, and industrial factors predict operating costs, and how local geographic, socioeconomic, and industrial factors predict e-commerce usage, which will significantly contribute to economic and regional science academic literature."
"Anomaly Detection of Particle Orbit in Accelerator using LSTM Deep
  Learning Technology","A stable, reliable, and controllable orbit lock system is crucial to an
electron (or ion) accelerator because the beam orbit and beam energy
instability strongly affect the quality of the beam delivered to experimental
halls. Currently, when the orbit lock system fails operators must manually
intervene. This paper develops a Machine Learning based fault detection
methodology to identify orbit lock anomalies and notify accelerator operations
staff of the off-normal behavior. Our method is unsupervised, so it does not
require labeled data. It uses Long-Short Memory Networks (LSTM) Auto Encoder to
capture normal patterns and predict future values of monitoring sensors in the
orbit lock system. Anomalies are detected when the prediction error exceeds a
threshold. We conducted experiments using monitoring data from Jefferson Lab's
Continuous Electron Beam Accelerator Facility (CEBAF). The results are
promising: the percentage of real anomalies identified by our solution is
68.6%-89.3% using monitoring data of a single component in the orbit lock
control system. The accuracy can be as high as 82%.","['Zhiyuan Chen', 'Wei Lu', 'Radhika Bhong', 'Yimin Hu', 'Brian Freeman', 'Adam Carpenter']",[],0,arXiv,http://arxiv.org/abs/2401.15543v1,False,True,False,False,False,1835,Craig Carpenter,Texas,Completed,2018,2023.0,"This study evaluates how local geographic, socioeconomic, and industrial factors impact the size of an industry in a county. We utilize data from the Business Register, the Longitudinal Business Database, the Annual Retail Trade Survey, and the Annual Survey of Manufactures, as well as other economic and demographic datasets to assess the size of an industry in a county. We model the unbiased minimum and total establishment counts due to the inclusion of nonemployers, and the increased accuracy in the measurement of industry size by using employment and payroll, rather than only the number of establishments, in a broader range of industries than previous research has addressed. We also examine how local geographic, socioeconomic, and industrial factors predict operating costs, and how local geographic, socioeconomic, and industrial factors predict e-commerce usage, which will significantly contribute to economic and regional science academic literature."
"Competing for Attention -- The Effect of Talk Radio on Elections and
  Political Polarization in the US","This paper studies the effects of talk radio, specifically the Rush Limbaugh
Show, on electoral outcomes and attitude polarization in the U.S. We propose a
novel identification strategy that considers the radio space in each county as
a market where multiple stations are competing for listeners' attention. Our
measure of competition is a spatial Herfindahl-Hirschman Index (HHI) in radio
frequencies. To address endogeneity concerns, we exploit the variation in
competition based on accidental frequency overlaps in a county, conditional on
the overall level of radio frequency competition. We find that counties with
higher exposure to the Rush Limbaugh Show have a systematically higher vote
share for Donald Trump in the 2016 and 2020 U.S. presidential elections.
Combining our county-level Rush Limbaugh Show exposure measure with individual
survey data reveals that self-identifying Republicans in counties with higher
exposure to the Show express more conservative political views, while
self-identifying Democrats in these same counties express more moderate
political views. Taken together, these findings provide some of the first
insights on the effects of contemporary talk radio on political outcomes, both
at the aggregate and individual level.","['Ashani Amarasinghe', 'Paul A. Raschky']",[],0,arXiv,http://arxiv.org/abs/2206.13675v1,False,True,False,False,False,1849,Jason Fletcher,Wisconsin,Active,2019,,"This project analyzes the effects of policy and environmental factors on contemporaneous and future schooling patterns. Our outcome variables of interest include a broad set of socioeconomic outcomes; we focus on educational attainment and household income level. Our two aims are to estimate (1) the impacts of five alternative policy/environmental contexts, holding constant each of the other contexts and (2) to explore interactive effects between some of the contexts of interest, where some birth cohorts in some counties could have been exposed to multiple contexts (i.e. both school desegregation and high pollution); in these cases, we will investigate evidence of statistical interaction in the contextual effects. Our focal contexts include school desegregation policies for school ages; exposure to pollutants in utero; hospital desegregation; introduction of welfare programs; and in utero exposure to salt iodization. We will use external, county level data to incorporate these contextual factors for individual in the ACS and Census data. In each case, the ""in utero"" period for each person is operationalized to be the 9-months before the birth date. These benefits cannot be completed with public data because public data do not contain county-of-birth information. Therefore, we are asking for access to restricted data.

Economists and other social scientists have long been interested in the life course impacts of early exposures to environmental and policy conditions. However, it is difficult to empirically uncover causal factors on later socioeconomic attainment. Our proposal contributes to our understanding of early precursors to these attainments by deploying a set of ""natural experiment"" research designs to combine information on exposures based on county of birth. We leverage space and time variation in these exposures to trace out the longer term impacts on adult outcomes"
"A scalable two-stage Bayesian approach accounting for exposure
  measurement error in environmental epidemiology","Accounting for exposure measurement errors has been recognized as a crucial
problem in environmental epidemiology for over two decades. Bayesian
hierarchical models offer a coherent probabilistic framework for evaluating
associations between environmental exposures and health effects, which take
into account exposure measurement errors introduced by uncertainty in the
estimated exposure as well as spatial misalignment between the exposure and
health outcome data. While two-stage Bayesian analyses are often regarded as a
good alternative to fully Bayesian analyses when joint estimation is not
feasible, there has been minimal research on how to properly propagate
uncertainty from the first-stage exposure model to the second-stage health
model, especially in the case of a large number of participant locations along
with spatially correlated exposures. We propose a scalable two-stage Bayesian
approach, called a sparse multivariate normal (sparse MVN) prior approach,
based on the Vecchia approximation for assessing associations between exposure
and health outcomes in environmental epidemiology. We compare its performance
with existing approaches through simulation. Our sparse MVN prior approach
shows comparable performance with the fully Bayesian approach, which is a gold
standard but is impossible to implement in some cases. We investigate the
association between source-specific exposures and pollutant (nitrogen dioxide
(NO$_2$))-specific exposures and birth outcomes for 2012 in Harris County,
Texas, using several approaches, including the newly developed method.","['Changwoo J. Lee', 'Elaine Symanski', 'Amal Rammah', 'Dong Hun Kang', 'Philip K. Hopke', 'Eun Sug Park']",[],0,arXiv,http://arxiv.org/abs/2401.00634v2,False,True,False,False,False,1849,Jason Fletcher,Wisconsin,Active,2019,,"This project analyzes the effects of policy and environmental factors on contemporaneous and future schooling patterns. Our outcome variables of interest include a broad set of socioeconomic outcomes; we focus on educational attainment and household income level. Our two aims are to estimate (1) the impacts of five alternative policy/environmental contexts, holding constant each of the other contexts and (2) to explore interactive effects between some of the contexts of interest, where some birth cohorts in some counties could have been exposed to multiple contexts (i.e. both school desegregation and high pollution); in these cases, we will investigate evidence of statistical interaction in the contextual effects. Our focal contexts include school desegregation policies for school ages; exposure to pollutants in utero; hospital desegregation; introduction of welfare programs; and in utero exposure to salt iodization. We will use external, county level data to incorporate these contextual factors for individual in the ACS and Census data. In each case, the ""in utero"" period for each person is operationalized to be the 9-months before the birth date. These benefits cannot be completed with public data because public data do not contain county-of-birth information. Therefore, we are asking for access to restricted data.

Economists and other social scientists have long been interested in the life course impacts of early exposures to environmental and policy conditions. However, it is difficult to empirically uncover causal factors on later socioeconomic attainment. Our proposal contributes to our understanding of early precursors to these attainments by deploying a set of ""natural experiment"" research designs to combine information on exposures based on county of birth. We leverage space and time variation in these exposures to trace out the longer term impacts on adult outcomes"
Allocation of COVID-19 Testing Budget on a Commute Network of Counties,"The screening testing is an effective tool to control the early spread of an
infectious disease such as COVID-19. When the total testing capacity is
limited, we aim to optimally allocate testing resources among n counties. We
build a (weighted) commute network on counties, with the weight between two
counties a decreasing function of their traffic distance. We introduce a
network-based disease model, in which the number of newly confirmed cases of
each county depends on the numbers of hidden cases of all counties on the
network. Our proposed testing allocation strategy first uses historical data to
learn model parameters and then decides the testing rates for all counties by
solving an optimization problem. We apply the method on the commute networks of
Massachusetts, USA and Hubei, China and observe its advantages over testing
allocation strategies that ignore the network structure. Our approach can also
be extended to study the vaccine allocation problem.","['Yaxuan Huang', 'Zheng Tracy Ke', 'Jiashun Jin']",[],0,arXiv,http://arxiv.org/abs/2110.04381v2,False,True,False,False,False,1849,Jason Fletcher,Wisconsin,Active,2019,,"This project analyzes the effects of policy and environmental factors on contemporaneous and future schooling patterns. Our outcome variables of interest include a broad set of socioeconomic outcomes; we focus on educational attainment and household income level. Our two aims are to estimate (1) the impacts of five alternative policy/environmental contexts, holding constant each of the other contexts and (2) to explore interactive effects between some of the contexts of interest, where some birth cohorts in some counties could have been exposed to multiple contexts (i.e. both school desegregation and high pollution); in these cases, we will investigate evidence of statistical interaction in the contextual effects. Our focal contexts include school desegregation policies for school ages; exposure to pollutants in utero; hospital desegregation; introduction of welfare programs; and in utero exposure to salt iodization. We will use external, county level data to incorporate these contextual factors for individual in the ACS and Census data. In each case, the ""in utero"" period for each person is operationalized to be the 9-months before the birth date. These benefits cannot be completed with public data because public data do not contain county-of-birth information. Therefore, we are asking for access to restricted data.

Economists and other social scientists have long been interested in the life course impacts of early exposures to environmental and policy conditions. However, it is difficult to empirically uncover causal factors on later socioeconomic attainment. Our proposal contributes to our understanding of early precursors to these attainments by deploying a set of ""natural experiment"" research designs to combine information on exposures based on county of birth. We leverage space and time variation in these exposures to trace out the longer term impacts on adult outcomes"
"A Generalist, Automated ALFALFA Baryonic Tully-Fisher Relation","The Baryonic Tully-Fisher Relation (BTFR) has applications in galaxy
evolution as a testbed for the galaxy-halo connection and in observational
cosmology as a redshift-independent secondary distance indicator. We use the
31,000+ galaxy ALFALFA sample -- which provides redshifts, velocity widths, and
HI content for a large number of gas-bearing galaxies in the local universe --
to fit and test an extensive local universe BTFR. This BTFR is designed to be
as inclusive of ALFALFA and comparable samples as possible. Velocity widths
measured via an automated method and $M_{b}$ proxies extracted from survey data
can be uniformly and efficiently measured for other samples, giving this
analysis broad applicability. We also investigate the role of sample
demographics in determining the best-fit relation. We find that the best-fit
relations are changed significantly by changes to the sample mass range and to
second order, mass sampling, gas fraction, different stellar mass and velocity
width measurements. We use a subset of ALFALFA with demographics that reflect
the full sample to measure a robust BTFR slope of $3.30\pm0.06$. We apply this
relation and estimate source distances, finding general agreement with
flow-model distances as well as average distance uncertainties of $\sim0.17$
dex for the full ALFALFA sample. We demonstrate the utility of these distance
estimates by applying them to a sample of sources in the Virgo vicinity,
recovering signatures of infall consistent with previous work.","['Catie J. Ball', 'Martha P. Haynes', 'Michael G. Jones', 'Bo Peng', 'Adriana Durbala', 'Rebecca A. Koopmann', 'Joseph Ribaudo', ""Aileen O'Donoghue""]",[],0,arXiv,http://arxiv.org/abs/2212.08728v1,False,True,False,False,False,1854,Martha J Bailey,Michigan,Completed,2017,2023.0,"This research uses 1960 Census and the March CPS supplement to create state and county level fertility rates across a number of decades and compares these with administrative records data from the National Center for Health Statistics. This will provide a better understanding of population trends and promote more accurate CPS-based population projections. This project will examine the economic, social, and demographic impacts of various policy and natural experiments."
Bayesian Spatial Change of Support for Count-Valued Survey Data,"We introduce Bayesian spatial change of support methodology for count-valued
survey data with known survey variances. Our proposed methodology is motivated
by the American Community Survey (ACS), an ongoing survey administered by the
U.S. Census Bureau that provides timely information on several key demographic
variables. Specifically, the ACS produces 1-year, 3-year, and 5-year
""period-estimates,"" and corresponding margins of errors, for published
demographic and socio-economic variables recorded over predefined geographies
within the United States. Despite the availability of these predefined
geographies it is often of interest to data users to specify customized
user-defined spatial supports. In particular, it is useful to estimate
demographic variables defined on ""new"" spatial supports in ""real-time."" This
problem is known as spatial change of support (COS), which is typically
performed under the assumption that the data follows a Gaussian distribution.
However, count-valued survey data is naturally non-Gaussian and, hence, we
consider modeling these data using a Poisson distribution. Additionally,
survey-data are often accompanied by estimates of error, which we incorporate
into our analysis. We interpret Poisson count-valued data in small areas as an
aggregation of events from a spatial point process. This approach provides us
with the flexibility necessary to allow ACS users to consider a variety of
spatial supports in ""real-time."" We demonstrate the effectiveness of our
approach through a simulated example as well as through an analysis using
public-use ACS data.","['Jonathan R. Bradley', 'Christopher K. Wikle', 'Scott H. Holan']",[],0,arXiv,http://arxiv.org/abs/1405.7227v2,True,True,False,False,True,1854,Martha J Bailey,Michigan,Completed,2017,2023.0,"This research uses 1960 Census and the March CPS supplement to create state and county level fertility rates across a number of decades and compares these with administrative records data from the National Center for Health Statistics. This will provide a better understanding of population trends and promote more accurate CPS-based population projections. This project will examine the economic, social, and demographic impacts of various policy and natural experiments."
"A Super-Fast Distributed Algorithm for Bipartite Metric Facility
  Location","The \textit{facility location} problem consists of a set of
\textit{facilities} $\mathcal{F}$, a set of \textit{clients} $\mathcal{C}$, an
\textit{opening cost} $f_i$ associated with each facility $x_i$, and a
\textit{connection cost} $D(x_i,y_j)$ between each facility $x_i$ and client
$y_j$. The goal is to find a subset of facilities to \textit{open}, and to
connect each client to an open facility, so as to minimize the total facility
opening costs plus connection costs. This paper presents the first
expected-sub-logarithmic-round distributed O(1)-approximation algorithm in the
$\mathcal{CONGEST}$ model for the \textit{metric} facility location problem on
the complete bipartite network with parts $\mathcal{F}$ and $\mathcal{C}$. Our
algorithm has an expected running time of $O((\log \log n)^3)$ rounds, where $n
= |\mathcal{F}| + |\mathcal{C}|$. This result can be viewed as a continuation
of our recent work (ICALP 2012) in which we presented the first
sub-logarithmic-round distributed O(1)-approximation algorithm for metric
facility location on a \textit{clique} network. The bipartite setting presents
several new challenges not present in the problem on a clique network. We
present two new techniques to overcome these challenges. (i) In order to deal
with the problem of not being able to choose appropriate probabilities (due to
lack of adequate knowledge), we design an algorithm that performs a random walk
over a probability space and analyze the progress our algorithm makes as the
random walk proceeds. (ii) In order to deal with a problem of quickly
disseminating a collection of messages, possibly containing many duplicates,
over the bipartite network, we design a probabilistic hashing scheme that
delivers all of the messages in expected-$O(\log \log n)$ rounds.","['James Hegeman', 'Sriram V. Pemmaraju']",[],0,arXiv,http://arxiv.org/abs/1308.2694v1,False,True,False,False,False,1866,James D Gaboardi,Atlanta,Completed,2018,2021.0,"In network-centric research, population is represented by large, aggregated census geographies to reduce computational complexity and accommodate data availability. However, it is well known in the literature that as spatial data are aggregated and precise locational information is lost, error occurs. The purpose of this research is to develop a novel method, PolyPop2Net or PP2N (Population Polygons to Networks), for allocating populations to networks by utilizing restricted-use 2010 Decennial Census microdata and the restricted-use Master Address File Extract as the benchmark truth. We will use publicly available 2010 decennial data to run Operations Research and Network Analysis models to evaluate the accuracy and validity of the PP2N. Specifically, we test the PP2N method using nested Census Bureau geographies from the publicly available 2010 Decennial Census for Leon County, Florida. We use the simulated examples along with the true Leon County examples to determine the generalizability of the new method. The new method will then be shared with others and will be available for use through integration into a spatial analysis package in Python."
Super-Fast Distributed Algorithms for Metric Facility Location,"This paper presents a distributed O(1)-approximation algorithm, with
expected-$O(\log \log n)$ running time, in the $\mathcal{CONGEST}$ model for
the metric facility location problem on a size-$n$ clique network. Though
metric facility location has been considered by a number of researchers in
low-diameter settings, this is the first sub-logarithmic-round algorithm for
the problem that yields an O(1)-approximation in the setting of non-uniform
facility opening costs. In order to obtain this result, our paper makes three
main technical contributions. First, we show a new lower bound for metric
facility location, extending the lower bound of B\u{a}doiu et al. (ICALP 2005)
that applies only to the special case of uniform facility opening costs. Next,
we demonstrate a reduction of the distributed metric facility location problem
to the problem of computing an O(1)-ruling set of an appropriate spanning
subgraph. Finally, we present a sub-logarithmic-round (in expectation)
algorithm for computing a 2-ruling set in a spanning subgraph of a clique. Our
algorithm accomplishes this by using a combination of randomized and
deterministic sparsification.","['Andrew Berns', 'James Hegeman', 'Sriram V. Pemmaraju']",[],0,arXiv,http://arxiv.org/abs/1308.2473v1,False,True,False,False,False,1866,James D Gaboardi,Atlanta,Completed,2018,2021.0,"In network-centric research, population is represented by large, aggregated census geographies to reduce computational complexity and accommodate data availability. However, it is well known in the literature that as spatial data are aggregated and precise locational information is lost, error occurs. The purpose of this research is to develop a novel method, PolyPop2Net or PP2N (Population Polygons to Networks), for allocating populations to networks by utilizing restricted-use 2010 Decennial Census microdata and the restricted-use Master Address File Extract as the benchmark truth. We will use publicly available 2010 decennial data to run Operations Research and Network Analysis models to evaluate the accuracy and validity of the PP2N. Specifically, we test the PP2N method using nested Census Bureau geographies from the publicly available 2010 Decennial Census for Leon County, Florida. We use the simulated examples along with the true Leon County examples to determine the generalizability of the new method. The new method will then be shared with others and will be available for use through integration into a spatial analysis package in Python."
Dynamic Facility Location via Exponential Clocks,"The \emph{dynamic facility location problem} is a generalization of the
classic facility location problem proposed by Eisenstat, Mathieu, and Schabanel
to model the dynamics of evolving social/infrastructure networks. The
generalization lies in that the distance metric between clients and facilities
changes over time. This leads to a trade-off between optimizing the classic
objective function and the ""stability"" of the solution: there is a switching
cost charged every time a client changes the facility to which it is connected.
While the standard linear program (LP) relaxation for the classic problem
naturally extends to this problem, traditional LP-rounding techniques do not,
as they are often sensitive to small changes in the metric resulting in
frequent switches.
  We present a new LP-rounding algorithm for facility location problems, which
yields the first constant approximation algorithm for the dynamic facility
location problem. Our algorithm installs competing exponential clocks on the
clients and facilities, and connect every client by the path that repeatedly
follows the smallest clock in the neighborhood. The use of exponential clocks
gives rise to several properties that distinguish our approach from previous
LP-roundings for facility location problems. In particular, we use \emph{no
clustering} and we allow clients to connect through paths of \emph{arbitrary
lengths}. In fact, the clustering-free nature of our algorithm is crucial for
applying our LP-rounding approach to the dynamic problem.","['Hyung-Chan An', 'Ashkan Norouzi-Fard', 'Ola Svensson']",[],0,arXiv,http://arxiv.org/abs/1411.4476v1,False,True,False,False,False,1866,James D Gaboardi,Atlanta,Completed,2018,2021.0,"In network-centric research, population is represented by large, aggregated census geographies to reduce computational complexity and accommodate data availability. However, it is well known in the literature that as spatial data are aggregated and precise locational information is lost, error occurs. The purpose of this research is to develop a novel method, PolyPop2Net or PP2N (Population Polygons to Networks), for allocating populations to networks by utilizing restricted-use 2010 Decennial Census microdata and the restricted-use Master Address File Extract as the benchmark truth. We will use publicly available 2010 decennial data to run Operations Research and Network Analysis models to evaluate the accuracy and validity of the PP2N. Specifically, we test the PP2N method using nested Census Bureau geographies from the publicly available 2010 Decennial Census for Leon County, Florida. We use the simulated examples along with the true Leon County examples to determine the generalizability of the new method. The new method will then be shared with others and will be available for use through integration into a spatial analysis package in Python."
"Autonomous Aerial Robotic Surveying and Mapping with Application to
  Construction Operations","In this paper we present an overview of the methods and systems that give
rise to a flying robotic system capable of autonomous inspection, surveying,
comprehensive multi-modal mapping and inventory tracking of construction sites
with high degree of systematicity. The robotic system can operate assuming
either no prior knowledge of the environment or by integrating a prior model of
it. In the first case, autonomous exploration is provided which returns a high
fidelity $3\textrm{D}$ map associated with color and thermal vision
information. In the second case, the prior model of the structure can be used
to provide optimized and repetitive coverage paths. The robot delivers its
mapping result autonomously, while simultaneously being able to detect and
localize objects of interest thus supporting inventory tracking tasks. The
system has been field verified in a collection of environments and has been
tested inside a construction project related to public housing.","['Huan Nguyen', 'Frank Mascarich', 'Tung Dang', 'Kostas Alexis']",[],0,arXiv,http://arxiv.org/abs/2005.04335v1,False,True,False,False,False,1871,Cuneyt Eroglu,Boston,Active,2022,,"Ever since the 1980's, consultants, experts, and trade press have consistently advised managers to pursue a lean strategy and to lower inventories regardless of industry, product or geographic differences. Yet, empirical evidence suggests that efforts to reduce inventories do not always lead to improved firm performance. It is conceivable that the relationship between inventories and firm performance is moderated by the environment in which a firm operates. The research question addressed in this proposal is to identify the environmental conditions that shape the inventory-performance relationship.

The project will estimate the effect of production environmental characteristics, operationalized through the Organizational Task Environment (OTE), on the relationship of inventory management and firm performance. The effect of inventory leanness is measured by regressing firm performance, measured as profitability or productivity, on an Empirical Leanness Indicator (ELI) estimated using turnover curves. The ELI is defined by the difference in a given firm's inventory as compared to the firm-size adjusted average inventory level for its industry. 

The moderating variables are environmental characteristics which are operationalized through the Organizational Task Environment (OTE). OTE is operationalized along three dimensions, namely munificence, dynamism and complexity. Munificence reflects the abundance or scarcity of production resources and capacity, generally estimated using sales growth. Dynamism refers to environmental volatility and comprises facets of instability and unpredictability, and will be measured by sales volatility over time. Complexity describes the difficulty to understand and manage a firm's environment, and will be proxied by measures of R&D intensity, information technology intensity, and management practices such as decentralization and data driven decision making. Numerous studies have found evidence of the moderating effects of environmental dynamism, complexity and munificence on the strategy-performance relationship of firms within their supply chains. Hence, the central tenet of this research project is that these OTE characteristics define the extent to which inventory leanness is associated with performance improvements in a given industry environment."
Voluntary Disclosure and Personalized Pricing,"Central to privacy concerns is that firms may use consumer data to price
discriminate. A common policy response is that consumers should be given
control over which firms access their data and how. Since firms learn about a
consumer's preferences based on the data seen and the consumer's disclosure
choices, the equilibrium implications of consumer control are unclear. We study
whether such measures improve consumer welfare in monopolistic and competitive
markets. We find that consumer control can improve consumer welfare relative to
both perfect price discrimination and no personalized pricing. First, consumers
can use disclosure to amplify competitive forces. Second, consumers can
disclose information to induce even a monopolist to lower prices. Whether
consumer control improves welfare depends on the disclosure technology and
market competitiveness. Simple disclosure technologies suffice in competitive
markets. When facing a monopolist, a consumer needs partial disclosure
possibilities to obtain any welfare gains.","['S. Nageeb Ali', 'Greg Lewis', 'Shoshana Vasserman']",[],0,arXiv,http://arxiv.org/abs/1912.04774v2,False,True,False,False,False,1873,Ryan C Lewis,Colorado,Active,2018,,"This research investigates the welfare implications of corporate actions by examining the effects of leverage, mergers, and acquisition on other nearby firms and local labor markets. In addition to leveraging well-established estimates of local commuting zones to assess spillover effects of these actions, we use data from Longitudinal Employer-Household Dynamics and the Longitudinal Business Database to compute granular overlapping zones and a new measure of local labor and firm competition that we call Regions of Spatial Competition, which should be more accurate for areas on the borders of existing commuting zones. We then compare the mobility and wages of workers in these markets who do and do not experience corporate shocks. If differences are found, results suggest that existing corporate finance theories focusing on privately optimal frameworks fail to capture ways that spillover impacts local markets more widely. "
An Economical Business-Cycle Model,"This paper develops a new model of business cycles. The model is economical
in that it is solved with an aggregate demand-aggregate supply diagram, and the
effects of shocks and policies are obtained by comparative statics. The model
builds on two unconventional assumptions. First, producers and consumers meet
through a matching function. Thus, the model features unemployment, which
fluctuates in response to aggregate demand and supply shocks. Second, wealth
enters the utility function, so the model allows for permanent zero-lower-bound
episodes. In the model, the optimal monetary policy is to set the interest rate
at the level that eliminates the unemployment gap. This optimal interest rate
is computed from the prevailing unemployment gap and monetary multiplier (the
effect of the nominal interest rate on the unemployment rate). If the
unemployment gap is exceedingly large, monetary policy cannot eliminate it
before reaching the zero lower bound, but a wealth tax can.","['Pascal Michaillat', 'Emmanuel Saez']",[],0,arXiv,http://arxiv.org/abs/1912.07163v2,False,True,False,False,False,1877,Daniel Tannenbaum,Nebraska,Active,2019,,"The predominant purpose of this research is to benefit the US Census Bureau by presenting updated statistics on employment, marriage and cohabitating patterns, labor force participation, and uptake of public assistance in response to a layoff at the household level. In conjunction with our primary analysis, we also examine unit non-response to the CPS-ASEC after a layoff event and create a crosswalk that links marital status to the LEHD for multiple years. To accomplish this goal, we request the use of restricted data from three datasets, each of which we propose to link separately to the LEHD. These datasets are: (1) the Decennial Census, 2000 and 2010, and 2020 if available, (2) the American Community Survey (ACS), 2008–2015 and 2016-2019 if available, and (3) the Annual Social and Economic Supplement to the Current Population Survey, 1985-2014, and 2015-2019 if available.
One of the goals of this project is to provide a detailed picture of the insurance role of the family. We plan to answer the following questions: 
• What is the effect of worker displacement on the long-run employment and earnings paths of family members, including spouses and adult children? 
• What are the relative roles of family members and public transfer programs over time in recovering the earnings losses of displaced workers? 
• Does worker displacement affect family structure, such as the likelihood of divorce or separation? How does a cohabiting individual differ in their responses to a displacement compared to a married individual? 
These questions are critical for understanding demographic and employment trends in the U.S., such as the decline of male labor force participation. The proposal will produce updated estimates of the effects of worker displacement on families. It will generate statistical estimates of several outcomes of interest: (i) long-run employment and earnings of family members, (ii) usage of public transfer programs, and (iii) family ruptures (such as separation or divorce).
A final and significant contribution of our research is to study how unemployment affects family structure, including the likelihood of separation or divorce, or the likelihood of marriage for non-married individuals. In the last half-century, the U.S. has experienced dramatic changes in family structure, including the rise of cohabiting couples and non-married couples with children.  Do cohabiting partners behave similarly to married partners in providing an insurance role in their labor supply decisions? The answer to this question is important for assessing the generosity of public assistance programs in the modern era of complex family arrangements. Prior work has found that job loss significantly raises the probability of divorce (Charles et al., 2004). Our proposed research will extend this analysis to study longer time horizons, and with an arguably improved identification strategy; in addition, we will include an analysis of cohabiting individuals to get a more complete picture of the dynamics of family structure in relation to unemployment."
"Investigating the effects of housing instability on depression, anxiety,
  and mental health treatment in childhood and adolescence","Housing instability is a widespread phenomenon in the United States. In
combination with other social determinants of health, housing instability
affects children's overall health and development. Drawing on data from the
2022 National Survey of Children's Health, we employed multiple logistic
regression models to understand how sociodemographic factors, especially
housing instability, affect mental health outcomes and treatment access for
youth aged 6-17 years. Our results show that youth facing housing instability
have a higher likelihood of experiencing anxiety (OR: 1.42, p<0.001) and
depression (OR: 1.57, p<0.001). Furthermore, youth experiencing both mental
health conditions and housing instability are significantly less likely to
receive mental health services in the past year, indicating the substantial
barriers they face in accessing mental health care. Based on our findings, we
highlight opportunities for digital mental health interventions to provide
children experiencing housing instability with more accessible and consistent
mental health services.","['Rachael Zehrung', 'Di Hu', 'Yawen Guo', 'Kai Zheng', 'Yunan Chen']",[],0,arXiv,http://arxiv.org/abs/2409.06011v2,False,False,False,False,True,1881,Sandra Newman,Maryland,Active,2020,,"This project will examine the housing, neighborhood, and housing market conditions of families with children living in federally assisted housing compared with similar families with children who do not receive housing assistance. Analysis will include difference in means across major subgroups (e.g., family size, income), and multivariate analysis predicting housing, neighborhood, and market characteristics of the assisted and unassisted samples separately to test whether there are differences in predictors of these outcomes. We will also test models that pool the assisted and unassisted groups to observe whether living in assisted housing has a large and significant effect on housing, neighborhood, and housing market outcomes."
"Automatically Detecting Self-Reported Birth Defect Outcomes on Twitter
  for Large-scale Epidemiological Research","In recent work, we identified and studied a small cohort of Twitter users
whose pregnancies with birth defect outcomes could be observed via their
publicly available tweets. Exploiting social media's large-scale potential to
complement the limited methods for studying birth defects, the leading cause of
infant mortality, depends on the further development of automatic methods. The
primary objective of this study was to take the first step towards scaling the
use of social media for observing pregnancies with birth defect outcomes,
namely, developing methods for automatically detecting tweets by users
reporting their birth defect outcomes. We annotated and pre-processed
approximately 23,000 tweets that mention birth defects in order to train and
evaluate supervised machine learning algorithms, including feature-engineered
and deep learning-based classifiers. We also experimented with various
under-sampling and over-sampling approaches to address the class imbalance. A
Support Vector Machine (SVM) classifier trained on the original, imbalanced
data set, with n-grams, word clusters, and structural features, achieved the
best baseline performance for the positive classes: an F1-score of 0.65 for the
""defect"" class and 0.51 for the ""possible defect"" class. Our contributions
include (i) natural language processing (NLP) and supervised machine learning
methods for automatically detecting tweets by users reporting their birth
defect outcomes, (ii) a comparison of feature-engineered and deep
learning-based classifiers trained on imbalanced, under-sampled, and
over-sampled data, and (iii) an error analysis that could inform classification
improvements using our publicly available corpus. Future work will focus on
automating user-level analyses for cohort inclusion.","['Ari Z. Klein', 'Abeed Sarker', 'Davy Weissenbacher', 'Graciela Gonzalez-Hernandez']",[],0,arXiv,http://arxiv.org/abs/1810.09506v1,False,True,False,False,False,1888,Andrew Barr,Texas,Completed,2018,2024.0,"This research estimates the long run impact of childhood age, education, and nutrition exposure on adulthood labor market and related outcomes. We measure childhood exposure based upon variation in program or institution availability within an individual’s county of birth. In particular, we consider ages 0 to 5 exposure to the rollout of Food Stamps, Head Start, and their interaction, ages 1 to 17 exposure to college openings, as well as childhood age and exposure to other nutrition and education programs and institutions. County of birth is identified using the restricted-use versions of demographic surveys linked to the Numident File, which contains a place of birth variable. Surveys considered include the Decennial Census, the American Community Survey, the Current Population Survey, and the Survey of Income and Program Participation. These surveys enable us to measure the effects of program and institution exposure on a wide array of outcomes including educational attainment, employment status, earnings as well as health, migration, adulthood program participation, and mortality. These estimates extend existing research that documents the short-run impacts of these programs and institutions by demonstrating the long-run impacts of these programs and institutions. Estimates of long-run impacts are necessary for understanding the full societal benefits of these large and costly programs and institutions."
"Labor Market Outcomes and Early Schooling: Evidence from School Entry
  Policies Using Exact Date of Birth","We use a rich, census-like Brazilian dataset containing information on
spatial mobility, schooling, and income in which we can link children to
parents to assess the impact of early education on several labor market
outcomes. Brazilian public primary schools admit children up to one year
younger than the national minimum age to enter school if their birthday is
before an arbitrary threshold, causing an exogenous variation in schooling at
adulthood. Using a Regression Discontinuity Design, we estimate one additional
year of schooling increases labor income in 25.8% - almost twice as large as
estimated using mincerian models. Around this cutoff there is also a gap of
9.6% on the probability of holding a college degree in adulthood, with which we
estimate the college premium and find a 201% increase in labor income. We test
the robustness of our estimates using placebo variables, alternative model
specifcations and McCrary Density Tests.","['Pedro Cavalcante Oliveira', 'Daniel Duque']",[],0,arXiv,http://arxiv.org/abs/1905.13281v1,False,True,False,False,False,1888,Andrew Barr,Texas,Completed,2018,2024.0,"This research estimates the long run impact of childhood age, education, and nutrition exposure on adulthood labor market and related outcomes. We measure childhood exposure based upon variation in program or institution availability within an individual’s county of birth. In particular, we consider ages 0 to 5 exposure to the rollout of Food Stamps, Head Start, and their interaction, ages 1 to 17 exposure to college openings, as well as childhood age and exposure to other nutrition and education programs and institutions. County of birth is identified using the restricted-use versions of demographic surveys linked to the Numident File, which contains a place of birth variable. Surveys considered include the Decennial Census, the American Community Survey, the Current Population Survey, and the Survey of Income and Program Participation. These surveys enable us to measure the effects of program and institution exposure on a wide array of outcomes including educational attainment, employment status, earnings as well as health, migration, adulthood program participation, and mortality. These estimates extend existing research that documents the short-run impacts of these programs and institutions by demonstrating the long-run impacts of these programs and institutions. Estimates of long-run impacts are necessary for understanding the full societal benefits of these large and costly programs and institutions."
Dynamic Multi-Factor Clustering of Financial Networks,"We investigate the tendency for financial instruments to form clusters when
there are multiple factors influencing the correlation structure. Specifically,
we consider a stock portfolio which contains companies from different
industrial sectors, located in several different countries. Both sector
membership and geography combine to create a complex clustering structure where
companies seem to first be divided based on sector, with geographical
subclusters emerging within each industrial sector. We argue that standard
techniques for detecting overlapping clusters and communities are not able to
capture this type of structure, and show how robust regression techniques can
instead be used to remove the influence of both sector and geography from the
correlation matrix separately. Our analysis reveals that prior to the 2008
financial crisis, companies did not tend to form clusters based on geography.
This changed immediately following the crisis, with geography becoming a more
important determinant of clustering.",['Gordon J. Ross'],[],0,arXiv,http://arxiv.org/abs/1505.01550v1,False,True,False,False,False,1890,Oren Ziv,Michigan,Active,2018,,"Over the course of the 20th century and continuing today, U.S. regions have experienced massive shifts to the geographic distribution of economic activity. In this research, we will seek to answer the following three questions: (1) How can we better catalogue these changes in the location of economic activity and industrial networks over the past half-century? (2) To what extent have changes in transportation costs, productivity spillovers, production networks, or exposure to international trade contributed to this reallocation and to regional divergence? (3) How have forces specifically internal to the firm contributed to regional reallocation? We will obtain establishment-level employment data and geographic information from the Longitudinal Business Database. Sales data from the Census of Manufactures, in conjunction with Commodity Flows Survey, will be used to produce causally identified industry-year estimates of agglomeration forces. "
"Multifractal analysis of racially-constrained population patterns and
  residential segregation in the US cities","A phenomenon of racial segregation in U.S. cities is a multifaceted area of
study. A recent advancement in this field is the development of a methodology
that transforms census population count-by-race data into a grid of monoracial
cells. This format enables assessment of heterogeneity of segregation within a
city. This paper leverages such a grid for the quantification of
race-constrained population patterns, allowing for the calculation and mapping
of binary segregation patterns within arbitrary region. A key innovation is the
application of Multifractal Analysis (MFA) to quantify the residency patterns
of race-constrained populations. The residency pattern is characterized by a
multifractal spectrum function, where the independent variable is a local
metric of pattern's ""gappiness"", and the dependent variable is proportional to
the size of the sub-pattern consisting of all locations having the same value
of this metric. In the context of binary populations, the gappiness of the
race-constrained population's pattern is intrinsically linked to its
segregation. This paper provides a comprehensive description of the
methodology, illustrated with examples focusing on the residency pattern of
Black population in the central region of Washington, DC. Further, the
methodology is demonstrated using a sample of residency patterns of Black
population in fourteen large U.S. cities. By numerically describing each
pattern through a multifractal spectrum, the fourteen patterns are clustered
into three distinct categories, each having unique characteristics. Maps of
local gappiness and segregation for each city are provided to show the
connection between the nature of the multifractal spectrum and the
corresponding residency and segregation patterns. This method offers an
excellent quantification of race-restricted residency and residential
segregation patterns within U.S. cities.","['Tomasz F. Stepinski', 'Anna Dmowska']",[],0,arXiv,http://arxiv.org/abs/2407.14977v1,False,True,False,False,False,1899,Nicole Jones,Missouri,Completed,2018,2022.0,"Research often shows that Blacks tend to experience the greatest disparity in gaining access into White neighborhoods, often residing in areas inferior to other minorities. This research uses Decennial Census and American Community Survey microdata to assess the impact individual (i.e., race) and social (i.e., socioeconomic status) characteristics have on Black residential outcomes over time. Two questions guide this study: (1) To what extent do individual and social characteristics affect Black residential outcomes over time? (2) Stratified by nativity, to what degree do individual and social characteristics affect foreign-born Black residential outcomes? The outcome variables are neighborhood racial/ethnic composition and neighborhood socioeconomic status. "
"Development of current estimated household data and agent-based
  simulation of the future population distribution of households in Japan","In response to the declining population and aging infrastructure in Japan,
local governments are implementing compact city policies such as the location
normalization plan. To optimize the reorganization of urban public
infrastructure, it is important to provide detailed and accurate forecasts of
the distribution of urban populations and households. However, many local
governments do not have the necessary data and forecasting capability.
Moreover, current forecasts of gender- and age-based population data only exist
at the municipal level, and household data are only available by family type at
the prefecture level. Meanwhile, the accuracy is limited with an assumption of
same change rate of population in all municipalities and within each city.
Therefore, the aim of this study was to develop an agent-based microsimulation
household transition model, with the household as the unit and agent, and
household data was estimated for all cities in Japan from 2015. Estimated
household data comprised the family type, house type, and address, age, and
gender of household members, obtained from the national census, and building
data. The resulting household transition model was used to forecast the
attributes of each household every five years. Simulations in Toyama and
Shizuoka Prefectures, Japan from 1980 to 2010 provided highly accurate
estimates of municipal-level population by age and household volume by family
type. The proposed model was also applied to predict the future distribution of
disappearing villages and vacant houses in Japan.","['Kajiwara Kento', 'Jue Ma', 'Toshikazu Seto', 'Yoshihide Sekimoto', 'Yoshiki Ogawa', 'Hiroshi Omata']",[],0,arXiv,http://arxiv.org/abs/2204.00198v1,False,True,False,False,False,1901,Jared Carbone,Colorado,Active,2020,,"This proposed study will benefit the Census Bureau under Title 13, Chapter 5. Specifically, this project will produce estimates that will provide a greater understanding of the effects of moving costs on relocation decisions for environmental reasons, which will be directly relevant for informing public policy related to environmental justice (criteria 3). Also, this research will provide a comprehensive understanding of geographic variation in environmental factors with a thorough description of the Californian population by geographic location and environmental quality (criteria 11).

Researchers consistently observe households sort themselves in response to regional differences in neighborhood characteristics such as environmental quality, crime rates, and racial mix of the population (Kuminoff et al., 2013; Tiebout, 1956). Environmental justice (EJ) literature has demonstrated that this sorting behavior can explain why certain types of demographic group such as low-income households disproportionately live near undesirable land uses (Banzhaf, 2012; Depro et al., 2015). If households find environmental harm undesirable, demand for housing near such areas will fall, driving local housing prices down. Low-income households may also dislike polluted areas, but they would be less willing to, or able to, pay for a clean environmental. Thus, they may move to locations with pollution to save money in housing expenditure and use it to purchase other goods.

This project will explore the role that barriers to moving in response to environmental harms may play in shaping the patterns found by the EJ literature. In other words, this paper addresses the question: To what extent do moving costs prohibit households from moving to cleaner locations? A recent finding in Lee (2017) suggests that lower income households, especially those with children, face higher moving costs than their wealthier/childless counterparts. Using counterfactual experiments, Lee verifies that lowering moving costs encourage households to move to locations with better air quality. "
"Estimation of the Effect of Carbon Tax Implementation on Household
  Income Distribution in Indonesia: Quantitative Analysis with Miyazawa Input-
  Output Approach","Climate change is a global challenge caused by greenhouse gas emissions from
fossil fuel use. Indonesia, as a developing country, faces major challenges in
implementing carbon tax policies to reduce emissions, especially related to
their regressive impacts on low-income households. Currently, there is little
in-depth research on how carbon tax policies impact household income
distribution in Indonesia. This study uses a quantitative approach with the
Input- Output model to analyze the impact of carbon tax on household income
based on 10 income groups, both in urban and rural areas. The results show that
carbon tax policies have a regressive impact, where low-income households bear
a proportionally greater burden. Household income in Class - 10 decreased by
IDR 19,144.85 million in urban areas and IDR 8,819.13 million in rural areas,
while households in Class - 1 decreased by IDR 954.23 million. Therefore,
mitigation policies such as cross subsidies are needed to reduce the impact on
vulnerable groups. These findings are important for policy makers in
formulating fair and effective fiscal policies, as well as ensuring social
justice in the context of sustainable development. This study has limitations
in the scope of analysis of long-term energy consumption behavior and certain
sectors, so further research is needed to deepen these aspects.",['Syahrituah Siregar'],[],0,arXiv,http://arxiv.org/abs/2501.08177v1,False,True,False,False,False,1901,Jared Carbone,Colorado,Active,2020,,"This proposed study will benefit the Census Bureau under Title 13, Chapter 5. Specifically, this project will produce estimates that will provide a greater understanding of the effects of moving costs on relocation decisions for environmental reasons, which will be directly relevant for informing public policy related to environmental justice (criteria 3). Also, this research will provide a comprehensive understanding of geographic variation in environmental factors with a thorough description of the Californian population by geographic location and environmental quality (criteria 11).

Researchers consistently observe households sort themselves in response to regional differences in neighborhood characteristics such as environmental quality, crime rates, and racial mix of the population (Kuminoff et al., 2013; Tiebout, 1956). Environmental justice (EJ) literature has demonstrated that this sorting behavior can explain why certain types of demographic group such as low-income households disproportionately live near undesirable land uses (Banzhaf, 2012; Depro et al., 2015). If households find environmental harm undesirable, demand for housing near such areas will fall, driving local housing prices down. Low-income households may also dislike polluted areas, but they would be less willing to, or able to, pay for a clean environmental. Thus, they may move to locations with pollution to save money in housing expenditure and use it to purchase other goods.

This project will explore the role that barriers to moving in response to environmental harms may play in shaping the patterns found by the EJ literature. In other words, this paper addresses the question: To what extent do moving costs prohibit households from moving to cleaner locations? A recent finding in Lee (2017) suggests that lower income households, especially those with children, face higher moving costs than their wealthier/childless counterparts. Using counterfactual experiments, Lee verifies that lowering moving costs encourage households to move to locations with better air quality. "
"Power to the teens? A model of parents' and teens' collective labor
  supply","Teens make life-changing decisions while constrained by the needs and
resources of the households they grow up in. Household behavior models
frequently delegate decision-making to the teen or their parents, ignoring
joint decision-making in the household. I show that teens and parents allocate
time and income jointly by using data from the Costa Rican Encuesta Nacional de
Hogares from 2011 to 2019 and a conditional cash transfer program. First, I
present gender differences in household responses to the transfer using a
marginal treatment effect framework. Second, I explain how the gender gap from
the results is due to the bargaining process between parents and teens. I
propose a collective household model and show that sons bargain cooperatively
with their parents while daughters do not. This result implies that sons have a
higher opportunity cost of attending school than daughters. Public policy
targeting teens must account for this gender disparity to be effective.",['José Alfonso Muñoz-Alvarado'],[],0,arXiv,http://arxiv.org/abs/2307.09634v1,False,True,False,False,False,1901,Jared Carbone,Colorado,Active,2020,,"This proposed study will benefit the Census Bureau under Title 13, Chapter 5. Specifically, this project will produce estimates that will provide a greater understanding of the effects of moving costs on relocation decisions for environmental reasons, which will be directly relevant for informing public policy related to environmental justice (criteria 3). Also, this research will provide a comprehensive understanding of geographic variation in environmental factors with a thorough description of the Californian population by geographic location and environmental quality (criteria 11).

Researchers consistently observe households sort themselves in response to regional differences in neighborhood characteristics such as environmental quality, crime rates, and racial mix of the population (Kuminoff et al., 2013; Tiebout, 1956). Environmental justice (EJ) literature has demonstrated that this sorting behavior can explain why certain types of demographic group such as low-income households disproportionately live near undesirable land uses (Banzhaf, 2012; Depro et al., 2015). If households find environmental harm undesirable, demand for housing near such areas will fall, driving local housing prices down. Low-income households may also dislike polluted areas, but they would be less willing to, or able to, pay for a clean environmental. Thus, they may move to locations with pollution to save money in housing expenditure and use it to purchase other goods.

This project will explore the role that barriers to moving in response to environmental harms may play in shaping the patterns found by the EJ literature. In other words, this paper addresses the question: To what extent do moving costs prohibit households from moving to cleaner locations? A recent finding in Lee (2017) suggests that lower income households, especially those with children, face higher moving costs than their wealthier/childless counterparts. Using counterfactual experiments, Lee verifies that lowering moving costs encourage households to move to locations with better air quality. "
"Import competition and domestic vertical integration: Theory and
  Evidence from Chinese firms","What impact does import competition have on firms' production organizational
choices? Existing literature has predominantly focused on the relationship
between import competition and firms' global production networks, with less
attention given to domestic. We first develop a Nash-bargaining model to guide
our empirical analysis, then utilize tariff changes as an exogenous shock to
test our theoretical hypotheses using a database of Chinese listed firms from
2000 to 2023. Our findings indicate that a decrease in downstream tariffs lead
to an increase in vertical integration. In our mechanism tests, we discover
that a reduction in upstream tariffs also enhances this effect. Moreover, the
impact of tariff reductions on vertical integration is primarily observed in
industries with high asset specificity, indicating that asset-specificity is a
crucial mechanism. We further explore whether import competition encourages
vertical integration for technological acquisition purpose, the effect is found
only among high-tech firms, while it's absent in non-high-tech firms. Our
research provides new perspectives and evidence on how firms optimize their
production organization in the process of globalization.","['Xin Du', 'Xiaoxia Shi']",[],0,arXiv,http://arxiv.org/abs/2408.13706v1,False,True,False,False,False,1908,Xavier A Giroud,Baruch,Completed,2018,2024.0,"China’s accession to the World Trade Organization in 2001 has led to a surge in U.S. imports from China and a decline in U.S. manufacturing employment in industries affected by Chinese import competition. While the implications of this “China Shock” for the U.S. manufacturing sector are well documented, little is known about how U.S. firms—the entities that own plants and employ workers—adjust to the China Shock. Do firms reallocate resources away from affected plants and toward plants in less affected industries? Or do they shift resources towards affected plants, allowing them to compete more effectively against Chinese imports? And what explains the direction and magnitude of the resource reallocation? This research will address these questions by using establishment-level microdata from the Longitudinal Business Database, Census of Manufactures, and Annual Survey of Manufactures, and worker-level data from the Longitudinal Employer-Household Dynamics program."
Production Networks and War,"How do severe shocks such as war alter the economy? We study how a country's
production network is affected by a devastating but localized conflict. Using
unique transaction-level data on Ukrainian railway shipments around the start
of the 2014 Russia-Ukraine crisis, we uncover several novel indirect effects of
conflict on firms. First, we document substantial propagation effects on
interfirm trade--trade declines even between partners outside the conflict
areas if one of them had traded with those areas before the conflict events.
The magnitude of such second-degree effect of conflict is one-fifth of the
first-degree effect. Ignoring this propagation would lead to an underestimate
of the total impact of conflict on trade by about 67\%. Second, war induces
sudden changes in the production-network structure that influence firm
performance. Specifically, we find that firms that exogenously became more
central--after the conflict practically cut off certain regions from the rest
of Ukraine--received a relative boost to their revenues. Finally, in a
production-network model, we separately estimate the effects of the exogenous
firm removal and the subsequent endogenous network adjustment on firm revenue
distribution. For a median firm, network adjustment compensates for 80\% of the
network destruction a year after the conflict onset.","['Vasily Korovkin', 'Alexey Makarin']",[],0,arXiv,http://arxiv.org/abs/2011.14756v4,False,True,False,False,False,1908,Xavier A Giroud,Baruch,Completed,2018,2024.0,"China’s accession to the World Trade Organization in 2001 has led to a surge in U.S. imports from China and a decline in U.S. manufacturing employment in industries affected by Chinese import competition. While the implications of this “China Shock” for the U.S. manufacturing sector are well documented, little is known about how U.S. firms—the entities that own plants and employ workers—adjust to the China Shock. Do firms reallocate resources away from affected plants and toward plants in less affected industries? Or do they shift resources towards affected plants, allowing them to compete more effectively against Chinese imports? And what explains the direction and magnitude of the resource reallocation? This research will address these questions by using establishment-level microdata from the Longitudinal Business Database, Census of Manufactures, and Annual Survey of Manufactures, and worker-level data from the Longitudinal Employer-Household Dynamics program."
"Education Opportunities for Rural Areas: Evidence from China's Higher
  Education Expansion","This paper explores the causal impact of education opportunities on rural
areas by exploiting the higher education expansion (HEE) in China in 1999. By
utilizing the detailed census data, the cohort-based difference-in-differences
design indicates that the HEE increased college attendance and encouraged more
people to attend senior high schools and that the effect is more significant in
rural areas. Then we apply a similar approach to a novel panel data set of
rural villages and households to examine the effect of education opportunities
on rural areas. We find contrasting impacts on income and life quality between
villages and households. Villages in provinces with higher HEE magnitudes
underwent a drop in the average income and worse living facilities. On the
contrary, households sending out migrants after the HEE experienced an increase
in their per capita income. The phenomenon where villages experienced a ``brain
drain'' and households with migrants gained after the HEE is explained by the
fact that education could serve as a way to overcome the barrier of rural-urban
migration. Our findings highlight the opposed impacts of education
opportunities on rural development and household welfare in rural areas.","['Ande Shen', 'Jiwei Zhou']",[],0,arXiv,http://arxiv.org/abs/2408.12915v1,False,True,False,False,False,1920,Seth G Sanders,Triangle,Completed,2018,2022.0,"We study differences in teenage motherhood and high school completion between races, and in particular how such differences relate to differences in housing. This research uses a combination of restricted-access data from the American Community Survey and Decennial Census, plus public data on household assets from corresponding years of the Federal Reserve’s Survey of Consumer Finances, to investigate omitted variable bias in the relationship between race, teenage motherhood, and high school completion. Our previous work indicates that among mobile home residents (a population group that on average has relatively few financial assets), rates of teenage motherhood and high school completion are similar between Blacks and Whites. The restricted-access data allow us to disentangle wealth effects from social interaction effects via fixed effect control variables created at the tract-level to (approximately) group mobile home residents into mobile home parks. In so doing, we test the hypothesis that omitted variable bias drives the correlation between race, teenage motherhood, and high school completion."
"""Computer Science for all"": Concepts to engage teenagers and non-CS
  students in technology","Knowledge in Computer Science (CS) is essential, and companies have increased
their demands for CS professionals. Despite this, many jobs remain unfilled.
Furthermore, employees with computational thinking (CT) skills are required,
even if they are not actual technicians. Moreover, the gender disparity in
technology related fields is a serious problem. Even if companies want to hire
women in technology, the number of women who enter these fields is remarkably
low. In high schools, most teenagers acquire only low-level skills in CS. Thus,
they may never understand the fundamental concepts of CS, have unrealistic
expectations or preconceptions, and are influenced by stereotype-based
expectations. Consequently, many teenagers exclude computing as a career path.
In this research study, we present two promising concepts to overcome these
challenges. First, we consider alternative paths to enter the field of CS. In
2018, a voluntary lecture ""Design your own app"" at the University of Graz for
students of all degree programs was introduced. In total, 202 students
participated. We applied a Game Development-Based Learning (GDBL) approach with
the visual coding tool Pocket Code, a mobile app developed at Graz University
of Technology. The students were supposed to create simple games directly on
smartphones. The course received positive evaluations and led to our second
concept; In January 2019, we started to design a MOOC (Massive Open Online
Course) with the title ""Get FIT in Computer Science"". First, this course can be
used to encourage young women who have little to no previous knowledge in CS.
Second, it should help all teenagers to get a more realistic picture of CS to
its basic concepts. Third, teachers can use the course materials to lead high
school classes (Open Educational Resources). Finally, the MOOC can be accessed
by everyone interested in this topic.","['Bernadette Spieler', 'Maria Grandl', 'Martin Ebner', 'Wolfgang Slany']",[],0,arXiv,http://arxiv.org/abs/1908.06637v2,False,True,False,False,False,1920,Seth G Sanders,Triangle,Completed,2018,2022.0,"We study differences in teenage motherhood and high school completion between races, and in particular how such differences relate to differences in housing. This research uses a combination of restricted-access data from the American Community Survey and Decennial Census, plus public data on household assets from corresponding years of the Federal Reserve’s Survey of Consumer Finances, to investigate omitted variable bias in the relationship between race, teenage motherhood, and high school completion. Our previous work indicates that among mobile home residents (a population group that on average has relatively few financial assets), rates of teenage motherhood and high school completion are similar between Blacks and Whites. The restricted-access data allow us to disentangle wealth effects from social interaction effects via fixed effect control variables created at the tract-level to (approximately) group mobile home residents into mobile home parks. In so doing, we test the hypothesis that omitted variable bias drives the correlation between race, teenage motherhood, and high school completion."
"Revisiting the thermal and superthermal two-class distribution of
  incomes: A critical perspective","This paper offers a two-pronged critique of the empirical investigation of
the income distribution performed by physicists over the past decade. Their
finding rely on the graphical analysis of the observed distribution of
normalized incomes. Two central observations lead to the conclusion that the
majority of incomes are exponentially distributed, but neither each individual
piece of evidence nor their concurrent observation robustly proves that the
thermal and superthermal mixture fits the observed distribution of incomes
better than reasonable alternatives. A formal analysis using popular measures
of fit shows that while an exponential distribution with a power-law tail
provides a better fit of the IRS income data than the log-normal distribution
(often assumed by economists), the thermal and superthermal mixture's fit can
be improved upon further by adding a log-normal component. The economic
implications of the thermal and superthermal distribution of incomes, and the
expanded mixture are explored in the paper.",['Markus P. A. Schneider'],[],0,arXiv,http://arxiv.org/abs/1804.06341v1,False,True,False,False,False,1935,Markus Schneider,Colorado,Completed,2018,2023.0,"This research aims to provide a more comprehensive understanding of the change in U.S. income inequality and labor market dynamics by characterizing the nation’s income distribution using an innovative set of distributional forms, namely a mixture model with a finite number of components. We use data from the Current Population Survey’s Annual Social and Economic Supplement to construct fine-grained, highly populated frequency histograms that allow us to estimate a multicomponent statistical model of the income distribution, which better capture the three generative processes that prior researchers have speculated shape the income distribution. Findings will identify the crucial characteristics of each of these processes and inform empirically-based theorizations of different modalities of income appropriation. This will ultimately cast light on the occurrence and consequences of unemployment or partial engagement with labor markets, segmentation and stratification in labor markets, and the relationship between functional and individual income. "
Bear Markets and Recessions versus Bull Markets and Expansions,"This paper examines the dynamic interaction between falling and rising
markets for both the real and the financial sectors of the largest economy in
the world using asymmetric causality tests. These tests require that each
underlying variable in the model be transformed into partial sums of the
positive and negative components. The positive components represent the rising
markets and the negative components embody the falling markets. The sample
period covers some part of the COVID19 pandemic. Since the data is non normal
and the volatility is time varying, the bootstrap simulations with leverage
adjustments are used in order to create reliable critical values when causality
tests are conducted. The results of the asymmetric causality tests disclose
that the bear markets are causing the recessions as well as the bull markets
are causing the economic expansions. The causal effect of bull markets on
economic expansions is higher compared to the causal effect of bear markets on
economic recessions. In addition, it is found that economic expansions cause
bull markets but recessions do not cause bear markets. Thus, the policies that
remedy the falling financial markets can also help the economy when it is in a
recession.",['Abdulnasser Hatemi-J'],[],0,arXiv,http://arxiv.org/abs/2009.01343v3,False,True,False,False,False,1942,Bryan A Stuart,Philadelphia,Active,2020,,"Growing evidence suggests that recessions catalyze lasting changes in the U.S. economy. For example, every recession since 1973 has led to a persistent decrease in earnings per capita in counties where the recession was more severe (Greenstone and Looney, 2010; Stuart, 2017). The overall consequence of this finding depends on why recessions lead to persistent declines in local economic activity. Local economic activity could decline because high-income individuals are more likely to migrate away from places experiencing negative economic shocks (Bound and Holzer, 2000; Notowidigdo, 2013), or because employers change their production process (Jaimovich and Siu, 2015; Hershbein and Kahn, 2017) or shut down (Foster, Grim, and Haltiwanger, 2016). Although a large literature documents the consequences of recessions, there is relatively little evidence that specifically examines why recessions lead to persistent declines in local economic activity.
This project will benefit the U.S. Census Bureau by enhancing the measurement of local economic activity and providing new evidence on the effects of recessions on workers, employers, and local labor markets. "
"Real implications of Quantitative Easing in the euro area: a
  complex-network perspective","The long-lasting socio-economic impact of the global financial crisis has
questioned the adequacy of traditional tools in explaining periods of financial
distress, as well as the adequacy of the existing policy response. In
particular, the effect of complex interconnections among financial institutions
on financial stability has been widely recognized. A recent debate focused on
the effects of unconventional policies aimed at achieving both price and
financial stability. In particular, Quantitative Easing (QE, i.e., the
large-scale asset purchase programme conducted by a central bank upon the
creation of new money) has been recently implemented by the European Central
Bank (ECB). In this context, two questions deserve more attention in the
literature. First, to what extent, by injecting liquidity, the QE may alter the
bank-firm lending level and stimulate the real economy. Second, to what extent
the QE may also alter the pattern of intra-financial exposures among financial
actors (including banks, investment funds, insurance corporations, and pension
funds) and what are the implications in terms of financial stability. Here, we
address these two questions by developing a methodology to map the
macro-network of financial exposures among institutional sectors across
financial instruments (e.g., equity, bonds, and loans) and we illustrate our
approach on recently available data (i.e., data on loans and private and public
securities purchased within the QE). We then test the effect of the
implementation of ECB's QE on the time evolution of the financial linkages in
the macro-network of the euro area, as well as the effect on macroeconomic
variables, such as output and prices.","['Chiara Perillo', 'Stefano Battiston']",[],0,arXiv,http://arxiv.org/abs/2004.09418v1,False,True,False,False,False,1958,Rustom D Manouchehri Irani,UIUC,Active,2018,,"A central tenet of U.S. corporate governance is that management should maximize shareholder value. However, as is now well-understood, shareholder maximization may impose costs on other stakeholders, including creditors and employees, that may not be internalized by shareholders. While there is recent evidence that the conflicts between shareholders and creditors—and even conflicts of interest among different classes of creditors—can have large impacts on corporate policies, there exists limited research analyzing precisely how these conflicts influence resource (mis)allocation in the economy. The purpose of this research is to conduct a micro-level analysis that documents how creditor control and borrower-lender relationships influence the investment, employment, and asset redeployment decisions, as well as employees of borrowing firms in (or in close proximity to) financial distress. The Census of Manufactures, Annual Survey of Manufactures, Longitudinal Business Database, Longitudinal Employer-Household Dynamics, Quarterly Financial Report, Auxiliary Establishment Survey, and Standard Statistical Establishment List will be used to quantify the effect of credit markets on individual firm behavior and performance, as well as worker earnings. We examine the influence of these conflicts on real activity and employees using two financial distress events that afford creditors greater control over corporate decision-making: bankruptcy and covenant violations (“technical default”). We complement these “ex post” analyses of creditor control with an “ex ante” analysis of The 1978 Bankruptcy Reform Act—a major piece of legislation that strengthened shareholders’ rights relative to creditors. "
"An optimal life insurance policy in the investment-consumption problem
  in an incomplete market","This paper considers an optimal life insurance for a householder subject to
mortality risk. The household receives a wage income continuously, which is
terminated by unexpected (premature) loss of earning power or (planned and
intended) retirement, whichever happens first. In order to hedge the risk of
losing income stream by householder's unpredictable event, the household enters
a life insurance contract by paying a premium to an insurance company. The
household may also invest their wealth into a financial market. The problem is
to determine an optimal insurance/investment/consumption strategy in order to
maximize the expected total, discounted utility from consumption and terminal
wealth. To reflect a real-life situation better, we consider an incomplete
market where the householder cannot trade insurance contracts continuously. To
our best knowledge, such a model is new in the insurance and finance
literature. The case of exponential utilities is considered in detail to derive
an explicit solution. We also provide numerical experiments for that particular
case to illustrate our results.","['Masahiko Egami', 'Hideki Iwaki']",[],0,arXiv,http://arxiv.org/abs/0801.0195v2,False,True,False,False,False,1971,Maxwell Kellogg,Chicago,Active,2022,,"This project proposes to analyze interactions between household work and U.S. disability insurance (DI) programs. Within the context of a household model of life cycle consumption and labor supply, we will use those interactions to examine the welfare and behavioral effects of changing the environment in which work, DI application, and consumption decisions are made. We propose to do this using the linked SIPP-SSA data. This proposal is motivated primarily by three features of DI programs in the United States which could generate substantial variation in the costs of applying for DI, depending on the characteristics of one's household. The first of these features is that DI application process is both long and uncertain (in terms of duration of the decision period and application outcome).  The second feature is that no constraints are placed on the earnings of other members of an applicant's household. Therefore, there is room for households to maintain income during the application process through other household members. The third feature is that access to medical insurance coverage is very limited during the application process. This feature is unique to DI programs of the United States, among developed countries; private medical insurance in the United States is closely tied to employment. These three features suggest that the (exogenously determined) composition of household structures in the United States has implications for the work disincentive effects and welfare value of the country's DI programs. With the unique interactions between medical insurance coverage and employment in the United States, the household may have a more important role to play in driving who applies for DI benefits in the United States than in other countries."
Information consumption and size in firms,"Social and biological collectives need to exchange information to persist and
to function. This happens across internal networks, whose structure represents
static channels through which information flows. Less studied is the quantity
and variety of information transmitted. We characterize a part of the
information flow, the information going into organizations, primarily business
firms. We measure what firms read using a data set of hundreds of millions of
records of news articles accessed by employees across millions of firms. We
measure and relate quantitatively three essential aspects: reading volume,
reading variety, and firm size. First we compare volume with firm size, showing
that firms grow sublinearly with the volume of their reading. The scaling means
that inequality in information volume exaggerates the classic Zipf's law
inequality in firm size, pointing to an economy of scale in information
consumption. Then, by connecting variety and volume, we show that the firms
vary in their reading habits to a limited degree. Firms above a certain size
become repetitive readers, consistent with the sudden onset of a coordination
cost between teams, not individual employees. Finally, we relate information
variety to size to show that large firms tend to increase investments in
existing areas of interest instead of divesting from them to move to new areas.
We argue that this reflects structural constraints in growth. The results
indicate how information consumption reflects the role of internal structure,
beyond individual employees, analogous to information processing in other
social and biological systems.","['Edward D. Lee', 'Alan P. Kwan', 'Rudolf Hanel', 'Anjali Bhatt', 'Frank Neffke']",[],0,arXiv,http://arxiv.org/abs/2210.07418v3,False,True,False,False,False,1975,Teresa C Fort,Boston,Active,2019,,"This project will use micro data to document firms’ organization of activities across space, and to assess how differences in firms’ geography and international trade activity affect their performance and innovation. A key element in the project is understanding firms’ decisions to own multiple establishments across different locations and industries. We will link detailed input usage data and product line information from the Census of Manufactures (CMF) and the Census of Retail Trade (CRT) to the international trade transactions databases. We will use these rich data to assess whether within-firm variation across establishments and international trade data can be used to impute missing data at the establishment level. We will also assess the extent to which the specific inputs or products that are pre-listed on the various economic census forms can be updated both to gather more relevant information and to reduce respondent burden. This project will also assess the extent to which various decisions, such as technology upgrading, take place at the establishment versus firm level. We will combine technology information from various surveys to document whether use of different technologies, such as electronic networks, are made at the establishment or firm level. By comparing technology responses across surveys, we can shed light on the accuracy of individual surveys. Most importantly, assessing the extent of heterogeneity within a firm and across establishments will provide the Census Bureau with valuable information on whether future technology surveys need to be conducted at the firm versus establishment level."
Real-Time Integrated Learning and Decision-Making for Asset Networks,"Problem definition: Unexpected component failures in industrial assets can
lead to significant financial losses and operational disruptions, making
preventive maintenance crucial for reducing unplanned downtime. This study
focuses on optimizing maintenance for economically coupled assets that degrade
according to a compound Poisson process, failing when degradation exceeds a
specified threshold. Degradation parameters vary across components and cannot
be directly observed, necessitating inference from real-time sensor data, with
maintenance limited to scheduled interventions. Academic/practical relevance:
We characterize optimal replacement policies that have access to the
degradation process parameters, which inform suitable initial policies for our
deep reinforcement learning (DRL) algorithm. The effectiveness of the trained
policies is demonstrated through a case study on interventional X-ray systems.
Methodology: The maintenance optimization problem is modeled as a partially
observable Markov decision process (POMDP). To manage the inherent
computational complexity, we reformulate it as a Bayesian Markov decision
process (BMDP) using conjugate pairs. This allows us to extend the DRL
algorithm to BMDPs. Additionally, we propose an open-loop feedback approach to
adapt the DRL algorithm to the POMDP setting when a BMDP reformulation is
infeasible. Results: Numerical results indicate that our DRL algorithm
effectively reduces maintenance costs and unplanned downtime by leveraging
real-time data and shared setup costs to learn state-of-the-art complex
opportunistic maintenance policies. Managerial implications: Our approach
demonstrates how integrating real-time learning and decision-making can
significantly lower costs in industrial asset networks, providing a practical
solution for improving operational efficiency amidst asset heterogeneity and
economic dependence.","['Peter Verleijsdonk', 'Collin Drent', 'Stella Kapodistria', 'Willem van Jaarsveld']",[],0,arXiv,http://arxiv.org/abs/2410.18246v1,False,True,False,False,False,1987,Kevin Mullally,Boston,Active,2022,,"This research proposes to study managerial incentives in the mutual fund industry by primarily investigating two questions: First, does the labor market for mutual fund managers discipline poorly performing managers? Second, how is managerial compensation determined? 
To answer these questions, the researchers will request a number of internal databases, most notably Longitudinal Employer-Household Database (LEHD), Longitudinal Business Database (LBD), Standard Statistical Establishment List (SSEL), and Compustat-SSEL Bridge files; and for external databases, Morningstar, LexisNexis Public Records, MFLinks, CRSP, and Thompson Reuters. The researchers either have subscription or membership for all external databases. All of the internal databases are requested for the years between 1990 and the latest year available, with the exception of LBD used to more accurately pin down firm age.  
The research will benefit the Census as the researchers plan to conduct a systematic assessment of the firm age variable by statistically comparing the Morningstar data with relevant data from various US Census programs. Morningstar provides a unique and an accurate way of calculating firm age for these mutual fund companies, which involves using a combination of advanced textual analysis to extract mutual funds' prospectus and tracking using a variety of different methods the inception dates of each fund affiliated with a mutual fund company. This comparison will shed light on the current methodology employed by Census to calculate its firm age variable in the LBD, i.e., firm age is counted as the age of the oldest establishment."
"Kindergarden quantum mechanics graduates (...or how I learned to stop
  gluing LEGO together and love the ZX-calculus)","This paper is a `spiritual child' of the 2005 lecture notes Kindergarten
Quantum Mechanics, which showed how a simple, pictorial extension of Dirac
notation allowed several quantum features to be easily expressed and derived,
using language even a kindergartner can understand. Central to that approach
was the use of pictures and pictorial transformation rules to understand and
derive features of quantum theory and computation. However, this approach left
many wondering `where's the beef?' In other words, was this new approach
capable of producing new results, or was it simply an aesthetically pleasing
way to restate stuff we already know?
  The aim of this sequel paper is to say `here's the beef!', and highlight some
of the major results of the approach advocated in Kindergarten Quantum
Mechanics, and how they are being applied to tackle practical problems on real
quantum computers. We will focus mainly on what has become the Swiss army knife
of the pictorial formalism: the ZX-calculus. First we look at some of the ideas
behind the ZX-calculus, comparing and contrasting it with the usual quantum
circuit formalism. We then survey results from the past 2 years falling into
three categories: (1) completeness of the rules of the ZX-calculus, (2)
state-of-the-art quantum circuit optimisation results in commercial and
open-source quantum compilers relying on ZX, and (3) the use of ZX in
translating real-world stuff like natural language into quantum circuits that
can be run on today's (very limited) quantum hardware.
  We also take the title literally, and outline an ongoing experiment aiming to
show that ZX-calculus enables children to do cutting-edge quantum computing
stuff. If anything, this would truly confirm that `kindergarten quantum
mechanics' wasn't just a joke.","['Bob Coecke', 'Dominic Horsman', 'Aleks Kissinger', 'Quanlong Wang']",[],0,arXiv,http://arxiv.org/abs/2102.10984v1,False,True,False,False,False,2040,Jade Jenkins,Irvine,Active,2019,,"This research examines the effects of state mandatory kindergarten requirements on long-run educational attainment and labor market outcomes. While in most states kindergarten began as a voluntary program, starting in the 1970s some states evolved to mandating kindergarten attendance. Several changes in state mandatory school entrance laws across—and in some instances, within—states over time provide an opportunity to causally identify the influence of an additional year of early-childhood education on important individual education and labor market outcomes, comparing states with mandatory attendance to those with voluntary attendance. Using a natural experiment design, we exploit variation in kindergarten requirements between 1970 and 2000 using pooled repeated cross-sections for individuals born between 1965 and 1995 observed in the 2000 Decennial Census and the 2001–2016 American Community Survey. We compare population-level outcomes for birth cohorts observed in the surveys over time. Our results will shed light on the anticipated impact of universal prekindergarten programs given the national trend towards preschool for all."
"Interactive Digital Learning Materials for Kindergarten Students in
  Bangladesh","The pedagogy of teaching and learning has changed with the proliferation of
communication technology and it is necessary to develop interactive learning
materials for children that may improve their learning, catching, and
memorizing capabilities. Perhaps, one of the most important innovations in the
age of technology is multimedia and its application. It is imperative to create
high quality and realistic learning environment for children. Interactive
learning materials can be easier to understand and deal with their first
learning. We developed some interactive learning materials in the form of a
video for Playgroup using multimedia application tools. This study investigated
the impact of students' abilities to acquire new knowledge or skills through
interactive learning materials. We visited one kindergartens (Nursery schools),
interviewed class teachers about their teaching methods and level of students'
ability of recognizing English alphabets, pictures, etc. The course teachers
were provided interactive learning materials to show their playgroups for a
number of sessions. The video included English alphabets with related words and
pictures, and motivational fun. We noticed that almost all children were very
interested to interact with their leaning video. The students were assessed
individually and asked to recognize the alphabets, and pictures. The students
adapted with their first alphabets very quickly. However, there were individual
differences in their cognitive development. This interactive multimedia can be
an alternative to traditional pedagogy for teaching playgroups.","['Md. Baharul Islam', 'Md. Kabirul Islam', 'Arif Ahmed', 'Abu Kalam Shamsuddin']",[],0,arXiv,http://arxiv.org/abs/1411.2075v1,False,True,False,False,False,2040,Jade Jenkins,Irvine,Active,2019,,"This research examines the effects of state mandatory kindergarten requirements on long-run educational attainment and labor market outcomes. While in most states kindergarten began as a voluntary program, starting in the 1970s some states evolved to mandating kindergarten attendance. Several changes in state mandatory school entrance laws across—and in some instances, within—states over time provide an opportunity to causally identify the influence of an additional year of early-childhood education on important individual education and labor market outcomes, comparing states with mandatory attendance to those with voluntary attendance. Using a natural experiment design, we exploit variation in kindergarten requirements between 1970 and 2000 using pooled repeated cross-sections for individuals born between 1965 and 1995 observed in the 2000 Decennial Census and the 2001–2016 American Community Survey. We compare population-level outcomes for birth cohorts observed in the surveys over time. Our results will shed light on the anticipated impact of universal prekindergarten programs given the national trend towards preschool for all."
"Ethics, Data Science, and Health and Human Services: Embedded Bias in
  Policy Approaches to Teen Pregnancy Prevention","Background: This study aims to evaluate the Chicago Teen Pregnancy Prevention
Initiative delivery optimization outcomes given policy-neutral and
policy-focused approaches to deliver this program to at-risk teens across the
City of Chicago. Methods: We collect and compile several datasets from public
sources including: Chicago Department of Public Health clinic locations, two
public health statistics datasets, census data of Chicago, list of Chicago
public high schools, and their Locations. Our policy-neutral approach will
consist of an equal distribution of funds and resources to schools and centers,
regardless of past trends and outcomes. The policy-focused approaches will
evaluate two models: first, a funding model based on prediction models from
historical data; and second, a funding model based on economic and social
outcomes for communities. Results: Results of this study confirms our initial
hypothesis, that even though the models are optimized from a machine learning
perspective, there is still possible that the models will produce wildly
different results in the real-world application. Conclusions: When ethics and
ethical considerations are extended beyond algorithmic optimization to
encompass output and societal optimization, the foundation and philosophical
grounding of the decision-making process become even more critical in the
knowledge discovery process.","['Davon Woodard', 'Huthaifa I. Ashqar', 'Taoran Ji']",[],0,arXiv,http://arxiv.org/abs/2006.04029v1,False,True,False,False,False,2041,Michael Lovenheim,Cornell,Completed,2019,2023.0,"The purpose of this project is to generate birth rates by age and race of mother for all school districts in the United States using the 1990 and 2000 US Censuses and the 2005-2012 American Community Surveys (ACS) The project will also provide a descriptive analysis using these data that will elucidate the correlation between teen birth rates and high school dropout rates and how these outcomes vary with the local area characteristics such as the poverty rate, racial/ethnic composition, per- capita income, unemployment rate, school spending, and industrial composition. While prior studies only could examine teen births in large counties at the county level due to their reliance on U.S. Vital Statistics birth data, we will examine teen births and its correlates in rural areas and at the school district level. Finally, the project will examine whether expanding teenagers' access to health care via school-based health centers (SBHCs) influences their fertility rates and their high school graduation rates."
"Function-on-Function Regression for the Identification of Epigenetic
  Regions Exhibiting Windows of Susceptibility to Environmental Exposures","The ability to identify time periods when individuals are most susceptible to
exposures, as well as the biological mechanisms through which these exposures
act, is of great public health interest. Growing evidence supports an
association between prenatal exposure to air pollution and epigenetic marks,
such as DNA methylation, but the timing and gene-specific effects of these
epigenetic changes are not well understood. Here, we present the first study
that aims to identify prenatal windows of susceptibility to air pollution
exposures in cord blood DNA methylation. In particular, we propose a
function-on-function regression model that leverages data from nearby DNA
methylation probes to identify epigenetic regions that exhibit windows of
susceptibility to ambient particulate matter less than 2.5 microns
(PM$_{2.5}$). By incorporating the covariance structure among both the
multivariate DNA methylation outcome and the time-varying exposure under study,
this framework yields greater power to detect windows of susceptibility and
greater control of false discoveries than methods that model probes
independently. We compare our method to a distributed lag model approach that
models DNA methylation in a probe-by-probe manner, both in simulation and by
application to motivating data from the Project Viva birth cohort. In two
epigenetic regions selected based on prior studies of air pollution effects on
epigenome-wide methylation, we identify windows of susceptibility to PM$_{2.5}$
exposure near the beginning and middle of the third trimester of pregnancy.","['Michele Zemplenyi', 'Mark J. Meyer', 'Andres Cardenas', 'Marie-France Hivert', 'Sheryl L. Rifas-Shiman', 'Heike Gibson', 'Itai Kloog', 'Joel Schwartz', 'Emily Oken', 'Dawn L. DeMeo', 'Diane R. Gold', 'Brent A. Coull']",[],0,arXiv,http://arxiv.org/abs/1912.07359v1,False,True,False,False,False,2041,Michael Lovenheim,Cornell,Completed,2019,2023.0,"The purpose of this project is to generate birth rates by age and race of mother for all school districts in the United States using the 1990 and 2000 US Censuses and the 2005-2012 American Community Surveys (ACS) The project will also provide a descriptive analysis using these data that will elucidate the correlation between teen birth rates and high school dropout rates and how these outcomes vary with the local area characteristics such as the poverty rate, racial/ethnic composition, per- capita income, unemployment rate, school spending, and industrial composition. While prior studies only could examine teen births in large counties at the county level due to their reliance on U.S. Vital Statistics birth data, we will examine teen births and its correlates in rural areas and at the school district level. Finally, the project will examine whether expanding teenagers' access to health care via school-based health centers (SBHCs) influences their fertility rates and their high school graduation rates."
Geography of Emotion: Where in a City are People Happier?,"Location-sharing services were built upon people's desire to share their
activities and locations with others. By ""checking-in"" to a place, such as a
restaurant, a park, gym, or train station, people disclose where they are,
thereby providing valuable information about land use and utilization of
services in urban areas. This information may, in turn, be used to design
smarter, happier, more equitable cities. We use data from Foursquare
location-sharing service to identify areas within a major US metropolitan area
with many check-ins, i.e., areas that people like to use. We then use data from
the Twitter microblogging platform to analyze the properties of these areas.
Specifically, we have extracted a large corpus of geo-tagged messages, called
tweets, from a major metropolitan area and linked them US Census data through
their locations. This allows us to measure the sentiment expressed in tweets
that are posted from a specific area, and also use that area's demographic
properties in analysis. Our results reveal that areas with many check-ins are
different from other areas within the metropolitan region. In particular, these
areas have happier tweets, which also encourage people from other areas to
commute longer distances to these places. These findings shed light on human
mobility patterns, as well as how physical environment influences human
emotions.","['Luciano Gallegos', 'Kristina Lerman', 'Arthur Huang', 'David Garcia']",[],0,arXiv,http://arxiv.org/abs/1507.07632v1,False,True,False,False,False,2050,Kyle E Walker,Dallas,Completed,2019,2026.0,"The purpose of this project is to investigate whether and how a new model of ""demographic inversion"" applies to internal and international migrants across metropolitan areas in the United States. Scholars and journalists have given significant attention in recent years to the declining relevance of traditional urban demographic models, which propose an association with upward mobility and suburban residential attainment. Empirical evidence to support this claim, however, is often limited by the coarse geographic resolution of publicly-available demographic data. This project will analyze the evolving dynamics of residential mobility in US metropolitan areas at micro-geographic levels using confidential data from the American Community Survey."
"Modeling Bike Share Station Activity: Effects of Nearby Businesses and
  Jobs on Trips to and from Stations","The purpose of this research is to identify correlates of bike station
activity for Nice Ride Minnesota, a bike share system in Minneapolis - St. Paul
Metropolitan Area in Minnesota. We obtained the number of trips to and from
each of the 116 bike share stations operating in 2011 from Nice Ride Minnesota.
Data for independent variables included in models come from a variety of
sources; including the 2010 US Census, the Metropolitan Council, a regional
planning agency, and the cities of Minneapolis and St. Paul. We use log-linear
and negative binomial regression models to evaluate the marginal effects of
these factors on average daily station trips. Our models have high goodness of
fit, and each of 13 independent variables is significant at the 10% level or
higher. The number of trips at Nice Ride stations is associated with
neighborhood socio demographics (i.e., age and race), proximity to the central
business district, proximity to water, accessibility to trails, distance to
other bike share stations, and measures of economic activity. Analysts can use
these results to optimize bike share operations, locate new stations, and
evaluate the potential of new bike share programs.","['Xize Wang', 'Greg Lindsey', 'Jessica E. Schoner', 'Andrew Harrison']",[],0,arXiv,http://arxiv.org/abs/2207.10577v1,False,True,False,False,False,2050,Kyle E Walker,Dallas,Completed,2019,2026.0,"The purpose of this project is to investigate whether and how a new model of ""demographic inversion"" applies to internal and international migrants across metropolitan areas in the United States. Scholars and journalists have given significant attention in recent years to the declining relevance of traditional urban demographic models, which propose an association with upward mobility and suburban residential attainment. Empirical evidence to support this claim, however, is often limited by the coarse geographic resolution of publicly-available demographic data. This project will analyze the evolving dynamics of residential mobility in US metropolitan areas at micro-geographic levels using confidential data from the American Community Survey."
Ambient Noise Full Waveform Inversion with Neural Operators,"Numerical simulations of seismic wave propagation are crucial for
investigating velocity structures and improving seismic hazard assessment.
However, standard methods such as finite difference or finite element are
computationally expensive. Recent studies have shown that a new class of
machine learning models, called neural operators, can solve the elastodynamic
wave equation orders of magnitude faster than conventional methods. Full
waveform inversion is a prime beneficiary of the accelerated simulations.
Neural operators, as end-to-end differentiable operators, combined with
automatic differentiation, provide an alternative approach to the adjoint-state
method. Since neural operators do not involve the Born approximation, when used
for full waveform inversion they have the potential to include additional
phases and alleviate cycle-skipping problems present in traditional
adjoint-state formulations. In this study, we demonstrate the first application
of neural operators for full waveform inversion on a real seismic dataset,
which consists of several nodal transects collected across the San Gabriel,
Chino, and San Bernardino basins in the Los Angeles metropolitan area.","['Caifeng Zou', 'Zachary E. Ross', 'Robert W. Clayton', 'Fan-Chi Lin', 'Kamyar Azizzadenesheli']",[],0,arXiv,http://arxiv.org/abs/2503.15013v2,False,True,False,False,False,2050,Kyle E Walker,Dallas,Completed,2019,2026.0,"The purpose of this project is to investigate whether and how a new model of ""demographic inversion"" applies to internal and international migrants across metropolitan areas in the United States. Scholars and journalists have given significant attention in recent years to the declining relevance of traditional urban demographic models, which propose an association with upward mobility and suburban residential attainment. Empirical evidence to support this claim, however, is often limited by the coarse geographic resolution of publicly-available demographic data. This project will analyze the evolving dynamics of residential mobility in US metropolitan areas at micro-geographic levels using confidential data from the American Community Survey."
Dynamics of racial segregation and gentrification in New York City,"Racial residential segregation is interconnected with several other phenomena
such as income inequalities, property values inequalities, and racial
disparities in health and in education. Furthermore, recent literature suggests
the phenomena of gentrification as a cause of perpetuation or increase of
racial residential segregation in some American cities. In this paper, we
analyze the dynamics of racial residential segregation for white, black, Asian,
and Hispanic citizens in New York City in the years of 1990, 2000, and 2010. It
was possible to observe that segregation between white and Hispanic citizens,
and discrimination between white and Asian ones has grown, while segregation
between white and black is quite stable. Furthermore, we analyzed the per
capita income and the Gini coefficient in each segregated zone, showing that
the highest inequalities occur in the zones where there is overlap of
high-density zones of pair of races. Focusing on census tracts that have
changed density of population during these twenty years, and, particularly, by
analyzing white and black people's segregation, our analysis reveals that a
positive flux of white (black) people is associated to a substantial increase
(decrease) of the property values, as compared with the city mean. Furthermore,
by clustering the region of high density of black citizens, we measured the
variation of area and displacement of the four biggest clusters in the period
from 1990 to 2010. The large displacements (~ 1.6 Km) observed for two of these
clusters, namely, one in the neighborhood of Harlem and the other inside the
borough of Brooklyn, led to the emergence of typically gentrified regions.","['Felipe G. Operti', 'André A. Moreira', 'Andrea Gabrielli', 'Hernan Makse', 'José S. Andrade Jr']",[],0,arXiv,http://arxiv.org/abs/1904.07205v2,False,True,False,False,False,2054,Anita Pena,Colorado,Completed,2019,2024.0,"The purpose of this research is to examine the effects of race/ethnicity, household characteristics and environment, and census tract, county, and state-level social and economic characteristics on voluntary and involuntary migration. The project will also produce estimates of the total U.S. population, stratified by race and ethnicity, that migrates over various distances and provide an analysis of the factors influencing this migration. In attempting to explain the rise of intrastate migration and reduced rate of interstate migration, researchers have highlighted the importance of both economic and non-economic factors. However, many of these factors are theorized to operate at more localized geographical levels than are available in public data. This has led to a dearth of research capable of examining the relative importance of economic and social factors on internal migration among major racial and ethnic groups. To analyze the neighborhood-level dynamics that theoretically drive differences in migration patterns, researchers must be able to identify the neighborhood of residence of individuals and households, as well as the aggregate characteristics of these places. Restricted Census data will be used to examine both inter- and instar-state migration dynamics at an appropriate unit of analysis."
Labor Informality and Credit Market Accessibility,"The paper investigates the effects of the credit market development on the
labor mobility between the informal and formal labor sectors. In the case of
Russia, due to the absence of a credit score system, a formal lender may set a
credit limit based on the verified amount of income. To get a loan, an informal
worker must first formalize his or her income (switch to a formal job), and
then apply for a loan. To show this mechanism, the RLMS data was utilized, and
the empirical method is the dynamic multinomial logit model of employment. The
empirical results show that a relaxation of credit constraints increases the
probability of transition from an informal to a formal job, and improved CMA
(by one standard deviation) increases the chances of informal sector workers to
formalize by 5.4 ppt. These results are robust in different specifications of
the model. Policy simulations show strong support for a reduction in informal
employment in response to better CMA in credit-constrained communities.","['Alina Malkova', 'Klara Sabirianova Peter', 'Jan Svejnar']",[],0,arXiv,http://arxiv.org/abs/2102.05803v1,False,True,False,False,False,2055,Manasa Gopal,Atlanta,Active,2019,,"We study the interaction between credit and labor markets with the objective of understanding the impact of financial imperfections on firm and worker outcomes. To do this, we use the Longitudinal Business Database, Longitudinal Employer-Household Dynamics, Census of Manufactures, and Annual Survey of Manufactures, along with data on loan originations in the United States. First, this research will evaluate the effect of credit supply shocks on firm investment, net employment, and the ability of the firm to retain human capital. For this, we create instruments of credit supply shock. We then track current and future labor market outcomes of workers employed at firms affected by credit supply shocks. Our research aims to identify the effects of credit supply shocks, both during and surrounding the financial crisis. Through this research, we also aim to understand the differences in firm and worker outcomes based on (1) lender type—specifically the role of banks vs. nonbanks and (2) the underlying collateral pledged by the firm. "
Visual Perception of Building and Household Vulnerability from Streets,"In developing countries, building codes often are outdated or not enforced.
As a result, a large portion of the housing stock is substandard and vulnerable
to natural hazards and climate related events. Assessing housing quality is key
to inform public policies and private investments. Standard assessment methods
are typically carried out only on a sample / pilot basis due to its high costs
or, when complete, tend to be obsolete due to the lack of compliance with
recommended updating standards or not accessible to most users with the level
of detail needed to take key policy or business decisions. Thus, we propose an
evaluation framework that is cost-efficient for first capture and future
updates, and is reliable at the block level. The framework complements existing
work of using street view imagery combined with deep learning to automatically
extract building information to assist the identification of housing
characteristics. We then check its potential for scalability and higher level
reliability. For that purpose, we create an index, which synthesises the
highest possible level of granularity of data at the housing unit and at the
household level at the block level, and assess whether the predictions made by
our model could be used to approximate vulnerability conditions with a lower
budget and in selected areas. Our results indicated that the predictions from
the images are clearly correlated with the index.","['Chaofeng Wang', 'Sarah Elizabeth Antos', 'Jessica Grayson Gosling Goldsmith', 'Luis Miguel Triveno']",[],0,arXiv,http://arxiv.org/abs/2205.14460v1,False,True,False,False,False,2061,Eileen Divringi,Philadelphia,Completed,2019,2022.0,"The need for home repairs is a critical quality-of-life challenge for many low- and moderate-income households and communities. Despite the clear utility for policymaking and program development, few publicly-available, neighborhood-level housing quality indicators exist. To address this information gap, this research develops small area estimates of home repair need for occupied housing units using American Housing Survey (AHS), American Community Survey (ACS), and proprietary repair cost data. We estimate the unit-level total cost of repairs by relating cost estimates to housing problems reported in the AHS. AHS units are then merged with publicly available tract- and region-level data from the ACS using geographic identifiers available in the restricted-use AHS file. We develop multilevel regression models to predict repair costs using characteristics of the unit, surrounding census tract, and region. We apply the resulting models to publicly-available data to produce aggregate tract-level estimates using a post-stratification approach. The resulting estimates will help decision makers understand the scope and magnitude of home repair needs and target resources accordingly. "
"A Repairable System Supported by Two Spare Units and Serviced by Two
  Types of Repairers","We study a one-unit repairable system, supported by two identical spare units
on cold standby, and serviced by two types of repairers. The model applies, for
instance, to ANSI (American National Standard Institute) centrifugal pumps in a
chemical plant. The failed unit undergoes repair either by an in-house repairer
within a random or deterministic patience time, or else by a visiting expert
repairer. The expert repairs one or all failed units before leaving, and does
so faster but at a higher cost rate than the regular repairer. Four models
arise depending on the number of repairs done by the expert and the nature of
the patience time. We compare these models based on the limiting availability
$A_{\infty}$, and the limiting profit per unit time $\omega$, using semi-Markov
processes, when all distributions are exponential. As anticipated, to maximize
$A_{\infty}$, the expert should repair all failed units. To maximize $\omega$,
a suitably chosen deterministic patience time is better than a random patience
time. Furthermore, given all cost parameters, we determine the optimum number
of repairs the expert should complete, and the optimum patience time given to
the regular repairer in order to maximize $\omega$.","['Vahid Andalib', 'Jyotirmoy Sarkar']",[],0,arXiv,http://arxiv.org/abs/1908.02547v1,False,True,False,False,False,2061,Eileen Divringi,Philadelphia,Completed,2019,2022.0,"The need for home repairs is a critical quality-of-life challenge for many low- and moderate-income households and communities. Despite the clear utility for policymaking and program development, few publicly-available, neighborhood-level housing quality indicators exist. To address this information gap, this research develops small area estimates of home repair need for occupied housing units using American Housing Survey (AHS), American Community Survey (ACS), and proprietary repair cost data. We estimate the unit-level total cost of repairs by relating cost estimates to housing problems reported in the AHS. AHS units are then merged with publicly available tract- and region-level data from the ACS using geographic identifiers available in the restricted-use AHS file. We develop multilevel regression models to predict repair costs using characteristics of the unit, surrounding census tract, and region. We apply the resulting models to publicly-available data to produce aggregate tract-level estimates using a post-stratification approach. The resulting estimates will help decision makers understand the scope and magnitude of home repair needs and target resources accordingly. "
"Prediction method of cigarette draw resistance based on correlation
  analysis","The cigarette draw resistance monitoring method is incomplete and single, and
the lacks correlation analysis and preventive modeling, resulting in
substandard cigarettes in the market. To address this problem without
increasing the hardware cost, in this paper, multi-indicator correlation
analysis is used to predict cigarette draw resistance. First, the monitoring
process of draw resistance is analyzed based on the existing quality control
framework, and optimization ideas are proposed. In addition, for the three
production units, the cut tobacco supply (VE), the tobacco rolling (SE), and
the cigarette-forming (MAX), direct and potential factors associated with draw
resistance are explored, based on the linear and non-linear correlation
analysis. Then, the correlates of draw resistance are used as inputs for the
machine learning model, and the predicted values of draw resistance are used as
outputs. Finally, this research also innovatively verifies the practical
application value of draw resistance prediction: the distribution
characteristics of substandard cigarettes are analyzed based on the prediction
results, the time interval of substandard cigarettes being produced is
determined, the probability model of substandard cigarettes being sampled is
derived, and the reliability of the prediction result is further verified by
the example. The results show that the prediction model based on correlation
analysis has good performance in three months of actual production.","['Linsheng Chen', 'Zhonghua Yu', 'Bo Zhang', 'Qiang Zhu', 'Hu Fan', 'Yucan Qiu']",[],0,arXiv,http://arxiv.org/abs/2304.06649v1,False,True,False,False,False,2061,Eileen Divringi,Philadelphia,Completed,2019,2022.0,"The need for home repairs is a critical quality-of-life challenge for many low- and moderate-income households and communities. Despite the clear utility for policymaking and program development, few publicly-available, neighborhood-level housing quality indicators exist. To address this information gap, this research develops small area estimates of home repair need for occupied housing units using American Housing Survey (AHS), American Community Survey (ACS), and proprietary repair cost data. We estimate the unit-level total cost of repairs by relating cost estimates to housing problems reported in the AHS. AHS units are then merged with publicly available tract- and region-level data from the ACS using geographic identifiers available in the restricted-use AHS file. We develop multilevel regression models to predict repair costs using characteristics of the unit, surrounding census tract, and region. We apply the resulting models to publicly-available data to produce aggregate tract-level estimates using a post-stratification approach. The resulting estimates will help decision makers understand the scope and magnitude of home repair needs and target resources accordingly. "
"An income-based approach to modeling commuting distance in the Toronto
  area","The purpose of this article is to propose a novel model of the effects of
changes in shelter and driving costs on car commuting distances in the
overheated Toronto housing market from 2011 to 2016. The model borrows from
theoretical concepts of microeconomics and urban geography to examine the
Toronto housing market. Using 2011 and 2016 Census data for census metropolitan
areas (CMAs) and census agglomerations (CAs) in Southern Ontario and computed
driving costs, the model of car commuting distance is based on variables of
allocation of monthly household income to monthly shelter costs and driving
costs as a function of the car driving distance to Toronto. Using this model,
we can predict the effect on car commuting distance due to changes in any of
the variables. The model also offers an explanation for communities of Toronto
car commuters beyond a driving radius that we might expect for daily commuting.
The model confirms that increases in shelter costs in the Toronto housing
market from 2011 to 2016 have forced the boundaries of feasible housing
locations outward, and forced households to move farther away, thus increasing
car commuting distance.",['Shawn Berry'],[],0,arXiv,http://arxiv.org/abs/2401.11343v1,False,True,False,False,False,2061,Eileen Divringi,Philadelphia,Completed,2019,2022.0,"The need for home repairs is a critical quality-of-life challenge for many low- and moderate-income households and communities. Despite the clear utility for policymaking and program development, few publicly-available, neighborhood-level housing quality indicators exist. To address this information gap, this research develops small area estimates of home repair need for occupied housing units using American Housing Survey (AHS), American Community Survey (ACS), and proprietary repair cost data. We estimate the unit-level total cost of repairs by relating cost estimates to housing problems reported in the AHS. AHS units are then merged with publicly available tract- and region-level data from the ACS using geographic identifiers available in the restricted-use AHS file. We develop multilevel regression models to predict repair costs using characteristics of the unit, surrounding census tract, and region. We apply the resulting models to publicly-available data to produce aggregate tract-level estimates using a post-stratification approach. The resulting estimates will help decision makers understand the scope and magnitude of home repair needs and target resources accordingly. "
"New Insights into Rental Housing Markets across the United States: Web
  Scraping and Analyzing Craigslist Rental Listings","Current sources of data on rental housing - such as the census or commercial
databases that focus on large apartment complexes - do not reflect recent
market activity or the full scope of the U.S. rental market. To address this
gap, we collected, cleaned, analyzed, mapped, and visualized 11 million
Craigslist rental housing listings. The data reveal fine-grained spatial and
temporal patterns within and across metropolitan housing markets in the U.S. We
find some metropolitan areas have only single-digit percentages of listings
below fair market rent. Nontraditional sources of volunteered geographic
information offer planners real-time, local-scale estimates of rent and housing
characteristics currently lacking in alternative sources, such as census data.","['Geoff Boeing', 'Paul Waddell']",[],0,arXiv,http://arxiv.org/abs/1605.05397v4,False,True,False,False,False,2080,Elizabeth Korver-Glenn,Penn State,Completed,2019,2024.0,"This study will examine how neighborhood and metropolitan area racial composition influence rental property management. We will document these relationships with respect to 1) forms of management (e.g. direct, by landlords; indirect, by property managers or others); 2) maintenance of properties; and, 3) tenant use of housing subsidies. We are requesting access to the Rental Housing Finance Survey (RHFS) Internal User Files (IUF) microdata, 2012 and 2015 waves, to examine these relationships because they are the only nationally representative data that contain information on who is responsible for the day-to-day management of rental properties as well as rental property and renter characteristics. We are also requesting access to the American Housing Survey (AHS) IUF data, 2013 wave, and CoreLogic tax roll records (if available). Gaining access to these restricted datasets will benefit Census by allowing us to provide feedback on key variables of interest across the RHFS and AHS (as well as the RHFS and the publicly available American Community Survey), thus improving Census products, and by providing estimates of property manager and renter populations."
Mueller-Navelet jets at LHC: the first complete NLL BFKL study,"Mueller Navelet jets were proposed 25 years ago as a decisive test of BFKL
dynamics at hadron colliders. We here present the first next-to-leading BFKL
study of the cross section and azimuthal decorrelation of these jets. This
includes both next-to-leading corrections to the Green's function and
next-to-leading corrections to the Mueller Navelet vertices. The obtained
results for standard observables proposed for studies of Mueller Navelet jets
show that both sources of corrections are of equal and big importance for final
magnitude and behavior of observables, in particular for the LHC kinematics
investigated here in detail. Our analysis reveals that the observables obtained
within the complete next-to-leading order BFKL framework of the present work
are quite similar to the same observables obtained within next-to-leading
logarithm DGLAP type treatment. There is still a noticeable difference in both
treatments for the ratio of the azimuthal angular moments < cos (2 phi) > / <
cos phi >.","['B. Ducloué', 'L. Szymanowski', 'S. Wallon']",[],0,arXiv,http://arxiv.org/abs/1208.6111v1,False,True,False,False,False,2081,Rebecca L Mueller,Penn State,Active,2020,,"This project studies the determinants of firm location decisions, including both where to operate and, for multi-establishment firms, how to allocate workers across locations. New firms have to consider local labor market conditions, infrastructure, taxes, and the regulatory environment when deciding where to locate their production facilities. This is also relevant for existing firms as location decisions are not permanent and incumbents respond to changes in the economic environment in which they operate. The Census data show the size and location of establishments in each year, as well as firm identifiers. We can use this to analyze the yearly movement of establishments."
"Nationwide frequency-dependent seismic site amplification models for
  Iceland","Seismic wave amplification due to localized site conditions is an important
aspect of regional seismic hazard assessment. Without systematic studies of
frequency-dependent site-effects during strong Icelandic earthquakes, various
local site proxies of large-scale studies in other seismic regions have been
used in Iceland. Recently, earthquake site-effects were rigorously quantified
for 34 stations in Southwest Iceland for the first time and correlated to
distinct Icelandic geological units of hard rock, rock, lava rock, and
sedimentary soil. These units are prevalent throughout Iceland and herein we
present 1) nationwide maps of proxies (slope, Vs30, geological units) that may
contribute to a better estimation of site effects and associated, 2)
frequency-dependent site-amplification maps of Iceland. The frequency-dependent
site factors for each geological unit are presented at 1-30 Hz and PGA.
Finally, we generate site amplification maps based on recent large-scale models
developed in other seismic regions (ESRM20) and various site proxies they are
based on (geology- and slope-based inferred Vs30, geomorphological sedimentary
thickness). We compare site-proxy maps and amplification maps from both
Icelandic and large-scale, non-Icelandic, models. Neither spatial patterns nor
amplification levels in either proxy or amplification maps from large-scale
non-Icelandic studies resemble those observed from local quantitative
strong-motion research as presented in this study. We attribute the discrepancy
primarily to the young geology of Iceland and its formation history.
Additionally, we compare model performance across frequencies by assessing the
bias of model predictions against empirical site amplifications in the South
Iceland Seismic Zone, accounting for site-to-site variability of residuals
indicating the superior performance of the local amplification model.","['Atefe Darzi', 'Benedikt Halldorsson', 'Fabrice Cotton', 'Sahar Rahpeyma']",[],0,arXiv,http://arxiv.org/abs/2407.09338v1,False,True,False,False,False,2082,John D Iceland,Penn State,Completed,2019,2022.0,"This research will examine whether the likelihood of reporting material hardships is affected by the relative affluence of one’s neighbors. We analyze this issue using data from the 2008 panel of the restricted-use version of the Survey of Income and Program (SIPP), where we can identify the neighborhood (census tract) of residence. One of the central aims of the proposed research is to provide information about the prevalence of seven different hardships among the population, and, in addition, how these patterns vary by the characteristics of the neighborhood of residence. This kind of neighborhood-level analysis is novel, and thus represents a unique contribution to our understanding of patterns of material hardship among the U.S. population."
"Is it possible to obtain reliable estimates for the prevalence of anemia
  and childhood stunting among children under 5 in the poorest districts in
  Peru?","In this article we describe and apply the Fay-Herriot model with spatially
correlated random area effects (Pratesi, M., & Salvati, N. (2008)), in order to
predict the prevalence of anemia and childhood stunting in Peruvian districts,
based on the data from the Demographic and Family Health Survey of the year
2019, which collects data about anemia and childhood stunting for children
under the age of 12 years, and the National Census carried out in 2017. Our
main objective is to produce reliable predictions for the districts, where
sample sizes are too small to provide good direct estimates, and for the
districts, which were not included in the sample. The basic Fay-Herriot model
(Fay & Herriot, 1979) tackles this problem by incorporating auxiliary
information, which is generally available from administrative or census
records. The Fay-Herriot model with spatially correlated random area effects,
in addition to auxiliary information, incorporates geographic information about
the areas, such as latitude and longitude. This permits modeling spatial
autocorrelations, which are not unusual in socioeconomic and health surveys. To
evaluate the mean square error of the above-mentioned predictors, we use the
parametric bootstrap procedure, developed in Molina et al. (2009).","['Anna Sikov', 'José Cerda-Hernández', 'Eduardo Haro']",[],0,arXiv,http://arxiv.org/abs/2311.04812v1,False,True,False,False,False,2086,Jan M Eberth,Triangle,Completed,2019,2023.0,"This research will benefit the U.S. Census Bureau by investigating the feasibility of producing model-based estimates of county-level and census tract-level childhood obesity rates using data from the 2016 National Survey of Children's Health (NSCH), and comparing these rates across urban versus rural designations nationally and across the nine U.S. Census divisions. In the process, the research will also benefit the Bureau by demonstrating the utility of the NSCH data for analyzing social conditions related to childhood obesity, which is associated with poorer health in childhood and predicts later obesity in adulthood.  These latter benefits will demonstrate the utility of the NSCH for the production of obesity estimates for American children at varying levels of geography, a task for which the NSCH is specifically designed.  Based on respondent demographic information, the team will utilize a small area estimation approach, harnessing the flexibility of spatially explicit information to provide model-based small area estimates and associated confidence intervals for the outcome of interest."
"Multi-time small-area estimation of oil and gas production capacities by
  Bayesian multilevel modeling","This paper presents a Bayesian multilevel modeling approach for estimating
well-level oil and gas production capacities across small geographic areas over
multiple time periods. Focusing on a basin, which is a geologically and
economically distinguishable drilling region, we model the production
capacities of its wells grouped by area and time. Regularizing our inferences
with priors, we model area-level and time-level variations as well as
well-level variations, incorporating lateral length, water usage, and sand
usage at each well. The Maidenhead Coordinate System is used to define uniform
geographic areas, many of which contain only a small number of wells in a given
time period. First, a Bayesian small-area model is built, using data from the
Bakken region from February 2012 to June 2024. Then, the model is expanded to
contain temporal dynamics in the production capacities. In addition to general
time components, water and sand usage intensities are modeled in estimating
production capabilities over time. We find the Bayesian multilevel modeling
approach provides a flexible and robust framework for modeling and estimating
oil and gas production capacities at area and time levels and for informing
area-time predictions with uncertainties.",['Hiroaki Minato'],[],0,arXiv,http://arxiv.org/abs/2408.11167v5,False,True,False,False,False,2086,Jan M Eberth,Triangle,Completed,2019,2023.0,"This research will benefit the U.S. Census Bureau by investigating the feasibility of producing model-based estimates of county-level and census tract-level childhood obesity rates using data from the 2016 National Survey of Children's Health (NSCH), and comparing these rates across urban versus rural designations nationally and across the nine U.S. Census divisions. In the process, the research will also benefit the Bureau by demonstrating the utility of the NSCH data for analyzing social conditions related to childhood obesity, which is associated with poorer health in childhood and predicts later obesity in adulthood.  These latter benefits will demonstrate the utility of the NSCH for the production of obesity estimates for American children at varying levels of geography, a task for which the NSCH is specifically designed.  Based on respondent demographic information, the team will utilize a small area estimation approach, harnessing the flexibility of spatially explicit information to provide model-based small area estimates and associated confidence intervals for the outcome of interest."
Innovation and imitation,"We study several models of growth driven by innovation and imitation by a
continuum of firms, focusing on the interaction between the two. We first
investigate a model on a technology ladder where innovation and imitation
combine to generate a balanced growth path (BGP) with compact support, and with
productivity distributions for firms that are truncated power-laws. We start
with a simple model where firms can adopt technologies of other firms with
higher productivities according to exogenous probabilities. We then study the
case where the adoption probabilities depend on the probability distribution of
productivities at each time. We finally consider models with a finite number of
firms, which by construction have firm productivity distributions with bounded
support. Stochastic imitation and innovation can make the distance of the
productivity frontier to the lowest productivity level fluctuate, and this
distance can occasionally become large. Alternatively, if we fix the length of
the support of the productivity distribution because firms too far from the
frontier cannot survive, the number of firms can fluctuate randomly.","['Jess Benhabib', 'Éric Brunet', 'Mildred Hager']",[],0,arXiv,http://arxiv.org/abs/2006.06315v2,False,True,False,False,False,2095,Seula Kim,Baruch,Active,2019,,"This project will study how firms use different types of innovation to survive and grow when faced with the changes in international trade environment -- increasing international competition and various trade policy changes -- and the aggregate implications of their strategic innovation decisions. This project will increase understanding recent changes in business dynamism -- including firm employment growth for young firms, and startup rates -- and the role of firm innovation in economic growth during periods of increased international trade and knowledge flows."
Online Assemblies: Civic Technologies Reshaping the Public Space,"Speaking or writing of political assemblies tends to evoke the action of
people gathering to deliberate, or the spaces in which this deliberation might
take place. One thing that is often overlooked, however, is the fact that these
spaces can be digital. Online assemblies have become more widespread in recent
years; from the first Web forums to civic technologies specifically designed to
host collective political debates. As digital services affect our possibilities
for political mobilization and participation, I will here attempt to define the
qualities specific to online assemblies, and to identify several patterns and
continuities in the design features of civic technologies offering online
spaces for debate.",['Tallulah Frappier'],[],0,arXiv,http://arxiv.org/abs/2303.13522v1,False,True,False,False,False,2110,David J Knight,Chicago,Active,2020,,"Social scientists have long studied how social policy affects electoral participation, but causal identification is difficult since citizens are not randomly assigned to receive government benefits. We plan to tackle this question by using data from the Moving to Opportunity (MTO) field experiment that sought to move low-income Americans away from concentrated poverty. Funding from the grant will allow us to merge national voter file data with the MTO data so that we can study the downstream effects of receiving housing vouchers on electoral participation for both adult subjects and former child subjects."
Civic Engagement among Early Internet Adopters: Trend or Phase?,"This paper brings evidence to bear on the question of the long-term effects
of Internet diffusion on civic engagement in geographic communities. It draws
on findings from survey data collected in four U.S. towns and cities in fall
2000 where community computer networking is established. The study shows that
early adopters of the Internet are more likely to engage in civic activities
and to have higher levels of community involvement than later adopters.
Further, early adopters are more likely to use the Internet to increase their
community involvement and political participation. Later adopters in all four
sites show less involvement in their local community and less interest in
political activity and information, online or offline. These findings reinforce
those of the Kohut (1999) study showing that later adopters are less civic
minded and more interested than early adopters in consumer and commercial
applications, such as shopping and entertainment. The evidence in these four
sites is consistent with earlier findings in Blacksburg, Virginia (Kavanaugh,
2000; Patterson and Kavanaugh, 2001; Kavanaugh and Patterson, 2001) and other
studies of early innovation adopters (Rogers, 1983; Kohut, 1999; Valente, 1995,
among others). The results reported in this paper lend weight to the argument
that increases in civic engagement and community involvement are due primarily
to the behavior of early adopters, making such increases a phase, not a trend.
As later adopters come on line, use of the Internet for community involvement
or civic engagement decreases. In the long term, we can expect that Internet
access may have only a modest effect on community involvement and civic
engagement in geographic communities.",['Andrea L. Kavanaugh'],[],0,arXiv,http://arxiv.org/abs/cs/0109087v2,False,True,False,False,False,2110,David J Knight,Chicago,Active,2020,,"Social scientists have long studied how social policy affects electoral participation, but causal identification is difficult since citizens are not randomly assigned to receive government benefits. We plan to tackle this question by using data from the Moving to Opportunity (MTO) field experiment that sought to move low-income Americans away from concentrated poverty. Funding from the grant will allow us to merge national voter file data with the MTO data so that we can study the downstream effects of receiving housing vouchers on electoral participation for both adult subjects and former child subjects."
"Mobilizing the Trump Train: Understanding Collective Action in a
  Political Trolling Community","Political trolls initiate online discord not only for the lulz (laughs) but
also for ideological reasons, such as promoting their desired political
candidates. Political troll groups recently gained spotlight because they were
considered central in helping Donald Trump win the 2016 US presidential
election, which involved difficult mass mobilizations. Political trolls face
unique challenges as they must build their own communities while simultaneously
disrupting others. However, little is known about how political trolls mobilize
sufficient participation to suddenly become problems for others. We performed a
quantitative longitudinal analysis of more than 16 million comments from one of
the most popular and disruptive political trolling communities, the subreddit
/r/The\_Donald (T\D). We use T_D as a lens to understand participation and
collective action within these deviant spaces. In specific, we first study the
characteristics of the most active participants to uncover what might drive
their sustained participation. Next, we investigate how these active
individuals mobilize their community to action. Through our analysis, we
uncover that the most active employed distinct discursive strategies to
mobilize participation, and deployed technical tools like bots to create a
shared identity and sustain engagement. We conclude by providing data-backed
design implications for designers of civic media.","['Claudia Flores-Saviaga', 'Brian C. Keegan', 'Saiph Savage']",[],0,arXiv,http://arxiv.org/abs/1806.00429v1,False,True,False,False,False,2110,David J Knight,Chicago,Active,2020,,"Social scientists have long studied how social policy affects electoral participation, but causal identification is difficult since citizens are not randomly assigned to receive government benefits. We plan to tackle this question by using data from the Moving to Opportunity (MTO) field experiment that sought to move low-income Americans away from concentrated poverty. Funding from the grant will allow us to merge national voter file data with the MTO data so that we can study the downstream effects of receiving housing vouchers on electoral participation for both adult subjects and former child subjects."
"Rental Housing Spot Markets: How Online Information Exchanges Can
  Supplement Transacted-Rents Data","Traditional US rental housing data sources such as the American Community
Survey and the American Housing Survey report on the transacted market - what
existing renters pay each month. They do not explicitly tell us about the spot
market - i.e., the asking rents that current homeseekers must pay to acquire
housing - though they are routinely used as a proxy. This study compares
governmental data to millions of contemporaneous rental listings and finds that
asking rents diverge substantially from these most recent estimates.
Conventional housing data understate current market conditions and
affordability challenges, especially in cities with tight and expensive rental
markets.","['Geoff Boeing', 'Jake Wegmann', 'Junfeng Jiao']",[],0,arXiv,http://arxiv.org/abs/2002.01578v1,False,False,False,False,True,2112,Jeffrey Shrader,Baruch,Completed,2019,2023.0,"Previous work has found that access to air conditioning dampens the relationship between heat and mortality, but it has used coarse, noisy measures of air conditioning adoption. We propose to develop annual, county-level measures of air conditioning adoption from the American Housing Survey. We will develop this measure by, first, estimating how air conditioning adoption depends, state by state, on respondents’ demographic characteristics and locations’ average weather and, second, combining these state-level estimates with county-level American Community Survey demographic information to calculate our county-level measure. We will use our new county-measure to improve previous estimates of the benefit of air conditioning, to estimate whether the availability of air conditioning makes weather forecasts more or less important for health outcomes, and to estimate whether the availability of air conditioning makes public health interventions more or less successful at reducing mortality from extreme heat."
"Equity in 311 Reporting: Understanding Socio-Spatial Differentials in
  the Propensity to Complain","Cities across the United States are implementing information communication
technologies in an effort to improve government services. One such innovation
in e-government is the creation of 311 systems, offering a centralized platform
where citizens can request services, report non-emergency concerns, and obtain
information about the city via hotline, mobile, or web-based applications. The
NYC 311 service request system represents one of the most significant links
between citizens and city government, accounting for more than 8,000,000
requests annually. These systems are generating massive amounts of data that,
when properly managed, cleaned, and mined, can yield significant insights into
the real-time condition of the city. Increasingly, these data are being used to
develop predictive models of citizen concerns and problem conditions within the
city. However, predictive models trained on these data can suffer from biases
in the propensity to make a request that can vary based on socio-economic and
demographic characteristics of an area, cultural differences that can affect
citizens' willingness to interact with their government, and differential
access to Internet connectivity. Using more than 20,000,000 311 requests -
together with building violation data from the NYC Department of Buildings and
the NYC Department of Housing Preservation and Development; property data from
NYC Department of City Planning; and demographic and socioeconomic data from
the U.S. Census American Community Survey - we develop a two-step methodology
to evaluate the propensity to complain: (1) we predict, using a gradient
boosting regression model, the likelihood of heating and hot water violations
for a given building, and (2) we then compare the actual complaint volume for
buildings with predicted violations to quantify discrepancies across the City.","['Constantine Kontokosta', 'Boyeong Hong', 'Kristi Korsberg']",[],0,arXiv,http://arxiv.org/abs/1710.02452v1,False,True,False,False,True,2112,Jeffrey Shrader,Baruch,Completed,2019,2023.0,"Previous work has found that access to air conditioning dampens the relationship between heat and mortality, but it has used coarse, noisy measures of air conditioning adoption. We propose to develop annual, county-level measures of air conditioning adoption from the American Housing Survey. We will develop this measure by, first, estimating how air conditioning adoption depends, state by state, on respondents’ demographic characteristics and locations’ average weather and, second, combining these state-level estimates with county-level American Community Survey demographic information to calculate our county-level measure. We will use our new county-measure to improve previous estimates of the benefit of air conditioning, to estimate whether the availability of air conditioning makes weather forecasts more or less important for health outcomes, and to estimate whether the availability of air conditioning makes public health interventions more or less successful at reducing mortality from extreme heat."
"FinRL: Deep Reinforcement Learning Framework to Automate Trading in
  Quantitative Finance","Deep reinforcement learning (DRL) has been envisioned to have a competitive
edge in quantitative finance. However, there is a steep development curve for
quantitative traders to obtain an agent that automatically positions to win in
the market, namely \textit{to decide where to trade, at what price} and
\textit{what quantity}, due to the error-prone programming and arduous
debugging. In this paper, we present the first open-source framework
\textit{FinRL} as a full pipeline to help quantitative traders overcome the
steep learning curve. FinRL is featured with simplicity, applicability and
extensibility under the key principles, \textit{full-stack framework,
customization, reproducibility} and \textit{hands-on tutoring}.
  Embodied as a three-layer architecture with modular structures, FinRL
implements fine-tuned state-of-the-art DRL algorithms and common reward
functions, while alleviating the debugging workloads. Thus, we help users
pipeline the strategy design at a high turnover rate. At multiple levels of
time granularity, FinRL simulates various markets as training environments
using historical data and live trading APIs. Being highly extensible, FinRL
reserves a set of user-import interfaces and incorporates trading constraints
such as market friction, market liquidity and investor's risk-aversion.
Moreover, serving as practitioners' stepping stones, typical trading tasks are
provided as step-by-step tutorials, e.g., stock trading, portfolio allocation,
cryptocurrency trading, etc.","['Xiao-Yang Liu', 'Hongyang Yang', 'Jiechao Gao', 'Christina Dan Wang']",[],0,arXiv,http://arxiv.org/abs/2111.09395v1,False,True,False,False,False,2117,Liu A Yang,Maryland,Active,2021,,"We propose to examine how factors such as cost of financing, supply of skilled human capital and market competition affect the innovative activity and performance at the firm level. We will use census datasets to track firm’s growth over time and to construct measures of innovation activity, innovation outcome and firm performance. We will merge census datasets with external datasets to exploit variation in the funding provided to firms, exogenous shocks in the supply of highly-skilled workers available, and changes in competition environment through tariffs to gauge the effect of various market friction on innovation at the firm level."
A Review on Internet of Things for Defense and Public Safety,"The Internet of Things (IoT) is undeniably transforming the way that
organizations communicate and organize everyday businesses and industrial
procedures. Its adoption has proven well suited for sectors that manage a large
number of assets and coordinate complex and distributed processes. This survey
analyzes the great potential for applying IoT technologies (i.e., data-driven
applications or embedded automation and intelligent adaptive systems) to
revolutionize modern warfare and provide benefits similar to those in industry.
It identifies scenarios where Defense and Public Safety (PS) could leverage
better commercial IoT capabilities to deliver greater survivability to the
warfighter or first responders, while reducing costs and increasing operation
efficiency and effectiveness. This article reviews the main tactical
requirements and the architecture, examining gaps and shortcomings in existing
IoT systems across the military field and mission-critical scenarios. The
review characterizes the open challenges for a broad deployment and presents a
research roadmap for enabling an affordable IoT for defense and PS.","['Paula Fraga-Lamas', 'Tiago M. Fernández-Caramés', 'Manuel Suárez-Albela', 'Luis Castedo', 'Miguel González-López']",[],0,arXiv,http://arxiv.org/abs/2402.03599v1,False,True,False,False,False,2120,Charles M Tolbert,Texas,Active,2019,,"This project will analyze the role credit availability plays in the development and growth of veteran-owned businesses over time and across space. Based on past research, we expect the performance of veteran-owned firms to be affected by the availability of credit for these ventures (either start-up capital or expansion capital). To better understand the effects of access to credit on veteran-owned businesses, the researchers will build on their past research, which identified local financial institutions to examine the impact different types of lenders in a geographic region have on small and young businesses. Care will be taken to identify more specifically those financial industries that are often coded in miscellaneous industry categories. The researchers have developed computer programs that parse names, abbreviations, and acronyms for information that may suggest a more specific classification. As part of our investigation, we will examine how survival and employment growth in military veteran owned businesses vary across disability status, gender, race, and rurality. The researchers propose to advance the scholarly understanding of military veteran business location and growth process and the effects of military veteran businesses on local economic performance in the United States"
"Junk News on Military Affairs and National Security: Social Media
  Disinformation Campaigns Against US Military Personnel and Veterans","Social media provides political news and information for both active duty
military personnel and veterans. We analyze the subgroups of Twitter and
Facebook users who spend time consuming junk news from websites that target US
military personnel and veterans with conspiracy theories, misinformation, and
other forms of junk news about military affairs and national security issues.
(1) Over Twitter we find that there are significant and persistent interactions
between current and former military personnel and a broad network of extremist,
Russia-focused, and international conspiracy subgroups. (2) Over Facebook, we
find significant and persistent interactions between public pages for military
and veterans and subgroups dedicated to political conspiracy, and both sides of
the political spectrum. (3) Over Facebook, the users who are most interested in
conspiracy theories and the political right seem to be distributing the most
junk news, whereas users who are either in the military or are veterans are
among the most sophisticated news consumers, and share very little junk news
through the network.","['John D. Gallacher', 'Vlad Barash', 'Philip N. Howard', 'John Kelly']",[],0,arXiv,http://arxiv.org/abs/1802.03572v1,False,True,False,False,False,2120,Charles M Tolbert,Texas,Active,2019,,"This project will analyze the role credit availability plays in the development and growth of veteran-owned businesses over time and across space. Based on past research, we expect the performance of veteran-owned firms to be affected by the availability of credit for these ventures (either start-up capital or expansion capital). To better understand the effects of access to credit on veteran-owned businesses, the researchers will build on their past research, which identified local financial institutions to examine the impact different types of lenders in a geographic region have on small and young businesses. Care will be taken to identify more specifically those financial industries that are often coded in miscellaneous industry categories. The researchers have developed computer programs that parse names, abbreviations, and acronyms for information that may suggest a more specific classification. As part of our investigation, we will examine how survival and employment growth in military veteran owned businesses vary across disability status, gender, race, and rurality. The researchers propose to advance the scholarly understanding of military veteran business location and growth process and the effects of military veteran businesses on local economic performance in the United States"
"Firm-level supply chains to minimize unemployment and economic losses in
  rapid decarbonization scenarios","Urgently needed carbon emissions reductions might lead to strict
command-and-control decarbonization strategies with potentially negative
economic consequences. Analysing the entire firm-level production network of a
European economy, we have explored how the worst outcomes of such approaches
can be avoided. We compared the systemic relevance of every firm in Hungary
with its annual CO2 emissions to identify optimal emission-reducing strategies
with a minimum of additional unemployment and economic losses. Setting specific
reduction targets, we studied various decarbonization scenarios and quantified
their economic consequences. We determined that for an emissions reduction of
20%, the most effective strategy leads to losses of about 2% of jobs and 2% of
economic output. In contrast, a naive scenario targeting the largest emitters
first results in 28% job losses and 33% output reduction for the same target.
This demonstrates that it is possible to use firm-level production networks to
design highly effective decarbonization strategies that practically preserve
employment and economic output.","['Johannes Stangl', 'András Borsos', 'Christian Diem', 'Tobias Reisch', 'Stefan Thurner']",[],0,arXiv,http://arxiv.org/abs/2302.08987v3,False,True,False,False,False,2127,Henry Friedman,UCLA,Active,2020,,"This project will analyze the employment diversity and establishment/firm productivity. It will produce some descriptive statistics and regression estimates of population diversity within firms and the effects on productivity that are not contained in existing publications. Employment diversity is one of the important aspects of the corporate culture. In recent years, the importance of the corporate culture for firms' performance attract more and more attention. The effects of employment diversity have not been fully investigated. There are different kinds of employment diversity such as ethnic diversity, age diversity, education diversity, and gender diversity. How do these different aspects of diversity affect firms' productivity? Do some of these diversities have positive effects while others have negative effects on firms' productivity?  The LEHD data are particularly well-suited to explore these questions. 
In this project the researcher will use the LEHD Individual Characteristics File to construct firm/establishment level age, gender, educational and ethnic diversity measures. The researcher will link these measures to firm (and when possible, establishment) records in the Economic Census, Annual Survey of Manufactures, Annual Retail Trade Survey and Annual Wholesale Trade Survey. Then the researcher will use these files to generate firm/establishment productivity measures. After generating the diversity and productivity measures, the project will show the descriptive statistics of all measures. Moreover, the study will develop regression estimates of the effects of firm/establishment level diversities on firm/establishment level productivity. "
Asset Prices and Risk Aversion,"The standard asset pricing models (the CCAPM and the Epstein-Zin non-expected
utility model) counterintuitively predict that equilibrium asset prices can
rise if the representative agent's risk aversion increases. If the income
effect, which implies enhanced saving as a result of an increase in risk
aversion, dominates the substitution effect, which causes the representative
agent to reallocate his portfolio in favour of riskless assets, the demand for
securities increases. Thus, asset prices are forced to rise when the
representative agent is more risk adverse. By disentangling risk aversion and
intertemporal substituability, we demonstrate that the risky asset price is an
increasing function of the coefficient of risk aversion only if the elasticity
of intertemporal substitution (EIS) exceeds unity. This result, which was first
proved par Epstein (1988) in a stationary economy setting with a constant risk
aversion, is shown to hold true for non-stationary economies with a variable or
constant risk aversion coefficient. The conclusion is that the EIS probably
exceeds unity.",['Dominique Pepin'],[],0,arXiv,http://arxiv.org/abs/1403.0851v1,False,True,False,False,False,2128,Benjamin Iverson,Utah,Active,2020,,"A growing literature highlights the huge dispersion in productivity across establishments in the US. New research demonstrates that resource reallocation is a key factor in growth and explains much of the variation in productivity. Despite a mounting recognition of the importance of asset allocation, existing measures of asset allocation rely on aggregate estimates that do not track how individual assets change hands and are ultimately reutilized. In this project, we construct new measures of the real estate assets used by firms from purchased US Postal Service data linked with restricted data from the LBD, SSEL, and economic surveys/censuses. We then use these measures to evaluate asset utilization and reallocation in the economy to determine how assets are used or redeployed following a variety of economic shocks, and how reallocation activity affects other measures of performance and economic activity with fixed effects models."
Ambiguous volatility and asset pricing in continuous time,"This paper formulates a model of utility for a continuous time framework that
captures the decision-maker's concern with ambiguity about both volatility and
drift. Corresponding extensions of some basic results in asset pricing theory
are presented. First, we derive arbitrage-free pricing rules based on hedging
arguments. Ambiguous volatility implies market incompleteness that rules out
perfect hedging. Consequently, hedging arguments determine prices only up to
intervals. However, sharper predictions can be obtained by assuming preference
maximization and equilibrium. Thus we apply the model of utility to a
representative agent endowment economy to study equilibrium asset returns. A
version of the C-CAPM is derived and the effects of ambiguous volatility are
described.","['Larry G. Epstein', 'Shaolin Ji']",[],0,arXiv,http://arxiv.org/abs/1301.4614v1,False,True,False,False,False,2128,Benjamin Iverson,Utah,Active,2020,,"A growing literature highlights the huge dispersion in productivity across establishments in the US. New research demonstrates that resource reallocation is a key factor in growth and explains much of the variation in productivity. Despite a mounting recognition of the importance of asset allocation, existing measures of asset allocation rely on aggregate estimates that do not track how individual assets change hands and are ultimately reutilized. In this project, we construct new measures of the real estate assets used by firms from purchased US Postal Service data linked with restricted data from the LBD, SSEL, and economic surveys/censuses. We then use these measures to evaluate asset utilization and reallocation in the economy to determine how assets are used or redeployed following a variety of economic shocks, and how reallocation activity affects other measures of performance and economic activity with fixed effects models."
Optimal pricing for carbon dioxide removal under inter-regional leakage,"Carbon dioxide removal (CDR) moves atmospheric carbon to geological or
land-based sinks. In a first-best setting, the optimal use of CDR is achieved
by a removal subsidy that equals the optimal carbon tax and marginal damages.
We derive second-best policy rules for CDR subsidies and carbon taxes when no
global carbon price exists but a national government implements a unilateral
climate policy. We find that the optimal carbon tax differs from an optimal CDR
subsidy because of carbon leakage and a balance of resource trade effect.
First, the optimal removal subsidy tends to be larger than the carbon tax
because of lower supply-side leakage on fossil resource markets. Second, net
carbon exporters exacerbate this wedge to increase producer surplus of their
carbon resource producers, implying even larger removal subsidies. Third, net
carbon importers may set their removal subsidy even below their carbon tax when
marginal environmental damages are small, to appropriate producer surplus from
carbon exporters.","['Max Franks', 'Matthias Kalkuhl', 'Kai Lessmann']",[],0,arXiv,http://arxiv.org/abs/2212.09299v1,False,True,False,False,False,2129,Rebecca Lester,Stanford,Active,2021,,"This project will study firms' internal capital markets by examining companies' responses to divisional cash shocks. Prior literature documents various frictions affecting the allocation of assets within a firm, such as regulatory requirements or agency costs. As a result, whether, to what extent, and where a cash flow shock will be spent by a firm is an open empirical question. Our ""shocks"" to capital flows are subsidies awarded to firms by regional (e.g., state, county, or city) governments, which occur frequently over our sample period. We consider two main types of subsidies - cash grants and regional tax exemptions - that either decrease a firm's operating costs for a certain period of time (in the case of regional tax exemptions) or increase a firm's available capital by providing a ""windfall"" (in the case of cash grants) in the specific subsidizing jurisdiction. Our dataset consists of a large sample of approximately 50,000 tax subsidies and cash grants awarded to over 2,000 companies since 2004. Additionally, we will use the Economic Census, the Annual Survey of Manufactures (ASM), the Annual Capital Expenditures Survey (ACES), and other Census datasets. Our tests provide insight into the extent to which firms efficiently redeploy their assets and the potentially different effects of the cash flow shocks provided by the two different types of subsidies we consider. We also intend to show how frictions, such as regulatory restrictions and agency costs, affect firms' internal asset allocations. Finally, we assess regional implications of cash flow shocks."
"Time-warped growth processes, with applications to the modeling of
  boom-bust cycles in house prices","House price increases have been steady over much of the last 40 years, but
there have been occasional declines, most notably in the recent housing bust
that started around 2007, on the heels of the preceding housing bubble. We
introduce a novel growth model that is motivated by time-warping models in
functional data analysis and includes a nonmonotone time-warping component that
allows the inclusion and description of boom-bust cycles and facilitates
insights into the dynamics of asset bubbles. The underlying idea is to model
longitudinal growth trajectories for house prices and other phenomena, where
temporal setbacks and deflation may be encountered, by decomposing such
trajectories into two components. A first component corresponds to underlying
steady growth driven by inflation that anchors the observed trajectories on a
simple first order linear differential equation, while a second boom-bust
component is implemented as time warping. Time warping is a commonly
encountered phenomenon and reflects random variation along the time axis. Our
approach to time warping is more general than previous approaches by admitting
the inclusion of nonmonotone warping functions. The anchoring of the
trajectories on an underlying linear dynamic system also makes the time-warping
component identifiable and enables straightforward estimation procedures for
all model components. The application to the dynamics of housing prices as
observed for 19 metropolitan areas in the U.S. from December 1998 to July 2013
reveals that the time setbacks corresponding to nonmonotone time warping vary
substantially across markets and we find indications that they are related to
market-specific growth rates.","['Jie Peng', 'Debashis Paul', 'Hans-Georg Müller']",[],0,arXiv,http://arxiv.org/abs/1411.5497v1,False,True,False,False,False,2133,Jose Carreno-Garcia,Berkeley,Active,2019,,This project analyzes the link between the housing bubble preceding the Great Recession and the recent U.S. productivity slowdown. This will consist of two stages. The first stage of this analysis will deal with the direct effects of the housing bust on aggregate demand and consumption. The second stage of the analysis will explore the channels that connect the aggregate demand/consumption slowdown to the productivity slowdown.
"Multidimensional Skills as a Measure of Human Capital: Evidence from
  LinkedIn Profiles","We measure human capital using the self-reported skill sets of 8.75 million
U.S. college graduates from professional profiles on the online platform
LinkedIn. We establish that these skills are systematically related to human
capital investments such as different types of schooling and work experience.
The average profile of the number of reported skills by age looks remarkably
similar to the well-established concave age-earnings profiles. More experienced
workers and those with higher educational degrees have larger shares of
occupation-specific skills, consistent with their acquisition through
professional-degree programs and on-the-job experience. Workers who report
more, and particularly more specific and managerial, skills are more likely to
hold highly paid jobs. Skill differences across workers can account for more
earnings variation than detailed vectors of education and experience. We also
document a substantial gender gap in reported skills, which starts to manifest
when young women reach typical ages of first motherhood. Gender differences in
skill profiles can rationalize a substantial proportion of the gender gap in
the propensity to work in highly paid jobs. Overall, the results are consistent
with an important role of multidimensional skills in accounting for several
well-known basic labor-market patterns.","['David Dorn', 'Florian Schoner', 'Moritz Seebacher', 'Lisa Simon', 'Ludger Woessmann']",[],0,arXiv,http://arxiv.org/abs/2409.18638v1,False,True,False,False,False,2136,Sarah Hamersma,Cornell,Active,2019,,"The Supplemental Nutrition Assistance Program (SNAP) is one of the most broadly-targeted anti-poverty programs in the U.S. While SNAP is available throughout the life course, it may play a distinct role in labor market decisions in early adulthood. In this research project, we model the relationship between SNAP participation and educational attainment for young adults, and examine whether differences in SNAP work requirements affect that relationship. In early adulthood, SNAP may play a role in improving the feasibility of time-intensive educational investments by helping meet basic needs. This role may be limited, however, by the restrictions on SNAP access placed on young adults without children, many of whom face work requirements for eligibility unless their state or county successfully applies for a federal waiver of the requirement. We will use the Survey of Income and Program Participation, combined with administrative data on SNAP from seven states and policy data on work-requirement waivers by county, to determine whether work requirements create a meaningful barrier to SNAP for young adults, and whether SNAP access plays an important role in educational attainment. Using a variety of quasi-experimental methods including regression analyses and instrumental variables models, we expect to see reduced SNAP take-up in areas with work requirements, and potentially less educational investment by young adults with such barriers to SNAP participation. This research will help policy makers better understand whether potential students emerging from disadvantaged households may lack access to food assistance that could otherwise help make college attendance feasible."
Incentive-Aware Models of Financial Networks,"Financial networks help firms manage risk but also enable financial shocks to
spread. Despite their importance, existing models of financial networks have
several limitations. Prior works often consider a static network with a simple
structure (e.g., a ring) or a model that assumes conditional independence
between edges. We propose a new model where the network emerges from
interactions between heterogeneous utility-maximizing firms. Edges correspond
to contract agreements between pairs of firms, with the contract size being the
edge weight. We show that, almost always, there is a unique ""stable network.""
All edge weights in this stable network depend on all firms' beliefs.
Furthermore, firms can find the stable network via iterative pairwise
negotiations. When beliefs change, the stable network changes. We show that
under realistic settings, a regulator cannot pin down the changed beliefs that
caused the network changes. Also, each firm can use its view of the network to
inform its beliefs. For instance, it can detect outlier firms whose beliefs
deviate from their peers. But it cannot identify the deviant belief: increased
risk-seeking is indistinguishable from increased expected profits. Seemingly
minor news may settle the dilemma, triggering significant changes in the
network.","['Akhil Jalan', 'Deepayan Chakrabarti', 'Purnamrita Sarkar']",[],0,arXiv,http://arxiv.org/abs/2212.06808v3,False,True,False,False,False,2154,Yao Liu,Baruch,Active,2022,,"The goal of this project is to study the effect of financial reporting regulation on firm dynamics based on different prior state-level disclosure-requirement conditions. To demonstrate firm dynamics, I first show within-firm efficiency and then examine across-firm efficiency and resource allocation. To show within-firm efficiency, I study firm-specific TFP and geographic mobility. For across-firm efficiency and resource allocation (i.e., shifting the allocation of labor and capital). I study whether resources move towards firms with higher productivity within local industries, whether there are disproportionate effects on smaller and younger businesses, and whether there are resource allocation dynamics between public and private establishments (e.g., standard general equilibrium effects).

Empirically analyzing the research question has been difficult because of both data and identification challenges. The analyses of ex ante regulatory design require state-level disclosure jurisdictions. To construct this measurement, I use machine learning techniques to dissect judicial opinions at state-level cases to document how rigorous and specific disclosure requirements are within the state. The analyses of ex post firm dynamics require micro-level data on small firms connecting both their real decisions to their productivity and age."
Observed mobility behavior data reveal social distancing inertia,"The research team has utilized an integrated dataset, consisting of
anonymized location data, COVID-19 case data, and census population
information, to study the impact of COVID-19 on human mobility. The study
revealed that statistics related to social distancing, namely trip rate, miles
traveled per person, and percentage of population staying at home have all
showed an unexpected trend, which we named social distancing inertia. The
trends showed that as soon as COVID-19 cases were observed, the statistics
started improving, regardless of government actions. This suggests that a
portion of population who could and were willing to practice social distancing
voluntarily and naturally reacted to the emergence of COVID-19 cases. However,
after about two weeks, the statistics saturated and stopped improving, despite
the continuous rise in COVID-19 cases. The study suggests that there is a
natural behavior inertia toward social distancing, which puts a limit on the
extent of improvement in the social-distancing-related statistics. The national
data showed that the inertia phenomenon is universal, happening in all the U.S.
states and for all the studied statistics. The U.S. states showed a
synchronized trend, regardless of the timeline of their statewide COVID-19 case
spreads or government orders.","['Sepehr Ghader', 'Jun Zhao', 'Minha Lee', 'Weiyi Zhou', 'Guangchen Zhao', 'Lei Zhang']",[],0,arXiv,http://arxiv.org/abs/2004.14748v1,False,True,False,False,False,2167,Linda Zhao,Chicago,Active,2020,,"We propose to estimate tract-level associations between (1) ethnic background and household income and (2) ethnic background and mobility, as measured by whether an individual lived in the same county five years prior. Individual-level data from the Decennial Censuses (1950, 1960, 1970, 1980, 1990, and 2000) will be used to construct measures of association. Since little is known about patterns of associations, we will start by describing trends over time. Then, we will merge association measures from the Census 2000 at the tract level with the 2000 Social Capital Community Benchmark Survey (SCCBS), which contain information on social cohesion. Using ordinal multilevel logistic regressions, we will investigate whether measures of association predict trust in neighbors, net of contextual controls that are derived from the Census 2000 and data from the Department of Justice (DOJ UCR 2000). Our work will contribute to the scholarly debate on the implications of ethnoracial diversity on social cohesion. Recent studies have argued that diversity in communities diminishes cohesion, for instance, by reducing trust in neighbors. Yet classical research on social integration suggests that it is the association, or ""consolidation"", of multiple dimensions of social life (i.e. ethnic background and household income) that actually impede integration by impeding the existence of multifaceted social groups and intersecting social norms. We expect to find, after introducing consolidation measures to the debate on trust and cohesion, that existing debate on diversity is too simplistic and that it is consolidation rather than diversity that best predicts trust within communities."
"Automated Identification of Eviction Status from Electronic Health
  Record Notes","Objective: Evictions are important social and behavioral determinants of
health. Evictions are associated with a cascade of negative events that can
lead to unemployment, housing insecurity/homelessness, long-term poverty, and
mental health problems. In this study, we developed a natural language
processing system to automatically detect eviction status from electronic
health record (EHR) notes.
  Materials and Methods: We first defined eviction status (eviction presence
and eviction period) and then annotated eviction status in 5000 EHR notes from
the Veterans Health Administration (VHA). We developed a novel model, KIRESH,
that has shown to substantially outperform other state-of-the-art models such
as fine-tuning pre-trained language models like BioBERT and BioClinicalBERT.
Moreover, we designed a novel prompt to further improve the model performance
by using the intrinsic connection between the two sub-tasks of eviction
presence and period prediction. Finally, we used the Temperature Scaling-based
Calibration on our KIRESH-Prompt method to avoid over-confidence issues arising
from the imbalance dataset.
  Results: KIRESH-Prompt substantially outperformed strong baseline models
including fine-tuning the BioClinicalBERT model to achieve 0.74672 MCC, 0.71153
Macro-F1, and 0.83396 Micro-F1 in predicting eviction period and 0.66827 MCC,
0.62734 Macro-F1, and 0.7863 Micro-F1 in predicting eviction presence. We also
conducted additional experiments on a benchmark social determinants of health
(SBDH) dataset to demonstrate the generalizability of our methods.
  Conclusion and Future Work: KIRESH-Prompt has substantially improved eviction
status classification. We plan to deploy KIRESH-Prompt to the VHA EHRs as an
eviction surveillance system to help address the US Veterans' housing
insecurity.","['Zonghai Yao', 'Jack Tsai', 'Weisong Liu', 'David A. Levy', 'Emily Druhl', 'Joel I Reisman', 'Hong Yu']",[],0,arXiv,http://arxiv.org/abs/2212.02762v3,False,True,False,False,False,2175,Ashley Gromis,Baruch,Completed,2019,2022.0,"We study the prevalence, causes, and consequences of eviction in the United States. We will use the 2017 American Housing Survey, which contains measures of informal as well as formal eviction, and the 2018 Rental Housing Finance Survey, when it becomes available. We will first produce national and regional estimates of the prevalence of types of evictions, comparing the frequency of formal and informal eviction. Next, we will document the causes of eviction, focusing on the importance of demographic and socioeconomic factors, including family type, gender, and income level. To inform policy and programmatic responses, we will then analyze the consequences of eviction on housing search, housing quality, and neighborhood quality. We will employ logistic regression models that account for other factors relevant to our outcomes."
Whack-a-Chip: The Futility of Hardware-Centric Export Controls,"U.S. export controls on semiconductors are widely known to be permeable, with
the People's Republic of China (PRC) steadily creating state-of-the-art
artificial intelligence (AI) models with exfiltrated chips. This paper presents
the first concrete, public evidence of how leading PRC AI labs evade and
circumvent U.S. export controls. We examine how Chinese companies, notably
Tencent, are not only using chips that are restricted under U.S. export
controls but are also finding ways to circumvent these regulations by using
software and modeling techniques that maximize less capable hardware.
Specifically, we argue that Tencent's ability to power its Hunyuan-Large model
with non-export controlled NVIDIA H20s exemplifies broader gains in efficiency
in machine learning that have eroded the moat that the United States initially
built via its existing export controls. Finally, we examine the implications of
this finding for the future of the United States' export control strategy.","['Ritwik Gupta', 'Leah Walker', 'Andrew W. Reddie']",[],0,arXiv,http://arxiv.org/abs/2411.14425v1,False,True,False,False,False,2181,Kelly Hewett,Atlanta,Active,2022,,"Firms predominantly choose exporting in their efforts to grow via international expansion. As managers formulate their overall exporting strategies, including what products to export, which markets represent the best opportunities for their next export venture(s), and in which markets they should cease export activities, accumulated export experience and related learning play an important role, a phenomenon often referred to in the marketing literature as learning-by-exporting. While the economic trade literature has explored issues associated with exporting a number of products and exporting to a number of markets,  there remain gaps regarding the extent to which exporting different products and exporting to different markets enhance learning from exporting and how much each type of learning (from markets and from products) influences export success. To fill these gaps, we will explore how exporting firms maximize learning based on their export experiences with market and with products.  Specifically, in this project, we will answer the following questions: Does learning from exporting the same product to different markets or different products to the same market contribute more to a firm's export capabilities; how will learning from exporting change over time; and do export capabilities affect export venture survival? To answer these questions, we will estimate multivariate regression models using the Longitudinal Firm Trade Transactions Database (LFTTD)."
Genomic Materials Design: CALculation of PHAse Dynamics,"The CALPHAD system of fundamental phase-level databases, now known as the
Materials Genome, has enabled a mature technology of computational materials
design and qualification that has already met the acceleration goals of the
national Materials Genome Initiative. As first commercialized by QuesTek
Innovations, the methodology combines efficient genomic-level parametric design
of new material composition and process specifications with multidisciplinary
simulation-based forecasting of manufacturing variation, integrating efficient
uncertainty management. Recent projects demonstrated under the
multi-institutional CHiMaD Design Center notably include novel alloys designed
specifically for the new technology of additive manufacturing. With the proven
success of the CALPHAD-based Materials Genome technology, current university
research emphasizes new methodologies for affordable accelerated expansion of
more accurate CALPHAD databases. Rapid adoption of these new capabilities by US
apex corporations has compressed the materials design and development cycle to
under 2 years, enabling a new ""materials concurrency"" integrated into a new
level of concurrent engineering supporting an unprecedented level of
manufacturing innovation.","['G. B Olson', 'Z. K. Liu']",[],0,arXiv,http://arxiv.org/abs/2305.05060v2,False,True,False,False,False,2185,Sharon Belenzon,Triangle,Active,2021,,"This research will investigate the evolution of the relationship between scientific research and manufacturing capabilities in American firms over the last few decades of the 20th century. The researchers will use internal U.S. Census Bureau microdata alongside a comprehensive firm-level, researcher-created listing of scientific articles published in ""hard science"" journals by corporate scientists. The linked data will allow the researchers to study the relationship between corporate research and manufacturing and investigate the mechanisms underlying that relationship, as they specifically estimate the impact of offshoring and other factors on scientific output by firms. The research will also describe the allocation of research activity by country, comparing the distribution of research and development (R&D) activity by company location (domestic versus international) to the location of the publishing author(s) in the external database."
"Hiring in the substance use disorder treatment related sector during the
  first five years of Medicaid expansion","Effective treatment strategies exist for substance use disorder (SUD),
however severe hurdles remain in ensuring adequacy of the SUD treatment (SUDT)
workforce as well as improving SUDT affordability, access and stigma. Although
evidence shows recent increases in SUD medication access from expanding
Medicaid availability under the Affordable Care Act, it is yet unknown whether
these policies also led to a growth in the changes in the nature of hiring in
SUDT related workforce, partly due to poor data availability. Our study uses
novel data to shed light on recent trends in a fast-evolving and
policy-relevant labor market, and contributes to understanding the current SUDT
related workforce and the effect of Medicaid expansion on hiring attempts in
this sector. We examine attempts over 2010-2018 at hiring in the SUDT and
related behavioral health sector as background for estimating the causal effect
of the 2014-and-beyond state Medicaid expansion on these outcomes through
""difference-in-difference"" econometric models. We use Burning Glass
Technologies (BGT) data covering virtually all U.S. job postings by employers.
Nationally, we find little growth in the sector's hiring attempts in 2010-2018
relative to the rest of the economy or to health care as a whole. However, this
masks diverging trends in subsectors, which saw reduction in hospital based
hiring attempts, increases towards outpatient facilities, and changes in
occupational hiring demand shifting from medical personnel towards counselors
and social workers. Although Medicaid expansion did not lead to any
statistically significant or meaningful change in overall hiring attempts,
there was a shift in the hiring landscape.","['Olga Scrivner', 'Thuy Nguyen', 'Kosali Simon', 'Esmé Middaugh', 'Bledi Taska', 'Katy Börner']",[],0,arXiv,http://arxiv.org/abs/1908.00216v1,False,True,False,False,False,2192,Lara Shore-Sheppard,Yale,Active,2022,,"This project uses 2008-2017 American Community Survey data to examine the impact of the 2010 Affordable Care Act (ACA), the largest health reform in a generation, on low-income individuals. The project focuses on three questions of interest: (1) how ACA Medicaid expansions affected health insurance coverage, (2) how ACA Medicaid expansions affected participation in other non-health safety net programs, and (3) how ACA Medicaid expansions affected employment and wages.  We use a county-border-pair design to assess how individuals living in states with ACA Medicaid eligibility expansions compare to those living just on the other side of a state border without such expansions."
A graph-based multimodal framework to predict gentrification,"Gentrification--the transformation of a low-income urban area caused by the
influx of affluent residents--has many revitalizing benefits. However, it also
poses extremely concerning challenges to low-income residents. To help
policymakers take targeted and early action in protecting low-income residents,
researchers have recently proposed several machine learning models to predict
gentrification using socioeconomic and image features. Building upon previous
studies, we propose a novel graph-based multimodal deep learning framework to
predict gentrification based on urban networks of tracts and essential
facilities (e.g., schools, hospitals, and subway stations). We train and test
the proposed framework using data from Chicago, New York City, and Los Angeles.
The model successfully predicts census-tract level gentrification with 0.9
precision on average. Moreover, the framework discovers a previously unexamined
strong relationship between schools and gentrification, which provides a basis
for further exploration of social factors affecting gentrification.","['Javad Eshtiyagh', 'Baotong Zhang', 'Yujing Sun', 'Linhui Wu', 'Zhao Wang']",[],0,arXiv,http://arxiv.org/abs/2312.15646v2,False,True,False,False,False,2212,Michelle Lam,Michigan,Active,2020,,"This paper studies gentrification at the block or block group level, using restricted American Community Survey (block group) and Decennial Census (block for 1970-2000, block group for2010) data linked to other block or block group level datasets available publicly. The block-level data will be used to estimate a quantitative general equilibrium new geography model, which will pin down what kind of shocks or ""changes"" are most important in explaining the gentrification patterns we see in cities today, and how large a role heterogeneous agents and/or agglomeration effects play in these patterns. In order to capture the empirically-observed rich spatial variation between small geographic units, using block or block group level data is critical. Moreover, the block-level data can be used to disentangle the competing results of micro- and macro-level studies of gentrification. Micro-level studies typically feature a narrative of displacement whereas macro-level studies find no significant displacement effect. Reconciling these findings by using micro-level data in a macro-level model which can aggregate results will also be an important contribution to the literature."
Ownership Chains in Multinational Enterprises,"This study examines how multinational enterprises (MNEs) structure ownership
chains to coordinate subsidiaries across multiple national borders. Using a
unique global dataset, we first document key stylized facts: 54% of
subsidiaries are controlled through indirect ownership, and ownership chains
can span up to seven countries. In particular, we find that subsidiaries
further down the hierarchy tend to be more geographically distant from the
parent and often operate in different time zones. This suggests that the ease
of communication along ownership chains is a critical determinant of their
structure. Motivated by these findings, we develop a location choice model in
which parent firms compete for corporate control of final subsidiaries. When
monitoring is costly, they may delegate control to an intermediate affiliate in
another jurisdiction. The model generates a two-stage empirical strategy: (i) a
trilateral equation that determines the location of an intermediate affiliate
conditional on the location of final subsidiaries; and (ii) a bilateral
equation that predicts the location of final investment. Our empirical
estimates confirm that the ease of communication at the country level
significantly influences the location decisions of affiliates along ownership
chains. These findings underscore the importance of organizational frictions in
shaping global corporate structures and provide new insights into the geography
of multinational ownership networks.","['Stefania Miricola', 'Armando Rungi', 'Gianluca Santoni']",[],0,arXiv,http://arxiv.org/abs/2305.12857v2,False,True,False,False,False,2214,Nicolas Crouzet,Chicago,Active,2019,,"This project will investigate differences in capital structure, investment rates, and rates of return between publicly traded and privately held firms. A long literature in corporate finance has studied the connection between firm ownership, capital structure, and investment. Going public helps firms diversify their funding sources, relaxing financial constraints (Pagano, Panetta, and Zingales 1998). On the other hand, going public increases separation between ownership and control possibly worsening agency problems (Jensen and Meckling 1976). However, evidence on the importance of these mechanisms, and on the net benefits of public ownership, is limited. This is because it is difficult to observe capital structure and investment decisions of private firms. Quantifying the benefits of going public has become a more pressing question in recent years since the number of publicly traded US corporations has declined substantially since the early 2000’s (Doidge, Karolyi, and Stulz 2017). Our goal in this project is to use data from the Quarterly Financial Report to provide extensive, long-run evidence on average differences in investment and capital structure between private and public firms, on the consequences of going public, and on the contribution of changes in ownership composition to trends in aggregate investment and returns to capital."
"Comparison of the Cost Metrics for Reversible and Quantum Logic
  Synthesis","A breadth-first search method for determining optimal 3-line circuits
composed of quantum NOT, CNOT, controlled-V and controlled-V+ (NCV) gates is
introduced. Results are presented for simple gate count and for technology
motivated cost metrics. The optimal NCV circuits are also compared to NCV
circuits derived from optimal NOT, CNOT and Toffoli (NCT) gate circuits. The
work presented here provides basic results and motivation for continued study
of the direct synthesis of NCV circuits, and establishes relations between
function realizations in different circuit cost metrics.","['Dmitri Maslov', 'D. Michael Miller']",[],0,arXiv,http://arxiv.org/abs/quant-ph/0511008v3,False,True,False,False,False,2215,Joshua Clapp,Colorado,Active,2019,,"The economic and social consequences of violent victimization for individuals are not well understood. As a major and deleterious life event often effecting one's mental, physical and emotional well-being, it is likely that victimization either directly (i.e. through missed work due to health complications) or indirectly (i.e. resulting from psychological trauma that negatively impacts living conditions) disrupts economic stability. We propose to exploit the panel nature of the NCVS data to create victimization histories for respondents over a 3 year window, which we can then use to assess how changes in employment, earnings, housing, and marital status are related to instances of criminal victimization, after adjusting for demographic and economic characteristics of their communities. Results will shed light on the previously-ignored socioeconomic burden of victimization for individuals, as well as for which types of people these negative impacts are felt the strongest. "
"A Non-Intrusive and Context-Based Vulnerability Scoring Framework for
  Cloud Services","Understanding the severity of vulnerabilities within cloud services is
particularly important for today service administrators.Although many systems,
e.g., CVSS, have been built to evaluate and score the severity of
vulnerabilities for administrators, the scoring schemes employed by these
systems fail to take into account the contextual information of specific
services having these vulnerabilities, such as what roles they play in a
particular service. Such a deficiency makes resulting scores unhelpful. This
paper presents a practical framework, NCVS, that offers automatic and
contextual scoring mechanism to evaluate the severity of vulnerabilities for a
particular service. Specifically, for a given service S, NCVS first
automatically collects S contextual information including topology,
configurations, vulnerabilities and their dependencies. Then, NCVS uses the
collected information to build a contextual dependency graph, named CDG, to
model S context. Finally, NCVS scores and ranks all the vulnerabilities in S by
analyzing S context, such as what roles the vulnerabilities play in S, and how
critical they affect the functionality of S. NCVS is novel and useful, because
1) context-based vulnerability scoring results are highly relevant and
meaningful for administrators to understand each vulnerability importance
specific to the target service; and 2) the workflow of NCVS does not need
instrumentation or modifications to any source code. Our experimental results
demonstrate that NCVS can obtain more relevant vulnerability scoring results
than comparable system, such as CVSS.","['Hao Zhuang', 'Florian Pydde']",[],0,arXiv,http://arxiv.org/abs/1611.07383v2,False,True,False,False,False,2215,Joshua Clapp,Colorado,Active,2019,,"The economic and social consequences of violent victimization for individuals are not well understood. As a major and deleterious life event often effecting one's mental, physical and emotional well-being, it is likely that victimization either directly (i.e. through missed work due to health complications) or indirectly (i.e. resulting from psychological trauma that negatively impacts living conditions) disrupts economic stability. We propose to exploit the panel nature of the NCVS data to create victimization histories for respondents over a 3 year window, which we can then use to assess how changes in employment, earnings, housing, and marital status are related to instances of criminal victimization, after adjusting for demographic and economic characteristics of their communities. Results will shed light on the previously-ignored socioeconomic burden of victimization for individuals, as well as for which types of people these negative impacts are felt the strongest. "
"Victim or Perpetrator? Analysis of Violent Characters Portrayals from
  Movie Scripts","Violent content in the media can influence viewers' perception of the
society. For example, frequent depictions of certain demographics as victims or
perpetrators of violence can shape stereotyped attitudes. We propose that
computational methods can aid in the large-scale analysis of violence in
movies. The method we develop characterizes aspects of violent content solely
from the language used in the scripts. Thus, our method is applicable to a
movie in the earlier stages of content creation even before it is produced.
This is complementary to previous works which rely on audio or video post
production. In this work, we identify stereotypes in character roles (i.e.,
victim, perpetrator and narrator) based on the demographics of the actor casted
for that role. Our results highlight two significant differences in the
frequency of portrayals as well as the demographics of the interaction between
victims and perpetrators : (1) female characters appear more often as victims,
and (2) perpetrators are more likely to be White if the victim is Black or
Latino. To date, we are the first to show that language used in movie scripts
is a strong indicator of violent content, and that there are systematic
portrayals of certain demographics as victims and perpetrators in a large
dataset. This offers novel computational tools to assist in creating awareness
of representations in storytelling","['Victor R Martinez', 'Krishna Somandepalli', 'Karan Singla', 'Anil Ramanakrishna', 'Yalda T. Uhls', 'Shrikanth Narayanan']",[],0,arXiv,http://arxiv.org/abs/2008.08225v2,False,True,False,False,False,2215,Joshua Clapp,Colorado,Active,2019,,"The economic and social consequences of violent victimization for individuals are not well understood. As a major and deleterious life event often effecting one's mental, physical and emotional well-being, it is likely that victimization either directly (i.e. through missed work due to health complications) or indirectly (i.e. resulting from psychological trauma that negatively impacts living conditions) disrupts economic stability. We propose to exploit the panel nature of the NCVS data to create victimization histories for respondents over a 3 year window, which we can then use to assess how changes in employment, earnings, housing, and marital status are related to instances of criminal victimization, after adjusting for demographic and economic characteristics of their communities. Results will shed light on the previously-ignored socioeconomic burden of victimization for individuals, as well as for which types of people these negative impacts are felt the strongest. "
Resurgence analysis of the Adler function at order $1/N_f^2$,"We compute non-perturbative contributions to the Adler function, the
derivative of the vacuum polarization function in gauge theory, using
resurgence methods and Borel-summed gauge field propagators. At 2-loop, to
order $1/N_f$, we construct the full 2-parameter transseries and perform the
sum over the non-perturbative sectors. We then introduce a convolution-based
method to derive the transseries structure of product series, which can also be
used to study higher orders in the expansion in $1/N_f$. We compute 3-loop
planar diagrams, at order $1/N_f^2$, and for each diagram study the asymptotic
behavior and resulting non-perturbative information in the transseries. A
structure emerges that, from a resurgence point of view, is quite different
from toy models hitherto studied. We study in particular the first and second
non-perturbative sectors, their relation to UV and IR renormalons, and how
their presence influences the perturbative expansions in neighbouring sectors.
Finally, finding that many non-perturbative sectors have asymptotic series, we
derive relations among all of them, thus providing an interesting new
perspective on the alien lattice for the Adler function.","['Eric Laenen', 'Coenraad Marinissen', 'Marcel Vonk']",[],0,arXiv,http://arxiv.org/abs/2302.13715v2,False,True,False,False,False,2222,Coenraad Pinkse,Penn State,Active,2021,,"We will link Census data on production and input use to external data on the prices of inputs and outputs and show how the linked data can be used to address common problems in productivity measurement.  Specifically, supply side techniques for measuring output market power will be extended to identify the degree of monopsony power in input markets (markdowns) simultaneously with output market markups, and a model of the optimal utilization of a fixed factor will be developed and estimated.  In each case, we will assess the bias in manufacturer productivity growth estimates that is associated with failure to consider a relevant factor: the need for physical quantities rather than revenues and expenditures, the existence of monopsony power in input markets, and the fact that capital is not always fully utilized."
On margin-based generalization prediction in deep neural networks,"Understanding generalization in deep neural networks is an active area of
research. A promising avenue of exploration has been that of margin
measurements: the shortest distance to the decision boundary for a given sample
or that sample's representation internal to the network. Margin-based
complexity measures have been shown to be correlated with the generalization
ability of deep neural networks in some circumstances but not others. The
reasons behind the success or failure of these metrics are currently unclear.
In this study, we examine margin-based generalization prediction methods in
different settings. We motivate why these metrics sometimes fail to accurately
predict generalization and how they can be improved.
  First, we analyze the relationship between margins measured in the input
space and sample noise. We find that different types of sample noise can have a
very different effect on the overall margin of a network that has modeled noisy
data.
  Following this, we empirically evaluate how robust margins measured at
different representational spaces are at predicting generalization. We find
that these metrics have several limitations and that a large margin does not
exhibit a strong correlation with empirical risk in many cases.
  Finally, we introduce a new margin-based measure that incorporates an
approximation of the underlying data manifold. It is empirically demonstrated
that this measure is generally more predictive of generalization than all other
margin-based measures. Furthermore, we find that this measurement also
outperforms other contemporary complexity measures on a well-known
generalization prediction benchmark. In addition, we analyze the utility and
limitations of this approach and find that this metric is well aligned with
intuitions expressed in prior work.",['Coenraad Mouton'],[],0,arXiv,http://arxiv.org/abs/2405.17445v1,False,True,False,False,False,2222,Coenraad Pinkse,Penn State,Active,2021,,"We will link Census data on production and input use to external data on the prices of inputs and outputs and show how the linked data can be used to address common problems in productivity measurement.  Specifically, supply side techniques for measuring output market power will be extended to identify the degree of monopsony power in input markets (markdowns) simultaneously with output market markups, and a model of the optimal utilization of a fixed factor will be developed and estimated.  In each case, we will assess the bias in manufacturer productivity growth estimates that is associated with failure to consider a relevant factor: the need for physical quantities rather than revenues and expenditures, the existence of monopsony power in input markets, and the fact that capital is not always fully utilized."
Sorting with Teams,"We fully solve a sorting problem with heterogeneous firms and multiple
heterogeneous workers whose skills are imperfect substitutes. We show that
optimal sorting, which we call mixed and countermonotonic, is comprised of two
regions. In the first region, mediocre firms sort with mediocre workers and
coworkers such that the output losses are equal across all these teams
(mixing). In the second region, a high skill worker sorts with low skill
coworkers and a high productivity firm (countermonotonicity). We characterize
the equilibrium wages and firm values. Quantitatively, our model can generate
the dispersion of earnings within and across US firms.","['Job Boerma', 'Aleh Tsyvinski', 'Alexander P. Zimin']",[],0,arXiv,http://arxiv.org/abs/2109.02730v3,False,True,False,False,False,2226,Michael A Zabek,Michigan,Completed,2020,2022.0,"This project will study the macroeconomic implications of worker quality and firm heterogeneity. Many macroeconomic phenomena--ranging from trends in productivity to relative declines in workers' earnings--are commonly examined through the lenses of models with either representative workers or representative firms. Understanding how deviations from these assumptions change our understanding of macroeconomic phenomena poses both a theoretical
challenge and, perhaps especially, an empirical challenge. Most commonly, researchers do not simultaneously observe detailed data both on firms and on the individual workers within those firms. By using combined employer and employee data from the U.S. Census Bureau, we will enhance understanding of how worker quality and firm heterogeneity interact in equilibrium to generate observed macroeconomic outcomes."
Integer Subspace Differential Privacy,"We propose new differential privacy solutions for when external
\emph{invariants} and \emph{integer} constraints are simultaneously enforced on
the data product. These requirements arise in real world applications of
private data curation, including the public release of the 2020 U.S. Decennial
Census. They pose a great challenge to the production of provably private data
products with adequate statistical usability. We propose \emph{integer subspace
differential privacy} to rigorously articulate the privacy guarantee when data
products maintain both the invariants and integer characteristics, and
demonstrate the composition and post-processing properties of our proposal. To
address the challenge of sampling from a potentially highly restricted discrete
space, we devise a pair of unbiased additive mechanisms, the generalized
Laplace and the generalized Gaussian mechanisms, by solving the Diophantine
equations as defined by the constraints. The proposed mechanisms have good
accuracy, with errors exhibiting sub-exponential and sub-Gaussian tail
probabilities respectively. To implement our proposal, we design an MCMC
algorithm and supply empirical convergence assessment using estimated upper
bounds on the total variation distance via $L$-lag coupling. We demonstrate the
efficacy of our proposal with applications to a synthetic problem with
intersecting invariants, a sensitive contingency table with known margins, and
the 2010 Census county-level demonstration data with mandated fixed state
population totals.","['Prathamesh Dharangutte', 'Jie Gao', 'Ruobin Gong', 'Fang-Yi Yu']",[],0,arXiv,http://arxiv.org/abs/2212.00936v1,False,True,False,False,False,2228,Jonathan Schroeder,Minnesota,Active,2019,,"We propose to address two major limitations of existing Census Bureau data products. First, the restricted-access 1990 and 2000 census microdata lack precise geographic coordinates for the residences of respondents. This limitation prevents Census analysts and others from conducting high-precision spatial analysis and modeling with 1990 and 2000 data. The second major limitation to be addressed is the problem of inconsistencies across time in the summarized geographic units in public census summary data. Where the extents of geographic units change between censuses, it can be impossible to determine how the units' populations have changed. To address this problem, several secondary data providers have produced geographically standardized census data series, which give estimates of population and housing characteristics across multiple census years for a single, fixed set of geographic units. The rapidly growing use of these products is concerning because errors in standardized data are potentially severe, but the real magnitude of errors remains undetermined. 
We would use restricted census microdata to address these issues in three ways. First, we will geocode all 1990 and 2000 census responses as nearly as possible to the standards of the current Master Address File. This will dramatically increase the quality and usefulness of restricted-access census microdata for these years. Second, for every 1990 and 2000 census response, we will identify the 2010 and 2020 census block of residence, enabling the production of high-accuracy geographically standardized time series within restricted environments. Third, we will summarize the accuracy of public sources of geographically standardized census tract data, establishing a foundational guideline on the fitness for use of publicly available standardized data products and demonstrating the relative merits of the different standardization approaches each product uses."
"Small domain estimation of census coverage: A case study in Bayesian
  analysis of complex survey data","Many countries conduct a full census survey to report official population
statistics. As no census survey ever achieves 100 per cent response rate, a
post-enumeration survey (PES) is usually conducted and analysed to assess
census coverage and produce official population estimates by geographic area
and demographic attributes. Considering the usually small size of PES, direct
estimation at the desired level of disaggregation is not feasible. Design-based
estimation with sampling weight adjustment is a commonly used method but is
difficult to implement when survey non-response patterns cannot be fully
documented and population benchmarks are not available. We overcome these
limitations with a fully model-based Bayesian approach applied to the New
Zealand PES. Although theory for the Bayesian treatment of complex surveys has
been described, published applications of individual level Bayesian models for
complex survey data remain scarce. We provide such an application through a
case study of the 2018 census and PES surveys. We implement a multilevel model
that accounts for the complex design of PES. We then illustrate how mixed
posterior predictive checking and cross-validation can assist with model
building and model selection. Finally, we discuss potential methodological
improvements to the model and potential solutions to mitigate dependence
between the two surveys.","['Joane S. Elleouet', 'Patrick Graham', 'Nikolai Kondratev', 'Abby K. Morgan', 'Rebecca M. Green']",[],0,arXiv,http://arxiv.org/abs/2205.12769v3,False,True,False,False,False,2228,Jonathan Schroeder,Minnesota,Active,2019,,"We propose to address two major limitations of existing Census Bureau data products. First, the restricted-access 1990 and 2000 census microdata lack precise geographic coordinates for the residences of respondents. This limitation prevents Census analysts and others from conducting high-precision spatial analysis and modeling with 1990 and 2000 data. The second major limitation to be addressed is the problem of inconsistencies across time in the summarized geographic units in public census summary data. Where the extents of geographic units change between censuses, it can be impossible to determine how the units' populations have changed. To address this problem, several secondary data providers have produced geographically standardized census data series, which give estimates of population and housing characteristics across multiple census years for a single, fixed set of geographic units. The rapidly growing use of these products is concerning because errors in standardized data are potentially severe, but the real magnitude of errors remains undetermined. 
We would use restricted census microdata to address these issues in three ways. First, we will geocode all 1990 and 2000 census responses as nearly as possible to the standards of the current Master Address File. This will dramatically increase the quality and usefulness of restricted-access census microdata for these years. Second, for every 1990 and 2000 census response, we will identify the 2010 and 2020 census block of residence, enabling the production of high-accuracy geographically standardized time series within restricted environments. Third, we will summarize the accuracy of public sources of geographically standardized census tract data, establishing a foundational guideline on the fitness for use of publicly available standardized data products and demonstrating the relative merits of the different standardization approaches each product uses."
Estimating Local Daytime Population Density from Census and Payroll Data,"Daytime population density reflects where people commute and spend their
waking hours. It carries significant weight as urban planners and engineers
site transportation infrastructure and utilities, plan for disaster recovery,
and assess urban vitality. Various methods with various drawbacks exist to
estimate daytime population density across a metropolitan area, such as using
census data, travel diaries, GPS traces, or publicly available payroll data.
This study estimates the San Francisco Bay Area's tract-level daytime
population density from US Census and LEHD LODES data. Estimated daytime
densities are substantially more concentrated than corresponding nighttime
population densities, reflecting regional land use patterns. We conclude with a
discussion of biases, limitations, and implications of this methodology.",['Geoff Boeing'],[],0,arXiv,http://arxiv.org/abs/1806.00665v1,False,True,False,False,False,2228,Jonathan Schroeder,Minnesota,Active,2019,,"We propose to address two major limitations of existing Census Bureau data products. First, the restricted-access 1990 and 2000 census microdata lack precise geographic coordinates for the residences of respondents. This limitation prevents Census analysts and others from conducting high-precision spatial analysis and modeling with 1990 and 2000 data. The second major limitation to be addressed is the problem of inconsistencies across time in the summarized geographic units in public census summary data. Where the extents of geographic units change between censuses, it can be impossible to determine how the units' populations have changed. To address this problem, several secondary data providers have produced geographically standardized census data series, which give estimates of population and housing characteristics across multiple census years for a single, fixed set of geographic units. The rapidly growing use of these products is concerning because errors in standardized data are potentially severe, but the real magnitude of errors remains undetermined. 
We would use restricted census microdata to address these issues in three ways. First, we will geocode all 1990 and 2000 census responses as nearly as possible to the standards of the current Master Address File. This will dramatically increase the quality and usefulness of restricted-access census microdata for these years. Second, for every 1990 and 2000 census response, we will identify the 2010 and 2020 census block of residence, enabling the production of high-accuracy geographically standardized time series within restricted environments. Third, we will summarize the accuracy of public sources of geographically standardized census tract data, establishing a foundational guideline on the fitness for use of publicly available standardized data products and demonstrating the relative merits of the different standardization approaches each product uses."
The Distributional Effects of Economic Uncertainty,"We study the distributional implications of uncertainty shocks by developing
a model that links macroeconomic aggregates to the US distribution of earnings
and consumption. We find that: initially, the fraction of low-earning workers
decreases, while the share of households reporting low consumption increases;
at longer horizons, the fraction of low-income workers increases, but the
consumption distribution reverts to its pre-shock shape. While the first phase
reduces income inequality and increases consumption inequality, in the second
stage income inequality rises, while the effects on consumption inequality
dissipate. Finally, we introduce Functional Local Projections and show that
they yield similar results.","['Florian Huber', 'Massimiliano Marcellino', 'Tommaso Tornese']",[],0,arXiv,http://arxiv.org/abs/2411.12655v1,False,True,False,False,False,2229,Jiashuo Feng,Utah,Active,2022,,"This project seeks to characterize the impact of trade on consumption, earnings and inequality in the United States. On consumption, we aim to use detailed micro-data to measure which consumers buy imported products or domestic products that use imported intermediate inputs, by building a new bridge between the Census data and product-level consumption data from the Nielsen Company. On earnings, we propose to comprehensively measure the exposure of U.S. workers to various facets of international trade, namely import competition, export opportunities, and the use of imported intermediate inputs using the Census' surveys of firms and customs data. On inequality, we use a model to infer the distributional consequences of trade via consumption and earnings from the empirical patterns observed in the data. The project relies on several Census datasets: SSL and LBD to match external consumption data to firm microdata; Economic Census (for both manufacturing and other sectors), Annual Surveys, and LFTTD to measure firms' exposure to international trade; LEHD to measure worker exposure to trade; and finally CFS, SIRD, and BRDIS to understand the role of geography of sales and innovation in how U.S. consumers benefit from trade."
LFS-GAN: Lifelong Few-Shot Image Generation,"We address a challenging lifelong few-shot image generation task for the
first time. In this situation, a generative model learns a sequence of tasks
using only a few samples per task. Consequently, the learned model encounters
both catastrophic forgetting and overfitting problems at a time. Existing
studies on lifelong GANs have proposed modulation-based methods to prevent
catastrophic forgetting. However, they require considerable additional
parameters and cannot generate high-fidelity and diverse images from limited
data. On the other hand, the existing few-shot GANs suffer from severe
catastrophic forgetting when learning multiple tasks. To alleviate these
issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can
generate high-quality and diverse images in lifelong few-shot image generation
task. Our proposed framework learns each task using an efficient task-specific
modulator - Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and
has a rich representation ability due to its unique reconstruction technique.
Furthermore, we propose a novel mode seeking loss to improve the diversity of
our model in low-data circumstances. Extensive experiments demonstrate that the
proposed LFS-GAN can generate high-fidelity and diverse images without any
forgetting and mode collapse in various domains, achieving state-of-the-art in
lifelong few-shot image generation task. Surprisingly, we find that our LFS-GAN
even outperforms the existing few-shot GANs in the few-shot image generation
task. The code is available at Github.","['Juwon Seo', 'Ji-Su Kang', 'Gyeong-Moon Park']",[],0,arXiv,http://arxiv.org/abs/2308.11917v2,False,True,False,False,False,2272,Seth Neller,Atlanta,Active,2020,,"In the last twenty years, a large and growing multidisciplinary body of literature has established that the conditions to which a child is exposed in early life and even in utero have immediate and often lifelong consequences (Almond et al, 2017). Despite the rapid progress of this literature, several major questions about human capital development remain unanswered (Currie & Almond, 2011). Perhaps most importantly: can economic and health interventions mitigate the damages from harmful early-life shocks? This project attempts to answer this question by examining long-term outcomes of children whose families were exposed to one of three early-life shocks in the early- or mid-20th Century. The three shocks affect children's health, healthcare access, and finally the interaction between wealth and environmental hazards. To evaluate their impacts, we will utilize records from the Decennial Censuses, the American Community Survey, the Current Population Survey, the American Housing Survey, and the Social Security NUMIDENT database. The Census products provide outcome and control variables, while the Social Security files provide detailed information on date and place of birth, which is used to identify treated individuals. The estimation strategy is a difference-in-differences approach, and the resulting analyses will show when early-life shocks matter most and how they potentially interact."
"Lifelong Vehicle Trajectory Prediction Framework Based on Generative
  Replay","Accurate trajectory prediction of vehicles is essential for reliable
autonomous driving. To maintain consistent performance as a vehicle driving
around different cities, it is crucial to adapt to changing traffic
circumstances and achieve lifelong trajectory prediction model. To realize it,
catastrophic forgetting is a main problem to be addressed. In this paper, a
divergence measurement method based on conditional Kullback-Leibler divergence
is proposed first to evaluate spatiotemporal dependency difference among varied
driving circumstances. Then based on generative replay, a novel lifelong
vehicle trajectory prediction framework is developed. The framework consists of
a conditional generation model and a vehicle trajectory prediction model. The
conditional generation model is a generative adversarial network conditioned on
position configuration of vehicles. After learning and merging trajectory
distribution of vehicles across different cities, the generation model replays
trajectories with prior samplings as inputs, which alleviates catastrophic
forgetting. The vehicle trajectory prediction model is trained by the replayed
trajectories and achieves consistent prediction performance on visited cities.
A lifelong experiment setup is established on four open datasets including five
tasks. Spatiotemporal dependency divergence is calculated for different tasks.
Even though these divergence, the proposed framework exhibits lifelong learning
ability and achieves consistent performance on all tasks.","['Peng Bao', 'Zonghai Chen', 'Jikai Wang', 'Deyun Dai', 'Hao Zhao']",[],0,arXiv,http://arxiv.org/abs/2111.07511v1,False,True,False,False,False,2272,Seth Neller,Atlanta,Active,2020,,"In the last twenty years, a large and growing multidisciplinary body of literature has established that the conditions to which a child is exposed in early life and even in utero have immediate and often lifelong consequences (Almond et al, 2017). Despite the rapid progress of this literature, several major questions about human capital development remain unanswered (Currie & Almond, 2011). Perhaps most importantly: can economic and health interventions mitigate the damages from harmful early-life shocks? This project attempts to answer this question by examining long-term outcomes of children whose families were exposed to one of three early-life shocks in the early- or mid-20th Century. The three shocks affect children's health, healthcare access, and finally the interaction between wealth and environmental hazards. To evaluate their impacts, we will utilize records from the Decennial Censuses, the American Community Survey, the Current Population Survey, the American Housing Survey, and the Social Security NUMIDENT database. The Census products provide outcome and control variables, while the Social Security files provide detailed information on date and place of birth, which is used to identify treated individuals. The estimation strategy is a difference-in-differences approach, and the resulting analyses will show when early-life shocks matter most and how they potentially interact."
Household Leverage Cycle Around the Great Recession,"This paper provides the first causal evidence that credit supply expansion
caused the 1999-2010 U.S. business cycle mainly through the channel of
household leverage (debt-to-income ratio). Specifically, induced by net export
growth, credit expansion in private-label mortgages, rather than
government-sponsored enterprise mortgages, causes a much stronger boom and bust
cycle in household leverage in the high net-export-growth areas. In addition,
such a stronger household leverage cycle creates a stronger boom and bust cycle
in the local economy, including housing prices, residential construction
investment, and house-related employment. Thus, our results are consistent with
the credit-driven household demand channel (Mian and Sufi, 2018). Further, we
show multiple pieces of evidence against the corporate channel, which is
emphasized by other business cycle theories (hypotheses).",['Bo Li'],[],0,arXiv,http://arxiv.org/abs/2407.01539v1,False,True,False,False,False,2280,Jonathan Halket,Texas,Active,2021,,"This project aims to use household/house level data from Corelogic (and BlackKnight), the American Community Survey (ACS), the American Housing Survey (AHS) and the 2010 Decennial Census to understand the geography of house prices, housing returns and household borrowing across neighborhoods within metropolitan areas. The primary goal of this project is to estimate how market house values, rents and borrowing behavior vary across housing and neighborhood characteristics using a rich set of characteristics including its precise location. Using these estimates, the project will compare how measures of value across these datasets. The project will also estimate the effects of selection by tenure on the hedonic estimates."
"Identifying Risk Patterns in Brazilian Police Reports Preceding
  Femicides: A Long Short Term Memory (LSTM) Based Analysis","Femicide refers to the killing of a female victim, often perpetrated by an
intimate partner or family member, and is also associated with gender-based
violence. Studies have shown that there is a pattern of escalating violence
leading up to these killings, highlighting the potential for prevention if the
level of danger to the victim can be assessed. Machine learning offers a
promising approach to address this challenge by predicting risk levels based on
textual descriptions of the violence. In this study, we employed the Long Short
Term Memory (LSTM) technique to identify patterns of behavior in Brazilian
police reports preceding femicides. Our first objective was to classify the
content of these reports as indicating either a lower or higher risk of the
victim being murdered, achieving an accuracy of 66%. In the second approach, we
developed a model to predict the next action a victim might experience within a
sequence of patterned events. Both approaches contribute to the understanding
and assessment of the risks associated with domestic violence, providing
authorities with valuable insights to protect women and prevent situations from
escalating.","['Vinicius Lima', 'Jaque Almeida de Oliveira']",[],0,arXiv,http://arxiv.org/abs/2401.12980v1,False,True,False,False,False,2283,Sarah small,Colorado,Active,2019,,"The proposed study uses the National Crime Victimization Surveys (NCVS) to investigate intimate partner violence victims' use of healthcare facilities and state-specific correlates associated with intimate partner violence (IPV) rates. We seek to increase the utility of Census Bureau data by analyzing the effects of healthcare access, income, employment, family, race, gender, and state-level characteristics on IPV rates and victims' healthcare facility utilization. We are particularly interested in whether state differences in healthcare professionals' reporting requirements explain meaningful variation in victims' likelihood of seeking professional medical services and overall rates of IPV victimization and reporting. Some argue that mandatory reporting is beneficial in that it helps law enforcement officers identify abusers and forces abusers to face the seriousness of their actions (Coulter & Chez, 1997). Others say mandatory reporting discourages victims from seeking medical help and robs them of their autonomy for safely reporting abuse, often putting victims further in harm's way. Many scholars (Sullivan & Hagen, 2005; Rodriguez et al., 2001; Glass et al., 2001; Gielen et al., 2000) have qualitatively studied patients' and healthcare workers' perceptions of mandatory reporting policies and found that most patients who have experienced IPV oppose these regulations. However, to our knowledge, this would be the first study that quantitatively examines the actual impacts of mandatory physician reporting laws on healthcare utilization rates of IPV victims using a quasi-experimental framework. In light of this debate, our central research questions are: (1) Do healthcare providers' mandatory reporting requirements impact the use of healthcare services for victims of IPV? And (2), do states with mandatory reporting requirements have more or less IPV per capita? In answering these questions we will also examine how police presence, proximity to care providers, local economic climate, family size, race, and gender impact the likelihood of experiencing IPV and seeking healthcare after an IPV incident. To answer these questions, we will make use of the restricted NCVS to obtain greater geographic precision about the counties and states that respondents reside in. We will prepare estimates of victim's healthcare utilization rates and IPV rates by state in light of variance in mandatory reporting laws. This will shed light on the costs and benefits of mandatory reporting and will provide a clearer insight into factors which determine healthcare utilization by IPV victims and state-level differences in IPV rates. Through our analysis of the data, we will also document the distribution of missing data related to IPV and how this missing data is related to the method of data collection to help increase the utility of the data to the Census Bureau.
There may be some concern about victimization generally, and IPV especially, constituting a sensitive topic of research. This concern is fair, though the NCVS is designed specifically to generate population estimates of victimization prevalence and the associated characteristics of victimized individuals. The present research falls within this purpose, and therefore represents a fairly routine use of these data. While we do plan to use these estimates to determine how state-level policies on mandatory reporting influence IPV reporting and medical care utilization, we are not assessing the impacts of any national policy issues or particularly current, sensitive political topics."
"Assessment of electrical and infrastructure recovery in Puerto Rico
  following hurricane Maria using a multisource time series of satellite
  imagery","Puerto Rico suffered severe damage from the category 5 hurricane (Maria) in
September 2017. Total monetary damages are estimated to be ~92 billion USD, the
third most costly tropical cyclone in US history. The response to this damage
has been tempered and slow moving, with recent estimates placing 45% of the
population without power three months after the storm. Consequently, we
developed a unique data-fusion mapping approach called the Urban Development
Index (UDI) and new open source tool, Comet Time Series (CometTS), to analyze
the recovery of electricity and infrastructure in Puerto Rico. Our approach
incorporates a combination of time series visualizations and change detection
mapping to create depictions of power or infrastructure loss. It also provides
a unique independent assessment of areas that are still struggling to recover.
For this workflow, our time series approach combines nighttime imagery from the
Suomi National Polar-orbiting Partnership Visible Infrared Imaging Radiometer
Suite (NPP VIIRS), multispectral imagery from two Landsat satellites, US Census
data, and crowd-sourced building footprint labels. Based upon our approach we
can identify and evaluate: 1) the recovery of electrical power compared to
pre-storm levels, 2) the location of potentially damaged infrastructure that
has yet to recover from the storm, and 3) the number of persons without power
over time. As of May 31, 2018, declined levels of observed brightness across
the island indicate that 13.9% +/- ~5.6% of persons still lack power and/or
that 13.2% +/- ~5.3% of infrastructure has been lost. In comparison, the Puerto
Rico Electric Power Authority states that less than 1% of their customers still
are without power.",['Jacob Shermeyer'],[],0,arXiv,http://arxiv.org/abs/1807.05854v1,False,True,False,False,False,2285,Elizabeth Fussell,Boston,Active,2020,,"Recent economic and weather shocks have highlighted a critical need to understand change in Puerto Rico's population. Our research will investigate the hypothesis that two major events - the economic crisis and the 2017 hurricanes - changed Puerto Rico residents' fertility and migration calculus, while affecting mortality in the short-term as well as over a multiple year period. In particular, we expect that the economic crisis and uncertainty after the 2017 hurricane season suppressed fertility, increased mortality in the short term and possible lowered mortality over a longer period, and increased out-migration.
We have three research aims. First, we will describe period and cohort trends in fertility, mortality and migration between 2000 and the latest year for which data are available. These components of demographic change are important for understanding how the sociodemographic characteristics of the population change, both through direct effects (birth, death, out-migration, in-migration) and indirect effects (changes in the population at risk of birth, death, and migration). Second, we will describe the residential locations of Puerto Ricans in Puerto Rico or other U.S. states and territories in each year between 2000 and the latest year for which data are available. Third, we will compare demographic, social, geographic, and economic statuses for out-migrants and comparable non-migrants in Puerto Rico to investigate how residential mobility to the US mainland is associated with mortality, fertility, health, household income, poverty, and residential outcomes."
"A Bayesian Projection of the Total Fertility Rate of Puerto Rico:
  2020-2050","The abrupt decline in the Total Fertility Rate (TFR) of Puerto Rico since
2000 makes the prospect of a sustained population decline a real possibility.
From 2000 to 2021 the TFR declined from 2.1 to 0.9 children per woman, one of
the lowest in the world. Population projections produced by the United States
Census Bureau and the United Nations Population Division show that the island
population may decline from 3.8 millions in 2000 to slightly above 2 million by
2050, a dramatic 47% population decline in 50 years. As dire as this prospect
may be, this may be an optimistic scenario. Both projections have the TFR
increasing to 1.5 by 2050, but a fertility projection conducted by us show that
fertility can remain much closer to 1.0 until 2050. Bayesian Hierarchical
Probabilistic Theory has been used by the United Nations to incorporate a way
to measure the uncertainty and to estimate the projection parameters. However,
the assumption that the fertility level in countries with low fertility will
eventually increase to 2.1 has been widely criticized as unrealistic and not
supported by evidence. We modified the assumptions used by the United Nations
considering countries with TFR similar to Puerto Rico and find that by 2050
Puerto Rico may have a TFR of 1.1 bounded by a 95% credibility interval
(0.56,1.77). This indicates that there may be a larger population decline than
what current projections show.","['Angélica Rosario Santos', 'Luis Pericchi Guerra', 'Hernando Mattei']",[],0,arXiv,http://arxiv.org/abs/2308.14168v1,True,True,False,False,False,2285,Elizabeth Fussell,Boston,Active,2020,,"Recent economic and weather shocks have highlighted a critical need to understand change in Puerto Rico's population. Our research will investigate the hypothesis that two major events - the economic crisis and the 2017 hurricanes - changed Puerto Rico residents' fertility and migration calculus, while affecting mortality in the short-term as well as over a multiple year period. In particular, we expect that the economic crisis and uncertainty after the 2017 hurricane season suppressed fertility, increased mortality in the short term and possible lowered mortality over a longer period, and increased out-migration.
We have three research aims. First, we will describe period and cohort trends in fertility, mortality and migration between 2000 and the latest year for which data are available. These components of demographic change are important for understanding how the sociodemographic characteristics of the population change, both through direct effects (birth, death, out-migration, in-migration) and indirect effects (changes in the population at risk of birth, death, and migration). Second, we will describe the residential locations of Puerto Ricans in Puerto Rico or other U.S. states and territories in each year between 2000 and the latest year for which data are available. Third, we will compare demographic, social, geographic, and economic statuses for out-migrants and comparable non-migrants in Puerto Rico to investigate how residential mobility to the US mainland is associated with mortality, fertility, health, household income, poverty, and residential outcomes."
Behavior of Liquidity Providers in Decentralized Exchanges,"Decentralized exchanges (DEXes) have introduced an innovative trading
mechanism, where it is not necessary to match buy-orders and sell-orders to
execute a trade. DEXes execute each trade individually, and the exchange rate
is automatically determined by the ratio of assets reserved in the market.
Therefore, apart from trading, financial players can also liquidity providers,
benefiting from transaction fees from trades executed in DEXes. Although
liquidity providers are essential for the functionality of DEXes, it is not
clear how liquidity providers behave in such markets. In this paper, we aim to
understand how liquidity providers react to market information and how they
benefit from providing liquidity in DEXes. We measure the operations of
liquidity providers on Uniswap and analyze how they determine their investment
strategy based on market changes. We also reveal their returns and risks of
investments in different trading pair categories, i.e., stable pairs, normal
pairs, and exotic pairs. Further, we investigate the movement of liquidity
between trading pools. To the best of our knowledge, this is the first work
that systematically studies the behavior of liquidity providers in DEXes.","['Lioba Heimbach', 'Ye Wang', 'Roger Wattenhofer']",[],0,arXiv,http://arxiv.org/abs/2105.13822v2,False,True,False,False,False,2299,Ivo Welch,UCLA,Active,2020,,"In the last few decades, globalization has advanced at an unprecedented pace and scope, and international trade has experienced tremendous growth. At the same time, nominal and real exchange rate volatilities are large. The sensitivity of international trade to US dollar currency changes and whether the financial market understands the sensitivity are important and still-unresolved questions. In this project, we use the Longitudinal Foreign Trade Transactions Database (LFTTD) --which contains detailed transaction-level data for all importing/exporting US firms--to assess, 1.  heterogeneity in trade prices, volumes, and revenues of US exporters and importers in response to exchange rate movements and 2. whether financial markets understand this sensitivity. 
Briefly speaking, we will use LFTTD to obtain export and import volume and unit value information for each transaction. We use import/export dates in the LFTTD to match transactions to exchange rate movements from the IMF Exchange Rates data, using a weighted average of the exchange rates firms experience. To recover firm-level characteristics, we link transactions to US firms in the Longitudinal Business Database (LDB) and via the Standard Statistical Establishment List/Business Register (SSEL) and the COMPUSTAT-SSEL Bridge (CSB) to the CRSP/COMPUSTAT merged database. We use data from the Penn World tables and IMF's International Financial Statistics to account for macroeconomic characteristics of US firms' foreign trading partners.  "
"The Transmission of US Monetary Policy Shocks: The Role of Investment &
  Financial Heterogeneity","This paper studies the transmission of US monetary policy shocks into
Emerging Markets emphasizing the role of investment and financial
heterogeneity. First, we use a panel SVAR model to show that a US interest
tightening leads to a persistent recession in Emerging Markets driven by a
sharp reduction in aggregate investment. Second, we study the role of firms'
financial heterogeneity in the transmission of US interest rate shocks by
exploiting detailed balance sheet dataset from Chile. We find that more
indebted firms experience greater drops in investment in response to a US
tightening shock than less indebted firms. This result is at odds with recent
evidence from US firms, even when using the same identification strategy and
econometric methods. Third, we rationalize this finding using a stylized model
of heterogeneous firms subject to a tightening leverage constraint. Finally, we
present evidence in support of this hypothesis as well as robustness checks to
our main results. Overall, our results suggests that the transmission channel
of US monetary policy shocks within and outside the US differ, a result novel
to the literature.","['Santiago Camara', 'Sebastian Ramirez Venegas']",[],0,arXiv,http://arxiv.org/abs/2209.11150v1,False,True,False,False,False,2306,Gideon Bornstein,Philadelphia,Active,2021,,"This projects investigates how the transmission of monetary policy to wages and employment depends on the size distribution of firms in the economy. We first study the firm-level wage and employment response to identified high-frequency monetary policy shocks. This allows us to examine whether these responses vary systematically along three dimensions: (i) the size of firms, (ii) the national market share of firms, and (iii) the market structure of the location in which the firm operates. To rationalize the findings, we construct a New-Keynesian model with heterogeneous firms. The model is estimated via indirect inference, targeting the estimates of the reduced form regressions. Finally, we use the model to assess how the observed shift in the US firm size distribution during the past three decades impacted the transmission of monetary policy."
"Time-Series Classification in Smart Manufacturing Systems: An
  Experimental Evaluation of State-of-the-Art Machine Learning Algorithms","Manufacturing is gathering extensive amounts of diverse data, thanks to the
growing number of sensors and rapid advances in sensing technologies. Among the
various data types available in SMS settings, time-series data plays a pivotal
role. Hence, TSC emerges is crucial in this domain. The objective of this study
is to fill this gap by providing a rigorous experimental evaluation of the SoTA
ML and DL algorithms for TSC tasks in manufacturing and industrial settings. We
first explored and compiled a comprehensive list of more than 92 SoTA
algorithms from both TSC and manufacturing literature. Following, we selected
the 36 most representative algorithms from this list. To evaluate their
performance across various manufacturing classification tasks, we curated a set
of 22 manufacturing datasets, representative of different characteristics that
cover diverse manufacturing problems. Subsequently, we implemented and
evaluated the algorithms on the manufacturing benchmark datasets, and analyzed
the results for each dataset. Based on the results, ResNet, DrCIF,
InceptionTime, and ARSENAL are the top-performing algorithms, boasting an
average accuracy of over 96.6% across all 22 manufacturing TSC datasets. These
findings underscore the robustness, efficiency, scalability, and effectiveness
of convolutional kernels in capturing temporal features in time-series data, as
three out of the top four performing algorithms leverage these kernels for
feature extraction. Additionally, LSTM, BiLSTM, and TS-LSTM algorithms deserve
recognition for their effectiveness in capturing features within time-series
data using RNN-based structures.","['Mojtaba A. Farahani', 'M. R. McCormick', 'Ramy Harik', 'Thorsten Wuest']",[],0,arXiv,http://arxiv.org/abs/2310.02812v2,False,True,False,False,False,2309,Kendra R Marcoux,Berkeley,Active,2019,,"This project aims to quantify the extent to which initial market conditions determine manufacturing establishments' subsequent input mix. We focus on the role of initial energy prices for short and long-run energy usage as an example of this technology lock-in. We propose to conduct this analysis using data from the Annual Survey of Manufactures, the Census of Manufactures, the Longitudinal Business Database, the Manufacturing Energy Consumption Survey, the Annual Capital Expenditure Survey, and the Survey of Pollution Abatement Costs & Expenditures from the Census Bureau, in combination with several external data sources on U.S. energy prices. First, we will use these data sets to provide new descriptive evidence on the importance of energy prices in the year of an establishment's entry for subsequent energy efficiency, summarizing the effects by entry cohort and by initial energy price. Second, we will estimate a dynamic structural model of entry, exit, and capital investment to simulate how the energy intensity of production would change for entrant and incumbent establishments under several counterfactual market conditions. Our goal is to use the structural model to study how raising energy prices and reducing capital adjustment costs affect the investment decisions and the input mix of U.S. manufacturing establishments. "
"The TWA 3 Young Triple System: Orbits, Disks, Evolution","We have characterized the spectroscopic orbit of the TWA 3A binary and
provide preliminary families of probable solutions for the TWA 3A visual orbit
as well as for the wide TWA 3A--B orbit. TWA 3 is a hierarchical triple located
at 34 pc in the $\sim$10 Myr old TW Hya association. The wide component
separation is 1.""55; the close pair was first identified as a possible binary
almost 20 years ago. We initially identified the 35-day period orbital solution
using high-resolution infrared spectroscopy which angularly resolved the A and
B components. We then refined the preliminary orbit by combining the infrared
data with a re-analysis of our high-resolution optical spectroscopy. The
orbital period from the combined spectroscopic solution is $\sim$35 days, the
eccentricity is $\sim$0.63, and the mass ratio is $\sim$0.84; although this
high mass ratio would suggest that optical spectroscopy alone should be
sufficient to identify the orbital solution, the presence of the tertiary B
component likely introduced confusion in the blended optical spectra. Using
millimeter imaging from the literature, we also estimate the inclinations of
the stellar orbital planes with respect to the TWA 3A circumbinary disk
inclination and find that all three planes are likely misaligned by at least
$\sim$30 degrees. The TWA 3A spectroscopic binary components have spectral
types of M4.0 and M4.5; TWA 3B is an M3. We speculate that the system formed as
a triple, is bound, and that its properties were shaped by dynamical
interactions between the inclined orbits and disk.","['Kendra Kellogg', 'L. Prato', 'Guillermo Torres', 'G. H. Schaefer', 'I. Avilez', 'D. Ruíz-Rodríguez', 'L. H. Wasserman', 'Alceste Z. Bonanos', 'E. W. Guenther', 'R. Neuhäuser', 'S. E. Levine', 'A. S. Bosh', 'Katie M. Morzinski', 'Laird Close', 'Vanessa Bailey', 'Phil Hinz', 'Jared R. Males']",[],0,arXiv,http://arxiv.org/abs/1707.00591v1,False,True,False,False,False,2309,Kendra R Marcoux,Berkeley,Active,2019,,"This project aims to quantify the extent to which initial market conditions determine manufacturing establishments' subsequent input mix. We focus on the role of initial energy prices for short and long-run energy usage as an example of this technology lock-in. We propose to conduct this analysis using data from the Annual Survey of Manufactures, the Census of Manufactures, the Longitudinal Business Database, the Manufacturing Energy Consumption Survey, the Annual Capital Expenditure Survey, and the Survey of Pollution Abatement Costs & Expenditures from the Census Bureau, in combination with several external data sources on U.S. energy prices. First, we will use these data sets to provide new descriptive evidence on the importance of energy prices in the year of an establishment's entry for subsequent energy efficiency, summarizing the effects by entry cohort and by initial energy price. Second, we will estimate a dynamic structural model of entry, exit, and capital investment to simulate how the energy intensity of production would change for entrant and incumbent establishments under several counterfactual market conditions. Our goal is to use the structural model to study how raising energy prices and reducing capital adjustment costs affect the investment decisions and the input mix of U.S. manufacturing establishments. "
News Sentiment as a Predictor for American Domestic Migration,"This paper goes into depth on the effect that US News Sentiment from national
newspapers has on US interstate migration trends. Through harnessing data from
the New York Times between 2010 and 2020, an average sentiment score was
calculated, allowing for data to be entered into a neural network. Then a
logistic regression model was used to predict interstate migration. The results
indicate the model was highly accurate as the mean margin of error was +/- 900
citizens. The predictions from the model were compared with the US Census data
from 2010 to 2020 that was used to train the model. Since the input for the
model was not exposed to any migration data, the model clearly demonstrated
that its results were drawn from sentiment data alone. These findings are
significant as they indicate that the role of the press could be used as a
predictor for domestic migration which can help the government and businesses
understand better what is influencing people to move to certain places.","['Benjamin Lane', 'Simeon Sayer']",[],0,arXiv,http://arxiv.org/abs/2502.15998v1,False,True,False,False,False,2323,Dan Black,Cornell,Active,2020,,"Retirement allows older adults to make location choices free from labor market implications. Prior research explores how financial resources affect migration choices and suggests that one motivation in particular applies to wealthier Americans - estate taxes. Due to the elimination of a tax credit in 2005 that allowed a dollar for dollar reduction in an individual's federal tax liability in the amount of that individual's state-level estate tax, enormous effective variation now exists in estate tax rates across the states that levy estate taxes. This research will use microdata from the U.S. Census Bureau to compare long- versus short-term state-to-state migration flows for older Americans, including return or circular migration. In particular, the research will provide an increased understanding of how state-levied estate taxes affect these migration patterns."
"Disentangling individual-level from location-based income uncovers
  socioeconomic preferential mobility and impacts segregation estimates","Segregation encodes information about society, such as social cohesion,
mixing, and inequality. However, most past and current studies tackled
socioeconomic (SE) segregation by analyzing static aggregated mobility
networks, often without considering further individual features beyond income
and, most importantly, without distinguishing individual-level from
location-based income. Accessing individual-level income may help mapping
macroscopic behavior into more granular mobility patterns, hence impacting
segregation estimates. Here we combine a mobile phone dataset of daily mobility
flows across Spanish districts stratified and adjusted by age, gender and
income with census data of districts median income. We build mobility-based SE
assortativity matrices for multiple demographics and observe mobility patterns
of three income groups with respect to location-based SE classes. We find that
SE assortativity differs when isolating the mobility of specific income groups:
we observe that groups prefer to visit areas with higher average income than
their own, which we call preferential mobility. Our analysis suggests
substantial differences between weekdays and weekends SE assortativity by age
class, with weekends characterized by higher SE assortativity. Our modeling
approach shows that the radiation model, which typically performs best at
reproducing inter-municipal population mobility, best fits middle income and
middle-aged flows, while performing worse on young and low income groups. Our
double-sided approach, focusing on assortativity patterns and mobility
modeling, suggests that state of the art mobility models fail at capturing
preferential mobility behavior. Overall, our work indicates that mobility
models considering the interplay of SE preferential behavior, age and gender
gaps may sensibly improve the state of the art models performance.","['Marc Duran-Sala', 'Anandu Koikkalethu Balachandran', 'Marta Morandini', 'Timur Naushirvanov', 'Adarsh Prabhakaran', 'Andrew Renninger', 'Mattia Mazzoli']",[],0,arXiv,http://arxiv.org/abs/2407.01799v1,False,True,False,False,False,2334,Martin Ruef,Triangle,Completed,2020,2023.0,"This research will measure levels and distributions of income segregation in the U.S. from 1980 to 2010. Restricted-use Decennial Census data and American Community Survey (ACS) microdata will be used to a) derive estimates of income segregation based on actual income numbers (and less top-coding) than the categorical income data available in Public Use Microdata Samples (PUMS) and published tabulations; and b) make use of finer geographical detail than public versions of these datasets provide. Using household-level data to model changes in metropolitan area income segregation as a function of structural and sociodemographic characteristics will allow the estimation of tract-level income segregation without being forced to make assumptions about how tract-level income distributions. Structural characteristics of interest include mean income, and education levels. Supplementary analyses will evaluate how income segregation estimates based on exact incomes differ from estimates based on the categorical public Census data. The researchers will also evaluate how sampling variation affects income segregation estimates."
"Understanding Complex Patterns in Social, Geographic, and Economic
  Inequities in COVID-19 Mortality at the County Level in the US Using
  Generalized Additive Models","I present three types of applications of generalized additive models (GAMs)
to COVID-19 mortality rates in the US for the purpose of advancing methods to
document inequities with respect to which communities suffered disproportionate
COVID-19 mortality rates at specific times during the first three years of the
pandemic. First, GAMs can be used to describe the changing relationship between
COVID-19 mortality and county-level covariates (sociodemographic, economic, and
political metrics) over time. Second, GAMs can be used to perform
spatiotemporal smoothing that pools information over time and space to address
statistical instability due to small population counts or stochasticity
resulting in a smooth, dynamic latent risk surface summarizing the mortality
risk associated with geographic locations over time. Third, estimation of
COVID-19 mortality associations with county-level covariates conditional on a
smooth spatiotemporal risk surface allows for more rigorous consideration of
how socio-environmental contexts and policies may have impacted COVID-19
mortality. Each of these approaches provides a valuable perspective to
documenting inequities in COVID-19 mortality by addressing the question of
which populations have suffered the worst burden of COVID-19 mortality taking
into account the nonlinear spatial, temporal, and social patterning of disease.",['Christian Testa'],[],0,arXiv,http://arxiv.org/abs/2211.16629v1,False,True,False,False,False,2338,Jason Fletcher,Wisconsin,Active,2020,,"Sociodemographic predictors of mortality rates for the US population have been estimated in prior research.  We know that these rates vary by age, sex, socioeconomic status, race/ethnicity, and geography, among other factors (Cutler et al. 2006).  However, many questions have only been partially addressed because of data limitations.  For instance, causal effects of socioeconomic status on mortality remain unclear.  The association of many of these factors on death, by cause, is also limited because of the large sample size needs to assess relationships.  Therefore, we will ask several interrelated research questions to expand our knowledge of the sociodemographic predictors of US mortality. We will do this by utilizing the restricted version of the National Longitudinal Mortality Study. First, the analysis will examine trends in differences in old-age mortality outcomes based on state-of-birth.  Second, using the up-to-30 year follow of mortality outcomes, the project will examine risk of mortality by cause-of-death for broad groups of occupational categories.  Third, compulsory schooling laws that vary by state and year will be merged with state-of-birth and year-of-birth information in the NLMS in order to use an instrumental variables approach to estimate the causal effects of educational attainment on mortality outcomes.  "
"Reconstructing Native American Migrations from Whole-genome and
  Whole-exome Data","There is great scientific and popular interest in understanding the genetic
history of populations in the Americas. We wish to understand when different
regions of the continent were inhabited, where settlers came from, and how
current inhabitants relate genetically to earlier populations. Recent studies
unraveled parts of the genetic history of the continent using genotyping arrays
and uniparental markers. The 1000 Genomes Project provides a unique opportunity
for improving our understanding of population genetic history by providing over
a hundred sequenced low coverage genomes and exomes from Colombian (CLM),
Mexican-American (MXL), and Puerto Rican (PUR) populations. Here, we explore
the genomic contributions of African, European, and Native American ancestry to
these populations. Estimated Native American ancestry is 48% in MXL, 25% in
CLM, and 13% in PUR. Native American ancestry in PUR is most closely related to
populations surrounding the Orinoco River basin, confirming the Southern
America ancestry of the Ta\'ino people of the Caribbean. We present new methods
to estimate the allele frequencies in the Native American fraction of the
populations, and model their distribution using a demographic model for three
ancestral Native American populations. These ancestral populations likely split
in close succession: the most likely scenario, based on a peopling of the
Americas 16 thousand years ago (kya), supports that the MXL Ancestors split
12.2kya, with a subsequent split of the ancestors to CLM and PUR 11.7kya. The
model also features effective populations of 62,000 in Mexico, 8,700 in
Colombia, and 1,900 in Puerto Rico. Modeling Identity-by-descent and ancestry
tract length, we show that post-contact populations differ markedly in their
effective sizes and migration patterns, with Puerto Rico showing the smallest
effective size and the earlier migration from Europe.","['Simon Gravel', 'Fouad Zakharia', 'Andres Moreno-Estrada', 'Jake K Byrnes', 'Marina Muzzio', 'Juan L. Rodriguez-Flores', 'Eimear E. Kenny', 'Christopher R. Gignoux', 'Brian K. Maples', 'Wilfried Guiblet', 'Julie Dutil', 'Marc Via', 'Karla Sandoval', 'Gabriel Bedoya', 'Taras K Oleksyk', 'Andres Ruiz-Linares', 'Esteban G Burchard', 'Juan Carlos Martinez-Cruzado', 'Carlos D. Bustamante', 'The 1000 Genomes Project']",[],0,arXiv,http://arxiv.org/abs/1306.4021v2,False,True,False,False,False,2343,Carolyn A Liebler,Minnesota,Active,2020,,"The future size and composition of the American Indian and Alaska Native (AIAN) population is difficult to predict using the demographic balancing equation. Race response change (or ""mental migration"") and racial misclassification on death certificates creates the need for additional adjustments. First, I will use cohort component analysis, including adjustments for race response change and mortality misclassification, to develop national AIAN population projections. Linked data to calculate rates of race response change will include decennial censuses, the American Community Survey, and the Current Population Survey (2000 to the present). This project applies my prior work on mental migration to a practical issue that affects policy makers and planners. Because mental migration is so common among AIAN people, the possible future population is much larger than usually estimated. Second, I will use the Navajo Nation Adult Voter data (NNAV; user-provided data) linked with the other data to develop population projections of Navajo people specifically. Tribal nation-specific projections can be more useful than nationwide projections. I expect Navajo rates of race response change to be lower than the overall AIAN rates and projections to show accordingly smaller variation. Third, expanding on my prior work on tribal non-response with unlinked data, I will investigate non-response to the tribe question among Navajo people. A tribe-specific focus is only possible with linked data, and allows more detailed (and thus useful) analysis of the reasons people do not always respond to this seemingly-important question. "
"Using Machine Learning to Evaluate Real Estate Prices Using Location Big
  Data","With everyone trying to enter the real estate market nowadays, knowing the
proper valuations for residential and commercial properties has become crucial.
Past researchers have been known to utilize static real estate data (e.g.
number of beds, baths, square footage) or even a combination of real estate and
demographic information to predict property prices. In this investigation, we
attempted to improve upon past research. So we decided to explore a unique
approach: we wanted to determine if mobile location data could be used to
improve the predictive power of popular regression and tree-based models. To
prepare our data for our models, we processed the mobility data by attaching it
to individual properties from the real estate data that aggregated users within
500 meters of the property for each day of the week. We removed people that
lived within 500 meters of each property, so each property's aggregated
mobility data only contained non-resident census features. On top of these
dynamic census features, we also included static census features, including the
number of people in the area, the average proportion of people commuting, and
the number of residents in the area. Finally, we tested multiple models to
predict real estate prices. Our proposed model is two stacked random forest
modules combined using a ridge regression that uses the random forest outputs
as predictors. The first random forest model used static features only and the
second random forest model used dynamic features only. Comparing our models
with and without the dynamic mobile location features concludes the model with
dynamic mobile location features achieves 3/% percent lower mean squared error
than the same model but without dynamic mobile location features.","['Walter Coleman', 'Ben Johann', 'Nicholas Pasternak', 'Jaya Vellayan', 'Natasha Foutz', 'Heman Shakeri']",[],0,arXiv,http://arxiv.org/abs/2205.01180v1,False,True,False,False,False,2358,Ashvin Gandhi,UCLA,Active,2020,,"The affordability and accessibility of housing has generated renewed interest following sustained increases in house prices in the largest American cities. While local governments have responded through various policies, understanding of housing and labor markets at a localized level is still relatively superficial. This project will develop a rich understanding of many aspects of local housing and labor markets. Using data available through the Census RDC--in particular the LEHD--this project proposes to understand (1) how households decide where to live within a city (e.g. proximity to employers, other location-based amenities, physical characteristics of housing, and price) and (2) how real estate stock passes through different geographies and demographic groups (i.e. the ripple effects of new development via the chains of vacancies created as households move). Because household and firm location choices may influence access to employment (including entrepreneurial entry and self-employment opportunities), this project will also produce population estimates that allow for understanding the labor market at a very localized level."
"Exploring differences in injury severity between occupant groups
  involved in fatal rear-end crashes: A correlated random parameter logit model
  with mean heterogeneity","Rear-end crashes are one of the most common crash types. Passenger cars
involved in rear-end crashes frequently produce severe outcomes. However, no
study investigated the differences in the injury severity of occupant groups
when cars are involved as following and leading vehicles in rear-end crashes.
Therefore, the focus of this investigation is to compare the key factors
affecting the injury severity between the front- and rear-car occupant groups
in rear-end crashes. First, data is extracted from the Fatality Analysis
Reporting System (FARS) for two types of rear-end crashes from 2017 to 2019,
including passenger cars as rear-end and rear-ended vehicles. Significant
injury severity difference between front- and rear-car occupant groups is found
by conducting likelihood ratio test. Moreover, the front- and rear-car occupant
groups are modelled by the correlated random parameter logit model with
heterogeneity in means (CRPLHM) and the random parameter logit model with
heterogeneity in means (RPLHM), respectively. From the modeling, the
significant factors are occupant positions, driver age, overturn, vehicle type,
etc. For instance, the driving and front-right positions significantly increase
the probability of severe injury when struck by another vehicle. Large
truck-strike-car tends to cause severe outcomes compared to car-strike-large
truck. This study provides an insightful knowledge of mechanism of occupant
injury severity in rear-end crashes, and propose some effective countermeasures
to mitigate the crash severity, such as implementing stricter seat belt laws,
improving the coverage of the streetlights, strengthening car driver's
emergency response ability.","['Renteng Yuan', 'Xin Gu', 'Zhipeng Peng', 'Qiaojun Xiang']",[],0,arXiv,http://arxiv.org/abs/2303.12159v3,False,True,False,False,False,2359,David B Richardson,Triangle,Active,2021,,"This project will use Decennial Census and American Community Survey (ACS) data in combination with researcher-provided data to study fatal occupational injury rates in North Carolina over the forty-year period 1977-2017. Data on occupational fatalities in North Carolina come from the state's medical examiner system as well as death certificate files from the North Carolina Division of Public Health. Information on fatal injuries describes the sex, race, year of birth, occupation, and industry of the decedent, as well as the year of fatal injury and death. A prior study by the researchers using public Decennial Census data analyzes fatal occupational injuries in North Carolina between 1977 and 1991, when North Carolina's population was 6.6 million. North Carolina now has 9.9 million residents and the state's economy has changed substantially since 1991, with employment shifting dramatically away from agriculture, textile, and furniture production to a more diversified economy with a diverse workforce. This research will encompass a comprehensive epidemiological study of fatal occupational injuries between 1977 and the present, comparing findings to - and updating - the previous research."
"Deriving Priors for Bayesian Prediction of Daily Response Propensity in
  Responsive Survey Design: Historical Data Analysis vs. Literature Review","Responsive Survey Design (RSD) aims to increase the efficiency of survey data
collection via live monitoring of paradata and the introduction of protocol
changes when survey errors and increased costs seem imminent. Daily predictions
of response propensity for all active sampled cases are among the most
important quantities for live monitoring of data collection outcomes, making
sound predictions of these propensities essential for the success of RSD.
Because it relies on real-time updates of prior beliefs about key design
quantities, such as predicted response propensities, RSD stands to benefit from
Bayesian approaches. However, empirical evidence of the merits of these
approaches is lacking in the literature, and the derivation of informative
prior distributions is required for these approaches to be effective. In this
paper, we evaluate the ability of two approaches to deriving prior
distributions for the coefficients defining daily response propensity models to
improve predictions of daily response propensity in a real data collection
employing RSD. The first approach involves analyses of historical data from the
same survey, and the second approach involves literature review. We find that
Bayesian methods based on these two approaches result in higher-quality
predictions of response propensity than more standard approaches ignoring prior
information. This is especially true during the early-to-middle periods of data
collection when interventions are often considered in an RSD framework.","['Brady T. West', 'James Wagner', 'Stephanie Coffey', 'Michael R. Elliott']",[],0,arXiv,http://arxiv.org/abs/1907.06560v4,False,True,False,False,False,2371,James R Wagner,Michigan,Active,2021,,"Sample surveys are a critical resource for social science and health research. Large-scale national surveys, such as the National Survey of Children's Health (NSCH) and the National Survey of College Graduates (NSCG), are essential for understanding the population. However, sample surveys are increasingly facing the dual pressure of rising nonresponse and shrinking budgets. This pressure threatens the quality of survey estimates.

Survey methodologists have responded to these pressures by proposing two new classes of designs which are aimed at increasing quality or controlling costs. The first new class of designs, known as responsive survey designs, uses incoming data from the field to trigger changes in the design. In effect, these responsive designs identify cases that are not responding well under the current protocol, and offering them a new protocol that is more likely to induce response. The second class of designs is known as adaptive survey design. These designs attempt to identify subgroups in the population for whom different designs may be more effective. The goal is to identify the optimal design, with respect to a stated objective, that is tailored to individuals--i.e., assigns different designs to subgroups. Both classes of designs rely upon inputs for decision-making. Often, these inputs are in the form of model predictions about the probability of response. Unfortunately, the quality of those inputs has not been evaluated in either of these new classes of survey designs. We propose to evaluate the quality of these inputs. In particular, we will evaluate the impact of model selection procedures on the effectiveness of responsive and adaptive survey designs.

We will use data from the largest ongoing survey in the U.S., the American Community Survey (ACS), to accelerate progress on evaluating different approaches to informing the data collection design. The ACS is a mandatory survey with a 95% response rate that also uses a phased design with multiple protocols,
making it ideal for this study. We will vary both the information being used to direct data collection at the sample case level, and the primary objective of the targeted use of more costly methods "
Toward Ethical Robotic Behavior in Human-Robot Interaction Scenarios,"This paper describes current progress on developing an ethical architecture
for robots that are designed to follow human ethical decision-making processes.
We surveyed both regular adults (folks) and ethics experts (experts) on what
they consider to be ethical behavior in two specific scenarios: pill-sorting
with an older adult and game playing with a child. A key goal of the surveys is
to better understand human ethical decision-making. In the first survey, folk
responses were based on the subject's ethical choices (""folk morality""); in the
second survey, expert responses were based on the expert's application of
different formal ethical frameworks to each scenario. We observed that most of
the formal ethical frameworks we included in the survey (Utilitarianism,
Kantian Ethics, Ethics of Care and Virtue Ethics) and ""folk morality"" were
conservative toward deception in the high-risk task with an older adult when
both the adult and the child had significant performance deficiencies.","['Shengkang Chen', 'Vidullan Surendran', 'Alan R. Wagner', 'Jason Borenstein', 'Ronald C. Arkin']",[],0,arXiv,http://arxiv.org/abs/2206.10727v1,False,True,False,False,False,2371,James R Wagner,Michigan,Active,2021,,"Sample surveys are a critical resource for social science and health research. Large-scale national surveys, such as the National Survey of Children's Health (NSCH) and the National Survey of College Graduates (NSCG), are essential for understanding the population. However, sample surveys are increasingly facing the dual pressure of rising nonresponse and shrinking budgets. This pressure threatens the quality of survey estimates.

Survey methodologists have responded to these pressures by proposing two new classes of designs which are aimed at increasing quality or controlling costs. The first new class of designs, known as responsive survey designs, uses incoming data from the field to trigger changes in the design. In effect, these responsive designs identify cases that are not responding well under the current protocol, and offering them a new protocol that is more likely to induce response. The second class of designs is known as adaptive survey design. These designs attempt to identify subgroups in the population for whom different designs may be more effective. The goal is to identify the optimal design, with respect to a stated objective, that is tailored to individuals--i.e., assigns different designs to subgroups. Both classes of designs rely upon inputs for decision-making. Often, these inputs are in the form of model predictions about the probability of response. Unfortunately, the quality of those inputs has not been evaluated in either of these new classes of survey designs. We propose to evaluate the quality of these inputs. In particular, we will evaluate the impact of model selection procedures on the effectiveness of responsive and adaptive survey designs.

We will use data from the largest ongoing survey in the U.S., the American Community Survey (ACS), to accelerate progress on evaluating different approaches to informing the data collection design. The ACS is a mandatory survey with a 95% response rate that also uses a phased design with multiple protocols,
making it ideal for this study. We will vary both the information being used to direct data collection at the sample case level, and the primary objective of the targeted use of more costly methods "
"Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of
  Urban Areas","Urban flood risk emerges from complex and nonlinear interactions among
multiple features related to flood hazard, flood exposure, and social and
physical vulnerabilities, along with the complex spatial flood dependence
relationships. Existing approaches for characterizing urban flood risk,
however, are primarily based on flood plain maps, focusing on a limited number
of features, primarily hazard and exposure features, without consideration of
feature interactions or the dependence relationships among spatial areas. To
address this gap, this study presents an integrated urban flood-risk rating
model based on a novel unsupervised graph deep learning model (called
FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among
areas and complex and nonlinear interactions among flood hazards and urban
features for specifying emergent flood risk. Using data from multiple
metropolitan statistical areas (MSAs) in the United States, the model
characterizes their flood risk into six distinct city-specific levels. The
model is interpretable and enables feature analysis of areas within each
flood-risk level, allowing for the identification of the three archetypes
shaping the highest flood risk within each MSA. Flood risk is found to be
spatially distributed in a hierarchical structure within each MSA, where the
core city disproportionately bears the highest flood risk. Multiple cities are
found to have high overall flood-risk levels and low spatial inequality,
indicating limited options for balancing urban development and flood-risk
reduction. Relevant flood-risk reduction strategies are discussed considering
ways that the highest flood risk and uneven spatial distribution of flood risk
are formed.","['Kai Yin', 'Ali Mostafavi']",[],0,arXiv,http://arxiv.org/abs/2309.14610v3,False,True,False,False,False,2377,Lala Ma,Kentucky,Active,2022,,"Floods trigger monumental losses across the US that are projected to increase with climate change. With the recent history of unprecedented flood losses from events including Hurricanes Katrina, Sandy, and Harvey, growing concern surrounds the detrimental consequences for some of the nation's most vulnerable communities. These distributional concerns are not new. Critically, little is known about how households may heterogeneously adjust to flood risk along several key housing margins including location choice and the decision to rent versus own, as well as how these decisions may be impacted by public welfare programs. The project will provide insight into these flooding issues through two empirical papers carefully motivated by theory. Both papers use data from the American Community Survey (2000-2023, as available).  Paper 1 applies a residential sorting model to data on recent movers in major metropolitan areas within the US to analyze the extent to which homebuyers and renters heterogeneously sort across flood risk based on their race/ethnicity, income, and educational attainment. These results will then be used to estimate the impacts of the National Flood Insurance Program and changes in flood risk on intra-US migration decisions and the resulting distribution of socioeconomic groups across flood risk in the US. Paper 2 examines how serious flood events affect migration decisions (e.g. whether, and where, to move within the US, and whether to rent versus buy) and mortgage impacts, and if participation in social insurance programs modifies the responses to flood-related natural disasters. These decisions can have long-term implications for risk exposure and wealth accumulation."
Transforming the Preparation of Physics GTAs: Curriculum Development,"Graduate Teaching Assistants (GTAs) are key partners in the education of
undergraduates. Given the potentially large impact GTAs can have on
undergraduate student learning, it is important to provide them with
appropriate preparation for teaching. But GTAs are students themselves, and not
all of them desire to pursue an academic career. Fully integrating GTA
preparation into the professional development of graduate students lowers the
barrier to engagement so that all graduate students may benefit from the
opportunity to explore teaching and its applications to many potential career
paths. In this paper we describe the design and implementation of a GTA
Preparation course for first-year Ph.D. students at the Georgia Tech School of
Physics. Through a yearly cycle of implementation and revision, guided by the
3P Framework we developed (Pedagogy, Physics, Professional Development), the
course has evolved into a robust and comprehensive professional development
program that is well-received by physics graduate students.","['Emily Alicea-Muñoz', 'Carol Subiño Sullivan', 'Michael F. Schatz']",[],0,arXiv,http://arxiv.org/abs/2109.00168v1,False,True,False,False,False,2383,Kevin Kniffin,Cornell,Active,2021,,"Academic disciplines play an important role in establishing methods for studying phenomena observed in the real world; however, there is a significant push from institutions, such as universities and grant funding agencies, for researchers to work with others outside of their disciplinary silos. Our objective is to study how the PhD market responds to this movement through a unique juxtaposition of datasets including UMETRICS, SED, and LEHD. This project will extend the prior literature on the near-term labor market penalty interdisciplinary dissertators in STEM fields tend to face in the first year after completing the PhD.  This project will also generate novel population estimates by addressing the following five Research Questions (RQs): RQ1: To what extent do interdisciplinary dissertators tend to face either penalties, rewards, more varied outcomes, or no differences in outcomes over the course of their careers (after completing an interdisciplinary dissertation)? RQ2: To what extent do the patterns that we examine through RQ1 vary over time? RQ3: How do graduate students who are funded by programs with an interdisciplinary focus do over the course of their careers when compared with graduate students funded by the NSF in similar but more traditional discipline-based programs? RQ4: How do members of (large) project-based teams do over the course of their careers when compared with peers who are not part of (large) project-based teams? RQ5:To what extent do the SED and ProQuest datasets correspond to each other with respect to classifying someone's dissertation as interdisciplinary?"
"Self-respecting worker in the precarious gig economy: A dynamic
  principal-agent model","We introduce a dynamic principal-agent model to understand the nature of
contracts between an employer and an independent gig worker. We model the
worker's self-respect with an endogenous backward-looking participation
constraint; he accepts a job offer if and only if its utility is at least as
large as his reference value, which is based on the average of previously
realized wages. If the dynamically changing reference value capturing the
worker's demand is too high, then no contract is struck until the reference
value hits a threshold. Below the threshold, contracts are offered and
accepted, and the worker's wage demand follows a stochastic process. We apply
our model to perfectly competitive and monopsonistic labor market structures
and investigate first-best and second-best solutions. We show that a
far-sighted employer with market power may sacrifice instantaneous profit to
regulate the agent's demand. Moreover, the far-sighted employer implements
increasing and path-dependent effort levels. Our model captures the worker's
bargaining power by a vulnerability parameter that measures the rate at which
his wage demand decreases when unemployed. With a low vulnerability parameter,
the worker can afford to go unemployed and need not take a job at all costs.
Conversely, a worker with high vulnerability can be exploited by the employer,
and in this case our model also exhibits self-exploitation.","['Zsolt Bihary', 'Péter Csóka', 'Péter Kerényi', 'Alexander Szimayer']",[],0,arXiv,http://arxiv.org/abs/1902.10021v4,False,True,False,False,False,2387,Gal Wettstein,Boston,Active,2021,,"Longer lives, the shift to defined contribution (DC) plans, and lower Social Security replacement rates mean Americans need to work longer to secure an adequate retirement. This project analyzes the ability of older workers to find employment in an environment where longer careers are likely to be necessary but where increasing firm concentration and a lack of demand from employers threaten their ability to do so. First, the project examines the relationship between employer concentration and labor force participation (LFP) for workers at various ages. To establish the mechanisms involved in this association, the project will test for heterogeneity in the correlation of concentration and LFP by two dimensions: education and unionization.  In both cases, the effect of concentration is expected to be smaller for workers with more bargaining power, such as the more educated and the unionized. Second, the project explores whether older workers are good for firms' productivity and profitability. The project will estimate the firm-level correlation of the share of workers over age 55 and the productivity of the firm, as measured by revenue per worker, the ratio of revenue to wages, and for the manufacturing sector total factor productivity and whether production targets were met.  The project will estimate the share of workers over age 55 by commuting zone (CZ), and focusing on the manufacturing sector will estimate the correlation of productivity, with the share of workers over age 55 instrumented by the share of the firm's CZ over age 55."
"Bounding the Photon Mass with Ultrawide Bandwidth Pulsar Timing Data and
  Dedispersed Pulses of Fast Radio Bursts","Exploring the concept of a massive photon has been an important area in
astronomy and physics. If photons have mass, their propagation in nonvacuum
space would be affected by both the nonzero mass $m_{\gamma}$ and the presence
of a plasma medium. This would lead to a delay time proportional to
$m_{\gamma}^2\nu^{-4}$, which deviates from the classical dispersion relation
(proportional to $\nu^{-2}$). For the first time, we have derived the
dispersion relation of a photon with a nonzero mass propagating in plasma. To
reduce the impact of variations in the dispersion measure (DM), we employed the
high-precision timing data to constrain the upper bound of the photon mass.
Specifically, the DM/time of arrival (TOA) uncertainties derived from ultrawide
bandwidth (UWB) observations conducted by the Parkes Pulsar Timing Array (PPTA)
are used. The dedispersed pulses from fast radio bursts (FRBs) with minimal
scattering effects are also used to constrain the upper bound of photon mass.
The stringent limit on the photon mass is determined by uncertainties of the
TOA of pulsars, with an optimum value of $9.52\times 10^{-46} \, \rm kg
\,\,(5.34 \times 10^{-10}\, \rm eV/c^2$). In the future, it is essential to
investigate the photon mass, as pulsar timing data are collected by PTA and UWB
receivers, or FRBs with wideband spectra are detected by UWB receivers.","['Yu-Bin Wang', 'Xia Zhou', 'Abdusattar Kurban', 'Fa-Yin Wang']",[],0,arXiv,http://arxiv.org/abs/2403.06422v3,False,True,False,False,False,2391,Danny Kurban,Colorado,Completed,2021,2022.0,"Economic theory suggests that trading firms should be particularly dependent on external and internal financing due to issues such as long shipment lags, currency and regulatory risks, and high initial fixed costs to set up a distribution network abroad. This implies that firms with tighter credit constraints should be significantly less likely to export to a given destination (extensive margin) and have significantly lower export shares (intensive margin) than comparable, but financially unconstrained, firms. However, existing strategies fail to document how financial constraints determine trade shares because they lack firm-level microdata. By making use of linked Economic Census, Quarterly Financial Report, LFTTD, and Compustat data, we can more directly measure firm-level constraints and determine how biased standard techniques for assessing this relationship at the industry-level have been. Results will better characterize the true contribution of financing constraints on trade shares in the US economy.  "
"Implications of the first evidence for coherent elastic scattering of
  reactor neutrinos","The recent evidence for coherent elastic neutrino-nucleus scattering
(CE$\nu$NS) in the NCC-1701 germanium detector using antineutrinos from the
Dresden-II nuclear reactor is in good agreement with standard model
expectations. However, we show that a $2\sigma$ improvement in the fit to the
data can be achieved if the quenching factor is described by a modified
Lindhard model. We also place constraints on the parameter space of a light
vector or scalar mediator that couples to neutrinos and quarks, and on a
neutrino magnetic moment. We demonstrate that the constraints are quite
sensitive to the quenching factor at low recoil energies by comparing
constraints for the standard Lindhard model with those by marginalizing over
the two parameters of the modified Lindhard model.","['Jiajun Liao', 'Hongkai Liu', 'Danny Marfatia']",[],0,arXiv,http://arxiv.org/abs/2202.10622v2,False,True,False,False,False,2391,Danny Kurban,Colorado,Completed,2021,2022.0,"Economic theory suggests that trading firms should be particularly dependent on external and internal financing due to issues such as long shipment lags, currency and regulatory risks, and high initial fixed costs to set up a distribution network abroad. This implies that firms with tighter credit constraints should be significantly less likely to export to a given destination (extensive margin) and have significantly lower export shares (intensive margin) than comparable, but financially unconstrained, firms. However, existing strategies fail to document how financial constraints determine trade shares because they lack firm-level microdata. By making use of linked Economic Census, Quarterly Financial Report, LFTTD, and Compustat data, we can more directly measure firm-level constraints and determine how biased standard techniques for assessing this relationship at the industry-level have been. Results will better characterize the true contribution of financing constraints on trade shares in the US economy.  "
Adapting End-to-End Speech Recognition for Readable Subtitles,"Automatic speech recognition (ASR) systems are primarily evaluated on
transcription accuracy. However, in some use cases such as subtitling, verbatim
transcription would reduce output readability given limited screen size and
reading time. Therefore, this work focuses on ASR with output compression, a
task challenging for supervised approaches due to the scarcity of training
data. We first investigate a cascaded system, where an unsupervised compression
model is used to post-edit the transcribed speech. We then compare several
methods of end-to-end speech recognition under output length constraints. The
experiments show that with limited data far less than needed for training a
model from scratch, we can adapt a Transformer-based ASR model to incorporate
both transcription and compression capabilities. Furthermore, the best
performance in terms of WER and ROUGE scores is achieved by explicitly modeling
the length constraints within the end-to-end ASR system.","['Danni Liu', 'Jan Niehues', 'Gerasimos Spanakis']",[],0,arXiv,http://arxiv.org/abs/2005.12143v1,False,True,False,False,False,2391,Danny Kurban,Colorado,Completed,2021,2022.0,"Economic theory suggests that trading firms should be particularly dependent on external and internal financing due to issues such as long shipment lags, currency and regulatory risks, and high initial fixed costs to set up a distribution network abroad. This implies that firms with tighter credit constraints should be significantly less likely to export to a given destination (extensive margin) and have significantly lower export shares (intensive margin) than comparable, but financially unconstrained, firms. However, existing strategies fail to document how financial constraints determine trade shares because they lack firm-level microdata. By making use of linked Economic Census, Quarterly Financial Report, LFTTD, and Compustat data, we can more directly measure firm-level constraints and determine how biased standard techniques for assessing this relationship at the industry-level have been. Results will better characterize the true contribution of financing constraints on trade shares in the US economy.  "
"Uncovering Urban Mobility and City Dynamics from Large-Scale Taxi
  Origin-Destination (O-D) Trips: Case Study in Washington DC Area","We perform a systematic analysis on the large-scale taxi trip data to uncover
urban mobility and city dynamics in multimodal urban transportation
environments. As a case study, we use the taxi origin-destination trip data and
some additional data sources in Washington DC area. We first study basic
characteristics of taxi trips, then focus on five important aspects. Three of
them concern urban mobility, which are respectively mobility and cost including
effect of traffic congestion, trip safety, and multimodal connectivity; the
other two pertain to city dynamics, which are respectively transportation
resilience and the relation between trip patterns and land use. For these
aspects, we use appropriate statistical methods and geographic techniques to
mine patterns and characteristics from taxi trip data for better understanding
qualitative and quantitative impacts of the inputs from key stakeholders on
available measures of effectiveness on urban mobility and city dynamics, where
key stakeholders include road users, system operators, and city. Finally, we
briefly summarize our findings and discuss some critical roles and implications
of the uncovered patterns and characteristics from the relation between taxi
system and key stakeholders. The results can support road users by providing
evidence-based information of trip cost, mobility, safety, multimodal
connectivity and transportation resilience, can assist taxi drivers and
operators to deliver transportation services in a higher quality of mobility,
safety and operational efficiency, and can also help city planners and policy
makers to transform multimodal transportation and to manage urban resources in
a more effective and better way.","['Xiao-Feng Xie', 'Zunjing Jenipher Wang']",[],0,arXiv,http://arxiv.org/abs/1812.09583v1,False,True,False,False,False,2403,Evan Mast,Philadelphia,Active,2020,,"Geographic mobility plays an important role in many economic issues, and thus mobility across metropolitan areas, states, and countries has been widely studied. Recent research increasingly suggests that mobility across neighborhoods is also important, for example in determining the effectiveness of place-based revitalization programs (Busso, Gregory, and Kline 2013) and influencing the long-term educational attainment and earnings of children (Chetty, Hendren, and Katz 2016; Chetty and Hendren 2018a,b; Chyn 2018). Yet despite this importance, studying cross-neighborhood mobility and its implications for individual outcomes has been challenging because of data limitations. This project combines data from the Census 2000 and 2010, American Community Surveys 2005-2018, and Master Address File (MAF) to obtain longitudinal individual observations with detailed location information, demographic characteristics, and outcomes. We first explore how different types of individuals move across different types of neighborhoods and how these mobility patterns shape individuals' longitudinal exposure to neighborhood characteristics such as poverty and segregation. We expect that mobility will lead to exposure measures that differ from measures using cross-sectional data. We then study the role of individual mobility in neighborhood change, and we expect this will reveal that neighborhoods are more dynamic than usually assumed. Finally, we use these data to study the effectiveness of place-based treatments for incumbent and future residents. Overall, our results will improve understanding of cross-neighborhood mobility, gentrification and neighborhood decline, and the effectiveness and distributional consequences of place-based treatments."
Crowdsourcing the Robin Hood effect in cities,"Socioeconomic inequalities in cities are embedded in space and result in
neighborhood effects, whose harmful consequences have proved very hard to
counterbalance efficiently by planning policies alone. Considering
redistribution of money flows as a first step toward improved spatial equity,
we study a bottom-up approach that would rely on a slight evolution of shopping
mobility practices. Building on a database of anonymized credit card
transactions in Madrid and Barcelona, we quantify the mobility effort required
to reach a reference situation where commercial income is evenly shared among
neighborhoods. The redirections of shopping trips preserve key properties of
human mobility, including travel distances. Surprisingly, for both cities only
a small fraction ($\sim 5 \%$) of trips need to be altered to reach equity
situations, improving even other sustainability indicators. The method could be
implemented in mobile applications that would assist individuals in reshaping
their shopping practices, to promote the spatial redistribution of
opportunities in the city.","['Thomas Louail', 'Maxime Lenormand', 'Juan Murillo Arias', 'José J. Ramasco']",[],0,arXiv,http://arxiv.org/abs/1604.08394v2,False,True,False,False,False,2403,Evan Mast,Philadelphia,Active,2020,,"Geographic mobility plays an important role in many economic issues, and thus mobility across metropolitan areas, states, and countries has been widely studied. Recent research increasingly suggests that mobility across neighborhoods is also important, for example in determining the effectiveness of place-based revitalization programs (Busso, Gregory, and Kline 2013) and influencing the long-term educational attainment and earnings of children (Chetty, Hendren, and Katz 2016; Chetty and Hendren 2018a,b; Chyn 2018). Yet despite this importance, studying cross-neighborhood mobility and its implications for individual outcomes has been challenging because of data limitations. This project combines data from the Census 2000 and 2010, American Community Surveys 2005-2018, and Master Address File (MAF) to obtain longitudinal individual observations with detailed location information, demographic characteristics, and outcomes. We first explore how different types of individuals move across different types of neighborhoods and how these mobility patterns shape individuals' longitudinal exposure to neighborhood characteristics such as poverty and segregation. We expect that mobility will lead to exposure measures that differ from measures using cross-sectional data. We then study the role of individual mobility in neighborhood change, and we expect this will reveal that neighborhoods are more dynamic than usually assumed. Finally, we use these data to study the effectiveness of place-based treatments for incumbent and future residents. Overall, our results will improve understanding of cross-neighborhood mobility, gentrification and neighborhood decline, and the effectiveness and distributional consequences of place-based treatments."
"One City, Two Tales: Using Mobility Networks to Understand Neighborhood
  Resilience and Fragility during the COVID-19 Pandemic","What predicts a neighborhood's resilience and adaptability to essential
public health policies and shelter-in-place regulations that prevent the
harmful spread of COVID-19? To answer this question, in this paper we present a
novel application of human mobility patterns and human behavior in a network
setting. We analyze mobility data in New York City over two years, from January
2019 to December 2020, and create weekly mobility networks between Census Block
Groups by aggregating Point of Interest level visit patterns. Our results
suggest that both the socioeconomic and geographic attributes of neighborhoods
significantly predict neighborhood adaptability to the shelter-in-place
policies active at that time. That is, our findings and simulation results
reveal that in addition to factors such as race, education, and income,
geographical attributes such as access to amenities in a neighborhood that
satisfy community needs were equally important factors for predicting
neighborhood adaptability and the spread of COVID-19. The results of our study
provide insights that can enhance urban planning strategies that contribute to
pandemic alleviation efforts, which in turn may help urban areas become more
resilient to exogenous shocks such as the COVID-19 pandemic.","['Hasan Alp Boz', 'Mohsen Bahrami', 'Selim Balcisoy', 'Burcin Bozkaya', 'Nina Mazar', 'Aaron Nichols', 'Alex Pentland']",[],0,arXiv,http://arxiv.org/abs/2210.04641v1,False,True,False,False,False,2403,Evan Mast,Philadelphia,Active,2020,,"Geographic mobility plays an important role in many economic issues, and thus mobility across metropolitan areas, states, and countries has been widely studied. Recent research increasingly suggests that mobility across neighborhoods is also important, for example in determining the effectiveness of place-based revitalization programs (Busso, Gregory, and Kline 2013) and influencing the long-term educational attainment and earnings of children (Chetty, Hendren, and Katz 2016; Chetty and Hendren 2018a,b; Chyn 2018). Yet despite this importance, studying cross-neighborhood mobility and its implications for individual outcomes has been challenging because of data limitations. This project combines data from the Census 2000 and 2010, American Community Surveys 2005-2018, and Master Address File (MAF) to obtain longitudinal individual observations with detailed location information, demographic characteristics, and outcomes. We first explore how different types of individuals move across different types of neighborhoods and how these mobility patterns shape individuals' longitudinal exposure to neighborhood characteristics such as poverty and segregation. We expect that mobility will lead to exposure measures that differ from measures using cross-sectional data. We then study the role of individual mobility in neighborhood change, and we expect this will reveal that neighborhoods are more dynamic than usually assumed. Finally, we use these data to study the effectiveness of place-based treatments for incumbent and future residents. Overall, our results will improve understanding of cross-neighborhood mobility, gentrification and neighborhood decline, and the effectiveness and distributional consequences of place-based treatments."
The Future of ChatGPT-enabled Labor Market: A Preliminary Study in China,"As a phenomenal large language model, ChatGPT has achieved unparalleled
success in various real-world tasks and increasingly plays an important role in
our daily lives and work. However, extensive concerns are also raised about the
potential ethical issues, especially about whether ChatGPT-like artificial
general intelligence (AGI) will replace human jobs. To this end, in this paper,
we introduce a preliminary data-driven study on the future of ChatGPT-enabled
labor market from the view of Human-AI Symbiosis instead of Human-AI
Confrontation. To be specific, we first conduct an in-depth analysis of
large-scale job posting data in BOSS Zhipin, the largest online recruitment
platform in China. The results indicate that about 28% of occupations in the
current labor market require ChatGPT-related skills. Furthermore, based on a
large-scale occupation-centered knowledge graph, we develop a semantic
information enhanced collaborative filtering algorithm to predict the future
occupation-skill relations in the labor market. As a result, we find that
additional 45% occupations in the future will require ChatGPT-related skills.
In particular, industries related to technology, products, and operations are
expected to have higher proficiency requirements for ChatGPT-related skills,
while the manufacturing, services, education, and health science related
industries will have lower requirements for ChatGPT-related skills.","['Lan Chen', 'Xi Chen', 'Shiyu Wu', 'Yaqi Yang', 'Meng Chang', 'Hengshu Zhu']",[],0,arXiv,http://arxiv.org/abs/2304.09823v4,False,True,False,False,False,2411,Xi Yang,Dallas,Active,2021,,"This project studies the labor market consequences of variation in housing wealth by exploring a wide range of labor market outcomes including labor supply, wage dynamics, geographic mobility, commuting behaviors, and unemployment duration. We use the restricted version of the Survey of Income and Program Participation from 1996 to 2014 which is linked with geographic variables (MSA and county codes). These geographic codes make it possible to better measure the fluctuation of housing value at the local level. This project contributes to the literature in two ways. First, our results improve our understanding of the consequences of housing market fluctuation and the mechanisms behind these effects. Second, by using the geographically linked SIPP, this project is able to investigate the data quality of housing wealth variables in the survey data."
The academic Great Gatsby Curve,"The Great Gatsby Curve measures the relationship between income inequality
and intergenerational income persistence. By utilizing genealogical data of
over 245,000 mentor-mentee pairs and their academic publications from 22
different disciplines, this study demonstrates that an academic Great Gatsby
Curve exists as well, in the form of a positive correlation between academic
impact inequality and the persistence of impact across academic generations. We
also provide a detailed breakdown of academic persistence, showing that the
correlation between the impact of mentors and that of their mentees has
increased over time, indicating an overall decrease in academic
intergenerational mobility. We analyze such persistence across a variety of
dimensions, including mentorship types, gender, and institutional prestige.","['Ye Sun', 'Fabio Caccioli', 'Xiancheng Li', 'Giacomo Livan']",[],0,arXiv,http://arxiv.org/abs/2310.08968v1,False,True,False,False,False,2413,Garrett Anstreicher,Wisconsin,Active,2020,,"While recent research has studied the impacts of the Great Recession on the wages and employment of young adults, less is known about how their parents protected them from the economic crash. By linking young adults in the American Community Survey to themselves in the Decennial Census to obtain their parental information, we intend to investigate how the impact of the Great Recession on the earnings, migration, and educational attainment of youth differed based on the socioeconomic status of their parents. We expect to find substantial heterogeneity within young adults, with the children of wealthier parents being more likely to take protective actions such as delaying labor force entry via the pursuit of higher education and leaving areas that were impacted worse by the Great Recession, while the children of disadvantaged parents being less so. These asymmetric responses and the flight of talented youth from areas adversely affected by the Great Recession may in turn be important in explaining the stark differences in post-recession recoveries observed between urban and rural locations in the United States."
"Accounting for recall bias in case-control studies: a causal inference
  approach","A case-control study is designed to help determine if an exposure is
associated with an outcome. However, since case-control studies are
retrospective, they are often subject to recall bias. Recall bias can occur
when study subjects do not remember previous events accurately. In this paper,
we first define the estimand of interest: the causal odds ratio (COR) for a
case-control study. Second, we develop estimation approaches for the COR and
present estimates as a function of recall bias. Third, we define a new quantity
called the \textit{R-factor}, which denotes the minimal amount of recall bias
that leads to altering the initial conclusion. We show that a failure to
account for recall bias can significantly bias estimation of the COR. Finally,
we apply the proposed framework to a case-control study of the causal effect of
childhood physical abuse on adulthood mental health.","['Kwonsang Lee', 'Francesca Dominici']",[],0,arXiv,http://arxiv.org/abs/2102.10537v1,False,True,False,False,False,2416,Abhay P Aneja,Berkeley,Active,2023,,"This project will quantify the impact of the Civil Rights Act and Voting Rights Act on the long-run economic and social status of racial and ethnic persons, using birth cohorts that were differentially affected due to the timing of policy implementation across counties. This project will also consider indirect effects on other subpopulations that may be affected via spillovers. Finally, this project will also consider mechanisms through which legal changes cause improvements in economic outcomes, such as labor market and housing outcomes. The researchers will quantify employment outcomes for various demographic groups, including minority groups that civil rights legislation targeted, as well as groups that may have been impacted indirectly."
"Exploring Tracer Study Service in Career Center Web Site of Indonesia
  Higher Education","Quality competence of worker the present do not meet labor market criteria
and the low level of labor productivity, the lack of communication between the
labor market with education, changing of socio-economic structure and global
political influence labor market, the development of science and technology
very rapidly lead to fundamental changes in terms of qualifications,
competencies and requirements for entering the workforce. Tracer Study results
can be used by universities to determine the success of the educational process
that has been done towards their students. Therefore, universities need a
technology services to support the optimization of the use of tracer study. One
of that is the use of a website to facilitate the conduct tracer study. Most
services tracer study provides information to college, like year graduated, got
a job waiting period, the first salary to work, first job, the relevance of the
curriculum to the work, and compliance with the major areas of work taken in
college. Tracer study feature in Career Center Website affect the popularity
website especially in traffic and rich file website.","['Renny', 'Reza Chandra', 'Syamsi Ruhama', 'Mochammad Wisuda Sarjono']",[],0,arXiv,http://arxiv.org/abs/1304.5869v2,False,True,False,False,False,2420,Shawn Martin,Michigan,Active,2021,,"We are interested in studying the labor market outcomes of college graduates. Specifically, we want to focus on how labor market outcomes vary by the selectivity of the college attended and college major (i.e type of college graduate). We will document differences across type of college graduate in the mean, growth and volatility of earnings in early- mid- and late-career. We will also analyze the extent to which outcomes vary geographically or have changed across college graduate cohorts. We will next analyze the sources of heterogenous labor market outcomes including the a worker's employer, the tasks performed on the job and
occupation. We will study the effect of local labor market conditions at time of graduation on a variety of short- and long-term labor market outcomes including earnings, employment, occupation, migration decisions and employer characteristics.

To accomplish this, restricted Census data from the Longitudinal Employer-Household Dynamics (LEHD) program, Decennial Census, American Community Survey (ACS), the National Survey of College Graduates (NSCG) and the UMETRICS University Research Data (UMT) will be linked together. The specific LEHD data sets being requested in this proposal include: Employer Characteristics Files (ECF), Employment History Files (EHF), Geocoded Address List (GAL), Individual Characteristics Files (ICF), Quarterly Workforce Indicators, and Unit-to-Worker (U2W). Additionally, PI-supplied data will include area-level measures of labor markets, institution-level information on for U.S. post-secondary institutions, occupation-level descriptors and publicly available measures of average earnings by college attended and
college major."
Career Path Suggestion using String Matching and Decision Trees,"High school and college graduates seemingly are often battling for the
courses they should major in order to achieve their target career. In this
paper, we worked on suggesting a career path to a graduate to reach his/her
dream career given the current educational status. Firstly, we collected the
career data of professionals and academicians from various career fields and
compiled the data set by using the necessary information from the data.
Further, this was used as the basis to suggest the most appropriate career path
for the person given his/her current educational status. Decision trees and
string matching algorithms were employed to suggest the appropriate career path
for a person. Finally, an analysis of the result has been done directing to
further improvements in the model.","['Akshay Nagpal', 'Supriya P. Panda']",[],0,arXiv,http://arxiv.org/abs/1505.06306v1,False,True,False,False,False,2420,Shawn Martin,Michigan,Active,2021,,"We are interested in studying the labor market outcomes of college graduates. Specifically, we want to focus on how labor market outcomes vary by the selectivity of the college attended and college major (i.e type of college graduate). We will document differences across type of college graduate in the mean, growth and volatility of earnings in early- mid- and late-career. We will also analyze the extent to which outcomes vary geographically or have changed across college graduate cohorts. We will next analyze the sources of heterogenous labor market outcomes including the a worker's employer, the tasks performed on the job and
occupation. We will study the effect of local labor market conditions at time of graduation on a variety of short- and long-term labor market outcomes including earnings, employment, occupation, migration decisions and employer characteristics.

To accomplish this, restricted Census data from the Longitudinal Employer-Household Dynamics (LEHD) program, Decennial Census, American Community Survey (ACS), the National Survey of College Graduates (NSCG) and the UMETRICS University Research Data (UMT) will be linked together. The specific LEHD data sets being requested in this proposal include: Employer Characteristics Files (ECF), Employment History Files (EHF), Geocoded Address List (GAL), Individual Characteristics Files (ICF), Quarterly Workforce Indicators, and Unit-to-Worker (U2W). Additionally, PI-supplied data will include area-level measures of labor markets, institution-level information on for U.S. post-secondary institutions, occupation-level descriptors and publicly available measures of average earnings by college attended and
college major."
"A Network Perspective on Attitude Strength: Testing the Connectivity
  Hypothesis","Attitude strength is a key characteristic of attitudes. Strong attitudes are
durable and impactful, while weak attitudes are fluctuating and
inconsequential. Recently, the Causal Attitude Network (CAN) model was proposed
as a comprehensive measurement model of attitudes, which conceptualizes
attitudes as networks of causally connected evaluative reactions (i.e.,
beliefs, feelings, and behavior toward an attitude object). Here, we test the
central postulate of the CAN model that highly connected attitude networks
correspond to strong attitudes. We use data from the American National Election
Studies 1980-2012 on attitudes toward presidential candidates (total n =
18,795). We first show that political interest predicts connectivity of
attitude networks toward presidential candidates. Second, we show that
connectivity is strongly related to two defining features of strong attitudes -
stability of the attitude and the attitude's impact on behavior. We conclude
that network theory provides a promising framework to advance the understanding
of attitude strength.","['Jonas Dalege', 'Denny Borsboom', 'Frenk van Harreveld', 'Han L. J. van der Maas']",[],0,arXiv,http://arxiv.org/abs/1705.00193v2,False,True,False,False,False,2424,Kelly G Strada,Chicago,Completed,2021,2024.0,"What were the main drivers of female labor force participation during World War II? Were there long-term consequences for women who entered the workforce and for their children? In order to address these questions, I make use of the Census Numident File combined with the 1940 Census to construct a novel intercensal measure of labor force entry--SSN first issuance--for the wives of draft-eligible men. After linking these data to the WWII Enlistment Records, I exploit the draft lottery to construct an individual-level instrument for married women's workforce entry. In doing so, I can disentangle the supply and demand-side drivers of female labor supply during WWII--the former being proxied by government spending by town--conditional on a rich set of covariates. Furthermore, I plan to leverage the longitudinal structure of PIK-ed mandatory surveys to assess the long-term consequences of female entry. Specifically, by using reduced form evidence and instrumental variable techniques, I intend to assess whether being a working woman increased the likelihood of divorce and whether the outcomes of children of working mothers differed substantially from those of non-working mothers. In order to address identification concerns, I plan to extend the analysis to elicit random variation in the entry of female neighbors. Preliminary results suggest that a man's draft status (and timing) strongly predicts his wife's labor force entry: by accessing RDC data I wish to evaluate whether women paid a social cost for working and if their children benefited from having a working mother."
"Social network analysis of electric vehicles adoption: a data-based
  approach","Mobility is undergoing dramatic transformations. Especially in the context of
urban areas, several significant changes are underway, driven by both new
mobility needs and environmental concerns. The most mature one, which still is
struggling to affirm itself is the process of the adoption of Electric Vehicles
(EVs), thus switching from fuel-based to battery-powered propulsion
technologies. Many social and economic barriers have proved to play a crucial
role in this process, ranging from level of education, environmental awareness,
age and census. This work aims at contributing to the study of this adoption
process through a data-based lens, using real mobility patterns to setup a
social-network analysis to model the spread of consensus among neighbouring
people that can enable the switch to EVs. In particular, we build the network
topology using proximity measures that emerge from the analysis of real trips,
and the initial disposition of the single agents towards the EV technology is
inferred from their real mobility patterns. Based on this network, a cascade
adoption model is simulated to investigate the dynamics of the adoption
process, and an incentive scheme is designed to show how different policies can
contribute to the opinion diffusion over time on the network.","['V. Breschi', 'M. Tanelli', 'C. Ravazzi', 'S. Strada', 'F. Dabbene']",[],0,arXiv,http://arxiv.org/abs/2001.09704v1,False,True,False,False,False,2424,Kelly G Strada,Chicago,Completed,2021,2024.0,"What were the main drivers of female labor force participation during World War II? Were there long-term consequences for women who entered the workforce and for their children? In order to address these questions, I make use of the Census Numident File combined with the 1940 Census to construct a novel intercensal measure of labor force entry--SSN first issuance--for the wives of draft-eligible men. After linking these data to the WWII Enlistment Records, I exploit the draft lottery to construct an individual-level instrument for married women's workforce entry. In doing so, I can disentangle the supply and demand-side drivers of female labor supply during WWII--the former being proxied by government spending by town--conditional on a rich set of covariates. Furthermore, I plan to leverage the longitudinal structure of PIK-ed mandatory surveys to assess the long-term consequences of female entry. Specifically, by using reduced form evidence and instrumental variable techniques, I intend to assess whether being a working woman increased the likelihood of divorce and whether the outcomes of children of working mothers differed substantially from those of non-working mothers. In order to address identification concerns, I plan to extend the analysis to elicit random variation in the entry of female neighbors. Preliminary results suggest that a man's draft status (and timing) strongly predicts his wife's labor force entry: by accessing RDC data I wish to evaluate whether women paid a social cost for working and if their children benefited from having a working mother."
Recent topics on the O'Hara energies,"The O'Hara energies, introduced by Jun O'Hara in 1991, were proposed to
answer the question of what is a ""good"" figure in a given knot type. A property
of the O'Hara energies is that the ""better"" the figure of a knot is, the less
the energy value is. In this article, we discuss two topics on the O'Hara
energies. First, we slightly generalize the O'Hara energies and consider a
characterization of its finiteness. The finiteness of the O'Hara energies was
considered by Blatt in 2012 who used the Sobolev-Slobodeckii space, and
naturally we consider a generalization of this space. Another fundamental
problem is to understand the minimizers of the O'Hara energies. This problem
has been addressed in several papers, some of them based on numerical
computations. In this direction, we discuss a discretization of the O'Hara
energies and give some examples of numerical computations. Particular one of
the O'Hara energies, called the M\""{o}bius energy thanks to its M\""{o}bius
invariance, was considered by Kim-Kusner in 1993, and Scholtes in 2014
established convergence properties. We apply their argument in general since
the argument does not rely on M\""{o}bius invariance.",['Shoya Kawakami'],[],0,arXiv,http://arxiv.org/abs/1908.11671v1,False,True,False,False,False,2436,Amy B O'Hara,Georgetown,Active,2023,,"The project demonstrates how linkages between court, federal, and state data build knowledge about the relationships between housing insecurity, racial inequities, and civil court participation. A deficit of high-quality accessible data impedes the study and development of evidence-based policies in the civil justice arena, and will cripple evidence building and policy responses during pandemic recovery unless more knowledge is created.  This project develops methods to produce statistics characterizing parties involved in civil court eviction cases, noting how they differ across sites and over time.

The project addresses knowledge and data gaps about the demographic and socioeconomic characteristics of court participants.  Such data are not collected in the civil courts themselves, unlike the situation in criminal courts where more information about defendants (e.g., age or date of birth, race and ethnicity) is captured. For civil cases, even if courts were to begin collecting these characteristics regularly, the data would not be comprehensive or representative because 40 percent of defendants in evictions cases do not show up for their court hearings.

We intend to use researcher-provided-data from 9 sites with demographic and geographic characteristics in this project.  We seek person-level matching to Census race and ethnicity data, federal and state administrative data, and commercial data to explore the availability and consistency of race and ethnicity data for defendants in the evictions cases, and statistics about residential mobility and labor force participation before and after eviction cases."
Low-Resource Court Judgment Summarization for Common Law Systems,"Common law courts need to refer to similar precedents' judgments to inform
their current decisions. Generating high-quality summaries of court judgment
documents can facilitate legal practitioners to efficiently review previous
cases and assist the general public in accessing how the courts operate and how
the law is applied. Previous court judgment summarization research focuses on
civil law or a particular jurisdiction's judgments. However, judges can refer
to the judgments from all common law jurisdictions. Current summarization
datasets are insufficient to satisfy the demands of summarizing precedents
across multiple jurisdictions, especially when labeled data are scarce for many
jurisdictions. To address the lack of datasets, we present CLSum, the first
dataset for summarizing multi-jurisdictional common law court judgment
documents. Besides, this is the first court judgment summarization work
adopting large language models (LLMs) in data augmentation, summary generation,
and evaluation. Specifically, we design an LLM-based data augmentation method
incorporating legal knowledge. We also propose a legal knowledge enhanced
evaluation metric based on LLM to assess the quality of generated judgment
summaries. Our experimental results verify that the LLM-based summarization
methods can perform well in the few-shot and zero-shot settings. Our LLM-based
data augmentation method can mitigate the impact of low data resources.
Furthermore, we carry out comprehensive comparative experiments to find
essential model components and settings that are capable of enhancing
summarization performance.","['Shuaiqi Liu', 'Jiannong Cao', 'Yicong Li', 'Ruosong Yang', 'Zhiyuan Wen']",[],0,arXiv,http://arxiv.org/abs/2403.04454v1,False,True,False,False,False,2436,Amy B O'Hara,Georgetown,Active,2023,,"The project demonstrates how linkages between court, federal, and state data build knowledge about the relationships between housing insecurity, racial inequities, and civil court participation. A deficit of high-quality accessible data impedes the study and development of evidence-based policies in the civil justice arena, and will cripple evidence building and policy responses during pandemic recovery unless more knowledge is created.  This project develops methods to produce statistics characterizing parties involved in civil court eviction cases, noting how they differ across sites and over time.

The project addresses knowledge and data gaps about the demographic and socioeconomic characteristics of court participants.  Such data are not collected in the civil courts themselves, unlike the situation in criminal courts where more information about defendants (e.g., age or date of birth, race and ethnicity) is captured. For civil cases, even if courts were to begin collecting these characteristics regularly, the data would not be comprehensive or representative because 40 percent of defendants in evictions cases do not show up for their court hearings.

We intend to use researcher-provided-data from 9 sites with demographic and geographic characteristics in this project.  We seek person-level matching to Census race and ethnicity data, federal and state administrative data, and commercial data to explore the availability and consistency of race and ethnicity data for defendants in the evictions cases, and statistics about residential mobility and labor force participation before and after eviction cases."
Geometry and complexity of O'Hara's algorithm,"In this paper we analyze O'Hara's partition bijection. We present three type
of results. First, we show that O'Hara's bijection can be viewed geometrically
as a certain scissor congruence type result. Second, we obtain a number of new
complexity bounds, proving that O'Hara's bijection is efficient in several
special cases and mildly exponential in general. Finally, we prove that for
identities with finite support, the map of the O'Hara's bijection can be
computed in polynomial time, i.e. much more efficiently than by O'Hara's
construction.","['Matjaz Konvalinka', 'Igor Pak']",[],0,arXiv,http://arxiv.org/abs/0710.1459v2,False,True,False,False,False,2436,Amy B O'Hara,Georgetown,Active,2023,,"The project demonstrates how linkages between court, federal, and state data build knowledge about the relationships between housing insecurity, racial inequities, and civil court participation. A deficit of high-quality accessible data impedes the study and development of evidence-based policies in the civil justice arena, and will cripple evidence building and policy responses during pandemic recovery unless more knowledge is created.  This project develops methods to produce statistics characterizing parties involved in civil court eviction cases, noting how they differ across sites and over time.

The project addresses knowledge and data gaps about the demographic and socioeconomic characteristics of court participants.  Such data are not collected in the civil courts themselves, unlike the situation in criminal courts where more information about defendants (e.g., age or date of birth, race and ethnicity) is captured. For civil cases, even if courts were to begin collecting these characteristics regularly, the data would not be comprehensive or representative because 40 percent of defendants in evictions cases do not show up for their court hearings.

We intend to use researcher-provided-data from 9 sites with demographic and geographic characteristics in this project.  We seek person-level matching to Census race and ethnicity data, federal and state administrative data, and commercial data to explore the availability and consistency of race and ethnicity data for defendants in the evictions cases, and statistics about residential mobility and labor force participation before and after eviction cases."
Endogenous Crashes as Phase Transitions,"This paper explores the mechanisms behind extreme financial events,
specifically market crashes, by employing the theoretical framework of phase
transitions. We focus on endogenous crashes, driven by internal market
dynamics, and model these events as first-order phase transitions critical,
stochastic, and dynamic. Through a comparative analysis of early warning
signals associated with each type of transition, we demonstrate that dynamic
phase transitions (DPT) offer a more accurate representation of market crashes
than critical (CPT) or stochastic phase transitions (SPT). Unlike existing
models, such as the Log-Periodic Power Law (LPPL) model, which often suffers
from overfitting and false positives, our approach grounded in DPT provides a
more robust prediction framework. Empirical findings, based on an analysis of
S&P 500 stocks from 2019 to 2024, reveal significant trends in volatility and
anomalous dimensions before crashes, supporting the superiority of the DPT
model. This work contributes to a deeper understanding of the predictive
signals preceding market crashes and offers a novel perspective on their
underlying dynamics.","['Revant Nayar', 'Minhajul Islam']",[],0,arXiv,http://arxiv.org/abs/2408.06433v1,False,True,False,False,False,2439,Nitya Pandalai Nayar,Austin,Active,2021,,"How do firms' worker characteristics and engagement in international markets affect each other? Aimed at answering this question, our proposed project will leverage a new method developed by the researchers apportioning firm-level exports in the LFTTD to establishments belonging to the firm in the LBD. By further connecting these establishments with their workers in the LEHD, we will obtain a direct mapping between an establishment's exports and its workers. These links will help us understand the nexus between trade and immigration, how engagement in international trade mediates how firms respond to fiscal stimulus, and how exposure to international markets affects wage inequality."
Soft x-ray magnetic circular dichroism study on Gd-doped EuO thin films,"We report on the growth and characterization of ferromagnetic Gd-doped EuO
thin films. We prepared samples with Gd concentrations up to 11% by means of
molecular beam epitaxy under distillation conditions, which allows a very
precise control of the doping concentration and oxygen stoichiometry. Using
soft x-ray magnetic circular dichroism at the Eu and Gd M4,5 edges, we found
that the Curie temperature ranged from 69 K for pure stoichiometric EuO to
about 170 K for the film with the optimal Gd doping of around 4%. We also show
that the Gd magnetic moment couples ferromagnetically to that of Eu.","['H. Ott', 'S. J. Heise', 'R. Sutarto', 'Z. Hu', 'C. F. Chang', 'H. H. Hsieh', 'H. -J. Lin', 'C. T. Chen', 'L. H. Tjeng']",[],0,arXiv,http://arxiv.org/abs/cond-mat/0509722v1,False,True,False,False,False,2441,Chang-Tai Hsieh,Chicago,Active,2020,,"This project will use the Economic Censuses (EC), the Business R&D Surveys (SIRD, BRDIS and the ABS), the Longitudinal Firm Trade Transactions Database (LFTTD), and the Revenue-Enhanced Longitudinal Business Database (LBD) to provide new series that shed light on two questions.   First, what are the sources of innovation?  What is the role of creative destruction, innovation on a firm's own products, and creation of new products?  How important is access to foreign markets for each type of innovation?  Second, what are the effects of top firm concentration on innovation, aggregate growth, and the spatial distribution of economic activity?

This project builds upon the work the researchers completed in their previous Census RDC projects.  In these projects, the researchers tackled two questions.  First, how do we measure the efficiency of resource allocation across firms for a given distribution of underlying firm productivity?  Second, what are the mechanisms behind the changes in the distribution of firm productivity over time?  How much of the growth in firm productivity takes the form of creative destruction, innovation on the firm's own products, and the creation of brand new products?  

In the current proposal, the researchers plan to address two questions.  First, the researchers will address three limitations of the previous census project.  In the previous project, the researchers inferred the contribution of each type of innovation from the distribution of job flows.  The researchers did not directly observe each type of innovation, but rather inferred the frequency of each type of innovation from the distribution of job creation and job destruction.  The researchers also assumed that the size of quality improvements was the same for creative destruction and own innovation.  A third limitation was that the researchers assumed innovations only built on the products owned by domestic firms.  In the proposed research project, the researchers will directly measure the frequency and improvement in quality from each type of innovation based on the business R&D surveys (ABS-BRDIS-SIRD).  In addition, the researchers will measure the contribution of access to foreign markets to innovation by merging the LFTTD with the LBD and EC.

A second objective of this research proposal is to measure the effect of the growing concentration of sales at top firms.  The researchers believe that growing firm concentration could be the consequence of new technologies that enable top firms to expand.  The researchers plan to examine this hypothesis by measuring the following moments.  First, the researchers will decompose the growth of top firms in an industry into the contribution of expansion into new product lines or markets.  Second, the researchers will measure industry productivity growth in sectors where top firm concentration has increased.  Third, the researchers will measure changes in the share of payments to labor in industry sales, and their correlation with the change in the market share of top firms in the sector.  Fourth, the researchers will measure changes in the market share of top national firms in local markets (defined as an MSA or a county), and the extent of local labor productivity growth in each local market.  Finally, the researchers will examine the correlation of job destruction with the change in the market share of top national firms in the sector. 
           	
These statistics will allow the researchers to measure the empirical importance of different mechanisms of innovation, the importance of exports in innovation, and the economic effects of the growing concentration of top firms."
"Epitaxy, stoichiometry, and magnetic properties of Gd-doped EuO films on
  YSZ (001)","We have succeeded in preparing high-quality Gd-doped single-crystalline EuO
films. Using Eu-distillation-assisted molecular beam epitaxy and a systematic
variation in the Gd and oxygen deposition rates, we have been able to observe
sustained layer-by-layer epitaxial growth on yttria-stabilized cubic zirconia
(001). The presence of Gd helps to stabilize the layer-by-layer growth mode. We
used soft x-ray absorption spectroscopy at the Eu and Gd M4,5 edges to confirm
the absence of Eu3+ contaminants and to determine the actual Gd concentration.
The distillation process ensures the absence of oxygen vacancies in the films.
  From magnetization measurements we found the Curie temperature to increase
smoothly as a function of doping from 70 K up to a maximum of 125 K. A
threshold behavior was not observed for concentrations as low as 0.2%.","['R. Sutarto', 'S. G. Altendorf', 'B. Coloru', 'M. Moretti Sala', 'T. Haupricht', 'C. F. Chang', 'Z. Hu', 'C. Schüßler-Langeheine', 'N. Hollmann', 'H. Kierspel', 'J. A. Mydosh', 'H. H. Hsieh', 'H. -J. Lin', 'C. T. Chen', 'L. H. Tjeng']",[],0,arXiv,http://arxiv.org/abs/0903.1632v3,False,True,False,False,False,2441,Chang-Tai Hsieh,Chicago,Active,2020,,"This project will use the Economic Censuses (EC), the Business R&D Surveys (SIRD, BRDIS and the ABS), the Longitudinal Firm Trade Transactions Database (LFTTD), and the Revenue-Enhanced Longitudinal Business Database (LBD) to provide new series that shed light on two questions.   First, what are the sources of innovation?  What is the role of creative destruction, innovation on a firm's own products, and creation of new products?  How important is access to foreign markets for each type of innovation?  Second, what are the effects of top firm concentration on innovation, aggregate growth, and the spatial distribution of economic activity?

This project builds upon the work the researchers completed in their previous Census RDC projects.  In these projects, the researchers tackled two questions.  First, how do we measure the efficiency of resource allocation across firms for a given distribution of underlying firm productivity?  Second, what are the mechanisms behind the changes in the distribution of firm productivity over time?  How much of the growth in firm productivity takes the form of creative destruction, innovation on the firm's own products, and the creation of brand new products?  

In the current proposal, the researchers plan to address two questions.  First, the researchers will address three limitations of the previous census project.  In the previous project, the researchers inferred the contribution of each type of innovation from the distribution of job flows.  The researchers did not directly observe each type of innovation, but rather inferred the frequency of each type of innovation from the distribution of job creation and job destruction.  The researchers also assumed that the size of quality improvements was the same for creative destruction and own innovation.  A third limitation was that the researchers assumed innovations only built on the products owned by domestic firms.  In the proposed research project, the researchers will directly measure the frequency and improvement in quality from each type of innovation based on the business R&D surveys (ABS-BRDIS-SIRD).  In addition, the researchers will measure the contribution of access to foreign markets to innovation by merging the LFTTD with the LBD and EC.

A second objective of this research proposal is to measure the effect of the growing concentration of sales at top firms.  The researchers believe that growing firm concentration could be the consequence of new technologies that enable top firms to expand.  The researchers plan to examine this hypothesis by measuring the following moments.  First, the researchers will decompose the growth of top firms in an industry into the contribution of expansion into new product lines or markets.  Second, the researchers will measure industry productivity growth in sectors where top firm concentration has increased.  Third, the researchers will measure changes in the share of payments to labor in industry sales, and their correlation with the change in the market share of top firms in the sector.  Fourth, the researchers will measure changes in the market share of top national firms in local markets (defined as an MSA or a county), and the extent of local labor productivity growth in each local market.  Finally, the researchers will examine the correlation of job destruction with the change in the market share of top national firms in the sector. 
           	
These statistics will allow the researchers to measure the empirical importance of different mechanisms of innovation, the importance of exports in innovation, and the economic effects of the growing concentration of top firms."
"Evaluating the Environmental Justice Dimensions of Odor in Denver,
  Colorado","Background Odors are a documented environmental justice challenge in Denver,
Colorado. Complaints are an important modality through which residents express
their concerns. Objective We investigated disparities in environmental justice
related-variables based on home and workplace census block groups
(race/ethnicity, education-levels, renter-occupied housing, median income and
median home values, gentrification) based on locations of odor complaints as
well as that of potentially malodorous facilities. We report key themes
identified in complaints. Methods We obtained odor complaints for 2014-2023,
and the locations of facilities required to submit odor management plans as of
2023 from the Denver Department of Public Health and Environment. We downloaded
residential census block group-level socioeconomic data from the 2016-2020
American Community Survey; and workplace-based socioeconomic data from the
Longitudinal Employer-Household Dynamics dataset for 2020. We categorized
neighborhoods as gentrified or not based on a typology produced by the Urban
Displacement Project. We assessed exposure to potentially malodorous facilities
and complaints within each census block group and investigated exposure
disparities by comparing distributions of environmental justice-related
variables based on if a facility or a complaint has been made, and census block
group-level odor intensity categories. Results Less privileged census block
groups were significantly disproportionately burdened with potentially
malodorous facilities. Importantly, our study also reveals disparities in the
location of facilities, not just in traditional residence/-based environmental
justice-related variables, but in workplace/-based factors, as well. We did not
observe similar disparities for odor complaints. However, complaints were
significantly higher in gentrified neighborhoods.","['Priyanka N. deSouza', 'Amanda Rees', 'Emilia Oscilowicz', 'Brendan Lawlor', 'William Obermann', 'Katherine Dickinson', 'Lisa M. McKenzie', 'Sheryl Magzamen', 'Shelly Miller', 'Michelle L. Bell']",[],0,arXiv,http://arxiv.org/abs/2402.03336v1,False,True,False,False,True,2442,William B Allshouse,Colorado,Active,2021,,"The development of Oil and Gas (O&G) resources in the US has risen dramatically in recent years as O&G companies have applied the extractive technologies of horizontal drilling and hydraulic fracturing (""fracking""). However, while development has produced a mix of diffuse economic benefits, it also raises concerns about locally-concentrated health risks and environmental justice (EJ) issues. Previous research has been unable to determine the extent of these concerns because of the lack of data with highly disaggregated residence locations that allow for calculating O&G proximity in a way that reflects exposures and health risks that occur within hundreds of feet. This project will use precise locational data from the Master Address File and associated American Community Survey responses to assess whether the risks associated with O&G production are distributed unevenly in the population. Results will help inform policy used to mitigate EJ issues due to targeted drilling or sorting of disadvantaged households near O&G sites."
Notification Timing for On-Demand Personnel Scheduling,"Modern business models have enabled service systems to leverage a large pool
of casual employees with flexible hours, paid based on piece rates, to fulfill
on-demand work. These systems have been successfully implemented in sectors
such as ride-sharing, delivery services, and microtasks. However, because
casual employees engage infrequently and may lack experience, maintaining
service quality remains a key challenge. We introduce a novel scheduling system
designed to provide experienced casual employees to service companies,
optimizing their operations through a dynamic, data-driven approach. Similar to
traditional on-call systems, it contacts casual personnel in order of seniority
to inform them about available work. However, our system offers greater
flexibility, allowing employees to take time to decide and freely select from
available shifts. Senior employees can also replace (bump) junior employees
from the schedule if no other preferred shift is available, subject to certain
conditions. While permitted, these replacements create disruptions and
dissatisfaction among employees. The management aims to efficiently assign all
shifts while minimizing bumps. However, uncertainty arises regarding when an
employee will select a shift. The key challenge is determining the optimal
timing to notify employees to reduce disruptions. We first establish that this
problem is $\mathcal{NP}$-complete even with perfect information. To address
this, we propose a two-stage stochastic formulation for the dynamic problem and
develop a heuristic algorithm that approximates the optimal policy using a
threshold-based structure. These policies are fine-tuned using offline
solutions with pre-known uncertainty, allowing for optimization. Testing on
real-world data demonstrates that our approach outperforms the current strategy
used by our industry partner.","['Prakash Gawas', 'Antoine Legrain', 'Louis-Martin Rousseau']",[],0,arXiv,http://arxiv.org/abs/2312.06139v2,False,True,False,False,False,2459,Stephanie Karol,Michigan,Active,2021,,"Despite the fact that the non-profit sector employs roughly 10% of the American workforce, making it the third largest workforce behind retail and manufacturing, relatively little is known about its employees. Indeed, very few studies conduct a detailed analysis of a particular sector. Those that do consider a narrower subset of workers, are mainly concerned with public sector workers, and do not focus exclusively on charity employees. This project will be the first to use administrative data to paint a picture of the U.S. non-profit sector, and answer important questions such as who works in the non-profit sector, and how do shocks and government policies affect its employees.

Up until now, researchers have used survey data to focus on the motivation of non-profit workers, and how their behavior differs from for-profit workers. Using British panel data, Gregg et al. (2011) show that non-profit workers are more likely to donate their labor (as measured by unpaid overtime), than their for-profit counterparts. In the U.S. context, Houston (2000, 2006) finds that public employees place a higher value on intrinsic reward, and government employees are more likely to volunteer for charities and donate blood than for-profit employees. However, they found no difference among public service and private employees in terms of individual philanthropy. This is in contrast to Buurman et al (2012), who conclude from a Dutch survey that public sector employees contribute less to charity because they feel that they contribute enough to society at work for too little pay.

Therefore, although we know something about the motivations of charity employees, we know very little about their basic demographics, how they interact with the charity labor market, and how they move between the for- and non-profit sectors. Furthermore, understanding how organization-level shocks affect charity employees, and how government policies affect non-profit employment decisions will add to our knowledge of the charity labor market and contribute to the policy debate over the benefits of public versus private provision of services.

The use of administrative data is crucial in our setting; the Longitudinal Employer Household Dynamics (LEHD) is the only data source that allows us to focus on the employer-employee relationship in the United States. Using the LEHD also provides several benefits over using survey data, such as larger sample sizes and avoiding concerns about non-random measurement error. Indeed, previous studies have found the size of the non-profit sector to be misrepresented in surveys (Millard and Machin, 2007)."
"Multilevel superconducting circuits as two-qubit systems: Operations,
  state preparation, and entropic inequalities","We theoretically study operations with a four-level superconducting circuit
as a two-qubit system. Using a mapping on a two-qubit system, we show how to
implement iSWAP gates and Hadamard gates through pulses on transitions between
particular pairs of energy levels. Our approach allows one to prepare pure
two-qubit entangled states with desired form of reduced density matrices of the
same purity and, in particular, arbitrary identical reduced states of qubits.
We propose using schemes for the Hadamard gate and two-qubit entangled states
with identical reduced density matrices in order to verify $\log{N}$
inequalities for Shannon and R\'enyi entropies for the considered noncomposite
quantum system.","['E. O. Kiktenko', 'A. K. Fedorov', ""O. V. Man'ko"", ""V. I. Man'ko""]",[],0,arXiv,http://arxiv.org/abs/1411.0157v3,False,True,False,False,False,2471,Shannon Rieger,Baruch,Active,2021,,"How is urban space used to reproduce inequality? It has been documented that when urban areas are carved up into small, independent governmental jurisdictions, those areas tend to have greater racial, ethnic and economic segregation, more urban sprawl, and lower job growth. Some evidence indicates that the fragmentation of urban space in other ways, such as through the creation of gated communities, may also generate inequality. For example, gated communities may encourage resource hoarding and undermine political will to engage in regional problem-solving efforts, both of which impose disproportionate burdens on residents of nearby low-income neighborhoods. This project will empirically investigate the effects of gated communities on socioeconomic inequality. Our primary goal is to contribute to knowledge about whether and how residential gating affects the life chances of low-income individuals living in the surrounding area. We propose to assess the relationship between gated communities and socioeconomic inequality at the level of the metropolitan statistical area (MSA), using internal-use AHS data linked to MSA level characteristics available in the public ACS and Decennial Census, as well as data on economic mobility published by Harvard's Opportunity Insights. We will use a fixed-effects approach to investigate relationships between changes in the prevalence of gated communities and changes in inequality."
"Rapid adjustment and post-processing of temperature forecast
  trajectories","Modern weather forecasts are commonly issued as consistent multi-day forecast
trajectories with a time resolution of 1-3 hours. Prior to issuing, statistical
post-processing is routinely used to correct systematic errors and
misrepresentations of the forecast uncertainty. However, once the forecast has
been issued, it is rarely updated before it is replaced in the next forecast
cycle of the numerical weather prediction (NWP) model. This paper shows that
the error correlation structure within the forecast trajectory can be utilized
to substantially improve the forecast between the NWP forecast cycles by
applying additional post-processing steps each time new observations become
available. The proposed rapid adjustment is applied to temperature forecast
trajectories from the UK Met Office's convective-scale ensemble MOGREPS-UK.
MOGREPS-UK is run four times daily and produces hourly forecasts for up to 36
hours ahead. Our results indicate that the rapidly adjusted forecast from the
previous NWP forecast cycle outperforms the new forecast for the first few
hours of the next cycle, or until the new forecast itself can be rapidly
adjusted, suggesting a new strategy for updating the forecast cycle.","['Nina Schuhen', 'Thordis Thorarinsdottir', 'Alex Lenkoski']",[],0,arXiv,http://arxiv.org/abs/1910.05101v1,False,True,False,False,False,2479,Xin Dai,Philadelphia,Active,2021,,"This project will examine how multi-unit firms' life-cycle stages influence analysts' forecasts. Whereas prior studies focus on the firm-level life cycle, the proposed study will be the first study to focus on the establishment-level life cycle. Specifically, we will investigate whether analysts' forecast accuracy is lower for multi-unit firms whose units are in different life-cycle stages than those whose units are in the same life-cycle stage. The expected findings will suggest that the forecasting difficulty of more diversified firms can be attributed to the different life-cycle stages of each establishment. Additionally, for firms whose units are in the same stage, we will examine whether analysts' forecast accuracy is lower if all the units are in earlier stages than if all the units are in later stages. Also, for firms whose units are in different stages, we will examine whether analysts' forecast accuracy is lower if the units in earlier stages are larger (i.e., generates more revenue) than the units in later stages. To estimate the establishment-level life-cycle stages, we will use the Census data (CMF) at the establishment level. As a comparison, we will also estimate the life-cycle stages using firms' segment classifications in their 10-K filings. For the additional analyses, we will investigate the industry dynamics associated with life-cycle stages."
Planning of integrated mobility-on-demand and urban transit networks,"We envision a multimodal transportation system where Mobility-on-Demand (MoD)
service is used to serve the first mile and last mile of transit trips. For
this purpose, the current research formulates an optimization model for
designing an integrated MoD and urban transit system. The proposed model is a
mixed-integer non-linear programming model that captures the strategic behavior
of passengers in a multimodal network through a passenger assignment model. It
determines which transit routes to operate, the frequency of the operating
routes, the fleet size of vehicles required in each transportation analysis
zone to serve the demand, and the passenger flow on both road and transit
networks. A Benders decomposition approach with several enhancements is
proposed to solve the given optimization program. Computational experiments are
presented for the Sioux Falls multimodal network. The results show a
significant improvement in the congestion in the city center with the
introduction and optimization of an integrated transportation system. The
proposed design allocates more vehicles to the outskirt zones in the network
(to serve the first mile and last mile of transit trips) and more frequency to
the transit routes in the city center. The integrated system significantly
improves the share of transit passengers and their level of service in
comparison to the base optimized transit system. The sensitivity analysis of
the bus and vehicle fleet shows that increasing the number of buses has more
impact on improving the level of service of passengers compared to increasing
the number of MoD vehicles. Finally, we provide managerial insights for
deploying such multimodal service.","['Pramesh Kumar', 'Alireza Khani']",[],0,arXiv,http://arxiv.org/abs/2106.11005v2,False,True,False,False,False,2481,Miles Finney,UCLA,Completed,2021,2024.0,"This research project examines the relationship between changes in the US immigrant population and the country's urban housing stock.  Using the American Housing Survey, the American Community Survey, and the Decennial Census, I estimate the demands for urban housing across immigrant groups, comparing them to demands by the larger US population.  The goal of the study is to relate potentially distinct housing demands by immigrants to changes in the physical structure of US urban areas."
"Stock fluctuations are correlated and amplified across networks of
  interlocking directorates","Traded corporations are required by law to have a majority of outside
directors on their board. This requirement allows the existence of directors
who sit on the board of two or more corporations at the same time, generating
what is commonly known as interlocking directorates. While research has shown
that networks of interlocking directorates facilitate the transmission of
information between corporations, little is known about the extent to which
such interlocking networks can explain the fluctuations of stock price returns.
Yet, this is a special concern since the risk of amplifying stock fluctuations
is latent. To answer this question, here we analyze the board composition,
traders' perception, and stock performance of more than 1500 US traded
corporations from 2007-2011. First, we find that the fewer degrees of
separation between two corporations in the interlocking network, the stronger
the temporal correlation between their stock price returns. Second, we find
that the centrality of traded corporations in the interlocking network
correlates with the frequency at which financial traders talk about such
corporations, and this frequency is in turn proportional to the corresponding
traded volume. Third, we show that the centrality of corporations was
negatively associated with their stock performance in 2008, the year of the big
financial crash. These results suggest that the strategic decisions made by
interlocking directorates are strongly followed by stock analysts and have the
potential to correlate and amplify the movement of stock prices during
financial crashes. These results may have relevant implications for scholars,
investors, and regulators.","['Serguei Saavedra', 'Luis J. Gilarranz', 'Rudolf P. Rohr', 'Michael Schnabel', 'Brian Uzzi', 'Jordi Bascompte']",[],0,arXiv,http://arxiv.org/abs/1410.6646v1,False,True,False,False,False,2484,Elena Patel,Utah,Active,2021,,"It has been well documented that women are under-represented among business leaders, facing a ""glass ceiling"" that prevents them from professional advancement to the highest levels including on corporate boards of directors. To what extent this lack of diversity in business leadership impacts firm-level family friendly policies and practices, including the hiring, firing, and promotion of female employees is of particular importance in light of the persistent gender-wage gap in the U.S and elsewhere. This project links externally available, researcher-provided data on gender diversity of corporate boards of directors to restricted micro-data available through the Federal Statistical Research Data Center program. Primary datasets to be linked are the Survey of Business Owners and the Longitudinal Employer-Household Dynamics among others. We utilize an instrumental variables empirical strategy to causally identify the impact of female representation as well as the impact of leadership diversity on within-firm earnings, promotion, and turnover differences among male and female employees."
Online Continual Learning on Hierarchical Label Expansion,"Continual learning (CL) enables models to adapt to new tasks and environments
without forgetting previously learned knowledge. While current CL setups have
ignored the relationship between labels in the past task and the new task with
or without small task overlaps, real-world scenarios often involve hierarchical
relationships between old and new tasks, posing another challenge for
traditional CL approaches. To address this challenge, we propose a novel
multi-level hierarchical class incremental task configuration with an online
learning constraint, called hierarchical label expansion (HLE). Our
configuration allows a network to first learn coarse-grained classes, with data
labels continually expanding to more fine-grained classes in various hierarchy
depths. To tackle this new setup, we propose a rehearsal-based method that
utilizes hierarchy-aware pseudo-labeling to incorporate hierarchical class
information. Additionally, we propose a simple yet effective memory management
and sampling strategy that selectively adopts samples of newly encountered
classes. Our experiments demonstrate that our proposed method can effectively
use hierarchy on our HLE setup to improve classification accuracy across all
levels of hierarchies, regardless of depth and class imbalance ratio,
outperforming prior state-of-the-art works by significant margins while also
outperforming them on the conventional disjoint, blurry and i-Blurry CL setups.","['Byung Hyun Lee', 'Okchul Jung', 'Jonghyun Choi', 'Se Young Chun']",[],0,arXiv,http://arxiv.org/abs/2308.14374v1,False,True,False,False,False,2485,Ryun Jung Lee,Texas,Active,2021,,"For urban planners and designers, New Urbanism has been a principle approach in promoting a sense of community and walkability. The principles of New Urbanism primarily deal with urban form and design aspects; however, what is typically lacking are social aspects in urban communities. A notable and arguable aspect of New Urbanism is the assumption of economic stability and population growth. In recent years, the assumption that cities must inevitably grow has been criticized as many cities rather experienced urban decline. How New Urbanist communities mitigate or respond to neighborhood abandonment and physical disorders is still understudied. 

To fill the gap in the New Urbanism literature, our project uses the American Housing Survey and the American Community Survey from 2009-2018 to investigate four sub-studies examining the relationship between the New Urbanist environment and neighborhood abandonment. Study 1 examines if communities with New Urbanist features are more resilient to abandonment than are other communities. Study 2 examines if New Urbanist features can mitigate or predict neighborhood disorders. Study 3 compares the effects of the New Urbanist environment on neighborhood abandonment across growing, shrinking, and stable metro areas. Study 4 examines the role of the New Urbanist features in the relationship between neighborhood change and abandonment at the neighborhood level."
Beyond Baby Blues: The Child Penalty in Mental Health in Switzerland,"This paper investigates the mental health penalty for women after childbirth
in Switzerland. Leveraging insurance data, we employ a staggered
difference-in-difference research design. The findings reveal a substantial
mental health penalty for women following the birth of their first child.
Approximately four years after childbirth, there is a one percentage point
(p.p.) increase in antidepressant prescriptions, representing a 50% increase
compared to pre-birth levels. This increase rises to 1.7 p.p. (a 70% increase)
six years postpartum. The mental health penalty is likely not only a direct
consequence of giving birth but also a consequence of the changed life
circumstances and time constraints that accompany it, as the penalty is rising
over time and is higher for women who are employed before childbirth.",['Nora Bearth'],[],0,arXiv,http://arxiv.org/abs/2410.20861v1,False,True,False,False,False,2488,Pilar Gonalons,Philadelphia,Active,2022,,"In this project we examine the long-run economic consequences of the transition to parenthood from a couple's perspective, with a particular interest in how they have changed over time for different socioeconomic groups and how they contribute to income inequality across households. We use successive panels from the Survey of Income and Program Participation (SIPP) linked to the Detailed Earnings Record (DER), the Numident, and Census Household Composition Key datasets to study how the transition to parenthood shapes within and between household economic inequalities. We use couple-level fixed effects regression models to study how coupled men's and women's earnings change with the transition to parenthood (and how these changes have evolved since the 1980s across different socioeconomic groups) and we use log-linear models and simulation/decomposition methods to study how the economic consequences of parenthood shape family income inequality. These questions represent critical gaps in the literature on gender, work, and family that limit our understanding of how family decision-making is shaped by changing norms and institutions and in turn how these processes play into increasing income inequality across households. We expect that the economic consequences of parenthood have declined most rapidly for highly skilled women, who have most incentives to stay attached to the labor force after parenthood, and that those changes might have contributed to exacerbate income inequality across households."
"Procurement in welfare programs: Evidence and implications from WIC
  infant formula contracts","This paper examines the impact of government procurement in social welfare
programs on consumers, manufacturers, and the government. We analyze the U.S.
infant formula market, where over half of the total sales are purchased by the
Women, Infants, and Children (WIC) program. The WIC program utilizes
first-price auctions to solicit rebates from the three main formula
manufacturers, with the winner exclusively serving all WIC consumers in the
winning state. The manufacturers compete aggressively in providing rebates
which account for around 85% of the wholesale price. To rationalize and
disentangle the factors contributing to this phenomenon, we model
manufacturers' retail pricing competition by incorporating two unique features:
price inelastic WIC consumers and government regulation on WIC brand prices.
Our findings confirm three sizable benefits from winning the auction: a notable
spill-over effect on non-WIC demand, a significant marginal cost reduction, and
a higher retail price for the WIC brand due to the price inelasticity of WIC
consumers. Our counterfactual analysis shows that procurement auctions affect
manufacturers asymmetrically, with the smallest manufacturer harmed the most.
More importantly, by switching from the current mechanism to a predetermined
rebate procurement, the government can still contain the cost successfully,
consumers' surplus is greatly improved, and the smallest manufacturer benefits
from the switch, promoting market competition.","['Yonghong An', 'David Davis', 'Yizao Liu', 'Ruli Xiao']",[],0,arXiv,http://arxiv.org/abs/2308.12479v1,False,True,False,False,False,2494,Anna Malinovskaya,Cornell,Active,2021,,"The Special Supplemental Nutrition Program for Women, Infants, and Children (WIC) has been in effect since the 1970s. Although there exists a body of research examining the short-term effects of WIC on participating children, little is still known about whether any short-term effects persist through adulthood. In this project, we exploit plausibly exogenous variation across time and space in WIC geographical roll-out in the 1970s to study the causal effects of the program on children's longer-term outcomes such as high school graduation, college enrollment and completion, and earnings. In particular, we estimate the average intend-to-treat effect of exposure to WIC that lasts for different periods of time and starts at different times during one's childhood, such as in-utero, in the first year of life or later, in the third-fifth year of life, etc. For this purpose, we combine data on adult outcomes of children born in the 1970s from the American Community Survey and 2000 Decennial with our data on WIC timing and geographical spread across counties and use a difference-in-differences research design. We utilize the Numident to link respondents to the WIC roll-out data based on date and place of birth. Our findings will reveal whether enrollment in WIC in childhood raises, on average, the probability of graduating from high school, enrolling in college, completing college, and having higher earnings in adulthood."
"PitVQA++: Vector Matrix-Low-Rank Adaptation for Open-Ended Visual
  Question Answering in Pituitary Surgery","Vision-Language Models (VLMs) in visual question answering (VQA) offer a
unique opportunity to enhance intra-operative decision-making, promote
intuitive interactions, and significantly advancing surgical education.
However, the development of VLMs for surgical VQA is challenging due to limited
datasets and the risk of overfitting and catastrophic forgetting during full
fine-tuning of pretrained weights. While parameter-efficient techniques like
Low-Rank Adaptation (LoRA) and Matrix of Rank Adaptation (MoRA) address
adaptation challenges, their uniform parameter distribution overlooks the
feature hierarchy in deep networks, where earlier layers, that learn general
features, require more parameters than later ones. This work introduces
PitVQA++ with an open-ended PitVQA dataset and vector matrix-low-rank
adaptation (Vector-MoLoRA), an innovative VLM fine-tuning approach for adapting
GPT-2 to pituitary surgery. Open-Ended PitVQA comprises around 101,803 frames
from 25 procedural videos with 745,972 question-answer sentence pairs, covering
key surgical elements such as phase and step recognition, context
understanding, tool detection, localization, and interactions recognition.
Vector-MoLoRA incorporates the principles of LoRA and MoRA to develop a
matrix-low-rank adaptation strategy that employs vector ranking to allocate
more parameters to earlier layers, gradually reducing them in the later layers.
Our approach, validated on the Open-Ended PitVQA and EndoVis18-VQA datasets,
effectively mitigates catastrophic forgetting while significantly enhancing
performance over recent baselines. Furthermore, our risk-coverage analysis
highlights its enhanced reliability and trustworthiness in handling uncertain
predictions. Our source code and dataset is available
at~\url{https://github.com/HRL-Mike/PitVQA-Plus}.","['Runlong He', 'Danyal Z. Khan', 'Evangelos B. Mazomenos', 'Hani J. Marcus', 'Danail Stoyanov', 'Matthew J. Clarkson', 'Mobarakol Islam']",[],0,arXiv,http://arxiv.org/abs/2502.14149v1,False,True,False,False,False,2495,Matthew J Wiswall,Wisconsin,Active,2021,,"While college attainment has been studied extensively in the economics literature, it has become increasingly apparent that there are many post-secondary education decisions - such as major choice, pursuing advanced degrees, and taking on student loan debt - beyond a simple binary college completion choice that are crucial in determining one's earnings and financial stability later in life. By linking young adults in the American Community Surveys and the National Surveys of College Graduates to themselves and their parents in the Decennial Censuses, we propose to study how family background and local economic conditions influence these behavioral margins. We expect that the children of wealthier households are more likely to pursue high-earning majors/degrees while accruing less student loan debt. Individuals may respond to adverse local economic shocks by pursuing more schooling and more generally applicable majors. Substantial heterogeneity in these effects may be present across sex and race. These responses may have important policy implications and will further our understanding of the sources of economic inequality in the United States."
"Impact of Artificial Intelligence on Businesses: from Research,
  Innovation, Market Deployment to Future Shifts in Business Models","The fast pace of artificial intelligence (AI) and automation is propelling
strategists to reshape their business models. This is fostering the integration
of AI in the business processes but the consequences of this adoption are
underexplored and need attention. This paper focuses on the overall impact of
AI on businesses - from research, innovation, market deployment to future
shifts in business models. To access this overall impact, we design a
three-dimensional research model, based upon the Neo-Schumpeterian economics
and its three forces viz. innovation, knowledge, and entrepreneurship. The
first dimension deals with research and innovation in AI. In the second
dimension, we explore the influence of AI on the global market and the
strategic objectives of the businesses and finally, the third dimension
examines how AI is shaping business contexts. Additionally, the paper explores
AI implications on actors and its dark sides.","['Neha Soni', 'Enakshi Khular Sharma', 'Narotam Singh', 'Amita Kapoor']",[],0,arXiv,http://arxiv.org/abs/1905.02092v1,False,True,False,False,False,2498,Ofer Eldar,Berkeley,Active,2021,,"This research will link internal Census data to data on external, location-based designations - area-level treatments applied to particular geographic areas - to analyze area-level development and economic growth measures before and after the application of these treatments. Treatment regions will be compared to non-treatment regions. Treatments to be studied take the form of various designations as part of efforts to spur investment or access to credit. These designations have been prevalent since 1994, when the first round of the Empowerment Zone program was implemented. Empowerment Zones and similar location-based designations have existed regularly since the early 1990s and continue to be relevant today. The researchers will study the effects of various location-based designations over the last 30 years on outcomes such as the number of jobs, average earnings per worker, the number of start-up establishments, total factor productivity (TFP), and labor productivity. The researchers will investigate both localized direct effects and indirect spillovers."
Statistical Arbitrage Risk Premium by Machine Learning,"How to hedge factor risks without knowing the identities of the factors? We
first prove a general theoretical result: even if the exact set of factors
cannot be identified, any risky asset can use some portfolio of similar peer
assets to hedge against its own factor exposures. A long position of a risky
asset and a short position of a ""replicate portfolio"" of its peers represent
that asset's factor residual risk. We coin the expected return of an asset's
factor residual risk as its Statistical Arbitrage Risk Premium (SARP). The
challenge in empirically estimating SARP is finding the peers for each asset
and constructing the replicate portfolios. We use the elastic-net, a machine
learning method, to project each stock's past returns onto that of every other
stock. The resulting high-dimensional but sparse projection vector serves as
investment weights in constructing the stocks' replicate portfolios. We say a
stock has high (low) Statistical Arbitrage Risk (SAR) if it has low (high)
R-squared with its peers. The key finding is that ""unique"" stocks have both a
higher SARP and higher excess returns than ""ubiquitous"" stocks: in the
cross-section, high SAR stocks have a monthly SARP (monthly excess returns)
that is 1.101% (0.710%) greater than low SAR stocks. The average SAR across all
stocks is countercyclical. Our results are robust to controlling for various
known priced factors and characteristics.","['Raymond C. W. Leung', 'Yu-Man Tam']",[],0,arXiv,http://arxiv.org/abs/2103.09987v1,False,True,False,False,False,2508,Lois Miller,Wisconsin,Active,2020,,"Several papers in the economics literature have documented large and persistent effects associated with graduating into a recession. However, comparatively less is known about how these effects vary over college quality. If these negative effects are disproportionately concentrated among individuals graduating from worse schools, then this heterogeneity may have important implications for income inequality and mobility. By using data on college attended and time of graduation in the restricted-use National Surveys of Graduates, we propose to study how the returns to higher-quality colleges vary over the business cycle. We expect that the earnings and employment premia associated with higher college quality increase during economic downturns, with potential drivers of this result including graduates of higher-quality colleges being more able to find jobs that match well with their skills or being more willing to move to better labor markets. These results may have important policy implications and will further our understanding of socioeconomic mobility and inequality in the United States."
"Reference-dependent asset pricing with a stochastic consumption-dividend
  ratio","We study a discrete-time consumption-based capital asset pricing model under
expectations-based reference-dependent preferences. More precisely, we consider
an endowment economy populated by a representative agent who derives utility
from current consumption and from gains and losses in consumption with respect
to a forward-looking, stochastic reference point. First, we consider a general
model in which the agent's preferences include both contemporaneous gain-loss
utility, that is, utility from the difference between current consumption and
previously held expectations about current consumption, and prospective
gain-loss utility, that is, utility from the difference between intertemporal
beliefs about future consumption. A semi-closed form solution for equilibrium
asset prices is derived for this case. We then specialize to a model in which
the agent derives contemporaneous gain-loss utility only, obtaining equilibrium
asset prices in closed form. Extensive numerical experiments show that, with
plausible values of risk aversion and loss aversion, our models can generate
equity premia that match empirical estimates. Interestingly, the models turn
out to be consistent with some well-known empirical facts, namely procyclical
variation in the price-dividend ratio and countercyclical variation in the
conditional expected equity premium and in the conditional volatility of the
equity premium. Furthermore, we find that prospective gain-loss utility is
necessary for the model to predict reasonable values of the price-dividend
ratio.","['Luca De Gennaro Aquino', 'Xuedong He', 'Moris Simon Strub', 'Yuting Yang']",[],0,arXiv,http://arxiv.org/abs/2401.12856v1,False,True,False,False,False,2508,Lois Miller,Wisconsin,Active,2020,,"Several papers in the economics literature have documented large and persistent effects associated with graduating into a recession. However, comparatively less is known about how these effects vary over college quality. If these negative effects are disproportionately concentrated among individuals graduating from worse schools, then this heterogeneity may have important implications for income inequality and mobility. By using data on college attended and time of graduation in the restricted-use National Surveys of Graduates, we propose to study how the returns to higher-quality colleges vary over the business cycle. We expect that the earnings and employment premia associated with higher college quality increase during economic downturns, with potential drivers of this result including graduates of higher-quality colleges being more able to find jobs that match well with their skills or being more willing to move to better labor markets. These results may have important policy implications and will further our understanding of socioeconomic mobility and inequality in the United States."
Wealth and Poverty: The Effect of Poverty on Communities,"This paper analyzes the differences in poverty in high wealth communities and
low wealth communities. We first discuss methods of measuring poverty and
analyze the causes of individual poverty and poverty in the Bay Area. Three
cases are considered regarding relative poverty. The first two cases involve
neighborhoods in the Bay Area while the third case evaluates two neighborhoods
within the city of San Jose, CA. We find that low wealth communities have more
crime, more teen births, and more cost-burdened renters because of high
concentrations of temporary and seasonal workers, extensive regulations on
greenhouse gas emissions, minimum wage laws, and limited housing supply. In the
conclusion, we review past attempts to alleviate the effects of poverty and
give suggestions on how future policy can be influenced to eventually create a
future free of poverty.","['Merrick Wang', 'Robert Johnston']",[],0,arXiv,http://arxiv.org/abs/2010.01335v1,False,True,False,False,False,2510,Charles Varner,Stanford,Active,2021,,"California recently introduced a state supplemental Earned Income Tax Credit (EITC) with the express purpose of lowering poverty. Similarly, localities within California have increased the minimum wage in the last few years with the goal of decreasing inequality and decreasing poverty. This project seeks to understand how the California EITC and higher minimum wages impact measures of poverty and the behavioral impacts on various outcomes. These will be investigated by looking at small area poverty estimates and by looking at how the EITC and minimum wage affect poverty by those with different demographic characteristics, such as race."
"The impact of the pandemic of Covid-19 on child poverty in North
  Macedonia: Simulation-based estimates","The objective of this paper is to estimate the expected effects of the
pandemic of Covid-19 for child poverty in North Macedonia. We rely on MK-MOD
Tax & Benefit Microsimulation Model for North Macedonia based on the Survey on
Income and Living Conditions 2019. The simulation takes into account the
development of income, as per the observed developments in the first three
quarters of 2020, derived from the Labor Force Survey, which incorporates the
raw effect of the pandemic and the government response. In North Macedonia,
almost no government measure directly aimed the income of children, however,
three key and largest measures addressed household income: the wage subsidy of
14.500 MKD per worker in the hardest hit companies, relaxation of the criteria
for obtaining the guaranteed minimum income, and one-off support to vulnerable
groups of the population in two occasions. Results suggest that the relative
child poverty rate is estimated to increase from 27.8 percent before the
pandemic to 32.4 percent during the pandemic. This increase puts additional
19,000 children below the relative poverty threshold. Results further suggest
that absolute poverty is likely to reduce primarily because of the automatic
stabilizers in the case of social assistance and because of the one-time cash
assistance.",['Marjan Petreski'],[],0,arXiv,http://arxiv.org/abs/2310.05110v1,False,True,False,False,False,2510,Charles Varner,Stanford,Active,2021,,"California recently introduced a state supplemental Earned Income Tax Credit (EITC) with the express purpose of lowering poverty. Similarly, localities within California have increased the minimum wage in the last few years with the goal of decreasing inequality and decreasing poverty. This project seeks to understand how the California EITC and higher minimum wages impact measures of poverty and the behavioral impacts on various outcomes. These will be investigated by looking at small area poverty estimates and by looking at how the EITC and minimum wage affect poverty by those with different demographic characteristics, such as race."
Latin American HECAP Physics Briefing Book,"For the first time the scientific community in Latin America working at the
forefront of research in high energy, cosmology and astroparticle physics
(HECAP) have come together to discuss and provide scientific input towards the
development of a regional strategy.
  The present document, the Latin American HECAP Physics Briefing Book, is the
result of this ambitious bottom-up effort. This report contains the work
performed by the Preparatory Group to synthesize the main contributions and
discussions for each of the topical working groups. This briefing book
discusses the relevant emerging projects developing in the region and considers
potentially impactful future initiatives and participation of the Latin
American HECAP community in international flagship projects to provide the
essential input for the creation of a long-term HECAP strategy in the region.","['H. Aihara', 'A. Aranda', 'R. Camacho Toro', 'M. Cambiaso', 'M. Carena', 'E. Carrera', ""J. C. D'Olivo"", 'A. Gago', 'T. Goncalves', 'G. Herrera', 'D. Lopez Nacir', 'M. Losada', 'J. Molina', 'M. Mulders', 'D. Restrepo', 'R. Rosenfeld', 'A. Sanchez', 'F. Sanchez', 'M. Soares-Santos', 'M. Subieta', 'H. Wahlberg', 'H. Yepes Ramirez', 'A. Zerwekh']",[],0,arXiv,http://arxiv.org/abs/2104.06852v1,False,True,False,False,False,2517,John R Logan,Baruch,Active,2020,,"This project has four main components.  In one component, newly available linkages between census records in different years will be used to study two dimensions of neighborhood change: changes in relative income levels (such as gentrification and community impoverishment) and in racial/ethnic composition (such as increasing or declining diversity). The general approach using data from 2000 and 2010 is to identify the location of very person enumerated in either decennial census in each year, then analyze the composition of movers and stayers. The same database will also be used to study selective mobility into and out of neighborhoods at the individual level. The second component is to improve the estimation of tract characteristics in a given year within the boundaries of tracts as they were defined in a later year. Confidential census data sets identify people's census tract location based on the tract boundaries at the time of enumeration, but they also provide information that can be used to place them in other boundaries. This project will develop estimates for tract characteristics in 1950-2000 within 2010 tract boundaries, then extend the estimates to 2020 tract boundaries. The third component seeks to improve estimates of segregation of people across census tracts in terms of social characteristics such as education and occupational standing, similar to recent studies of income segregation.  These measures are susceptible to systematic bias associated with sample size and weighting, and the analysis will introduce methods of bias correction in order to evaluate change over time accurately.  The fourth component is small area estimation (SAE) for characteristics of individual census tracts.  We will examine Bayesian SAE models to increase reliability of point estimates of various population characteristics at the census tract scale that are based on sample data. The result will be an improved understanding of issues of sampling variability at the tract scale, methods for improving estimates, and disclosure of alternative estimates for a select set of variables."
Building Communication Skills in a Theoretical Statistics Course,"The traditional theoretical statistics course which develops the theoretical
underpinnings of the discipline (usually following a probability course) is
undergoing near-continuous revision in the statistics community. In particular,
recent versions of this course have incorporated more and more computation. We
take a look at a different aspect of the revision - building student
communication skills in the course, in both written and verbal forms, to allow
students to demonstrate their ability to explain statistical concepts. Two
separate projects are discussed, both of which were engaged in by a class of
size 17 in Spring 2015. The first project had a computational aspect (performed
using R), a statistical theory component, and a writing component, and was
based on the historical German tank problem. The second project involved a
class presentation and written report summarizing, critiquing, and/or
explaining an article selected from The American Statistician.",['Amy Wagaman'],[],0,arXiv,http://arxiv.org/abs/1612.02252v1,False,True,False,False,False,2517,John R Logan,Baruch,Active,2020,,"This project has four main components.  In one component, newly available linkages between census records in different years will be used to study two dimensions of neighborhood change: changes in relative income levels (such as gentrification and community impoverishment) and in racial/ethnic composition (such as increasing or declining diversity). The general approach using data from 2000 and 2010 is to identify the location of very person enumerated in either decennial census in each year, then analyze the composition of movers and stayers. The same database will also be used to study selective mobility into and out of neighborhoods at the individual level. The second component is to improve the estimation of tract characteristics in a given year within the boundaries of tracts as they were defined in a later year. Confidential census data sets identify people's census tract location based on the tract boundaries at the time of enumeration, but they also provide information that can be used to place them in other boundaries. This project will develop estimates for tract characteristics in 1950-2000 within 2010 tract boundaries, then extend the estimates to 2020 tract boundaries. The third component seeks to improve estimates of segregation of people across census tracts in terms of social characteristics such as education and occupational standing, similar to recent studies of income segregation.  These measures are susceptible to systematic bias associated with sample size and weighting, and the analysis will introduce methods of bias correction in order to evaluate change over time accurately.  The fourth component is small area estimation (SAE) for characteristics of individual census tracts.  We will examine Bayesian SAE models to increase reliability of point estimates of various population characteristics at the census tract scale that are based on sample data. The result will be an improved understanding of issues of sampling variability at the tract scale, methods for improving estimates, and disclosure of alternative estimates for a select set of variables."
"Twitter and Census Data Analytics to Explore Socioeconomic Factors for
  Post-COVID-19 Reopening Sentiment","Investigating and classifying sentiments of social media users towards an
item, situation, and system are very popular among the researchers. However,
they rarely discuss the underlying socioeconomic factor associations for such
sentiments. This study attempts to explore the factors associated with positive
and negative sentiments of the people about reopening the economy, in the
United States (US) amidst the COVID-19 global crisis. It takes into
consideration the situational uncertainties (i.e., changes in work and travel
pattern due to lockdown policies), economic downturn and associated trauma, and
emotional factors such as depression. To understand the sentiment of the people
about the reopening economy, Twitter data was collected, representing the 51
states including Washington DC of the US. State-wide socioeconomic
characteristics of the people, built environment data, and the number of
COVID-19 related cases were collected and integrated with Twitter data to
perform the analysis. A binary logit model was used to identify the factors
that influence people toward a positive or negative sentiment. The results from
the logit model demonstrate that family households, people with low education
levels, people in the labor force, low-income people, and people with higher
house rent are more interested in reopening the economy. In contrast,
households with a high number of members and high income are less interested to
reopen the economy. The model can correctly classify 56.18% of the sentiments.
The Pearson chi2 test indicates that overall this model has high
goodness-of-fit. This study provides a clear indication to the policymakers
where to allocate resources and what policy options they can undertake to
improve the socioeconomic situations of the people and mitigate the impacts of
pandemics in the current situation and as well as in the future.","['Md. Mokhlesur Rahman', 'G. G. Md. Nawaz Ali', 'Xue Jun Li', 'Kamal Chandra Paul', 'Peter H. J. Chong']",[],0,arXiv,http://arxiv.org/abs/2007.00054v1,False,True,False,False,False,2522,Sabrina Nasir,Irvine,Completed,2021,2024.0,"This research examines the demographic, economic and social conditions of Micronesians from the Freely Associated States who live in Hawaii. The U.S. federal government signed a Compact of Free Association [COFA] with these Micronesian nations in the 1980s and 1990s which allowed COFA citizens to live and work in the U.S. without a visa. Media accounts discuss a growing stigma against COFA citizens in Hawaii and their unique immigration status reduces their access to important social services. I ask the following questions: 1) how does the immigration status of COFA migrants inform their housing, employment, poverty, and healthcare outcomes and do these outcomes change during times of significant legislative change that affects access to social services and 2) how does the intersection of structural inequalities based on race, class, gender and migration status shape COFA migrant resource access in Hawaii? Using 1990-2020 Decennial Census and American Community Surveys, I conduct logistic regression analyses to isolate the effect of COFA status on poverty, employment and healthcare access and use the multi-group entropy index and index of net difference to examine patterns of segregation in Honolulu County. Findings support that COFA status is associated with more poverty, lower employment, less healthcare, and that these outcomes worsen over time. Additionally, COFA citizens are increasingly segregated in lower-income areas of Honolulu County. This adds to existing literature by analyzing how immigration status beyond the undocumented/citizen binary influences degrees of exclusion which speaks to broader research on how immigration status affects immigrant incorporation."
"Simultaneous Corn and Soybean Yield Prediction from Remote Sensing Data
  Using Deep Transfer Learning","Large-scale crop yield estimation is, in part, made possible due to the
availability of remote sensing data allowing for the continuous monitoring of
crops throughout their growth cycle. Having this information allows
stakeholders the ability to make real-time decisions to maximize yield
potential. Although various models exist that predict yield from remote sensing
data, there currently does not exist an approach that can estimate yield for
multiple crops simultaneously, and thus leads to more accurate predictions. A
model that predicts the yield of multiple crops and concurrently considers the
interaction between multiple crop yields. We propose a new convolutional neural
network model called YieldNet which utilizes a novel deep learning framework
that uses transfer learning between corn and soybean yield predictions by
sharing the weights of the backbone feature extractor. Additionally, to
consider the multi-target response variable, we propose a new loss function. We
conduct our experiment using data from 1,132 counties for corn and 1,076
counties for soybean across the United States. Numerical results demonstrate
that our proposed method accurately predicts corn and soybean yield from one to
four months before the harvest with a MAE being 8.74% and 8.70% of the average
yield, respectively, and is competitive to other state-of-the-art approaches.","['Saeed Khaki', 'Hieu Pham', 'Lizhi Wang']",[],0,arXiv,http://arxiv.org/abs/2012.03129v3,False,True,False,False,False,2529,Abraham Flaxman,Seattle,Active,2022,,"Vision loss and blindness are global public health concerns. In 2015, an estimated 216.6 million people had moderate to severe visual impairment and 36 million people were blind. In the United States specifically, in 2017, it has been estimated that 7.73 million Americans, 2.37 percent of the total population, are currently living with some form of uncorrectable vision loss in their better-seeing eye and 1.10 million, 0.34% of the total population, are blind. However, there are still substantial gaps in knowledge regarding the burden of vision loss and blindness in the U.S. The purpose of this study is to use vision loss data from the American Community Survey (ACS) to produce model-based, county-level estimates of the prevalence of vision loss and blindness in the U.S. stratified by sex, age, race/ethnicity and group quarters to highlight variation across the country. With more detailed and granular knowledge of the burden of vision loss and blindness, more evidence-based public health policy decisions can be made."
Automating Seccomp Filter Generation for Linux Applications,"Software vulnerabilities in applications undermine the security of
applications. By blocking unused functionality, the impact of potential
exploits can be reduced. While seccomp provides a solution for filtering
syscalls, it requires manual implementation of filter rules for each individual
application. Recent work has investigated automated approaches for detecting
and installing the necessary filter rules. However, as we show, these
approaches make assumptions that are not necessary or require overly
time-consuming analysis.
  In this paper, we propose Chestnut, an automated approach for generating
strict syscall filters for Linux userspace applications with lower requirements
and limitations. Chestnut comprises two phases, with the first phase consisting
of two static components, i.e., a compiler and a binary analyzer, that extract
the used syscalls during compilation or in an analysis of the binary. The
compiler-based approach of Chestnut is up to factor 73 faster than previous
approaches without affecting the accuracy adversely. On the binary analysis
level, we demonstrate that the requirement of position-independent binaries of
related work is not needed, enlarging the set of applications for which
Chestnut is usable. In an optional second phase, Chestnut provides a dynamic
refinement tool that allows restricting the set of allowed syscalls further. We
demonstrate that Chestnut on average blocks 302 syscalls (86.5%) via the
compiler and 288 (82.5%) using the binary-level analysis on a set of 18 widely
used applications. We found that Chestnut blocks the dangerous exec syscall in
50% and 77.7% of the tested applications using the compiler- and binary-based
approach, respectively. For the tested applications, Chestnut prevents
exploitation of more than 62% of the 175 CVEs that target the kernel via
syscalls. Finally, we perform a 6 month long-term study of a sandboxed Nginx
server.","['Claudio Canella', 'Mario Werner', 'Daniel Gruss', 'Michael Schwarz']",[],0,arXiv,http://arxiv.org/abs/2012.02554v1,False,True,False,False,False,2540,Joe Chestnut,Colorado,Active,2021,,"The majority of research on job accessibility is limited by the Modifiable Areal Unit Problem (MAUP), where a reliance on aggregate data obscures important differences in commuting times that vary at the sub-block level. This is especially true for commutes made using public transit, where walking makes up a substantial portion of commutes. We propose to improve estimates of commuting time and accessibility by worker income and industry, as well as estimating the extent of the MAUP in comparable studies, by using precise origin-destination pairs derived from linked American Community Survey and Longitudinal Employer-Household Dynamics data. We supplement these data with detailed, external information about transit networks and associated travel times.  Results will yield improved estimates of job accessibility by income, industry, and time of day for the nation as a whole as well as for several different city and county typologies related to transit availability and commuting patterns."
Tracking the $\ell_2$ Norm with Constant Update Time,"The \emph{$\ell_2$ tracking problem} is the task of obtaining a streaming
algorithm that, given access to a stream of items $a_1,a_2,a_3,\ldots$ from a
universe $[n]$, outputs at each time $t$ an estimate to the $\ell_2$ norm of
the \textit{frequency vector} $f^{(t)}\in \mathbb{R}^n$ (where $f^{(t)}_i$ is
the number of occurrences of item $i$ in the stream up to time $t$). The
previous work [Braverman-Chestnut-Ivkin-Nelson-Wang-Woodruff, PODS 2017] gave
an streaming algorithm with (the optimal) space using
$O(\epsilon^{-2}\log(1/\delta))$ words and $O(\epsilon^{-2}\log(1/\delta))$
update time to obtain an $\epsilon$-accurate estimate with probability at least
$1-\delta$. We give the first algorithm that achieves update time of $O(\log
1/\delta)$ which is independent of the accuracy parameter $\epsilon$, together
with the nearly optimal space using $O(\epsilon^{-2}\log(1/\delta))$ words. Our
algorithm is obtained using the \textsf{CountSketch} of
[Charilkar-Chen-Farach-Colton, ICALP 2002].","['Chi-Ning Chou', 'Zhixian Lei', 'Preetum Nakkiran']",[],0,arXiv,http://arxiv.org/abs/1807.06479v3,False,True,False,False,False,2540,Joe Chestnut,Colorado,Active,2021,,"The majority of research on job accessibility is limited by the Modifiable Areal Unit Problem (MAUP), where a reliance on aggregate data obscures important differences in commuting times that vary at the sub-block level. This is especially true for commutes made using public transit, where walking makes up a substantial portion of commutes. We propose to improve estimates of commuting time and accessibility by worker income and industry, as well as estimating the extent of the MAUP in comparable studies, by using precise origin-destination pairs derived from linked American Community Survey and Longitudinal Employer-Household Dynamics data. We supplement these data with detailed, external information about transit networks and associated travel times.  Results will yield improved estimates of job accessibility by income, industry, and time of day for the nation as a whole as well as for several different city and county typologies related to transit availability and commuting patterns."
An Australian DER Bill of Rights and Responsibilities,"Australia's world-leading penetration of distributed solar photovoltaics (PV)
is now impacting power system security and as a result how customers can use
and export their own PV-generated energy. Several programs of Australian
regulatory reform for distributed energy resources (DER) have emphasised the
importance of placing consumers at the centre of any energy transition, but
this has occurred against a haphazard backdrop of proposals for solar export
taxes, updated inverter standards, and diminishing feed-in-tariffs. Absent from
the discussion is a coherent espousal of reasonable consumer expectations with
practical technical definitions of how these may be applied. Whilst American
legislation has enshrined initial rights to connect PV, they do not consider
the evolution of rights in a DER-dominated future. This paper proposes a first
attempt at a 'DER Bill of Rights and Responsibilities' for both passive and
active participation in energy markets, using Australia as an example
jurisdiction. The intent is that clarity on rights to self-consumption and
passive participation will support customer trust in the guiderails of active
DER integration and control. Guiding principles are presented with practical
definitions referencing existing instruments including inverter standards,
network connection agreements, the reliability standard, and central ancillary
service markets. We highlight how these proposed rights are already being
breached regularly in Australia, before outlining a pathway to enshrine them
for a DER-dominated future with broad sector endorsement. These questions are
critical for Australia to address now; it is likely other countries will be
required to do so in the near future.","['Niraj Lal', 'Lee Brown']",[],0,arXiv,http://arxiv.org/abs/2112.04855v4,False,True,False,False,False,2547,Celeste Carruthers,Atlanta,Active,2022,,"Among the leading explanations for Black income convergence in the 20th century U.S., the improved quality of southern schools and a suite of Civil Rights era employment protections are dominant. We assess the impact of each change, alone and in concert, by following Americans born between 1902 and 1932 through their work lives. Our earlier work indicates that poor school quality was the proximate cause of income inequality for this group at labor market entry in 1940. We merge school quality data from the segregated pre-1940 South with restricted access Census and Social Security earnings data to investigate the long-term legacy of separate and unequal schooling. We take advantage of cross-cohort and geographic variability in the quality of pre-War Black schools, along with teacher salary schedules and Rosenwald school-building campaigns that drove some of that variation quasi-experimentally, to uncover causal effects of school quality on the shape of long-term age-earnings profiles.

Since the ""long term"" for these cohorts overlapped with the Civil Rights era, we can additionally explore whether there were corrective effects of Civil Rights era legislation, in part by measuring whether the race-specific and race-by-gender-specific returns to school quality and experience changed after the mid-1960s. 

Specific requested data files include the Current Population Survey Annual Social and Economic Supplement (1973 in full and 1979, 1981-1985 if and when available); the corresponding extract of the Social Security Administration's Summary Earnings Record (SSA-SER); the Census Numident File; and the 1940 Census of Population."
"The pulse of the city through Twitter: relationships between land use
  and spatiotemporal demographics","Social network data offer interesting opportunities in urban studies. In this
study, we used Twitter data to analyse city dynamics over the course of the
day. Users of this social network were grouped according to city zone and time
slot in order to analyse the daily dynamics of the city and the relationship
between this and land use. First, daytime activity in each zone was compared
with activity at night in order to determine which zones showed increased
activity in each of the time slots. Then, typical Twitter activity profiles
were obtained based on the predominant land use in each zone, indicating how
land uses linked to activities were activated during the day, but at different
rates depending on the type of land use. Lastly, a multiple regression analysis
was performed to determine the influence of the different land uses on each of
the major time slots (morning, afternoon, evening and night) through their
changing coefficients. Activity tended to decrease throughout the day for most
land uses (e.g. offices, education, health and transport), but remained
constant in parks and increased in retail and residential zones. Our results
show that social network data can be used to improve our understanding of the
link between land use and urban dynamics.","['Juan Carlos Garcia-Palomares', 'Maria Henar Salas-Olmedo', 'Borja Moya-Gomez', 'Ana Condeco-Melhorado', 'Javier Gutierrrez']",[],0,arXiv,http://arxiv.org/abs/1705.07956v1,False,True,False,False,False,2550,Rachel Meltzer,Boston,Active,2021,,"This project seeks to understand how the rise of the Internet and e-commerce changes the physical location and clustering of the establishments in which in-person consumer purchases take place.  We are interested in establishments in the ""customer-facing"" sector, by which we mean both traditional in-person retail, as well as in-person food and in-person personal services. The once-in-a-generation technological change brought by the Internet fundamentally alters the frictions consumers face in searching for goods. Specifically, we seek to answer three main research questions: (1) Since consumption search and matching costs, rather than firm input costs, play a more prominent role in location for customer-facing firms, how do patterns of customer-facing agglomeration differ from those for manufacturing?; (2) Has the rise of the Internet caused a change in the spatial pattern of customer-facing agglomeration and establishment co-location, especially in urban settings?; and (3) Has the rise of the Internet changed the fundamental building blocks of cities - land use and property value?  

We answer these questions using establishment-specific location data from the Census Bureau, combined with administrative property data and proprietary CoStar lease level data for the two largest metropolitan areas in the United States: New York and Los Angeles. The bedrock of this analysis is the Census restricted establishment data (LBD/ILBD), which allow us to understand patterns of establishment location over a longer period and at greater geographic specificity than any work to date.  Supplementary restricted Census data, such as the CRT, CSR, CFI, BES, and SAS, provide important details on the industrial, economic and organizational features of establishments, which can mediate and help to explain the hypothesized changes in the spatial organization of those customer-facing establishments."
"FUSU: A Multi-temporal-source Land Use Change Segmentation Dataset for
  Fine-grained Urban Semantic Understanding","Fine urban change segmentation using multi-temporal remote sensing images is
essential for understanding human-environment interactions in urban areas.
Although there have been advances in high-quality land cover datasets that
reveal the physical features of urban landscapes, the lack of fine-grained land
use datasets hinders a deeper understanding of how human activities are
distributed across the landscape and the impact of these activities on the
environment, thus constraining proper technique development. To address this,
we introduce FUSU, the first fine-grained land use change segmentation dataset
for Fine-grained Urban Semantic Understanding. FUSU features the most detailed
land use classification system to date, with 17 classes and 30 billion pixels
of annotations. It includes bi-temporal high-resolution satellite images with
0.2-0.5 m ground sample distance and monthly optical and radar satellite time
series, covering 847 km^2 across five urban areas in the southern and northern
of China with different geographical features. The fine-grained land use
pixel-wise annotations and high spatial-temporal resolution data provide a
robust foundation for developing proper deep learning models to provide
contextual insights on human activities and urbanization. To fully leverage
FUSU, we propose a unified time-series architecture for both change detection
and segmentation. We benchmark FUSU on various methods for several tasks.
Dataset and code are available at: https://github.com/yuanshuai0914/FUSU.","['Shuai Yuan', 'Guancong Lin', 'Lixian Zhang', 'Runmin Dong', 'Jinxiao Zhang', 'Shuang Chen', 'Juepeng Zheng', 'Jie Wang', 'Haohuan Fu']",[],0,arXiv,http://arxiv.org/abs/2405.19055v3,False,True,False,False,False,2550,Rachel Meltzer,Boston,Active,2021,,"This project seeks to understand how the rise of the Internet and e-commerce changes the physical location and clustering of the establishments in which in-person consumer purchases take place.  We are interested in establishments in the ""customer-facing"" sector, by which we mean both traditional in-person retail, as well as in-person food and in-person personal services. The once-in-a-generation technological change brought by the Internet fundamentally alters the frictions consumers face in searching for goods. Specifically, we seek to answer three main research questions: (1) Since consumption search and matching costs, rather than firm input costs, play a more prominent role in location for customer-facing firms, how do patterns of customer-facing agglomeration differ from those for manufacturing?; (2) Has the rise of the Internet caused a change in the spatial pattern of customer-facing agglomeration and establishment co-location, especially in urban settings?; and (3) Has the rise of the Internet changed the fundamental building blocks of cities - land use and property value?  

We answer these questions using establishment-specific location data from the Census Bureau, combined with administrative property data and proprietary CoStar lease level data for the two largest metropolitan areas in the United States: New York and Los Angeles. The bedrock of this analysis is the Census restricted establishment data (LBD/ILBD), which allow us to understand patterns of establishment location over a longer period and at greater geographic specificity than any work to date.  Supplementary restricted Census data, such as the CRT, CSR, CFI, BES, and SAS, provide important details on the industrial, economic and organizational features of establishments, which can mediate and help to explain the hypothesized changes in the spatial organization of those customer-facing establishments."
Rural Healthcare Access and Supply Constraints: A Causal Analysis,"Certificate-of-need (CON) laws require that healthcare providers receive
approval from a state board before offering additional services in a given
community. Proponents of CON laws claim that these laws are needed to prevent
the oversupply of healthcare services in urban areas and to increase access in
rural areas, which are predominantly underserved. Yet, the policy could lower
rural access if used by incumbents to limit entry from competitors. We explore
the repeal of these regulations in five U.S. states to offer the first estimate
of the causal effects of CON laws on rural and urban healthcare access. We find
that repealing CON laws causes a substantial increase in hospitals in both
rural and urban areas. We also find that the repeal leads to fewer beds and
smaller hospitals on average, suggesting an increase in entry and competition
in both rural and urban areas.","['Vitor Melo', 'Liam Sigaud', 'Elijah Neilson', 'Markus Bjoerkheim']",[],0,arXiv,http://arxiv.org/abs/2405.08168v1,False,True,False,False,False,2552,Alison Davis,Kentucky,Active,2021,,"Hospitals are an important driver of local economic growth in rural communities. Hospitals, and other healthcare facilities, help communities maintain a high quality of life and a healthy workforce, which are important factors for firms choosing a location. In this project, we will describe changes in hospital markets and how those changes affect economic growth, particularly in rural communities. Using data from the Longitudinal Business Database, the Integrated Longitudinal Business Database, and the County Business Patterns Business Register from 1987 through 2021 as available, we will examine the short-term and long-term effects that hospital entries and exits have on the number of establishments located in a community and the level of employment and payroll for both non-hospital health care sectors and non-health care sectors. We will also use a double hurdle model to estimate the impact that a hospital has on firm location decisions. This model allows us to measure both the probability that firms would not locate in a community without a hospital and the probability that firms would not locate in the community even if a hospital were present. Estimates of probabilities will be provided for the each 2-digit NAICS code."
"A Pathophysiological Model-Driven Communication for Dynamic Distributed
  Medical Best Practice Guidance Systems","There is a great divide between rural and urban areas, particularly in
medical emergency care. Although medical best practice guidelines exist in
hospital handbooks, they are often lengthy and difficult to apply clinically.
The challenges are exaggerated for doctors in rural areas and emergency medical
technicians (EMT) during patient transport.
  In this paper, we propose the concept of distributed executable medical best
practice guidance systems to assist adherence to best practice from the time
that a patient first presents at a rural hospital, through diagnosis and
ambulance transfer to arrival and treatment at a regional tertiary hospital
center. We codify complex medical knowledge in the form of simplified
distributed executable disease automata, from the thin automata at rural
hospitals to the rich automata in the regional center hospitals. However, a
main challenge is how to efficiently and safely synchronize distributed best
practice models as the communication among medical facilities, devices, and
professionals generates a large number of messages. This complex problem of
patient diagnosis and transport from rural to center facility is also fraught
with many uncertainties and changes resulting in a high degree of dynamism. To
address this situation, we propose a pathophysiological model-driven message
exchange communication architecture that ensures the real-time and dynamic
requirements of synchronization among distributed emergency best-practice
models are met in a reliable and safe manner. Taking the signs, symptoms, and
progress of stroke patients transported across a geographically distributed
healthcare network as the motivating use case, we implement our communication
system and apply it to our developed best practice automata using laboratory
simulations. Our proof-of-concept experiments shows there is potential for the
use of our system in a wide variety of domains.","['Mohammad Hosseini', 'Yu Jiang', 'Poliang Wu', 'Richard B. Berlin Jr.', 'Shangping Ren', 'Lui Sha']",[],0,arXiv,http://arxiv.org/abs/1608.04661v1,False,True,False,False,False,2552,Alison Davis,Kentucky,Active,2021,,"Hospitals are an important driver of local economic growth in rural communities. Hospitals, and other healthcare facilities, help communities maintain a high quality of life and a healthy workforce, which are important factors for firms choosing a location. In this project, we will describe changes in hospital markets and how those changes affect economic growth, particularly in rural communities. Using data from the Longitudinal Business Database, the Integrated Longitudinal Business Database, and the County Business Patterns Business Register from 1987 through 2021 as available, we will examine the short-term and long-term effects that hospital entries and exits have on the number of establishments located in a community and the level of employment and payroll for both non-hospital health care sectors and non-health care sectors. We will also use a double hurdle model to estimate the impact that a hospital has on firm location decisions. This model allows us to measure both the probability that firms would not locate in a community without a hospital and the probability that firms would not locate in the community even if a hospital were present. Estimates of probabilities will be provided for the each 2-digit NAICS code."
Measuring Gender Bias in Educational Videos: A Case Study on YouTube,"Students are increasingly using online materials to learn new subjects or to
supplement their learning process in educational institutions. Issues regarding
gender bias have been raised in the context of formal education and some
measures have been proposed to mitigate them. However, online educational
materials in terms of possible gender bias and stereotypes which may appear in
different forms are yet to be investigated in the context of search bias in a
widely-used search platform. As a first step towards measuring possible gender
bias in online platforms, we have investigated YouTube educational videos in
terms of the perceived gender of their narrators. We adopted bias measures for
ranked search results to evaluate educational videos returned by YouTube in
response to queries related to STEM (Science, Technology, Engineering, and
Mathematics) and NON-STEM fields of education. Gender is a research area by
itself in social sciences which is beyond the scope of this work. In this
respect, for annotating the perceived gender of the narrator of an
instructional video we used only a crude classification of gender into Male,
and Female. Then, for analysing perceived gender bias we utilised bias measures
that have been inspired by search platforms and further incorporated rank
information into our analysis. Our preliminary results demonstrate that there
is a significant bias towards the male gender on the returned YouTube
educational videos, and the degree of bias varies when we compare STEM and
NON-STEM queries. Finally, there is a strong evidence that rank information
might affect the results.","['Gizem Gezici', 'Yucel Saygin']",[],0,arXiv,http://arxiv.org/abs/2206.09987v1,False,True,False,False,False,2565,Mufaddal H Baxamusa,Minnesota,Active,2021,,"I will investigate the question: Does a more educated workforce in a metropolitan statistical area (MSA)  enhances the effectiveness of R&D spending by companies in that location and lead to improved firm performance.? The answer to this question has very different implications.  If a more educated workforce leads to increased R&D, then locations (cities) may need to attract a more educated workforce by, for example, investing in local universities.  On the other hand, if more R&D leads to a more educated workforce, then locations may need to invest in R&D tax credits or other incentives for companies.  The major datasets needed for this research are:  Business Research & Development and Innovation Survey (BRDIS), Longitudinal Business Database,  Census of Manufactures (CMF), Annual Survey of Manufactures (ASM).  This project will develop measures of accuracy and possible sources of error in the BRDIS survey. To accomplish this task, an algorithm will be written that will use the establishment's industry code to classify firms as single R&D locations, multiple R&D locations, or un-categorized R&D location. I expect to find that R&D expenditures has a positive relationship to the level of education of the workforce in that location. Furthermore, I expect to find that productivity, stock returns and cash flows of firms located in areas with a more educated workforce perform better because of R&D."
Mukai bundles on Fano threefolds,"We give a proof of Mukai's Theorem on the existence of certain exceptional
vector bundles on prime Fano threefolds. To our knowledge this is the first
complete proof in the literature. The result is essential for Mukai's biregular
classification of prime Fano threefolds, and for the existence of
semiorthogonal decompositions in their derived categories.
  Our approach is based on Lazarsfeld's construction that produces vector
bundles on a variety from globally generated line bundles on a divisor, on
Mukai's theory of stable vector bundles on K3 surfaces, and on Brill--Noether
properties of curves and (in the sense of Mukai) of K3 surfaces.","['Arend Bayer', 'Alexander Kuznetsov', 'Emanuele Macrì']",[],0,arXiv,http://arxiv.org/abs/2402.07154v1,False,True,False,False,False,2568,Patrick J Bayer,Triangle,Active,2021,,"This research will use the U.S. Census Bureau's Decennial Census and American Community Survey (ACS) data to examine residential location decisions and the extent to which choice set constraints driven by market size affect the ability of households to choose their preferred bundle of household and neighborhood characteristics when choosing where to live. The analysis will be conducted by jointly estimating a structural model of residential choice decisions across several housing markets of varying sizes, yielding households' preferences for particular residential characteristics (e.g., characteristics of the house, neighborhood, and local public goods) that comprise the residential bundle.  Restricting analysis to markets for which residential preferences are similar conditional on household type, the analysis then characterizes the welfare costs of choice set constraints by calculating households' implied willingness-to-pay (WTP) for the predicted residential choices that could instead be obtained in a market of a different size. Additional counterfactual simulations will consider how the consumption of particular amenities would change in a market if bundling frictions are relaxed by de-linking the consumption of some amenities from the choice of residence."
Bring your friend! Real or virtual?,"A monopolist faces a partially uninformed population of consumers,
interconnected through a directed social network. In the network, the
monopolist offers rewards to informed consumers (influencers) conditional on
informing uninformed consumers (influenced). Rewards are needed to bear a
communication cost. We investigate the incentives for the monopolist to move to
a denser network and the impact of this decision on social welfare. Social
welfare increases in information diffusion which, for given communication
incentives, is higher in denser networks. However, the monopolist internalizes
transfers and thus may prefer an environment with less competition between
informed consumers. The presence of highly connected influencers (hubs) is the
main driver that aligns monopolist incentives and welfare.","['Elias Carroni', 'Paolo Pin', 'Simone Righi']",[],0,arXiv,http://arxiv.org/abs/1710.08693v1,False,True,False,False,False,2570,Xavier A Giroud,Baruch,Active,2021,,"In this project, the researchers will explore whether an increase in market power across industries, as measured by markups at the firm and establishment level has led to an increase in income inequality at the county, MSA, and Census tract level. The research will examine the impact of competitiveness of industries on labor movement across establishments (reallocation, hiring, and firing rates), profits of firms, changes in the patterns of sources of financing and inputs for production, and pollution at the firm and establishment level. Moreover, this project will characterize how the impact on these socio-economic outcomes varies depending on firm-, establishment-, and worker-level characteristics, and will further examine the implications for productivity and economic growth. To measure competition across industries, the researchers will also use other measures such as the Herfindahl-Hirschman Index (HHI), the number of establishments per square mile, and the number of firms per square mile, among others."
Radial analysis and scaling of housing prices in French urban areas,"Urban scaling laws summarize how urban attributes evolve with city size.
Recent criticism questions notably the aggregate view of this approach, which
leads to neglecting the internal structure of cities. This is all the more
relevant for housing prices due to their important variations across space.
Based on a dataset compiling millions of real estate transactions over the
period 2017-2021, we investigate the regularities of the radial
(center-periphery) profiles of housing prices across cities, with respect to
their size. Results are threefold. First, they corroborate prior findings in
the urban scaling literature stating that largest cities agglomerate higher
housing prices. Second, we find that housing price radial profiles scale in
three dimensions with the power 1/5 of city population. After rescaling, great
regularities between radial profiles can be observed, although some locational
amenities have a significant impact on prices. Third, it appears that our
rescaled profiles approach fails to explain housing price variations in the
city center across cities. In fact, prices near the city center rise much
faster with city size than those in the periphery. This has strong implications
for low-income households seeking homeownership, because prohibitive prices in
the center may contribute to pushing them out into peripheral locations.","['Gaëtan Laziou', 'Rémi Lemoy', 'Marion Le Texier']",[],0,arXiv,http://arxiv.org/abs/2405.14503v1,False,True,False,False,False,2582,Ning Zhang,Penn State,Completed,2021,2022.0,"This paper studies the effect of U.S. Housing Choice Voucher Program Section 8 on low income people's labor supply, family formation and homeownership. I analyze this issue using data from 2014 Panel and 2018 Panel of the restricted-use Survey of Income and Program Participation (SIPP). My economic approach is to use the policy assigning housing vouchers based on an income cutoff as an instrument to study the effect of housing vouchers on low-income people's employment, family formation and homeownership. The assignment policy states that households with income lower than 50% of the median income for the MSA/county area are eligible for housing vouchers. In order to infer the household eligibility status, I need the household MSA/county code, which is accessible through the SIPP restricted-use data. With household eligibility status, I compare the households whose incomes are slightly below the income cutoff (eligible households) with households whose incomes are slightly above the income cutoff (ineligible households) to identify the effect of housing vouchers on employment, family formation and homeownership. This project will contribute to understanding the effect of Section 8 Housing Voucher on low-income households' labor supply, family formation and homeownership decisions as well as the welfare implication of such housing assistance programs on low income households.  "
"First order formalism for thick branes in modified gravity with Lagrange
  multiplier","This work discuss the construction of braneworld solutions in modified
gravity with Lagrange multipliers. We examine the general aspects of the model
and present a first order formalism that help us to find analytic solutions of
the equations of motion. We also investigate some explicit models, analyse
linear stability of the metric and comment on how to relate models investigated
in other works to the ones examined in the present study.","['D. Bazeia', 'D. A. Ferreira', 'D. C. Moreira']",[],0,arXiv,http://arxiv.org/abs/2002.00229v2,False,True,False,False,False,2586,Sara Patricia Ferreira Moreira,Chicago,Active,2022,,"This project examines how the distribution of firm size in the economy responds to shocks and other changes in the economic environment. Using the Longitudinal Business Database, along with other Census Bureau micro-level data that cover nearly the universe of firms operating in the U.S. since the late 1970s, we provide evidence on how important are differences in firm size in explaining heterogeneity in how firms respond to long-term demand shocks. The analysis emphasizes the role of firms' scope - the number of locations, establishments, product lines, or products operated by a firm - in this heterogeneous response. Motivated by the findings in the data, we propose a theory of firm size, where both scope and productivity are chosen endogenously."
"On the finite space blow up of the solutions of the Swift-Hohenberg
  equation","The aim of this paper is to study the finite space blow up of the solutions
for a class of fourth order differential equations. Our results answer a
conjecture in [F. Gazzola and R. Pavani. Wide oscillation finite time blow up
for solutions to nonlinear fourth order differential equations. Arch. Ration.
Mech. Anal., 207(2):717--752, 2013] and they have implications on the
nonexistence of beam oscillation given by traveling wave profile at low speed
propagation.","['Vanderley Ferreira Jr', 'Ederson Moreira dos Santos']",[],0,arXiv,http://arxiv.org/abs/1408.3436v1,False,True,False,False,False,2586,Sara Patricia Ferreira Moreira,Chicago,Active,2022,,"This project examines how the distribution of firm size in the economy responds to shocks and other changes in the economic environment. Using the Longitudinal Business Database, along with other Census Bureau micro-level data that cover nearly the universe of firms operating in the U.S. since the late 1970s, we provide evidence on how important are differences in firm size in explaining heterogeneity in how firms respond to long-term demand shocks. The analysis emphasizes the role of firms' scope - the number of locations, establishments, product lines, or products operated by a firm - in this heterogeneous response. Motivated by the findings in the data, we propose a theory of firm size, where both scope and productivity are chosen endogenously."
The parenthood effect in urban mobility,"The modelling of human mobility is vital for the understanding of the
complexity of urban dynamics and guiding effective interventions to improve
quality of life. Traditional modelling approaches focus on `average citizens,'
which overlook the multitude of experiences from distinct sociodemographic
groups. Recent studies have unveiled significant variations in mobility
patterns related to gender and socioeconomic status, yet the impact of
parenthood remains under-explored. Parenthood brings profound changes to daily
routines, influenced by factors such as increased caregiving responsibilities,
altered work-life balance, and the need for family-friendly environments.
Parents often prioritise considerations such as cost of living, social
wellbeing, environmental quality, and safety. Quantifying how `friendly' a city
is becomes more and more important for parents, especially in the context of
rising remote work opportunities which, in turn, reverberate on the choices on
where to settle. This work investigates whether these considerations lead to
distinct mobility patterns between parents and non-parents, also accounting for
the impact of partnership. Using extensive census data across American cities,
we analyse how parenthood and partnership reshape their urban experiences. Our
findings indicate that cities can indeed be classified by their level of
friendliness towards parents and partners. For example, Dallas and Nashville
can be more suited for single individuals, New York and Chicago can be more
accommodating to parents, while Washington and Baltimore favour married people.
These insights contribute to the growing body of research advocating for more
nuanced and equitable urban planning. By recognising the diverse needs of
different demographic groups, particularly parents, our study underscores the
importance of tailored urban design strategies over universal solutions.","['Mariana Macedo', 'Ronaldo Menezes', 'Alessio Cardillo']",[],0,arXiv,http://arxiv.org/abs/2501.02299v1,False,True,False,False,False,2593,Brenden D Timpe,Nebraska,Active,2022,,"Social scientists have long observed that labor-market outcomes and the accumulation of human capital are closely related to decisions about fertility. Yet much is still unknown about the way families balance the responsibilities of child-bearing, child-rearing, and career planning. Less still is known about the potential spillover effects on firms, co-workers, and social networks. This project will enhance Census data products and advance the academic literature by using Census household roster data to link family members to one another, allowing information on household relationships to be incorporated in analyses using the LEHD and other large-scale administrative data. We will use these linked data to study the relationship between child-bearing and the demographic, social, and economic characteristics of parents, their co-workers, and firms."
"Enrollment Forecast for Clinical Trials at the Portfolio Planning Phase
  Based on Site-Level Historical Data","Accurate forecast of a clinical trial enrollment timeline at the planning
phase is of great importance to both corporate strategic planning and trial
operational excellence. While predictions of key milestones such as last
subject first dose date can inform strategic decision-making, detailed
predictive insights (e.g., median number of enrolled subjects by month for a
country) can facilitate the planning of clinical trial operation activities and
promote execution excellence. The naive approach often calculates an average
enrollment rate from historical data and generates an inaccurate prediction
based on a linear trend with the average rate. The traditional statistical
approach utilizes the simple Poisson-Gamma model that assumes time-invariant
site activation rates and it can fail to capture the underlying nonlinear
patterns (e.g., up and down site activation pattern). We present a novel
statistical approach based on generalized linear mixed-effects models and the
use of non-homogeneous Poisson processes through Bayesian framework to model
the country initiation, site activation and subject enrollment sequentially in
a systematic fashion. We validate the performance of our proposed enrollment
modeling framework based on a set of preselected 25 studies from four
therapeutic areas. Our modeling framework shows a substantial improvement in
prediction accuracy in comparison to the traditional statistical approach.
Furthermore, we show that our modeling and simulation approach calibrates the
data variability appropriately and gives correct coverage rates for prediction
intervals of various nominal levels. Finally, we demonstrate the use of our
approach to generate the predicted enrollment curves through time with
confidence bands overlaid.","['Sheng Zhong', 'Yunzhao Xing', 'Mengjia Yu', 'Li Wang']",[],0,arXiv,http://arxiv.org/abs/2301.01351v1,False,True,False,False,False,2600,Andrea Wysocki,Chicago,Completed,2021,2023.0,"This project aims to fill gaps in knowledge about Medicare Savings Programs (MSPs) by producing estimates of trends in the MSP participation rate, characteristics of the population eligible for and enrolled in MSPs, including information about out-of-pocket (OOP) spending, and examining the effect of a key policy change that aimed to increase MSP participation. We require non-public Census Bureau data to complete this project including the 2006-2017 Survey of Income and Program Participation (SIPP), 2006-2017 Medicare Enrollment DataBase (EDB), and 2006-2017 Medicaid Statistical Information System (MSIS) data. The SIPP data contain a large sample with accurate information on income, assets, marital status, and state of residence needed to simulate most criteria to identify the MSP-eligible population. The Medicare EDB and Medicaid MSIS data are needed to link to the SIPP to enable estimation of the final MSP-eligible population based on eligibility for Medicare Part A and to identify the MSP-enrolled population. Without the ability to link the non-public SIPP, EDB, and MSIS data sources, we would be unable to identify who is enrolled in MSPs to address any of the research questions of interest."
Measurement Models For Sailboats Price vs. Features And Regional Areas,"In this study, we investigated the relationship between sailboat technical
specifications and their prices, as well as regional pricing influences.
Utilizing a dataset encompassing characteristics like length, beam, draft,
displacement, sail area, and waterline, we applied multiple machine learning
models to predict sailboat prices. The gradient descent model demonstrated
superior performance, producing the lowest MSE and MAE. Our analysis revealed
that monohulled boats are generally more affordable than catamarans, and that
certain specifications such as length, beam, displacement, and sail area
directly correlate with higher prices. Interestingly, lower draft was
associated with higher listing prices. We also explored regional price
determinants and found that the United States tops the list in average sailboat
prices, followed by Europe, Hong Kong, and the Caribbean. Contrary to our
initial hypothesis, a country's GDP showed no direct correlation with sailboat
prices. Utilizing a 50% cross-validation method, our models yielded consistent
results across test groups. Our research offers a machine learning-enhanced
perspective on sailboat pricing, aiding prospective buyers in making informed
decisions.","['Jiaqi Weng', 'Chunlin Feng', 'Yihan Shao']",[],0,arXiv,http://arxiv.org/abs/2309.14994v1,False,True,False,False,False,2601,Jing Deng,Georgetown,Active,2021,,"Studies suggest that clusters, or the geographic concentration of firms in the same or related industries, facilitate entrepreneurship. Less understood is why some startups founded in clusters relocate out of clusters, and how the timing and destination of relocation impacts economic success. This study combines external data on startup funding and founder characteristics with the Longitudinal Business Register, Longitudinal Employer Household Dynamics, and Census data on commercial innovation to determine when and why startups relocate, how this impacts their workforce, and whether relocation affects startup performance. Results will shed light on previously unclear factors tied to the success or failure of startups, both at the company and worker-level, particularly in the tech industry."
Obamacare and a Fix for the IRS Iteration,"We model the quantities appearing in Internal Revenue Service (IRS) tax
guidance for calculating the health insurance premium tax credit created by the
Patient Protection and Affordable Care Act, also called Obamacare. We ask the
question of whether there is a procedure, computable by hand, which can
calculate the appropriate premium tax credit for any household with
self-employment income. We motivate current IRS tax guidance, which has had
self-employed taxpayers use a fixed point iteration to calculate their premium
tax credits since 2014. Then, we give an example showing that the IRS iteration
can lead to a divergent sequence of iterates. As a consequence, IRS guidance
does not calculate appropriate premium tax credits for tax returns in certain
income intervals, adversely affecting eligible beneficiaries. A bisection
procedure for calculating premium tax credits is proposed. We prove that this
procedure calculates appropriate premium tax credits for a model of simple tax
returns. This is generalized to the case where premium tax credits are received
in advance, which is the most common one in applications. We outline the
problem of calculating appropriate premium tax credits for models of general
tax returns. While the bisection procedure will work with the tax code in its
current configuration, it could fail, eg, in states which have not expanded
Medicaid, if a new deduction with certain properties were to arise.",['Samuel J. Ferguson'],[],0,arXiv,http://arxiv.org/abs/2008.03355v1,False,True,False,False,False,2605,Troup Howard,Utah,Active,2021,,"Taxes on business property are a key source of revenue for local governments in the US, yet little is known about how these taxes affect economic activity. This project exploits large historical changes in tax legislation and regional institutional features of tax assessment in order to obtain shifts in taxation that are uncorrelated with business conditions. In conjunction with microdata on establishment-level outcomes from the U.S. Census Bureau, this variation will yield estimates of a range of business responses to taxes, including changes to wages, employment, revenue, prices, and location decisions. The central empirical design relies on merging an external dataset, comprised of publicly available regional property and transfer tax rates and parcel-level property records, to multiple U.S. Census Bureau datasets, including the County Business Patterns Business Register, the Longitudinal Business Database, and the Economic Census. This linkage will permit estimation of establishment-level event-study models as well as comparisons between Census records and the external data. This study will yield the first rigorous empirical estimates of responses to taxes on business property. These estimates are key for understanding the economic incidence of a funding source which, at $500 billion, represents approximately one-third of aggregate state and local tax revenue."
"On the Potential and Limitations of Few-Shot In-Context Learning to
  Generate Metamorphic Specifications for Tax Preparation Software","Due to the ever-increasing complexity of income tax laws in the United
States, the number of US taxpayers filing their taxes using tax preparation
software (henceforth, tax software) continues to increase. According to the
U.S. Internal Revenue Service (IRS), in FY22, nearly 50% of taxpayers filed
their individual income taxes using tax software. Given the legal consequences
of incorrectly filing taxes for the taxpayer, ensuring the correctness of tax
software is of paramount importance. Metamorphic testing has emerged as a
leading solution to test and debug legal-critical tax software due to the
absence of correctness requirements and trustworthy datasets. The key idea
behind metamorphic testing is to express the properties of a system in terms of
the relationship between one input and its slightly metamorphosed twinned
input. Extracting metamorphic properties from IRS tax publications is a tedious
and time-consuming process. As a response, this paper formulates the task of
generating metamorphic specifications as a translation task between properties
extracted from tax documents - expressed in natural language - to a contrastive
first-order logic form. We perform a systematic analysis on the potential and
limitations of in-context learning with Large Language Models(LLMs) for this
task, and outline a research agenda towards automating the generation of
metamorphic specifications for tax preparation software.","['Dananjay Srinivas', 'Rohan Das', 'Saeid Tizpaz-Niari', 'Ashutosh Trivedi', 'Maria Leonor Pacheco']",[],0,arXiv,http://arxiv.org/abs/2311.11979v1,False,True,False,False,False,2605,Troup Howard,Utah,Active,2021,,"Taxes on business property are a key source of revenue for local governments in the US, yet little is known about how these taxes affect economic activity. This project exploits large historical changes in tax legislation and regional institutional features of tax assessment in order to obtain shifts in taxation that are uncorrelated with business conditions. In conjunction with microdata on establishment-level outcomes from the U.S. Census Bureau, this variation will yield estimates of a range of business responses to taxes, including changes to wages, employment, revenue, prices, and location decisions. The central empirical design relies on merging an external dataset, comprised of publicly available regional property and transfer tax rates and parcel-level property records, to multiple U.S. Census Bureau datasets, including the County Business Patterns Business Register, the Longitudinal Business Database, and the Economic Census. This linkage will permit estimation of establishment-level event-study models as well as comparisons between Census records and the external data. This study will yield the first rigorous empirical estimates of responses to taxes on business property. These estimates are key for understanding the economic incidence of a funding source which, at $500 billion, represents approximately one-third of aggregate state and local tax revenue."
Supervised learning for the prediction of firm dynamics,"Thanks to the increasing availability of granular, yet high-dimensional, firm
level data, machine learning (ML) algorithms have been successfully applied to
address multiple research questions related to firm dynamics. Especially
supervised learning (SL), the branch of ML dealing with the prediction of
labelled outcomes, has been used to better predict firms' performance. In this
contribution, we will illustrate a series of SL approaches to be used for
prediction tasks, relevant at different stages of the company life cycle. The
stages we will focus on are (i) startup and innovation, (ii) growth and
performance of companies, and (iii) firms exit from the market. First, we
review SL implementations to predict successful startups and R&D projects.
Next, we describe how SL tools can be used to analyze company growth and
performance. Finally, we review SL applications to better forecast financial
distress and company failure. In the concluding Section, we extend the
discussion of SL methods in the light of targeted policies, result
interpretability, and causality.","['Falco J. Bargagli-Stoffi', 'Jan Niederreiter', 'Massimo Riccaboni']",[],0,arXiv,http://arxiv.org/abs/2009.06413v1,False,True,False,False,False,2608,Anthony Papac,UCLA,Completed,2021,2022.0,"Over the last ten years, research on worker mobility has uncovered a number of facts about the direction and composition of worker flows in the US economy. For example, Goetz et al. (2015) found that workers tend to move from older to younger firms, and Crane (2014) discovered that workers with higher previous wages tend to move from slower growing to faster growing firms. However, little work has been done to investigate the wage recruitment strategies causing so many high wage workers to move to younger, faster-growing firms. This project fills the gap in the literature by investigating the wage recruitment strategies that young, high growth firms use to attract and ultimately poach workers from other firms. Using linked employer-employee data from the Longitudinal Employer Household Dynamics and Longitudinal Business Database, the researchers of this project plan to decompose the starting salary of workers using the Abowd, Kramarz, and Margolis (AKM 1999) model of additive worker and firm fixed effects. Moreover, the researchers will use the AKM 1999 model to estimate the returns to tenure at firms of different ages and growth trajectories. Finally, the researchers will use information on the number of hours worked for workers in the LEHD who were also surveyed by the American Community Survey to compare the starting hourly wages and hourly wage growth of workers at younger, faster growing firms with those of workers at older, slower-growing firms."
"Venture Capital investments through the lens of Network and Functional
  Data Analysis","In this paper we characterize the performance of venture capital-backed firms
based on their ability to attract investment. The aim of the study is to
identify relevant predictors of success built from the network structure of
firms' and investors' relations. Focusing on deal-level data for the health
sector, we first create a bipartite network among firms and investors, and then
apply functional data analysis (FDA) to derive progressively more refined
indicators of success captured by a binary, a scalar and a functional outcome.
More specifically, we use different network centrality measures to capture the
role of early investments for the success of the firm. Our results, which are
robust to different specifications, suggest that success has a strong positive
association with centrality measures of the firm and of its large investors,
and a weaker but still detectable association with centrality measures of small
investors and features describing firms as knowledge bridges. Finally, based on
our analyses, success is not associated with firms' and investors' spreading
power (harmonic centrality), nor with the tightness of investors' community
(clustering coefficient) and spreading ability (VoteRank).","['Christian Esposito', 'Marco Gortan', 'Lorenzo Testa', 'Francesca Chiaromonte', 'Giorgio Fagiolo', 'Andrea Mina', 'Giulio Rossetti']",[],0,arXiv,http://arxiv.org/abs/2202.12859v2,False,True,False,False,False,2627,Federico Esposito,Boston,Active,2022,,"The goal of this project is to investigate how local shocks propagate across regions and examine the role of fiscal policy in their spatial percolation. To identify plant-level shocks, we obtain information on natural disasters that have hit U.S. counties since 1992 from NOAA. To trace the propagation of these shocks through firms' internal networks, we aim to construct a spatial network of the firm's internal organization using confidential plant-level data at from the Census Bureau's Longitudinal Business Database. We first investigate whether there are spillover effects of the shock across plants, i.e. whether plants in non-treated regions change their level of capital and employment following a shock occurred to other plants located in another region but that belong to the same firm. Second, we aim to identify the local government intervention using data on the loans given by the Small Business Administration to firms hit by a natural disaster. To estimate the spillover effect of the policy, we compare untreated plants belonging to firms that were hit by a disaster and were provided public funds, against untreated plants belonging to firms equally hit by a comparable disaster that were not helped. We aim to build a general equilibrium model to quantify the contribution of these spillover effects on the national fiscal multiplier."
"Uncovering Regulatory Affairs Complexity in Medical Products: A
  Qualitative Assessment Utilizing Open Coding and Natural Language Processing
  (NLP)","This study investigates the complexity of regulatory affairs in the medical
device industry, a critical factor influencing market access and patient care.
Through qualitative research, we sought expert insights to understand the
factors contributing to this complexity. The study involved semi-structured
interviews with 28 professionals from medical device companies, specializing in
various aspects of regulatory affairs. These interviews were analyzed using
open coding and Natural Language Processing (NLP) techniques. The findings
reveal key sources of complexity within the regulatory landscape, divided into
five domains: (A) Regulatory language complexity, (B) Intricacies within the
regulatory process, (C) Global-level complexities, (D) Database-related
considerations, and (E) Product-level issues. The participants highlighted the
need for strategies to streamline regulatory compliance, enhance interactions
between regulatory bodies and industry players, and develop adaptable
frameworks for rapid technological advancements. Emphasizing interdisciplinary
collaboration and increased transparency, the study concludes that these
elements are vital for establishing coherent and effective regulatory
procedures in the medical device sector.","['Yu Han', 'Aaron Ceross', 'Jeroen H. M. Bergmann']",[],0,arXiv,http://arxiv.org/abs/2401.02975v1,False,True,False,False,False,2638,Aymeric Bellon,Triangle,Active,2021,,"How do secure lenders' exposure to the environmental clean-up costs of their debtors affect their debtors' financial and economic outcomes? The relationship is theoretically ambiguous. Banks could reduce the quantity and maturity of credit, making financial constraints more binding and thus increasing the incentive of firms to pollute. However, banks could monitor their debtors and incentive them to reduce pollution. We could also expect a small and non-significant effect, as these constraints could not be binding. This paper studies this question by estimating difference-in-differences model exploiting several court rulings from the last three decades on a dataset constructed using EPA forms, Compustats as well as the LBD. The results are expected to shed light on the role of banks in affecting their debtor's collateral and inform policymakers on how to implement cost-effective policies to implement better pollution management practices."
"City-Scale Assessment of Pedestrian Exposure to Air Pollution: A Case
  Study in Barcelona","Air pollution is a pressing environmental risk to public health, particularly
in cities where population density and pollution levels are high. Traditional
methods for exposure analysis often rely on census data, but recent studies
highlight the impact of daily mobility on individuals' exposure. Here, we
develop a methodology to determine unprecedented pedestrian exposure estimates
at the city scale by combining sidewalk pedestrian flows with high-resolution
(25 m x 25 m) NO2 data from bias-corrected predictions of the air quality
system CALIOPE-Urban. Applied to Barcelona (Spain) for the year 2019, we show
that pedestrian flow and NO2 levels exhibit negligible temporal correlation.
While short-term (hourly) exposure is driven by pedestrian mobility, long-term
(monthly) exposure is dominated by NO2 patterns. We identify strong spatial
gradients of exposure, highlighting the importance for high-resolution
solutions at the sidewalks scale. Finally, we determine that exposure
mitigation strategies should consider different citizen subgroups based on
their mobility and preferred routes, as significant differences were found
between residential and pedestrian exposure. Our results provide exposure
indicators designed for city planners and policymakers, helping to prioritize
mitigation measures where and when they are most needed.","['Jan Mateu Armengol', 'Cristina Carnerero', 'Clément Rames', 'Álvaro Criado', 'Javier Borge-Holthoefer', 'Albert Soret', 'Albert Solé-Ribalta']",[],0,arXiv,http://arxiv.org/abs/2410.22015v1,False,True,False,False,False,2644,Catherine L Connolly,Baruch,Active,2022,,"Housing assistance is associated with improved physical and mental health for children and adults and may impact health by improving housing quality and the residential environment. However, few studies have examined the impact of housing assistance on residential environmental exposures and their determinants, such as indoor and outdoor pollutant sources, product usage, resident activity patterns, and the design and maintenance of building systems. In addition, there are limited comprehensive measures of residential environmental exposures at the national level.
This study will investigate the relationship between federal housing assistance and residential environmental exposures. We hypothesize that residential environmental exposures will be significantly different between residents living in HUD-assisted housing programs and residents living in other types of low-income housing due to differences in: compliance with federal regulations and policies specific to public housing and privately-owned multifamily properties, management and maintenance practices, and physical attributes of the housing stock. 

To conduct this analysis, we developed a national, multidimensional Housing and Environmental Quality Index (HEQI) from the public American Housing Survey (AHS) based on questions about housing quality, physical infrastructure, indoor hazards, product usage, and resident satisfaction. Using the geocodes available in the restricted-use AHS files, we will examine the associations of the HEQI with neighborhood measures of ambient air pollution, socioeconomic, and environmental conditions. Information about HUD assistance program type in the restricted AHS files will allow for comparisons of mean HEQI scores by type of assistance, as well as across income levels, housing type, sociodemographic subgroups, and neighborhood conditions. We will run multivariable models to estimate the relative risks of having poor HEQI scores, overall and within each HEQI domain, between assisted and unassisted households. The proposed study will advance our research understanding of whether housing assistance is an effective tool for reducing socioeconomic inequalities in residential environmental exposures, with important implications for social, housing, and public health policy"
"Efficient Economic Model Predictive Control of Water Treatment Process
  with Learning-based Koopman Operator","Used water treatment plays a pivotal role in advancing environmental
sustainability. Economic model predictive control holds the promise of
enhancing the overall operational performance of the water treatment
facilities. In this study, we propose a data-driven economic predictive control
approach within the Koopman modeling framework. First, we propose a deep
learning-enabled input-output Koopman modeling approach, which predicts the
overall economic operational cost of the wastewater treatment process based on
input data and available output measurements that are directly linked to the
operational costs. Subsequently, by leveraging this learned input-output
Koopman model, a convex economic predictive control scheme is developed. The
resulting predictive control problem can be efficiently solved by leveraging
quadratic programming solvers, and complex non-convex optimization problems are
bypassed. The proposed method is applied to a benchmark wastewater treatment
process. The proposed method significantly improves the overall economic
operational performance of the water treatment process. Additionally, the
computational efficiency of the proposed method is significantly enhanced as
compared to benchmark control solutions.","['Minghao Han', 'Jingshi Yao', 'Adrian Wing-Keung Law', 'Xunyuan Yin']",[],0,arXiv,http://arxiv.org/abs/2405.12478v2,False,True,False,False,False,2657,Alison Davis,Kentucky,Active,2021,,"This project proposes to estimate the effect of drinking water and water pollution violations on population flows and economic development. The advancing age of much of the U.S. water infrastructure as well as the population loss and shrinking tax base of many cities and rural communities make this a pressing issue requiring investigation. To study this question, we will link public water violation data from the Environmental Protection Agency with restricted-use Census data from the Longitudinal Business Database, County Business Patterns Business Register, and the American Community Survey from 2005-2019. Our estimates regarding changing location decisions of residents and businesses will also aid in properly modeling changing population dynamics of rural and micropolitan areas. To analyze firm location decisions and population shifts, we will use a count model such as a double-hurdle model as well as spatial equilibrium model where firms maximize profits and individuals maximize utility. We will analyze all counties and/or Census tracts in the US, but we will also perform sub-analysis by NAICS industry codes as well as urban rural divides as permitted by necessary disclosure avoidance."
"Safety-margin-based design and redesign considering mixed epistemic
  model uncertainty and aleatory parameter uncertainty","At the initial design stage engineers often rely on low-fidelity models that
have high epistemic uncertainty. Traditional safety-margin-based deterministic
design resorts to testing (e.g. prototype experiment, evaluation of
high-fidelity simulation, etc.) to reduce epistemic uncertainty and achieve
targeted levels of safety. Testing is used to calibrate models and prescribe
redesign when tests are not passed. After calibration, reduced epistemic model
uncertainty can be leveraged through redesign to restore safety or improve
design performance; however, redesign may be associated with substantial costs
or delays. In this paper, a methodology is described for optimizing the
safety-margin-based design, testing, and redesign process to allow the designer
to tradeoff between the risk of future redesign and the possible performance
and reliability benefits. The proposed methodology represents the epistemic
model uncertainty with a Kriging surrogate and is applicable in a wide range of
design problems. The method is illustrated on a cantilever beam bending example
and then a sounding rocket example. It is shown that redesign acts as a type of
quality control measure to prevent an undesirable initial design from being
accepted as the final design. It is found that the optimal design/redesign
strategy for maximizing expected design performance includes not only redesign
to correct an initial design that is later revealed to be unsafe, but also
redesign to improve performance when an initial design is later revealed to be
too conservative (e.g. too heavy).","['Nathaniel B. Price', 'Mathieu Balesdent', 'Sébastien Defoort', 'Rodolphe Le Riche', 'Nam H. Kim', 'Raphael T. Haftka']",[],0,arXiv,http://arxiv.org/abs/1904.08978v1,False,True,False,False,False,2660,Derek Young,Kentucky,Active,2022,,"Survey researchers are always interested in reducing costs associated with sample surveys as well as improving the quality of data obtained from such surveys. One way to accomplish these difficult tasks is to embed an experimental design within a sample survey that has very focused questions, or rather ""hypotheses,"" thus informing future redesigns of such surveys. This project will develop methodology that allows estimation of effects due to different survey implementations (such as two consecutive Annual Survey of Manufactures redesigns), or the effect of different types of question wording (again, potentially different questions or wording on consecutive ASM redesigns) on the variable(s) of interest. The focus will be on these variables to further address potential improvements in survey quality and reduction in costs, which will be quantified by the effects on response rates and/or completion rates. This will be accomplished by designing optimal experiments to evaluate the data collection, curation, and estimates produced as part of the ongoing ASM. Emphasis of the analyses will be on developing and estimating meaningful contrasts as well as employing a counterfactual approach to estimate treatment effects, where a specific redesign of the ASM is treated as the ""actual"" scenario (control group) and a subsequent redesign is treated as the ""hypothetical"" scenario (treatment group). The analyses in this project will use data from the restricted ASM, Census of Manufacturers (CMF), and Longitudinal Business Database (LBD) microdata, for the years 2000-2025."
"Identification of the Breach of Short-term Rental Regulations in Irish
  Rent Pressure Zones","The housing crisis in Ireland has rapidly grown in recent years. To make a
more significant profit, many landlords are no longer renting out their houses
under long-term tenancies but under short-term tenancies. The shift from
long-term to short-term rentals has harmed the supply of private housing
rentals. Regulating rentals in Rent Pressure Zones with the highest and rising
rents is becoming a tricky issue.
  In this paper, we develop a breach identifier to check short-term rentals
located in Rent Pressure Zones with potential breaches only using publicly
available data from Airbnb (an online marketplace focused on short-term
home-stays). First, we use a Residual Neural Network to filter out outdoor
landscape photos that negatively impact identifying whether an owner has
multiple rentals in a Rent Pressure Zone. Second, a Siamese Neural Network is
used to compare the similarity of indoor photos to determine if multiple rental
posts correspond to the same residence. Next, we use the Haversine algorithm to
locate short-term rentals within a circle centered on the coordinate of a
permit. Short-term rentals with a permit will not be restricted. Finally, we
improve the occupancy estimation model combined with sentiment analysis, which
may provide higher accuracy.
  Because Airbnb does not disclose accurate house coordinates and occupancy
data, it is impossible to verify the accuracy of our breach identifier. The
accuracy of the occupancy estimator cannot be verified either. It only provides
an estimate within a reasonable range. Users should be skeptical of short-term
rentals that are flagged as possible breaches.","['Guowen Liu', 'Inmaculada Arnedillo-Sanchez', 'Zhenshuo Chen']",[],0,arXiv,http://arxiv.org/abs/2211.16617v1,False,True,False,False,False,2661,Christina Stacy,Georgetown,Active,2022,,"We will generate the first cross-city panel dataset of rent control and inclusionary zoning reforms and estimate their effect on housing supply and rents. To generate the reform data, we will use machine learning algorithms to analyze newspaper articles across the U.S. between 2000 and 2021. We will then merge these reform data with a new dataset that we will generate from restricted census microdata from the Decennial Census and the American Community Survey on the number of rental units within each city that are affordable to households of different area median incomes. We will then estimate a fixed effects model, synthetic control models, and instrumental variable models (using state reforms as instruments) to examine the effect of different types of rent control and inclusionary zoning regulations on measures of housing affordability."
"PatentMiner: Patent Vacancy Mining via Context-enhanced and
  Knowledge-guided Graph Attention","Although there are a small number of work to conduct patent research by
building knowledge graph, but without constructing patent knowledge graph using
patent documents and combining latest natural language processing methods to
mine hidden rich semantic relationships in existing patents and predict new
possible patents. In this paper, we propose a new patent vacancy prediction
approach named PatentMiner to mine rich semantic knowledge and predict new
potential patents based on knowledge graph (KG) and graph attention mechanism.
Firstly, patent knowledge graph over time (e.g. year) is constructed by
carrying out named entity recognition and relation extrac-tion from patent
documents. Secondly, Common Neighbor Method (CNM), Graph Attention Networks
(GAT) and Context-enhanced Graph Attention Networks (CGAT) are proposed to
perform link prediction in the constructed knowledge graph to dig out the
potential triples. Finally, patents are defined on the knowledge graph by means
of co-occurrence relationship, that is, each patent is represented as a fully
connected subgraph containing all its entities and co-occurrence relationships
of the patent in the knowledge graph; Furthermore, we propose a new patent
prediction task which predicts a fully connected subgraph with newly added
prediction links as a new pa-tent. The experimental results demonstrate that
our proposed patent predic-tion approach can correctly predict new patents and
Context-enhanced Graph Attention Networks is much better than the baseline.
Meanwhile, our proposed patent vacancy prediction task still has significant
room to im-prove.","['Gaochen Wu', 'Bin Xu', 'Yuxin Qin', 'Fei Kong', 'Bangchang Liu', 'Hongwen Zhao', 'Dejie Chang']",[],0,arXiv,http://arxiv.org/abs/2107.04880v1,False,True,False,False,False,2669,Konhee Chang,Berkeley,Active,2022,,"This project exploits a recent court decision that limited the patent eligibility of software and business method patents to measure what happens to market entry for firms in software-oriented markets. This sheds light on the role of intellectual property rights for economic decision making, so far underexplored in existing data and surveys provided by the Census. To this end, this project will generate reduced form estimates on the impact of patent protection on firm entry and exit, as well as detailed insight into how business models, investments, and financial and innovation decisions are impacted by the ability to patent certain types of business methods and software applications."
The evolving boundary of green technology,"Green patents are a key indicator to track technological efforts aimed at
fighting climate change. Using an original dataset that merges different
Patstat releases, we identify three mechanisms that may bias green patent
statistics, potentially leading to contradictory findings. First, patent
reclassifications due to updates in (green) classification codes result in an
9.2\% increase in the number of green patents when using the most recent
classification structure. Second, delays in the adoption of the Cooperative
Patent Classification (CPC) system introduce regional biases, as approximately
10\% of green patents from late-adopting countries remain undetected in less
recent versions of the database. Third, we provide evidence that quality
thresholds used to identify high-value inventions significantly shape observed
trends in green patenting. Analyzing these mechanisms, our paper reveals that
in many studies a substantial number of green patents is systematically
overlooked, with the strongest effects observed for recent years and patents
originating from Asian patent offices. These findings lead to relevant policy
implications. Our results indicate not only that the global rate of green
innovation has accelerated, but also that its epicenter has shifted, with an
increasing share of green patents originating from emerging technological
leaders, particularly in Asia.","['Nicolò Barbieri', 'Kerstin Hötte', 'Peter Persoon']",[],0,arXiv,http://arxiv.org/abs/2503.21310v1,False,True,False,False,False,2669,Konhee Chang,Berkeley,Active,2022,,"This project exploits a recent court decision that limited the patent eligibility of software and business method patents to measure what happens to market entry for firms in software-oriented markets. This sheds light on the role of intellectual property rights for economic decision making, so far underexplored in existing data and surveys provided by the Census. To this end, this project will generate reduced form estimates on the impact of patent protection on firm entry and exit, as well as detailed insight into how business models, investments, and financial and innovation decisions are impacted by the ability to patent certain types of business methods and software applications."
Completeness Thresholds for Memory Safety of Array Traversing Programs,"We report on intermediate results of -- to the best of our knowledge -- the
first study of completeness thresholds for (partially) bounded memory safety
proofs. Specifically, we consider heap-manipulating programs that iterate over
arrays without allocating or freeing memory. In this setting, we present the
first notion of completeness thresholds for program verification which reduce
unbounded memory safety proofs to (partially) bounded ones. Moreover, we
demonstrate that we can characterise completeness thresholds for simple classes
of array traversing programs. Finally, we suggest avenues of research to scale
this technique theoretically, i.e., to larger classes of programs (heap
manipulation, tree-like data structures), and practically by highlighting
automation opportunities.","['Tobias Reinhard', 'Justus Fasse', 'Bart Jacobs']",[],0,arXiv,http://arxiv.org/abs/2305.03606v1,False,True,False,False,False,2680,Lea Bart,Michigan,Active,2022,,"This project studies the labor market experiences of the very low-income population and how features of the social safety net affect these experiences and long-term outcomes for families. Bringing the American Community Survey and Decennial Censuses together with longitudinal earnings data from the LEHD and detailed histories of program receipt will allow for more robust analysis of the interaction between program design and labor market outcomes than any dataset individually. In addition to labor market outcomes, this project will also study the population dynamics and long-term population outcomes that result from the design of transfer programs."
"AEGIS: Towards Formalized and Practical Memory-Safe Execution of C
  programs via MSWASM","Programs written in unsafe languages such as C are prone to memory safety
errors, which can lead to program compromises and serious real-world security
consequences. Recently, Memory-Safe WebAssembly (MSWASM) is introduced as a
general-purpose intermediate bytecode with built-in memory safety semantics.
Programs written in C can be compiled into MSWASM to get complete memory safety
protection. In this paper, we present our extensions on MSWASM, which improve
its semantics and practicality. First, we formalize MSWASM semantics in
Coq/Iris, extending it with inter-module interaction, showing that MSWASM
provides fine-grained isolation guarantees analogous to WASM's coarse-grained
isolation via linear memory. Second, we present Aegis, a system to adopt the
memory safety of MSWASM for C programs in an interoperable way. Aegis pipeline
generates Checked C source code from MSWASM modules to enforce spatial memory
safety. Checked C is a recent binary-compatible extension of C which can
provide guaranteed spatial safety. Our design allows Aegis to protect C
programs that depend on legacy C libraries with no extra dependency and with
low overhead. Aegis pipeline incurs 67% runtime overhead and near-zero memory
overhead on PolyBenchC programs compared to native.","['Shahram Esmaeilsabzali', 'Arayi Khalatyan', 'Zhijun Mo', 'Sruthi Venkatanarayanan', 'Shengjie Xu']",[],0,arXiv,http://arxiv.org/abs/2503.03698v1,False,True,False,False,False,2680,Lea Bart,Michigan,Active,2022,,"This project studies the labor market experiences of the very low-income population and how features of the social safety net affect these experiences and long-term outcomes for families. Bringing the American Community Survey and Decennial Censuses together with longitudinal earnings data from the LEHD and detailed histories of program receipt will allow for more robust analysis of the interaction between program design and labor market outcomes than any dataset individually. In addition to labor market outcomes, this project will also study the population dynamics and long-term population outcomes that result from the design of transfer programs."
Evaluating utility in synthetic banking microdata applications,"Financial regulators such as central banks collect vast amounts of data, but
access to the resulting fine-grained banking microdata is severely restricted
by banking secrecy laws. Recent developments have resulted in mechanisms that
generate faithful synthetic data, but current evaluation frameworks lack a
focus on the specific challenges of banking institutions and microdata. We
develop a framework that considers the utility and privacy requirements of
regulators, and apply this to financial usage indices, term deposit yield
curves, and credit card transition matrices. Using the Central Bank of
Paraguay's data, we provide the first implementation of synthetic banking
microdata using a central bank's collected information, with the resulting
synthetic datasets for all three domain applications being publicly available
and featuring information not yet released in statistical disclosure. We find
that applications less susceptible to post-processing information loss, which
are based on frequency tables, are particularly suited for this approach, and
that marginal-based inference mechanisms to outperform generative adversarial
network models for these applications. Our results demonstrate that synthetic
data generation is a promising privacy-enhancing technology for financial
regulators seeking to complement their statistical disclosure, while
highlighting the crucial role of evaluating such endeavors in terms of utility
and privacy requirements.","['Hugo E. Caceres', 'Ben Moews']",[],0,arXiv,http://arxiv.org/abs/2410.22519v1,False,True,False,False,False,2686,Diana Weng,Baruch,Active,2022,,"Using the technology module from the Annual Business Survey, bank call report data from the Federal Financial Institutions Examination Council, and loan-level origination data from government-sponsored enterprises and the Home Mortgage Disclosure Act, we study the determinants of banks' use of fintech and expect to find that greater availability of fintech and market pressure from shadow banks is associated with higher use of fintech. We also study the consequences of banks' use of fintech and expect to find that greater use is associated with greater risk-taking, risk assessment (ambiguous prediction), better risk management, and better quantity and quality of credit supply."
American Hate Crime Trends Prediction with Event Extraction,"Social media platforms may provide potential space for discourses that
contain hate speech, and even worse, can act as a propagation mechanism for
hate crimes. The FBI's Uniform Crime Reporting (UCR) Program collects hate
crime data and releases statistic report yearly. These statistics provide
information in determining national hate crime trends. The statistics can also
provide valuable holistic and strategic insight for law enforcement agencies or
justify lawmakers for specific legislation. However, the reports are mostly
released next year and lag behind many immediate needs. Recent research mainly
focuses on hate speech detection in social media text or empirical studies on
the impact of a confirmed crime. This paper proposes a framework that first
utilizes text mining techniques to extract hate crime events from New York
Times news, then uses the results to facilitate predicting American
national-level and state-level hate crime trends. Experimental results show
that our method can significantly enhance the prediction performance compared
with time series or regression methods without event-related factors. Our
framework broadens the methods of national-level and state-level hate crime
trends prediction.","['Songqiao Han', 'Hailiang Huang', 'Jiangwei Liu', 'Shengsheng Xiao']",[],0,arXiv,http://arxiv.org/abs/2111.04951v1,False,True,False,False,False,2687,Sylwia Piatkowska,Florida,Active,2022,,"In the proposed research, we seek to enhance knowledge on hate crime by circumventing police-based data on hate crimes with multilevel data from a restricted version of the area-identified National Crime Victimization Survey (NCVS) to examine the relationship between contextual features of places and the risk of hate crime victimization and reporting. Specifically, we will examine whether the likelihood of hate crime victimization and reporting associates with contextual characteristics for all racial/ethnic/gender categories. In addition, we will investigate whether the potential effects of contextual characteristics are different by race, ethnicity, and gender, and whether the possible relationships between the race/ethnicity of the victim and the race/ethnicity of the offender and hate crime victimization and reporting are conditioned by the contextual characteristics. Drawing upon the literature and theory of hate crime victimization and reporting, we develop hypotheses regarding the relationships between hate crime victimization and reporting and contextual characteristics. To achieve our goals, we will combine data on hate crime victimization and crime reporting from the 2005-2015 NCVS and community-/county-/state-level data from multiple sources, including the publicly available Decennial Census, American Community Survey, and Current Population Survey (CPS) Voting and Registration Supplement. We will analyze these data using the survey Heckprobit selection model. The results of this analysis will generate information that can be used to address an expanded set of concerns related to hate crime victimization and reporting, to inform public policy decisions, and to analyze public programs that pertain specifically to hate crime victimization and reporting."
"Deep Contrastive Learning for Feature Alignment: Insights from
  Housing-Household Relationship Inference","Housing and household characteristics are key determinants of social and
economic well-being, yet our understanding of their interrelationships remains
limited. This study addresses this knowledge gap by developing a deep
contrastive learning (DCL) model to infer housing-household relationships using
the American Community Survey (ACS) Public Use Microdata Sample (PUMS). More
broadly, the proposed model is suitable for a class of problems where the goal
is to learn joint relationships between two distinct entities without
explicitly labeled ground truth data. Our proposed dual-encoder DCL approach
leverages co-occurrence patterns in PUMS and introduces a bisect K-means
clustering method to overcome the absence of ground truth labels. The
dual-encoder DCL architecture is designed to handle the semantic differences
between housing (building) and household (people) features while mitigating
noise introduced by clustering. To validate the model, we generate a synthetic
ground truth dataset and conduct comprehensive evaluations. The model further
demonstrates its superior performance in capturing housing-household
relationships in Delaware compared to state-of-the-art methods. A
transferability test in North Carolina confirms its generalizability across
diverse sociodemographic and geographic contexts. Finally, the post-hoc
explainable AI analysis using SHAP values reveals that tenure status and
mortgage information play a more significant role in housing-household matching
than traditionally emphasized factors such as the number of persons and rooms.","['Xiao Qian', 'Shangjia Dong', 'Rachel Davidson']",[],0,arXiv,http://arxiv.org/abs/2502.11205v1,False,True,False,False,True,2689,AJ Golio,Atlanta,Active,2022,,"Recent contributions to the study of neighborhood gentrification remain heavily divided along methodological lines. Quantitative scholars consistently find little evidence of physical displacement within gentrifying areas, but are rarely able to assess the social impacts of gentrification on a wide scale. In this study, I aim to reconcile this gap through a two-step structural equation model and maximum likelihood regression analysis of the 2013 American Housing Survey's Neighborhood Social Capital variables. First, I assess how the multi-dimensional social costs and benefits of neighborhood change might be measured by incorporating these 21 variables into a latent factor analysis. Based on previous literature, I expect to find 5 latent measures of social experience. Then, utilizing longitudinal tract-level data from the American Community Survey, I classify each respondents' neighborhood as gentrifying or not. Using this classification as the primary independent variable, and controlling for various other household- and neighborhood-level factors from the AHS and ACS, I analyze the impact on the social experiences of long-time resident households. I expect to find that living in a gentrifying area is negatively associated with social capital, ties, and efficacy. Therefore, I refer to these impacts as the social costs of gentrification, and provide more nuance to quantitative research beyond the tracking of displaced households."
"Machine Learning Analysis of the Impact of Increasing the Minimum Wage
  on Income Inequality in Spain from 2001 to 2021","This paper analyzes the impact of the National Minimum Wage from 2001 to
2021. The MNW increased from 505.7/month (2001) to 1,108.3/month (2021). Using
the data provided by the Spanish Tax Administration Agency, databases that
represent the entire population studied can be analyzed. More accurate results
and more efficient predictive models are provided by these counts. This work is
characterized by the database used, which is a national census and not a sample
or projection. Therefore, the study reflects results and analyses based on
historical data from the Spanish Salary Census 2001-2021. Various
machine-learning models show that income inequality has been reduced by raising
the minimum wage. Raising the minimum wage has not led to inflation or
increased unemployment. On the contrary, it has been consistent with increased
net employment, contained prices, and increased corporate profit margins. The
most important conclusion is that an increase in the minimum wage in the period
analyzed has led to an increase in the wealth of the country, increasing
employment and company profits, and is postulated, under the conditions
analyzed, as an effective method for the redistribution of wealth.",['Marcos Lacasa Cazcarra'],[],0,arXiv,http://arxiv.org/abs/2402.02402v1,False,True,False,False,False,2693,David Neumark,Irvine,Active,2022,,"Our research will focus on four areas: (1) a higher-frequency (annual) look at more recent data , especially in light of recent variation in programs of interest to the research; (2) adding analysis of the earned income tax credit (EITC) in addition to the minimum wage; (3) analysis of the many local minimum wages adopted in recent years (see, e.g., Neumark and Yen, 2020); and (4) differentiating effects at the tract level based on the surrounding geography (e.g., are effects different in low-income/low-socioeconomic status (SES) tracts that are near high-income/high-SES tracts vs. low-income/low-SES tracts in a large area of other low-income tracts). We will use American Community Survey (ACS) data (2005-2019, and through 2027 as new data become available during the course of the project).

This study will generate population estimates from neighborhood-level models of employment outcomes. These estimates, based on underlying microdata aggregated to the Census tract-level, will increase the Bureau's understanding of the quality of the data contained in the ACS. The estimates produced will not be released into the public domain, but will instead be used in models that examine neighborhood-level employment and poverty status following changes in the minimum wage and the EITC. The research will focus on whether any resulting changes in employment status disproportionally affect socioeconomically disadvantaged neighborhoods, either in isolation or when compared to population estimates that are grouped into spatially larger levels of geography. These estimates, constructed from models with Census tract as the unit of observation, will also demonstrate how alternatively aggregated microdata can be used to increase the utility of Census Bureau data for analyzing demographic, economic, and social conditions.

The researchers will exploit publicly-available tract-level aggregates and create their own (non-comparable) aggregates using confidential data. The investigators will generate a panel dataset across spatially and temporally harmonized Census tracts that will allow them to examine local-level outcomes as a function of minimum wage increases. Utilizing microdata from the ACS, the researchers will estimate a model of tract-level employment and employment/income-related outcomes as a function of tract-level labor market features as well as other factors."
"Causal exposure-response curve estimation with surrogate confounders: a
  study of air pollution and children's health in Medicaid claims data","In this paper, we undertake a case study to estimate a causal
exposure-response function (ERF) for long-term exposure to fine particulate
matter (PM$_{2.5}$) and respiratory hospitalizations in socioeconomically
disadvantaged children using nationwide Medicaid claims data. These data
present specific challenges. First, family income-based Medicaid eligibility
criteria for children differ by state, creating socioeconomically distinct
populations and leading to clustered data. Second, Medicaid enrollees'
socioeconomic status, a confounder and an effect modifier of the
exposure-response relationships under study, is not measured. However, two
surrogates are available: median household income of each enrollee's zip code
and state-level Medicaid family income eligibility thresholds for children. We
introduce a customized approach for causal ERF estimation called MedMatch,
building on generalized propensity score (GPS) matching methods. MedMatch
adapts these methods to (1) leverage the surrogate variables to account for
potential confounding and/or effect modification by socioeconomic status and
(2) address practical challenges presented by differing exposure distributions
across clusters. We also propose a new hyperparameter selection criterion for
MedMatch and traditional GPS matching methods. Through extensive simulation
studies, we demonstrate the strong performance of MedMatch relative to
conventional approaches in this setting. We apply MedMatch to estimate the
causal ERF between PM$_{2.5}$ and respiratory hospitalization among children in
Medicaid, 2000-2012. We find a positive association, with a steeper curve at
lower PM$_{2.5}$ concentrations that levels off at higher concentrations.","['Jenny J. Lee', 'Xiao Wu', 'Francesca Dominici', 'Rachel C. Nethery']",[],0,arXiv,http://arxiv.org/abs/2308.00812v2,False,True,False,False,False,2695,Ethan Jenkins,Chicago,Active,2022,,"The United States incarceration rate is more than six times the incarceration rate of a typical OECD country. Additionally, the United States spends around $250 on total correctional expenditures per capita. This number has tripled since 1980. In addition to this fiscal cost, incarceration has lasting negative effects on an individual's future. Given the large fiscal and social cost of incarceration, understanding cost-effective interventions that reduce criminality is important.

I plan to identify the effect of access to Medicaid on subsequent criminal behavior using a regression discontinuity design (RDD) exploiting a policy discontinuity where several Medicaid expansions in the 1980s and early 1990s only applied to those born after September 30, 1983. This variation is almost certainly exogenous since these expansions occurred well after 1983, making it impossible to sort around the cutoff. This policy discontinuity was first used by Card and Shore-Sheppard (2004) to identify changes in insurance coverage and healthcare utilization. This discontinuity has been used to show that Medicaid eligibility reduces future teen mortality and adulthood hospitalizations. I would implement this RDD using day-of-birth cohort incarceration rates as the unit of observation.
An alternative identification strategy is to leverage differences across states and time in Medicaid expansion through the 1980s and 1990s using a two-way fixed effects model with simulated Medicaid eligibility. This approach has been widely used to study the effects of childhood Medicaid expansion. To use this strategy the unit of observation would be state month-of-birth cohorts.

The primary data set needed for this project is the Criminal Justice Administrative Records System (CJARS). Using CJARS, I can construct day-of-birth cohort totals for variables such as ever incarcerated, number of years incarcerated, ever charged with a crime, ever found guilty of crime, and ever arrested. Additionally, I would like to disaggregate these totals by race and sex. For the arrest and adjudication tables, I would like to disaggregate by offense type to explore what types of crime are averted. In addition to CJARS, I plan to use the public-use National Vital Statistics System (NVSS) Birth Data Files. Using this data, I can calculate the number of people in each day-of-birth cohort for the states with the relevant CJARS coverage. With the criminal involvement totals from CJARS and total population from NVSS Birth Data, I can create rates for each cohort such as percent ever incarcerated. I can use the cohort rates to estimate an RDD.

To estimate a two-way fixed effects model, I can construct similar totals but for day-of-birth state cohorts. No variation in Medicaid eligibility changes within month, so month-of-birth state cohorts is also workable. I will then merge these rates simulated Medicaid eligibility from Brown, Kowalski, and Lurie (2020). This simulated Medicaid eligibility data contains the number of years eligible for Medicaid holding population characteristics fixed. Simulated Medicaid eligibility does not vary within state-month cohorts. It captures changes in Medicaid eligibility due to state law changes. Neither the RD nor two-way fixed-effects approaches need the CJARS data to be linked at the individual level."
Tax Mechanisms and Gradient Flows,"We demonstrate how a static optimal income taxation problem can be analyzed
using dynamical methods. Specifically, we show that the taxation problem is
intimately connected to the heat equation. Our first result is a new property
of the optimal tax which we call the fairness principle. The optimal tax at any
income is invariant under a family of properly adjusted Gaussian averages (the
heat kernel) of the optimal taxes at other incomes. That is, the optimal tax at
a given income is equal to the weighted by the heat kernels average of optimal
taxes at other incomes and income densities. Moreover, this averaging happens
at every scale tightly linked to each other providing a unified weighting
scheme at all income ranges. The fairness principle arises not due to equality
considerations but rather it represents an efficient way to smooth the burden
of taxes and generated revenues across incomes. Just as nature wants to
distribute heat evenly, the optimal way for a government to raise revenues is
to distribute the tax burden and raised revenues evenly among individuals. We
then construct a gradient flow of taxes -- a dynamic process changing the
existing tax system in the direction of the increase in tax revenues -- and
show that it takes the form of a heat equation. The fairness principle holds
also for the short-term asymptotics of the gradient flow, where the averaging
is done over the current taxes. The gradient flow we consider can be viewed as
a continuous process of a reform of the nonlinear income tax schedule and thus
unifies the variational approach to taxation and optimal taxation. We present
several other characteristics of the gradient flow focusing on its smoothing
properties.","['Stefan Steinerberger', 'Aleh Tsyvinski']",[],0,arXiv,http://arxiv.org/abs/1904.13276v1,False,True,False,False,False,2697,Alexander Siebert,Atlanta,Active,2023,,"We aim to investigate an explanation for a well-established, but hitherto unexplained, empirical regularity in the tax literature: that progressive income taxation fails to meaningfully impact income inequality.  We will investigate whether an additional unexpected empirical finding, that firms do not fully exercise their labor market monopsony power, can provide an explanation by enabling firms to adjust their within-firm wage distributions to counteract the intended equity effects of progressive taxation.  To answer this question, we propose a model where a measure of real pre-tax within-firm income inequality (constructed using the LEHD's EHF) is regressed on an interaction term constituted of instrumented measures of within-firm income-tax progressivity (constructed using the LEHD's EHF and researcher-provided tax data) and firm-level labor market monopsony power (constructed using the LEHD's EHF, and establishment-level QWI).  The coefficient on this interaction term will tell us what portion of a change in a firm's income distribution can be attributed to changes in the exercise of monopsony power that are induced by changes in income taxes.  We expect this coefficient to show evidence of widening pre-tax inequality in response to an increase in progressivity.  We then also propose running additional models to investigate the impact of this estimated change on the annual earnings growth of certain demographic groups (defined using information in the LEHD's ICF).  These additional regressions will tell us what groups of workers are most likely to be adversely affected by the unintended consequences of progressive taxation that we estimate in the main regressions."
"Business and Regulatory Responses to Artificial Intelligence: Dynamic
  Regulation, Innovation Ecosystems and the Strategic Management of Disruptive
  Technology","Identifying and then implementing an effective response to disruptive new AI
technologies is enormously challenging for any business looking to integrate AI
into their operations, as well as regulators looking to leverage AI-related
innovation as a mechanism for achieving regional economic growth. These
business and regulatory challenges are particularly significant given the broad
reach of AI, as well as the multiple uncertainties surrounding such
technologies and their future development and effects. This article identifies
two promising strategies for meeting the AI challenge, focusing on the example
of Fintech. First, dynamic regulation, in the form of regulatory sandboxes and
other regulatory approaches that aim to provide a space for responsible
AI-related innovation. An empirical study provides preliminary evidence to
suggest that jurisdictions that adopt a more proactive approach to Fintech
regulation can attract greater investment. The second strategy relates to
so-called innovation ecosystems. It is argued that such ecosystems are most
effective when they afford opportunities for creative partnerships between
well-established corporations and AI-focused startups and that this aspect of a
successful innovation ecosystem is often overlooked in the existing discussion.
The article suggests that these two strategies are interconnected, in that
greater investment is an important element in both fostering and signaling a
well-functioning innovation ecosystem and that a well-functioning ecosystem
will, in turn, attract more funding. The resulting synergies between these
strategies can, therefore, provide a jurisdiction with a competitive edge in
becoming a regional hub for AI-related activity.","['Mark Fenwick', 'Erik P. M. Vermeulen', 'Marcelo Corrales Compagnucci']",[],0,arXiv,http://arxiv.org/abs/2407.19439v1,False,True,False,False,False,2699,Kassandra M McLean,Colorado,Active,2022,,"Past research has attempted to estimate the impact that business closures have on local economies and labor markets, but these efforts have been hampered by limited public data availability and a lack of linked employer-employee data. The current study combines external, open-source mapping data with restricted establishment and worker-level data from the Longitudinal Business Database (LBD), County Business Patterns Business Register (CBPBR), and Longitudinal Employer Household Dynamics (LEHD) files to assess the impacts of such closures in very small areas called Hyper-Local Economic Ecosystems, or HLEEs. Using a synthetic controls design to compare HLEEs that do and do not experience closures over a given period, we can estimate how closures impact other local businesses, assess how these impacts differ by business type (such as in/out of sector of the closing business), and extend these findings to worker and resident outcomes that have not been explored previously."
"Global evidence on the income elasticity of willingness to pay, relative
  price changes and public natural capital values","While the global economy continues to grow, ecosystem services tend to
stagnate or decline. Economic theory has shown how such shifts in relative
scarcities can be reflected in project appraisal and accounting, but empirical
evidence has been sparse to put theory into practice. To estimate relative
price changes in ecosystem services to be used for making such adjustments, we
perform a global meta-analysis of contingent valuation studies to derive income
elasticities of marginal willingness to pay (WTP) for ecosystem services to
proxy the degree of limited substitutability. Based on 735 income-WTP pairs
from 396 studies, we find an income elasticity of WTP of around 0.6. Combined
with good-specific growth rates, we estimate relative price change of ecosystem
services of around 1.7 percent per year. In an application to natural capital
valuation of forest ecosystem services by the World Bank, we show that natural
capital should be uplifted by around 40 percent. Our assessment of aggregate
public natural capital yields a larger value adjustment of between 58 and 97
percent, depending on the discount rate. We discuss implications for policy
appraisal and for estimates of natural capital in comprehensive wealth
accounts.","['Moritz A. Drupp', 'Zachary M. Turk', 'Ben Groom', 'Jonas Heckenhahn']",[],0,arXiv,http://arxiv.org/abs/2308.04400v3,False,True,False,False,False,2699,Kassandra M McLean,Colorado,Active,2022,,"Past research has attempted to estimate the impact that business closures have on local economies and labor markets, but these efforts have been hampered by limited public data availability and a lack of linked employer-employee data. The current study combines external, open-source mapping data with restricted establishment and worker-level data from the Longitudinal Business Database (LBD), County Business Patterns Business Register (CBPBR), and Longitudinal Employer Household Dynamics (LEHD) files to assess the impacts of such closures in very small areas called Hyper-Local Economic Ecosystems, or HLEEs. Using a synthetic controls design to compare HLEEs that do and do not experience closures over a given period, we can estimate how closures impact other local businesses, assess how these impacts differ by business type (such as in/out of sector of the closing business), and extend these findings to worker and resident outcomes that have not been explored previously."
"Hyperspectral Unmixing Overview: Geometrical, Statistical, and Sparse
  Regression-Based Approaches","Imaging spectrometers measure electromagnetic energy scattered in their
instantaneous field view in hundreds or thousands of spectral channels with
higher spectral resolution than multispectral cameras. Imaging spectrometers
are therefore often referred to as hyperspectral cameras (HSCs). Higher
spectral resolution enables material identification via spectroscopic analysis,
which facilitates countless applications that require identifying materials in
scenarios unsuitable for classical spectroscopic analysis. Due to low spatial
resolution of HSCs, microscopic material mixing, and multiple scattering,
spectra measured by HSCs are mixtures of spectra of materials in a scene. Thus,
accurate estimation requires unmixing. Pixels are assumed to be mixtures of a
few materials, called endmembers. Unmixing involves estimating all or some of:
the number of endmembers, their spectral signatures, and their abundances at
each pixel. Unmixing is a challenging, ill-posed inverse problem because of
model inaccuracies, observation noise, environmental conditions, endmember
variability, and data set size. Researchers have devised and investigated many
models searching for robust, stable, tractable, and accurate unmixing
algorithms. This paper presents an overview of unmixing methods from the time
of Keshava and Mustard's unmixing tutorial [1] to the present. Mixing models
are first discussed. Signal-subspace, geometrical, statistical, sparsity-based,
and spatial-contextual unmixing algorithms are described. Mathematical problems
and potential solutions are described. Algorithm characteristics are
illustrated experimentally.","['José M. Bioucas-Dias', 'Antonio Plaza', 'Nicolas Dobigeon', 'Mario Parente', 'Qian Du', 'Paul Gader', 'Jocelyn Chanussot']",[],0,arXiv,http://arxiv.org/abs/1202.6294v2,False,True,False,False,False,2703,Matthew S Hall,Cornell,Active,2022,,"Given the steady growth in the number of U.S.-born children with parents from two different major ethno-racial categories, it is crucial to examine procedures for identifying mixed family backgrounds in census data.  Using the 2015-19 American Community Survey (ACS) sample data, this project will investigate the degree to which the identification of children from mixed family backgrounds is altered by adding information beyond the race and Hispanic-origin data stipulated for the children.  In a first step, the ancestry data provided for the children will be considered.  In a second step, the race, Hispanic-origin, and ancestry data for their parents will be added.  The use of the CHCK file will allow the analysis to include data for a sample of non-coresident fathers.  At each step, the children's ethno-racial classification will be reconsidered in light of any new descent information. A second major focus of the project is on the neighborhood contexts where mixed children grow up.  This choice is dictated by the critical role of social segregation in the transmission of inequalities from one generation to the next.   By merging the 2015-19 ACS file of census-tract characteristics with the individual children's records, we will estimate the characteristics of the neighborhoods where mixed children are growing up (e.g., percent of adults with college degrees) and compare them to the equivalent estimates for children belonging to unmixed groups (e.g, neighborhoods of Asian-white children compared to those of Asian-only children on the one hand and of non-Hispanic white children, on the other).   Using multivariate analyses, where neighborhood characteristics constitute the dependent variables, we will also compare mixed and unmixed ethno-racial categories, while controlling for individual and family characteristics, such as parental education and family income."
Remote Mentoring Young Females in STEM through MAGIC,"The limited representation of women in STEM workforce is a concerning
national issue. It has been found that the gender stratification is not due to
the lack of talent amongst young females, but due to the lack of access to
female role models. To this end, ""remote mentoring"" is an effective way to
offer nation-wide personalized STEM mentoring to young females from all
segments of the society. In this paper, we introduce MAGIC, an organization
dedicated to mentoring young females in STEM through remote methods. We conduct
a retrospective study of MAGIC's formative years and present our experience in
remotely establishing 23 highly tailored mentor-mentee pairs. We provide
several key findings on STEM remote mentoring, such as popular communication
tools, frequently sought STEM skills among girls, and projects that could be
accomplished through remote mentoring. Furthermore, we present key challenges
faced by mentors and mentees, notable outcomes, and lessons learnt about remote
mentoring.","['Ritu Khare', 'Esha Sahai', 'Ira Pramanick']",[],0,arXiv,http://arxiv.org/abs/1304.7547v1,False,True,False,False,False,2705,Byeongdon Oh,Berkeley,Active,2023,,"Improving STEM learning and broadening participation in STEM are essential for growing the economy and remaining globally competitive. Nonetheless, after six decades of federal emphasis, the STEM workforce remains non-representative of the US population. With the federal emphasis often on collective benefits, the individual benefits of STEM degrees and occupations are less clear, whether in terms of socioeconomic attainment or well-being. The individual benefits of STEM degrees and occupations for diverse young adults, those under-represented in STEM, are particularly unclear. The structural and interpersonal characteristics of STEM environments that differentiate individual benefits likely also shape selection and persistence in STEM degrees and occupations. We investigate three broad questions: 1) What aspects of STEM environments (e.g., discipline, field) increase the likelihood that students choose and persist in STEM degrees and STEM occupations? 2) Do STEM degrees and STEM occupations relate to socioeconomic attainment and well-being? 3) How do these relationships vary depending on individuals' social position (SES, race, immigration status, and gender)? We seek to utilize restricted-use data from the National Survey of College Graduates (NSCG) linked to the American Community Survey (ACS). The NSCG is a unique source of extensive information on college graduates' experiences in both undergraduate and graduate school. With its rich measures of labor force outcomes, we will be able to investigate how STEM degrees relate to STEM occupations, and then to socioeconomic attainment. Our focus is also consistent with the aims of the NSCG, investigating trends in employment opportunities and salaries to support the formulation of education and employment policies aimed at modifying scientific and technical curricula and reinforcing the national engines of innovation."
Understanding Childhood Vulnerability in The City of Surrey,"Understanding the community conditions that best support universal access and
improved childhood outcomes allows ultimately to improve decision-making in the
areas of planning and investment across the early stages of childhood
development. Here we describe two different data-driven approaches to
visualizing the lived experiences of children throughout the City of Surrey,
combining data derived from both public and private sources. In one approach,
we find specifically that the Early Development Instrument measuring childhood
vulnerabilities across varying domains can be used to cluster neighborhoods,
and that census variables can help explain similarities between neighborhoods
within these clusters. In our second approach, we use program registration data
from the City of Surrey's Community and Recreation Services Division. We also
find a critical age of entry and exit for each program related to early
childhood development and beyond, and find that certain neighborhoods and
recreational programs have larger retention rates than others. This report
details the journey of using data to tell the story of these neighborhoods, and
provides a lens to which community initiatives can be strategically crafted
through their use.","['Cody Griffith', 'Varoon Mathur', 'Catherine Lin', 'Kevin Zhu']",[],0,arXiv,http://arxiv.org/abs/1903.09639v1,False,True,False,False,False,2706,Riley Wilson,Utah,Active,2022,,"This study examines how access to early childhood education affects a mother's joint decisions about family formation, education, mobility, labor supply and career trajectory for up to 25 years. As both employment and childrearing require substantial time and resource commitments, women facing these decisions likely face trade-offs. Gaining access to public school for children could relax mothers' time constraints, potentially affecting work decisions, the ability to move in response to labor market conditions, and the feasibility and timing of additional children. This study combines information on a child's eligibility for public early childhood education with restricted microdata from the 2000 Decennial Census, 2010 Decennial Census, American Community Survey and LEHD. We use a regression discontinuity design based on child's birthday relative to a school cutoff date to determine how just barely gaining access to early child education one year earlier affects mothers' decisions in the short-, medium-, and long-run from 2000-2025. This project provides new insight into the persistence of family-level effects of early childhood education provision as we evaluate many family outcomes, explore a long time horizon, and study mediating pathways in children's outcomes."
Situated Live Programming for Human-Robot Collaboration,"We present situated live programming for human-robot collaboration, an
approach that enables users with limited programming experience to program
collaborative applications for human-robot interaction. Allowing end users,
such as shop floor workers, to program collaborative robots themselves would
make it easy to ""retask"" robots from one process to another, facilitating their
adoption by small and medium enterprises. Our approach builds on the paradigm
of trigger-action programming (TAP) by allowing end users to create rich
interactions through simple trigger-action pairings. It enables end users to
iteratively create, edit, and refine a reactive robot program while executing
partial programs. This live programming approach enables the user to utilize
the task space and objects by incrementally specifying situated trigger-action
pairs, substantially lowering the barrier to entry for programming or
reprogramming robots for collaboration. We instantiate situated live
programming in an authoring system where users can create trigger-action
programs by annotating an augmented video feed from the robot's perspective and
assign robot actions to trigger conditions. We evaluated this system in a study
where participants (n = 10) developed robot programs for solving collaborative
light-manufacturing tasks. Results showed that users with little programming
experience were able to program HRC tasks in an interactive fashion and our
situated live programming approach further supported individualized strategies
and workflows. We conclude by discussing opportunities and limitations of the
proposed approach, our system implementation, and our study and discuss a
roadmap for expanding this approach to a broader range of tasks and
applications.","['Emmanuel Senft', 'Michael Hagenow', 'Robert Radwin', 'Michael Zinn', 'Michael Gleicher', 'Bilge Mutlu']",[],0,arXiv,http://arxiv.org/abs/2108.03592v1,False,True,False,False,False,2711,Nancey G Leigh,Atlanta,Active,2023,,"While the development of robotic technology is rapidly progressing, the diffusion of technology has been uneven and relatively slow. The disparities in robot adoption rates between large-, and Small and Medium-sized Manufacturers (SMM)s are becoming a substantial issue in the U.S. However, there are surprisingly few empirical studies highlighting such imbalance. To increase understanding of robot diffusion, this study seeks to contribute empirical evidence of factors that influence SMMs' decisions on robot adoption. We plan to incorporate various datasets, including microdata from the Annual Survey of Manufacturers (ASM) and Annual Business Survey (ABS), as well as Real-Time Labor Market Information (RTLMI), to build a comprehensive dataset for multi-level analysis. Using the integrated dataset, we will 1) evaluate the robotic data collected in ASM and ABS, 2) examine establishment- and regional-level factors that influence robot adoption, and 3) investigate cross-interaction between firm sizes and regional characteristics. We expect to find regional advantages that play a critical role in facilitating robot adoption and SMMs are more susceptible to such influence. As the first robot adoption study conducted with a large-scale sample of U.S. manufacturers, this study is expected to offer the necessary knowledge for developing a supportive industrial policy."
"Quantitative law describing market dynamics before and after
  interest-rate change","We study the behavior of U.S. markets both before and after U.S. Federal Open
Market Committee (FOMC) meetings, and show that the announcement of a U.S.
Federal Reserve rate change causes a financial shock, where the dynamics after
the announcement is described by an analogue of the Omori earthquake law. We
quantify the rate n(t) of aftershocks following an interest rate change at time
T, and find power-law decay which scales as n(t-T) (t-T)^-$\Omega$, with
$\Omega$ positive. Surprisingly, we find that the same law describes the rate
n'(|t-T|) of ""pre-shocks"" before the interest rate change at time T. This is
the first study to quantitatively relate the size of the market response to the
news which caused the shock and to uncover the presence of quantifiable
preshocks. We demonstrate that the news associated with interest rate change is
responsible for causing both the anticipation before the announcement and the
surprise after the announcement. We estimate the magnitude of financial news
using the relative difference between the U. S. Treasury Bill and the Federal
Funds Effective rate. Our results are consistent with the ""sign effect,"" in
which ""bad news"" has a larger impact than ""good news."" Furthermore, we observe
significant volatility aftershocks, confirming a ""market underreaction"" that
lasts at least 1 trading day.","['Alexander M. Petersen', 'Fengzhong Wang', 'Shlomo Havlin', 'H. Eugene Stanley']",[],0,arXiv,http://arxiv.org/abs/0903.0010v3,False,True,False,False,False,2715,Richard Thakor,Minnesota,Active,2022,,"We aim to investigate the role of labor market power in determining firms' cost of capital and response to interest rate shocks. We hypothesize that firms with higher labor market power (1) have lower cost of capital, and (2) respond more aggressively to negative interest rate shocks. We plan to merge Census Bureau data with Compustat/CRSP to investigate these hypotheses. Confirmation of these hypotheses will deepen our understanding of the determinants of firms' cost of capital and the economic effects of interest rate shocks."
"Expanding Scanning Frequency Range of Josephson Parametric Amplifier
  Axion Haloscope Readout with Schottky Diode Bias Circuit","The axion search experiments in the microwave frequency range require high
sensitive detectors with intrinsic noise close to quantum noise limit.
Josephson parametric amplifiers (JPAs) are the most valuable candidates for the
role of the first stage amplifier in the measurement circuit of the microwave
frequency range, as they are well-known in superconducting quantum circuits
readout. To increase the frequency range, a challenging scientific task
involves implementing an assembly with parallel connection of several single
JPAs, which requires matching the complex RF circuit at microwaves and ensuring
proper DC flux bias. In this publication, we present a new DC flux bias setup
based on a Schottky diode circuit for a JPA assembly consisting of two JPAs. We
provide a detailed characterization of the diodes at cryogenic temperatures
lower than 4 K. Specifically, we selected two RF Schottky diodes with desirable
characteristics for the DC flux bias setup, and our results demonstrate that
the Schottky diode circuit is a promising method for achieving proper DC flux
bias in JPA assemblies.","['Minsu Ko', 'Sergey V. Uchaikin', 'Boris I. Ivanov', 'JinMyeong Kim', 'Seonjeong Oh', 'Violeta Gkika', 'Yannis K. Semertzidis']",[],0,arXiv,http://arxiv.org/abs/2307.01480v1,False,True,False,False,False,2744,Minsu Ko,Kentucky,Active,2023,,"The aim of this paper is to investigate the effect of production factors' aging on the technology adoption of a firm. We plan to show that technology adoption is slower for a firm with older labor and old capital. To do so, we will examine new patents in the same industry to analyze the productivity difference of establishments with older production factors compared to establishments with younger production factors. We will also investigate the reaction of firms to delayed technology adoption. If firms fail to promptly renew the factor structure, they will suffer from decreased earnings and firm value. Eventually, the age of production factor will work as a risk characteristic of the firm and is priced in the stock market. Considering the destructive effects of automation innovation on the factor market, the effect of automation will be evaluated separately. This research will contribute to the literature by providing foundations for life-cycle theories of corporate policies in the aging society."
Universal Voting Protocol Tweaks to Make Manipulation Hard,"Voting is a general method for preference aggregation in multiagent settings,
but seminal results have shown that all (nondictatorial) voting protocols are
manipulable. One could try to avoid manipulation by using voting protocols
where determining a beneficial manipulation is hard computationally. A number
of recent papers study the complexity of manipulating existing protocols. This
paper is the first work to take the next step of designing new protocols that
are especially hard to manipulate. Rather than designing these new protocols
from scratch, we instead show how to tweak existing protocols to make
manipulation hard, while leaving much of the original nature of the protocol
intact. The tweak studied consists of adding one elimination preround to the
election. Surprisingly, this extremely simple and universal tweak makes typical
protocols hard to manipulate! The protocols become NP-hard, #P-hard, or
PSPACE-hard to manipulate, depending on whether the schedule of the preround is
determined before the votes are collected, after the votes are collected, or
the scheduling and the vote collecting are interleaved, respectively. We prove
general sufficient conditions on the protocols for this tweak to introduce the
hardness, and show that the most common voting protocols satisfy those
conditions. These are the first results in voting settings where manipulation
is in a higher complexity class than NP (presuming PSPACE $\neq$ NP).","['Vincent Conitzer', 'Tuomas Sandholm']",[],0,arXiv,http://arxiv.org/abs/cs/0307018v1,False,True,False,False,False,2747,Vincent Pons,Boston,Active,2023,,"How important are childhood environments in shaping civic engagement and voting behavior? Motivated by existing evidence that the neighborhood in which we grow up can have long-lasting effects on our economic and social outcomes, we will ask whether civic engagement - measured by whether an individual registers to vote and participates in elections - is also shaped by our childhood environment, above and beyond the influence of our family. In addition to measuring overall effects of childhood neighborhoods on adult political behavior, we will also investigate whether neighborhood's influence is larger at certain ages. Furthermore, we will measure heterogeneous effects by demographic characteristics and location to uncover the mechanisms by which childhood environments shape political behavior. To answer these questions, we aim to first combine nationwide voter file data with Census Bureau information on family linkages and address histories. We will then track households that relocate across areas to assess the extent to which young adults' probability of registering and voting depends on the amount of time they spent in the origin and destination neighborhoods. Because our project requires linking Census Bureau data to external administrative data on political participation, we will finally be able to analyze the accuracy of self-reported information on voter registration and turnout present in the CPS and to identify individual-level correlates of misreporting which could be used to improve future turnout estimates."
"A data-driven econo-financial stress-testing framework to estimate the
  effect of supply chain networks on financial systemic risk","Supply chain disruptions constitute an often underestimated risk for
financial stability. As in financial networks, systemic risks in production
networks arises when the local failure of one firm impacts the production of
others and might trigger cascading disruptions that affect significant parts of
the economy. Here, we study how systemic risk in production networks translates
into financial systemic risk through a mechanism where supply chain contagion
leads to correlated bank-firm loan defaults. We propose a financial
stress-testing framework for micro- and macro-prudential applications that
features a national firm level supply chain network in combination with
interbank network layers. The model is calibrated by using a unique data set
including about 1 million firm-level supply links, practically all bank-firm
loans, and all interbank loans in a small European economy. As a showcase we
implement a real COVID-19 shock scenario on the firm level. This model allows
us to study how the disruption dynamics in the real economy can lead to
interbank solvency contagion dynamics. We estimate to what extent this
amplifies financial systemic risk. We discuss the relative importance of these
contagion channels and find an increase of interbank contagion by 70% when
production network contagion is present. We then examine the financial systemic
risk firms bring to banks and find an increase of up to 28% in the presence of
the interbank contagion channel. This framework is the first financial systemic
risk model to take agent-level dynamics of the production network and shocks of
the real economy into account which opens a path for directly, and event-driven
understanding of the dynamical interaction between the real economy and
financial systems.","['Jan Fialkowski', 'Christian Diem', 'András Borsos', 'Stefan Thurner']",[],0,arXiv,http://arxiv.org/abs/2502.17044v1,False,True,False,False,False,2762,Nuri Ersahin,Dallas,Active,2023,,"The modern economy is characterized by a complex network of customer and supplier relationships.  Idiosyncratic shocks affecting one firm are known to propagate upstream and downstream over supply chains. Direct and indirect customers of firms hit by idiosyncratic shocks, such as natural disasters, experience significant declines in sales growth and profitability. However, while the existing literature documents the risk of propagation, little is known on how firms react when negative shocks affect their customers and suppliers. The purpose of this project to conduct a micro-level analysis that documents how shocks that disrupt supply chains affect customers' investment, employment, and asset redeployment decisions. The project will conduct this analysis in two parts that complement each other: First, we conduct an ""ex post"" analysis by examining how realized disruptions of suppliers' production affect customers' investment, employment, and asset redeployment decisions as well as their productivity.  Second, we complement the ""ex post"" analysis in the first part with an ""ex ante"" analysis. We construct a new measure of supply chain risk faced by firms and analyze what firms do to manage the risk that suppliers will not be able to deliver the inputs required from them. The Census of Manufacturers and Annual Survey of Manufacturers, the Longitudinal Business Database, the Longitudinal Employer-Household Dynamics data, the Quarterly Financial Report Census Years, the Commodity  Flow Survey, the Manufacturers' Shipments, Inventories, and Orders, the Quarterly Survey of Plant Capacity Utilization, the Survey of Industrial Research and Development, the Business Research & Development and Innovation Survey, and the Census of Auxiliary Establishments and Standard Statistical Establishment List will be used to quantify the effect of supply chain risks on firm behavior and performance."
"Facilitating Battery Swapping Services for Freight Trucks with
  Spatial-Temporal Demand Prediction","Electrifying heavy-duty trucks offers a substantial opportunity to curtail
carbon emissions, advancing toward a carbon-neutral future. However, the
inherent challenges of limited battery energy and the sheer weight of
heavy-duty trucks lead to reduced mileage and prolonged charging durations.
Consequently, battery-swapping services emerge as an attractive solution for
these trucks. This paper employs a two-fold approach to investigate the
potential and enhance the efficacy of such services. Firstly, spatial-temporal
demand prediction models are adopted to predict the traffic patterns for the
upcoming hours. Subsequently, the prediction guides an optimization module for
efficient battery allocation and deployment. Analyzing the heavy-duty truck
data on a highway network spanning over 2,500 miles, our model and analysis
underscore the value of prediction/machine learning in facilitating future
decision-makings. In particular, we find that the initial phase of implementing
battery-swapping services favors mobile battery-swapping stations, but as the
system matures, fixed-location stations are preferred.","['Linyu Liu', 'Zhen Dai', 'Shiji Song', 'Xiaocheng Li', 'Guanting Chen']",[],0,arXiv,http://arxiv.org/abs/2310.04440v2,False,True,False,False,False,2763,Cuicui Chen,Cornell,Active,2022,,"Heavy-duty vehicles comprise one-tenth of road vehicles in the U.S. but contribute almost half of local air pollution. An important motivation for electrifying the trucking industry is to relieve the disproportionate burden of local air pollution on disadvantaged communities. We study the role of charging infrastructure in determining the distributional consequences of truck electrification. We first build a micro-founded framework that measures pollution inequality in conjunction with the underlying income inequality. This framework allows us to decompose welfare into income size and inequality, analyze inequality within and between demographic groups, and investigate the long-term effects of inequality given the human capital cost of pollution. To apply this framework to truck electrification, we estimate a transport mode choice model based on Commodity Flow Survey data, simulate which shipments would adopt the electric truck mode under various charging infrastructure designs, and calculate the economic and pollution effects of those adoption patterns.  Our inequality framework then combines those effects of truck electrification with American Community Survey data to quantify the distributional consequences of truck electrification and identify charging infrastructure designs that best support equality and efficiency."
"Forecasting Automotive Supply Chain Shortfalls with Heterogeneous Time
  Series","Operational disruptions can significantly impact companies performance. Ford,
with its 37 plants globally, uses 17 billion parts annually to manufacture six
million cars and trucks. With up to ten tiers of suppliers between the company
and raw materials, any extended disruption in this supply chain can cause
substantial financial losses. Therefore, the ability to forecast and identify
such disruptions early is crucial for maintaining seamless operations. In this
study, we demonstrate how we construct a dataset consisting of many
multivariate time series to forecast first-tier supply chain disruptions,
utilizing features related to capacity, inventory, utilization, and processing,
as outlined in the classical Factory Physics framework. This dataset is
technically challenging due to its vast scale of over five hundred thousand
time series. Furthermore, these time series, while exhibiting certain
similarities, also display heterogeneity within specific subgroups. To address
these challenges, we propose a novel methodology that integrates an enhanced
Attention Sequence to Sequence Deep Learning architecture, using Neural Network
Embeddings to model group effects, with a Survival Analysis model. This model
is designed to learn intricate heterogeneous data patterns related to
operational disruptions. Our model has demonstrated a strong performance,
achieving 0.85 precision and 0.8 recall during the Quality Assurance (QA) phase
across Ford's five North American plants. Additionally, to address the common
criticism of Machine Learning models as black boxes, we show how the SHAP
framework can be used to generate feature importance from the model
predictions. It offers valuable insights that can lead to actionable strategies
and highlights the potential of advanced machine learning for managing and
mitigating supply chain risks in the automotive industry.","['Bach Viet Do', 'Xingyu Li', 'Chaoye Pan', 'Oleg Gusikhin']",[],0,arXiv,http://arxiv.org/abs/2407.16739v2,False,True,False,False,False,2768,Tao Li,Florida,Active,2023,,"The project will investigate the impacts of tariff shocks along several dimensions: (1) trade activities, such as import values, changes in trade varieties, and trading partners, (2) stock price reactions, and (3) operational performance, employment, and investment. Using data from the Longitudinal Firm Trade Transaction Database from 2010 to present day along with a collection of data products covering tariff schedules and actions, this project will investigate the heterogeneous impacts increased tariffs have on domestic firms. These data will be accessed through the Florida Research Data Center."
"Same environment, stratified impacts? Air pollution, extreme
  temperatures, and birth weight in south China","This paper investigates whether associations between birth weight and
prenatal ambient environmental conditions--pollution and extreme
temperatures--differ by 1) maternal education; 2) children's innate health; and
3) interactions between these two. We link birth records from Guangzhou, China,
during a period of high pollution, to ambient air pollution (PM10 and a
composite measure) and extreme temperature data. We first use mean regressions
to test whether, overall, maternal education is an ""effect modifier"" in the
relationships between ambient air pollution, extreme temperature, and birth
weight. We then use conditional quantile regressions to test for effect
heterogeneity according to the unobserved innate vulnerability of babies after
conditioning on other confounders. Results show that 1) the negative
association between ambient exposures and birth weight is twice as large at
lower conditional quantiles of birth weights as at the median; 2) the
protection associated with college-educated mothers with respect to pollution
and extreme heat is heterogeneous and potentially substantial: between 0.02 and
0.34 standard deviations of birth weights, depending on the conditional
quantiles; 3) this protection is amplified under more extreme ambient
conditions and for infants with greater unobserved innate vulnerabilities.","['Xiaoying Liu', 'Jere R. Behrman', 'Emily Hannum', 'Fan Wang', 'Qingguo Zhao']",[],0,arXiv,http://arxiv.org/abs/2204.00219v1,False,True,False,False,False,2776,Qingli Fan,Cornell,Active,2023,,"Radiation is recognized as having an influence on human outcomes, especially in relation to infant and child development. In Japan and Europe, events ranging from the 1945 Hiroshima atomic explosion and the 1986 Chernobyl accident to the very recent 2011 Fukushima nuclear plant damage incident have drawn long-lasting attention to the effect of radiation on public health. In the United States, numerous nuclear weapon tests have been conducted and contributed considerable radioactive fallouts over the country; however, no study on the influence of radioactive fallouts on U.S. general population has been done. Combining radiation data in the 1960s, cohort survey response from ACS, CPS, SIPP, and administrative records from the SSA, we examine the impact of prenatal radiation exposure on long-term education attainment and labor market performance exploiting county-level variation in the timing and severity of radiation. Insights from the medical literature suggest that cognitive development is most susceptible to ionizing radiation during the second trimester, we thus expect human outcomes to be negatively impacted if the subjects were exposed to high doses of radiation during such critical gestation periods."
"Location inference on social media data for agile monitoring of public
  health crises: An application to opioid use and abuse during the Covid-19
  pandemic","The Covid-19 pandemic has intersected with the opioid epidemic to create a
unique public health crisis, with the health and economic consequences of the
virus and associated lockdowns compounding pre-existing social and economic
stressors associated with rising opioid and heroin use and abuse. In order to
better understand these interlocking crises, we use social media data to
extract qualitative and quantitative insights on the experiences of opioid
users during the Covid-19 pandemic. In particular, we use an unsupervised
learning approach to create a rich geolocated data source for public health
surveillance and analysis. To do this we first infer the location of 26,000
Reddit users that participate in opiate-related sub-communities (subreddits) by
combining named entity recognition, geocoding, density-based clustering, and
heuristic methods. Our strategy achieves 63 percent accuracy at state-level
location inference on a manually-annotated reference dataset. We then leverage
the geospatial nature of our user cohort to answer policy-relevant questions
about the impact of varying state-level policy approaches that balance economic
versus health concerns during Covid-19. We find that state government
strategies that prioritized economic reopening over curtailing the spread of
the virus created a markedly different environment and outcomes for opioid
users. Our results demonstrate that geospatial social media data can be used
for agile monitoring of complex public health crises.","['Angela E. Kilby', 'Charlie Denhart']",[],0,arXiv,http://arxiv.org/abs/2111.01778v1,False,True,False,False,False,2782,Maryann P Feldman,Triangle,Active,2023,,"This research examines how a public health crisis affects firm entry and exit, entrepreneurship, innovation, and self-employment. Specifically, the researchers consider the following questions, where local economies will be observed via region-level firm dynamism, entrepreneurial activity, innovation, and self-employment rates. How does local opioid use affect local economies, and how has the COVID crisis affected local economies? The researchers will first estimate the effects of specific public health crises - the opioid crisis and COVID pandemic - on local economies through their effects on firm dynamism. The researchers will then conduct a series of analyses that examine the interactive effects of local economic conditions and public health crises on those economies."
"Where is Dmitry going? Framing 'migratory' decisions in the criminal
  underground","The cybercriminal underground consists of hundreds of forum communities that
function as marketplaces and information-exchange platforms for both
established and wannabe cybercriminals. The ecosystem is continuously evolving,
with users migrating between forums and platforms. The emergence of cybercrime
communities in Telegram and Discord only highlights the rising fragmentation
and adaptability of the ecosystem. In this position paper, we explore the
economic incentives and trust-building mechanisms that may drive a participant
(hereafter, Dmitry) of the cybercriminal underground ecosystem to migrate from
one forum or platform to another. What are the market signals that matter to
Dmitry's decision of joining a specific community, and what roles and purposes
do these communities or platforms play within the broader ecosystem?
Ultimately, we build towards our thesis that by studying these mechanisms we
could explain, and therefore act upon, Dmitry's choice of joining a criminal
community rather than another. To build this argument, we first discuss
previous work evaluating differences in trust signals depicted in criminal
forums. We then present preliminary results evaluating criminal channels on
Telegram using those same lenses. Further, we analyze the different roles these
channels play in the criminal ecosystem. We then discuss implications for
future research.","['Luca Allodi', 'Roy Ricaldi', 'Jai Wientjes', 'Adriana Radu']",[],0,arXiv,http://arxiv.org/abs/2411.16291v1,False,True,False,False,False,2793,Conrad C Miller,Berkeley,Active,2023,,"This project will investigate racially motivated criminal justice policy using the mid-20th century Great Migration (a phenomenon where millions of Black Americans left largely rural communities in the southern United States in search of economic, social, and political freedom in northern and western cities) as a natural experiment. Researchers will quantify the causal relationship between area-level Great Migration population inflows and local punishment severity answering the question to what extent did increases in criminal justice punitiveness arise in response to growing urban Black communities in northern and western metropolitan areas?"
When a `rat race' implies an intergenerational wealth trap,"Two critical questions about intergenerational outcomes are: one, whether
significant barriers or traps exist between different social or economic
strata; and two, the extent to which intergenerational outcomes do (or can be
used to) affect individual investment and consumption decisions. We develop a
model to explicitly relate these two questions, and prove the first such `rat
race' theorem, showing that a fundamental relationship exists between high
levels of individual investment and the existence of a wealth trap, which traps
otherwise identical agents at a lower level of wealth. Our simple model of
intergenerational wealth dynamics involves agents which balance current
consumption with investment in a single descendant. Investments then determine
descendant wealth via a potentially nonlinear and discontinuous competitiveness
function about which we do not make concavity assumptions. From this model we
demonstrate how to infer such a competitiveness function from investments,
along with geometric criteria to determine individual decisions. Additionally
we investigate the stability of a wealth distribution, both to local
perturbations and to the introduction of new agents with no wealth.",['Joel Nishimura'],[],0,arXiv,http://arxiv.org/abs/1805.01019v1,False,True,False,False,False,2797,Jeremy A Kirk,Wisconsin,Active,2022,,"My project aims to study the importance of parental financial resources on the post-secondary education outcomes of their children. To study this question, I will link adult children from the American Community Survey (ACS) and the National Survey of College Graduates (NSCG) to their parents in the 2000 Decennial Long Form and Short Form Censuses and the 2010 Decennial Long Form Census. With this data, I will use a generalized difference-in-differences framework to study the impact of local house price changes experienced by parents on undergraduate attendance and degree attainment, student loan debt burden to finance education, the quality and cost of undergraduate institution attended, and graduate school attendance.  My project will contribute to an existing literature studying the impact of parental financial resources on human capital investment by evaluating the importance of changes to parental wealth at different points in the life-cycle of adult children and by evaluating how the impact of local house-price changes has evolved during the 21st Century. In line with existing research, I expect to find that exposure to large positive increases in local housing values in the early 2000s leads to increased college attainment, and that exposure to large negative shocks from The Great Recession depressed human capital investment. It is unclear if the re-acceleration of housing values in the mid to late 2010s will have the same positive impact as increases in the early 2000s, as housing wealth has been less liquid during this time period."
"Two Burning Questions on COVID-19: Did shutting down the economy help?
  Can we (partially) reopen the economy without risking the second wave?","As we reach the apex of the COVID-19 pandemic, the most pressing question
facing us is: can we even partially reopen the economy without risking a second
wave? We first need to understand if shutting down the economy helped. And if
it did, is it possible to achieve similar gains in the war against the pandemic
while partially opening up the economy? To do so, it is critical to understand
the effects of the various interventions that can be put into place and their
corresponding health and economic implications. Since many interventions exist,
the key challenge facing policy makers is understanding the potential
trade-offs between them, and choosing the particular set of interventions that
works best for their circumstance. In this memo, we provide an overview of
Synthetic Interventions (a natural generalization of Synthetic Control), a
data-driven and statistically principled method to perform what-if scenario
planning, i.e., for policy makers to understand the trade-offs between
different interventions before having to actually enact them. In essence, the
method leverages information from different interventions that have already
been enacted across the world and fits it to a policy maker's setting of
interest, e.g., to estimate the effect of mobility-restricting interventions on
the U.S., we use daily death data from countries that enforced severe mobility
restrictions to create a ""synthetic low mobility U.S."" and predict the
counterfactual trajectory of the U.S. if it had indeed applied a similar
intervention. Using Synthetic Interventions, we find that lifting severe
mobility restrictions and only retaining moderate mobility restrictions (at
retail and transit locations), seems to effectively flatten the curve. We hope
this provides guidance on weighing the trade-offs between the safety of the
population, strain on the healthcare system, and impact on the economy.","['Anish Agarwal', 'Abdullah Alomar', 'Arnab Sarker', 'Devavrat Shah', 'Dennis Shen', 'Cindy Yang']",[],0,arXiv,http://arxiv.org/abs/2005.00072v2,False,True,False,False,False,2801,Timothy Dore,Fed Board,Active,2023,,"We will study the effects of the COVID-19 pandemic on the U.S. businesses. We will link a variety of Census datasets, including the Longitudinal Business Database (LBD) and the Small Business Pulse Survey (SBPS) with external data on participation in government support programs such as the Paycheck Protection Program (PPP). 
In addition to studying SBPS survey nonresponse, we will ask several questions. First, what were the productivity effects of the pandemic? Our empirical approach will mirror Baily et al. (1992) and Foster, Haltiwanger and Krizan (2001). We expect that the pandemic will have increased the entry and exit rates of firms, particularly at the lower end of the productivity distribution, although government support programs may have lessened the degree of reallocation.
Second, we will study the effects of the pandemic, such as on revenues, investment, and participation in government programs, on different types of firms, such as differences across industries, size, and owner characteristics. This analysis will use OLS and IV regressions, where we will instrument participation in government programs with measures of pre-existing banking market characteristics. We expect that the effects of the pandemic will be highly heterogenous, with firm productivity and access to capital playing important roles.
Third, we will study how businesses respond to restrictions on business activity using a discontinuity approach around the borders of states with different levels of restrictions. We expect that restrictions likely negatively affected business operations, although these effects likely varied depending on factors such as industry and government program participation."
"When Wireless Communication Faces COVID-19: Combating the Pandemic and
  Saving the Economy","The year 2020 is experiencing a global health and economic crisis due to the
COVID-19 pandemic. Countries across the world are using digital technologies to
fight this global crisis. These digital technologies, in one way or another,
strongly rely on the availability of wireless communication technologies. In
this paper, we present the role of wireless communications in the COVID-19
pandemic from different perspectives. First, we show how these technologies are
helping to combat this pandemic, including monitoring of the virus spread,
enabling healthcare automation, and allowing virtual education and
conferencing. Also, we show the importance of digital inclusiveness in the
pandemic and possible solutions to connect the unconnected. Next, we discuss
the challenges faced by wireless technologies, including privacy, security, and
misinformation. Then, we present the importance of wireless communication
technologies in the survival of the global economy, such as automation of
industries and supply chain, e-commerce, and supporting occupations that are at
risk. Finally, we reveal that how the technologies developed during the
pandemic can be helpful in the post-pandemic era.","['Nasir Saeed', 'Ahmed Bader', 'Tareq Y. Al-Naffouri', 'Mohamed-Slim Alouini']",[],0,arXiv,http://arxiv.org/abs/2005.06637v2,False,True,False,False,False,2801,Timothy Dore,Fed Board,Active,2023,,"We will study the effects of the COVID-19 pandemic on the U.S. businesses. We will link a variety of Census datasets, including the Longitudinal Business Database (LBD) and the Small Business Pulse Survey (SBPS) with external data on participation in government support programs such as the Paycheck Protection Program (PPP). 
In addition to studying SBPS survey nonresponse, we will ask several questions. First, what were the productivity effects of the pandemic? Our empirical approach will mirror Baily et al. (1992) and Foster, Haltiwanger and Krizan (2001). We expect that the pandemic will have increased the entry and exit rates of firms, particularly at the lower end of the productivity distribution, although government support programs may have lessened the degree of reallocation.
Second, we will study the effects of the pandemic, such as on revenues, investment, and participation in government programs, on different types of firms, such as differences across industries, size, and owner characteristics. This analysis will use OLS and IV regressions, where we will instrument participation in government programs with measures of pre-existing banking market characteristics. We expect that the effects of the pandemic will be highly heterogenous, with firm productivity and access to capital playing important roles.
Third, we will study how businesses respond to restrictions on business activity using a discontinuity approach around the borders of states with different levels of restrictions. We expect that restrictions likely negatively affected business operations, although these effects likely varied depending on factors such as industry and government program participation."
"On spectral algorithms for community detection in stochastic blockmodel
  graphs with vertex covariates","In network inference applications, it is often desirable to detect community
structure, namely to cluster vertices into groups, or blocks, according to some
measure of similarity. Beyond mere adjacency matrices, many real networks also
involve vertex covariates that carry key information about underlying block
structure in graphs. To assess the effects of such covariates on block
recovery, we present a comparative analysis of two model-based spectral
algorithms for clustering vertices in stochastic blockmodel graphs with vertex
covariates. The first algorithm uses only the adjacency matrix, and directly
estimates the block assignments. The second algorithm incorporates both the
adjacency matrix and the vertex covariates into the estimation of block
assignments, and moreover quantifies the explicit impact of the vertex
covariates on the resulting estimate of the block assignments. We employ
Chernoff information to analytically compare the algorithms' performance and
derive the information-theoretic Chernoff ratio for certain models of interest.
Analytic results and simulations suggest that the second algorithm is often
preferred: we can often better estimate the induced block assignments by first
estimating the effect of vertex covariates. In addition, real data examples
also indicate that the second algorithm has the advantages of revealing
underlying block structure and taking observed vertex heterogeneity into
account in real applications. Our findings emphasize the importance of
distinguishing between observed and unobserved factors that can affect block
structure in graphs.","['Cong Mu', 'Angelo Mele', 'Lingxin Hao', 'Joshua Cape', 'Avanti Athreya', 'Carey E. Priebe']",[],0,arXiv,http://arxiv.org/abs/2007.02156v3,False,True,False,False,False,2824,Lingxin x Hao,Maryland,Active,2023,,"This research project applies the unleashed modeling capabilities and statistical inference for large-scale network models to the Longitudinal Employer and Household Dynamics (LEHD). Our central research question regards the impact of immigration on the macro, likely segmented, structure of the U.S. labor market and workers' labor market outcome inequality over the last two decades. Methodologically, the team is developing further the latent position graph modeling - extending from static graph modeling to dynamic graph modeling. Asymptotic spectral theory and computationally efficient procedures for understanding massive bipartite networks are also being developed. The empirical application to the LEHD will extend methods from the usual one-mode network setting (such as social media networks) to the bipartite network setting (such as worker-employer affiliation networks). We propose data combination methods to solve the problem in a lack of critical information on hours worked in the LEHD and to augment the SIPP survey data with the estimated individual memberships in the clusters of workers' latent positions in the labor market space. Our theoretical and empirical analyses will be complemented by the development and dissemination of open source software that implements the tractable, scalable and computationally efficient methods in analyzing large-scale social and economic networks."
Delay Analysis of Hybrid WiFi-LiFi System,"Heterogeneous wireless networks are capable of effectively leveraging
different access technologies to provide a wide variety of coverage areas. In
this paper, the coexistence of WiFi and visible light communication (VLC) is
investigated as a paradigm. The delay of two configurations of such
heterogeneous system has been evaluated. In the first configuration, the
non-aggregated system, any request is either allocated to WiFi or VLC. While in
the second configuration, the aggregated system, each request is split into two
pieces, one is forwarded to WiFi and the other is forwarded to VLC. Under the
assumptions of Poisson arrival process of requests and the exponential
distribution of requests size, it is mathematically proved that the aggregated
system provides lower minimum average system delay than that of the
non-aggregated system. For the non-aggregated system, the optimal traffic
allocation ratio is derived. For the aggregated system, an efficient solution
for the splitting ratio is proposed. Empirical results show that the solution
proposed here incurs a delay penalty (less than 3\%) over the optimal result.","['Sihua Shao', 'Abdallah Khreishah', 'Moussa Ayyash']",[],0,arXiv,http://arxiv.org/abs/1510.00740v1,False,True,False,False,False,2837,Peng Shao,Atlanta,Active,2023,,"I study the US aggregate markup from 1999 to 2019 under the presence of multidimensional unobserved heterogeneity. Accounting for heterogeneity provides a more robust measurement of the firm's markup, and markup is a useful metric to gauge a sector's competitiveness. The standard approach to markup estimation assumes the firm's unobserved heterogeneity only at the productivity factor. My project utilizes a clustering method to detect the firm's unobserved heterogeneity beyond just productivity, e.g., elasticity. I use the data covering the manufacturing, wholesale trader, and retail trader sectors from the Census Bureau. My research aims to document the differences in markup unexplained by the standard methodology. For example, my results will show the widened dispersion of the markup distribution after accounting for additional unobserved heterogeneity."
Gender Differences in Wage Expectations,"Using a survey on wage expectations among students at two Swiss institutions
of higher education, we examine the wage expectations of our respondents along
two main lines. First, we investigate the rationality of wage expectations by
comparing average expected wages from our sample with those of similar
graduates; we further examine how our respondents revise their expectations
when provided information about actual wages. Second, using causal mediation
analysis, we test whether the consideration of a rich set of personal and
professional controls, namely concerning family formation and children in
addition to professional preferences, accounts for the difference in wage
expectations across genders. We find that males and females overestimate their
wages compared to actual ones, and that males respond in an overconfident
manner to information about outside wages. Despite the attenuation of the
gender difference in wage expectations brought about by the comprehensive set
of controls, gender generally retains a significant direct, unexplained effect
on wage expectations.","['Ana Fernandes', 'Martin Huber', 'Giannina Vaccaro']",[],0,arXiv,http://arxiv.org/abs/2003.11496v1,False,True,False,False,False,2842,Isaac Cohen,Cornell,Active,2023,,"In this paper, I will use event study methods to study the dynamic relationships between labor market absences caused by family formation events and physicians' labor market outcomes, including their earnings, labor supply, accumulation of task-specific experience, and specialization. In order to study these relationships, I will create a longitudinal, quarterly dataset of individual physicians' careers spanning 2008-2020 by using the Numident File and Master Address Files from the US Census Bureau, together with the AMA Masterfile, to link collapsed physician-level data from the restricted-use version of the Medicare carrier file to physician-level information about household structure and demographics from the Census Household Composition Key File and the restricted-access American Community Survey. Using this dataset, I will estimate event studies with rich controls to characterize the average dynamic paths of physicians' labor market behavior around the time of their family formation decisions including child births and marriages. I will use the results of these event studies to study how much the accumulation of task-specific work experience affects gender gaps in labor market outcomes among physicians over the career cycle."
"Patterns of Sociotechnical Design Preferences of Chatbots for
  Intergenerational Collaborative Innovation : A Q Methodology Study","Chatbot technology is increasingly emerging as a virtual assistant. Chatbots
could allow individuals and organizations to accomplish objectives that are
currently not fully optimized for collaboration across an intergenerational
context. This paper explores the preferences of chatbots as a companion in
intergenerational innovation. The Q methodology was used to investigate
different types of collaborators and determine how different choices occur
between collaborators that merge the problem and solution domains of chatbots'
design within intergenerational settings. The study's findings reveal that
various chatbot design priorities are more diverse among younger adults than
senior adults. Additionally, our research further outlines the principles of
chatbot design and how chatbots will support both generations. This research is
the first step towards cultivating a deeper understanding of different age
groups' subjective design preferences for chatbots functioning as a companion
in the workplace. Moreover, this study demonstrates how the Q methodology can
guide technological development by shifting the approach from an age-focused
design to a common goal-oriented design within a multigenerational context.","['Irawan Nurhas', 'Pouyan Jahanbin', 'Jan Pawlowski', 'Stephen Wingreen', 'Stefan Geisler']",[],0,arXiv,http://arxiv.org/abs/2212.03485v1,False,True,False,False,False,2849,Zachary Ward,Texas,Active,2023,,"This project will produce consistent estimates of intragenerational, intergenerational, and multigenerational mobility between 1850 and today. We will combine data from the 1850 to 1940 United States Decennial Censuses with post-1940 data from the Decennial Census, the Current Population Survey-Annual Social and Economic Supplements (ASEC), and the American Community Survey. Our estimates will improve upon the existing literature by including women and non-white individuals across the entire period. Studies do not include women and non-white individuals in pre-1940 data due to data limitations, but we can overcome these limitations with a new dataset based on genealogical data that allows us to link across historical censuses. We also contribute to the literature by producing estimates that account for measurement error when measuring social status, error that may vary across data sources. In addition to estimates of social mobility for the overall population, we will estimate how mobility varies by nativity status, which will contribute to understanding how economic assimilation has changed across immigration cohorts in the 19th Century, early 20th Century, and today. Overall, our project will allow us to measure how ""equality of opportunity"" has changed across race, ethnicity, and gender over three centuries."
"Exploring the Opportunities of AR for Enriching Storytelling with Family
  Photos between Grandparents and Grandchildren","Storytelling with family photos, as an important mode of reminiscence-based
activities, can be instrumental in promoting intergenerational communication
between grandparents and grandchildren by strengthening generation bonds and
shared family values. Motivated by challenges that existing technology
approaches encountered for improving intergenerational storytelling (e.g., the
need to hold the tablet, the potential view detachment from the physical world
in Virtual Reality (VR)), we sought to find new ways of using Augmented Reality
(AR) to support intergenerational storytelling, which offers new capabilities
(e.g., 3D models, new interactivity) to enhance the expression for the
storyteller. We conducted a two-part exploratory study, where pairs of
grandparents and grandchildren 1) participated in an in-person storytelling
activity with a semi-structured interview 2) and then a participatory design
session with AR technology probes that we designed to inspire their
exploration. Our findings revealed insights into the possible ways of
intergenerational storytelling, the feasibility and usages of AR in
facilitating it, and the key design implications for leveraging AR in
intergenerational storytelling.","['Zisu Li', 'Li Feng', 'Chen Liang', 'Yuru Huang', 'Mingming Fan']",[],0,arXiv,http://arxiv.org/abs/2309.03533v1,False,True,False,False,False,2849,Zachary Ward,Texas,Active,2023,,"This project will produce consistent estimates of intragenerational, intergenerational, and multigenerational mobility between 1850 and today. We will combine data from the 1850 to 1940 United States Decennial Censuses with post-1940 data from the Decennial Census, the Current Population Survey-Annual Social and Economic Supplements (ASEC), and the American Community Survey. Our estimates will improve upon the existing literature by including women and non-white individuals across the entire period. Studies do not include women and non-white individuals in pre-1940 data due to data limitations, but we can overcome these limitations with a new dataset based on genealogical data that allows us to link across historical censuses. We also contribute to the literature by producing estimates that account for measurement error when measuring social status, error that may vary across data sources. In addition to estimates of social mobility for the overall population, we will estimate how mobility varies by nativity status, which will contribute to understanding how economic assimilation has changed across immigration cohorts in the 19th Century, early 20th Century, and today. Overall, our project will allow us to measure how ""equality of opportunity"" has changed across race, ethnicity, and gender over three centuries."
"The long-term impact of (un)conditional cash transfers on labour market
  outcomes in Ecuador","Despite the popularity of conditional cash transfers in low- and
middle-income countries, evidence on their long-term effects remains scarce.
This study assesses the impact of Ecuador's Human Development Grant on the
formal sector labor market outcomes of children in eligible households. This
grant, one of the first of its kind, is characterized by weak enforcement of
its eligibility criteria. Using a regression discontinuity design, we find that
the programme had no impact on formal employment rates and labour income earned
in the formal sector around a decade after exposure, and thus not affecting the
intergenerational transmission of poverty. We discuss possible explanations for
the lack of significant results considering how the programme contributes to
persistence in school in the medium term.","['Juan Ponce', 'José-Ignacio Antón', 'Mercedes Onofa', 'Roberto Castillo']",[],0,arXiv,http://arxiv.org/abs/2309.17216v2,False,True,False,False,False,2854,Andrew Joung,Michigan,Active,2023,,"Our project will examine how prenatal and early-life exposure to the social safety net and poor economic conditions shape the long-run and intergenerational outcomes of youth. To estimate this effect, we will use a standard event-study framework and leverage quasi-experimental variation in the timing of anti-poverty program rollouts and reforms, and exposure to negative economic shocks across cohorts. We will separately identify the effect of these early-life shocks on future outcomes for directly affected youth as well as intergenerational spillover effects onto their children. We will implement this approach with rich administrative set, linking earnings records from the LEHD and SSA, criminal justice records from CJARS, public program participation records from the SSR and state agencies, and survey-based outcomes from the ACS, CPS, and SIPP."
An Agent-Based Model for Poverty and Discrimination Policy-Making,"The deceleration of global poverty reduction in the last decades suggests
that traditional redistribution policies are losing their effectiveness.
Alternative ways to work towards the #1 United Nations Sustainable Development
Goal (poverty eradication) are required. NGOs have insistingly denounced the
criminalization of poverty, and the social science literature suggests that
discrimination against the poor (a phenomenon known as aporophobia) could
constitute a brake to the fight against poverty. This paper describes a
proposal for an agent-based model to examine the impact that aporophobia at the
institutional level has on poverty levels. This aporophobia agent-based model
(AABM) will first be applied to a case study in the city of Barcelona. The
regulatory environment is central to the model, since aporophobia has been
identified in the legal framework. The AABM presented in this paper constitutes
a cornerstone to obtain empirical evidence, in a non-invasive way, on the
causal relationship between aporophobia and poverty levels. The simulations
that will be generated based on the AABM have the potential to inform a new
generation of poverty reduction policies, which act not only on the
redistribution of wealth but also on the discrimination of the poor.","['Nieves Montes', 'Georgina Curto', 'Nardine Osman', 'Carles Sierra']",[],0,arXiv,http://arxiv.org/abs/2303.13994v1,False,True,False,False,False,2854,Andrew Joung,Michigan,Active,2023,,"Our project will examine how prenatal and early-life exposure to the social safety net and poor economic conditions shape the long-run and intergenerational outcomes of youth. To estimate this effect, we will use a standard event-study framework and leverage quasi-experimental variation in the timing of anti-poverty program rollouts and reforms, and exposure to negative economic shocks across cohorts. We will separately identify the effect of these early-life shocks on future outcomes for directly affected youth as well as intergenerational spillover effects onto their children. We will implement this approach with rich administrative set, linking earnings records from the LEHD and SSA, criminal justice records from CJARS, public program participation records from the SSR and state agencies, and survey-based outcomes from the ACS, CPS, and SIPP."
"Smiling Women Pitching Down: Auditing Representational and
  Presentational Gender Biases in Image Generative AI","Generative AI models like DALL-E 2 can interpret textual prompts and generate
high-quality images exhibiting human creativity. Though public enthusiasm is
booming, systematic auditing of potential gender biases in AI-generated images
remains scarce. We addressed this gap by examining the prevalence of two
occupational gender biases (representational and presentational biases) in
15,300 DALL-E 2 images spanning 153 occupations, and assessed potential bias
amplification by benchmarking against 2021 census labor statistics and Google
Images. Our findings reveal that DALL-E 2 underrepresents women in
male-dominated fields while overrepresenting them in female-dominated
occupations. Additionally, DALL-E 2 images tend to depict more women than men
with smiling faces and downward-pitching heads, particularly in
female-dominated (vs. male-dominated) occupations. Our computational algorithm
auditing study demonstrates more pronounced representational and presentational
biases in DALL-E 2 compared to Google Images and calls for feminist
interventions to prevent such bias-laden AI-generated images to feedback into
the media ecology.","['Luhang Sun', 'Mian Wei', 'Yibing Sun', 'Yoo Ji Suh', 'Liwei Shen', 'Sijia Yang']",[],0,arXiv,http://arxiv.org/abs/2305.10566v1,False,True,False,False,False,2862,Henry Downes,Chicago,Active,2023,,"The fundamental causes of the American Baby Boom are not well-understood. In particular, standard theories fail to predict its timing. I propose to shed new light on the origins of the Baby Boom by exploring a new channel: the effect of labor union membership on fertility. Following the passage of the 1935 Wagner Act, union membership rates more than tripled, and by the peak of the Baby Boom one-third of American workers were members of a labor union. Using the Decennial Census and novel historical data sources of union membership, I will test whether unionization is causally related to completed fertility and birth rates during this period."
From signaling to interviews in random matching markets,"In many two-sided labor markets, interviews are conducted before matches are
formed. An increase in the number of interviews in the market for medical
residencies raised the demand for signaling mechanisms, in which applicants can
send a limited number of signals to communicate interest. We study the role of
signaling mechanisms in reducing the number of interviews in centralized random
matching markets with post-interview shocks. For the market to clear we focus
on interim stability, which extends the notion of stability to ensure that
agents do not regret not interviewing with each other. A matching is almost
interim stable if it is interim stable after removing a vanishingly small
fraction of agents.
  We first study signaling mechanisms in random matching markets when agents on
the short side, long side, or both sides signal their top~$d$ preferred
partners. Interviews graphs are formed by including all pairs where at least
one party has signaled the other. We show that when $d = \omega(1)$, short-side
signaling leads to almost interim stable matchings. Long-side signaling is only
effective when the market is almost balanced. Conversely, when the interview
shocks are negligible and $d = o(\log n)$, both-side signaling fails to achieve
almost interim stability. For larger $d \ge \Omega(\log^2 n)$, short-side
signaling achieves perfect interim stability, while long-side signaling fails
in imbalanced markets. We build on our findings to propose a signaling
mechanism for multi-tiered random markets. Our analysis identifies conditions
under which signaling mechanisms are incentive compatible. A technical
contribution is the analysis of a message-passing algorithm that efficiently
determines interim stability and matching outcomes by leveraging local
neighborhood structures.","['Maxwell Allman', 'Itai Ashlagi', 'Amin Saberi', 'Sophie H. Yu']",[],0,arXiv,http://arxiv.org/abs/2501.14159v1,False,True,False,False,False,2863,Maxwell Miller,Boston,Active,2023,,"We investigate the relationship between government spending shocks and product-market concentration at the local level using data from the Longitudinal Business Database and the associated economic censuses from the Census Bureau. To estimate causal effects of local government spending, we exploit the fact that many federal spending programs to local areas depend on local population levels, and that local population is estimated with two different methodologies depending on whether the year is a Census or a non-Census year. Our preliminary findings suggest a permanent increase in local product-market concentration following positive shocks to government spending. We then develop a model in the spirit of Aghion and Howitt (1992) where competition discourages laggard firms from innovating but encourages neck-and-neck firms to innovate. Together with the effect of competition on the equilibrium industry structure, we provide the conditions under which a permanent increase in government spending leads to higher industry concentration. We then expect to show that these theoretical conditions hold in the data. Our results shed light on the unintended consequences of increased government spending, and its potential relationship to increasing concentration and profits in the United States."
"Supply Chain Digital Twin Framework Design: An Approach of Supply Chain
  Operations Reference Model and System of Systems","Digital twin technology has been regarded as a beneficial approach in supply
chain development. Different from traditional digital twin (temporal dynamic),
supply chain digital twin is a spatio-temporal dynamic system. This paper
explains what is 'twined' in supply chain digital twin and how to 'twin' them
to handle the spatio-temporal dynamic issue. A supply chain digital twin
framework is developed based on the theories of system of systems and supply
chain operations reference model. This framework is universal and can be
applied in various types of supply chain systems. We firstly decompose the
supply chain system into unified standard blocks preparing for the adoption of
digital twin. Next, the idea of supply chain operations reference model is
adopted to digitise basic supply chain activities within each block and explain
how to use existing information system. Then, individual sub-digital twin is
established for each member in supply chain system. After that, we apply the
concept of system of systems to integrate and coordinate sub-digital twin into
supply chain digital twin from the views of supply chain business integration
and information system integration. At last, one simple supply chain system is
applied to illustrate the application of the proposed model.","['Jie Zhang', 'Alexandra Brintrup', 'Anisoara Calinescu', 'Edward Kosasih', 'Angira Sharma']",[],0,arXiv,http://arxiv.org/abs/2107.09485v1,False,True,False,False,False,2878,Bin Wei,Atlanta,Active,2024,,"In this project, we propose to investigate the interplay between the firms' price- and wage-setting behavior and supply chains, and examine the implications of this interaction for inflation dynamics and the associated wage-price spirals. We construct a novel firm-level wage and price data by merging the wage data from the Census Bureau, the price data underlying  producer price indexes (PPIs) and import/export price indexes (IPP) from the Bureau of Labor Statistics (BLS), as well as the dataset on supply chains collected based on SEC filings. We plan to study the following questions: How do pandemic-induced changes in a supplier-firm's price-setting behavior affect the price-setting behavior of its customer-firms? How do changes in the structure of supplier-customer networks impact price-setting behavior of firms in supply-chain networks? How do price pressures impact wage-setting behavior of firms in supply-chain networks? The answers to these questions help us better understand how price pressures transmit through supply chains and helps us resolve the debate on causation between wage and price inflation."
"Flipping the Script on Criminal Justice Risk Assessment: An actuarial
  model for assessing the risk the federal sentencing system poses to
  defendants","In the criminal justice system, algorithmic risk assessment instruments are
used to predict the risk a defendant poses to society; examples include the
risk of recidivating or the risk of failing to appear at future court dates.
However, defendants are also at risk of harm from the criminal justice system.
To date, there exists no risk assessment instrument that considers the risk the
system poses to the individual. We develop a risk assessment instrument that
""flips the script."" Using data about U.S. federal sentencing decisions, we
build a risk assessment instrument that predicts the likelihood an individual
will receive an especially lengthy sentence given factors that should be
legally irrelevant to the sentencing decision. To do this, we develop a
two-stage modeling approach. Our first-stage model is used to determine which
sentences were ""especially lengthy."" We then use a second-stage model to
predict the defendant's risk of receiving a sentence that is flagged as
especially lengthy given factors that should be legally irrelevant. The factors
that should be legally irrelevant include, for example, race, court location,
and other socio-demographic information about the defendant. Our instrument
achieves comparable predictive accuracy to risk assessment instruments used in
pretrial and parole contexts. We discuss the limitations of our modeling
approach and use the opportunity to highlight how traditional risk assessment
instruments in various criminal justice settings also suffer from many of the
same limitations and embedded value systems of their creators.","['Mikaela Meyer', 'Aaron Horowitz', 'Erica Marshall', 'Kristian Lum']",[],0,arXiv,http://arxiv.org/abs/2205.13505v2,False,True,False,False,False,2882,William J von Geldern,Seattle,Active,2023,,"Research from a variety of academic disciplines has demonstrated that low-income Americans disproportionately experience both criminal justice system involvement and housing instability. Extant research has not, however, evaluated the connection between the two. For this project, I hope to link CJARS criminal record  and LSC eviction data, which will enable examination of whether eviction and other housing outcomes lead to subsequent criminal justice system involvement. Building on past work in criminology, sociology, and policy studies, this work will generate new evidence about the linkages between housing and criminal justice outcomes in the U.S."
"A Comprehensive Survey of Artificial Intelligence Techniques for Talent
  Analytics","In today's competitive and fast-evolving business environment, it is a
critical time for organizations to rethink how to make talent-related decisions
in a quantitative manner. Indeed, the recent development of Big Data and
Artificial Intelligence (AI) techniques have revolutionized human resource
management. The availability of large-scale talent and management-related data
provides unparalleled opportunities for business leaders to comprehend
organizational behaviors and gain tangible knowledge from a data science
perspective, which in turn delivers intelligence for real-time decision-making
and effective talent management at work for their organizations. In the last
decade, talent analytics has emerged as a promising field in applied data
science for human resource management, garnering significant attention from AI
communities and inspiring numerous research efforts. To this end, we present an
up-to-date and comprehensive survey on AI technologies used for talent
analytics in the field of human resource management. Specifically, we first
provide the background knowledge of talent analytics and categorize various
pertinent data. Subsequently, we offer a comprehensive taxonomy of relevant
research efforts, categorized based on three distinct application-driven
scenarios: talent management, organization management, and labor market
analysis. In conclusion, we summarize the open challenges and potential
prospects for future research directions in the domain of AI-driven talent
analytics.","['Chuan Qin', 'Le Zhang', 'Yihang Cheng', 'Rui Zha', 'Dazhong Shen', 'Qi Zhang', 'Xi Chen', 'Ying Sun', 'Chen Zhu', 'Hengshu Zhu', 'Hui Xiong']",[],0,arXiv,http://arxiv.org/abs/2307.03195v2,False,True,False,False,False,2890,Jessica Bai,Boston,Active,2023,,"The project uses a combination of Census Bureau and external, proprietary microdata on venture
capital funding to study labor mobility across the market cycle for technology and growth firms.
Specifically, it will produce novel estimates on the consequences of early-stage financing for
employment dynamics among high-growth, entrepreneurial businesses, an important subset of the
economy that the Census Bureau's current data infrastructure provides relatively little information
about. To produce these estimates, we leverage LBD and LEHD data that allow us to observe
patterns of mobility and displacement over a long period of time and over multiple boom and bust
episodes. When combined with the most comprehensive data available on venture capital funding,
we will be able to recover new estimates on the consequences of technology cycles for the
allocation of human capital in addition to producing causal estimates of the effect of a market
correction or ""bust"" on labor market outcomes."
"Credit risk and companies' inter-organizational networks: Assessing
  impact of suppliers and buyers on CDS spreads","Companies do not operate in a vacuum. As companies move towards an
increasingly specialized production function and their reach is becoming truly
global, their aptitude in managing and shaping their inter-organizational
network is a determining factor in measuring their health. Current models of
company financial health often lack variables explaining the
inter-organizational network, and as such, assume that (1) all networks are the
same and (2) the performance of partners do not impact companies. This paper
aims to be a first step in the direction of removing these assumptions.
Specifically, the impact is illustrated by examining the effects of customer
and supplier concentrations and partners' credit risk on credit-default swap
(CDS) spreads while controlling for credit risk and size. We rely upon
supply-chain data from Bloomberg that provides insight into companies'
relationships. The empirical results show that a well diversified customer
network lowers CDS spread, while having stable partners with low default
probabilities increase spreads. The latter result suggests that successful
companies do not focus on building a stable eco-system around themselves, but
instead focus on their own profit maximization at the cost of the financial
health of their suppliers' and customers'. At a more general level, the results
indicate the importance of considering the inter-organizational networks, and
highlight the value of including network variables in credit risk models.","['Tore Opsahl', 'William Newton']",[],0,arXiv,http://arxiv.org/abs/1602.06585v1,False,True,False,False,False,2891,Judson Caskey,UCLA,Active,2023,,"Corporate actions can affect firms' top managers, investors, consumers, and other firms in the same industry in various ways. However, there is little evidence of the effects of different types of corporate actions on international trade. In this project, we plan to use the Longitudinal Foreign Trade Transactions Database (LFTTD) to investigate whether different types of corporate actions affect companies' customer-supplier relationships, especially in the international market. Specifically, we will examine the impacts on prices, trade volumes, total exports/imports, the number of trading partners, and the number of destination countries."
Measuring times to determine positions,"Among the first devices used to measure the time we find the sundials and the
water-clocks, as told by Vitruvius in his book on the Architecture. The
sundials work because of the shadows cast by a rod or pole, the gnomon, on
their basements. Besides being an instrument able to measure the time
intervals, the sundial provided information on Earth and heaven to the ancient
astronomers. Here we discuss how this device is able to determine the latitude
and the north-south direction. The problem of the longitude is also shortly
discussed.",['Amelia Carolina Sparavigna'],[],0,arXiv,http://arxiv.org/abs/1202.2746v2,False,True,False,False,False,2892,Chinedum O Ojinnaka,Irvine,Active,2024,,"The Supplemental Nutrition Assistance Program (SNAP), the largest safety-net program that targets food insecurity in the United States, reduces food insecurity and poverty and improves health outcomes among participants. The importance of SNAP is highlighted by the Families First Coronavirus Response Act (FFCRA) which was enacted at the onset of the COVID-19. This project uses the 2014-2021 North Carolina and South Carolina data to analyze the relationship between stable SNAP participation and population-level factors (food deserts, rural/urban residence, residential segregation). We will determine: 1) trends in SNAP participation patterns (2014-2021); 2) the relationship between population-level factors and measures of stable SNAP participation before the COVID-19 pandemic; 3) the association between population-level factors and stable SNAP participation during the pandemic. For each aim, we will also determine any difference between White and Black clients. Stable SNAP participation will be measured over a 12-month period and will include: 1) number of months on SNAP; 2) churning-exit and re-entry into SNAP within four months (yes, no) and 3) participation pattern (continuous vs discontinued/intermittent)."
Unusually High Metallicity Host Of The Dark LGRB 051022,"We present spectroscopy of the host of GRB 051022 with GMOS nod and shuffle
on Gemini South and NIRSPEC on Keck II. We determine a metallicity for the host
of log(O/H)+12 = 8.77 using the R23 method (Kobulnicky & Kewley 2004 scale)
making this the highest metallicity long burst host yet observed. The galaxy
itself is unusually luminous for a LGRB host with a rest frame B band absolute
magnitude -21.5 and has the spectrum of a rapidly star-forming galaxy. Our work
raises the question of whether other dark burst hosts will show high
metallicities.","['J. F. Graham', 'A. S. Fruchter', 'L. J. Kewley', 'E. M. Levesque', 'A. J. Levan', 'N. R. Tanvir', 'D. E. Reichart', 'M. Nysewander']",[],0,arXiv,http://arxiv.org/abs/0903.5544v1,False,True,False,False,False,2892,Chinedum O Ojinnaka,Irvine,Active,2024,,"The Supplemental Nutrition Assistance Program (SNAP), the largest safety-net program that targets food insecurity in the United States, reduces food insecurity and poverty and improves health outcomes among participants. The importance of SNAP is highlighted by the Families First Coronavirus Response Act (FFCRA) which was enacted at the onset of the COVID-19. This project uses the 2014-2021 North Carolina and South Carolina data to analyze the relationship between stable SNAP participation and population-level factors (food deserts, rural/urban residence, residential segregation). We will determine: 1) trends in SNAP participation patterns (2014-2021); 2) the relationship between population-level factors and measures of stable SNAP participation before the COVID-19 pandemic; 3) the association between population-level factors and stable SNAP participation during the pandemic. For each aim, we will also determine any difference between White and Black clients. Stable SNAP participation will be measured over a 12-month period and will include: 1) number of months on SNAP; 2) churning-exit and re-entry into SNAP within four months (yes, no) and 3) participation pattern (continuous vs discontinued/intermittent)."
"Bayesian Safe Policy Learning with Chance Constrained Optimization:
  Application to Military Security Assessment during the Vietnam War","Algorithmic decisions and recommendations are used in many high-stakes
decision-making settings such as criminal justice, medicine, and public policy.
We investigate whether it would have been possible to improve a security
assessment algorithm employed during the Vietnam War, using outcomes measured
immediately after its introduction in late 1969. This empirical application
raises several methodological challenges that frequently arise in high-stakes
algorithmic decision-making. First, before implementing a new algorithm, it is
essential to characterize and control the risk of yielding worse outcomes than
the existing algorithm. Second, the existing algorithm is deterministic, and
learning a new algorithm requires transparent extrapolation. Third, the
existing algorithm involves discrete decision tables that are difficult to
optimize over.
  To address these challenges, we introduce the Average Conditional Risk
(ACRisk), which first quantifies the risk that a new algorithmic policy leads
to worse outcomes for subgroups of individual units and then averages this over
the distribution of subgroups. We also propose a Bayesian policy learning
framework that maximizes the posterior expected value while controlling the
posterior expected ACRisk. This framework separates the estimation of
heterogeneous treatment effects from policy optimization, enabling flexible
estimation of effects and optimization over complex policy classes. We
characterize the resulting chance-constrained optimization problem as a
constrained linear programming problem. Our analysis shows that compared to the
actual algorithm used during the Vietnam War, the learned algorithm assesses
most regions as more secure and emphasizes economic and political factors over
military factors.","['Zeyang Jia', 'Eli Ben-Michael', 'Kosuke Imai']",[],0,arXiv,http://arxiv.org/abs/2307.08840v2,False,True,False,False,False,2896,Melissa Desfor,Philadelphia,Active,2023,,"Recent trends in global migration have raised questions surrounding immigrants' national identification and assimilation into the American mainstream. What explains variation in immigrants' integration choices? Although seminal theories in political science argue that military service is a critical driver of assimilation, some scholars have challenged the empirical basis and theoretical logic underpinning this relationship. A major obstacle bedeviling the study of military service and integration is self-selection: immigrants who are better assimilated may be more likely to join the military in the first place. We address the selection problem by examining the effects of military conscription during the Vietnam War using an instrumental variables approach. Conscription during the crucial years 1970-1972 was decided on the basis of national draft lotteries, which assigned draft numbers based on an individual's date of birth. We use the draft lottery to instrument for military service and estimate the causal effect of service on a range of integration outcomes using granular data from the decennial censuses. Our study thus contributes novel evidence to key debates on the implications of military service for assimilation and national identification, while also highlighting a potential role for public policy to encourage immigrant incorporation via national service."
"Quantifying Global Food Trade: A Net Caloric Content Approach to Food
  Trade Network Analysis","As the global population and the per capita demand for resource intensive
diets continues to grow, the corresponding increase in food demand challenges
the global food system, enhancing its reliance on trade. Most previous research
typically constructed either unweighted networks or weighted solely by tonnage
to represent food trade, and focused on bilateral trade relationships between
pairs of countries. This study investigates the properties of global food trade
constructed in terms of total food calories associated with all the main food
products exchanged along each trade link (edge of the food trade network).
Utilizing data from the Food and Agriculture Organization between 1986 and
2022, we construct a directed, weighted network of net caloric flows between
countries. This approach highlights the importance of considering nutritional
value in discussions of food security and trade policies, offering a more
holistic view of global food trade dynamics. Our analysis reveals significant
heterogeneity in trade patterns, with certain countries emerging as major
exporters or importers of food calories. Moreover, we employ network measures,
including network connectivity, network heterogeneity, network modularity, and
node correlation similarity, to elucidate the structural dynamics of global net
food calorie trade networks that are relevant to the stability and resilience
of the global food system. Our work provides a more nuanced understanding of
global food trade dynamics, emphasizing the need for comprehensive strategies
to enhance the resilience and sustainability of food trade networks.","['Xiaopeng Wang', 'Chengyi Tu', 'Shuhao Chen', 'Sicheng Wang', 'Ying Fan', 'Samir Suweis', ""Paolo D'Odorico""]",[],0,arXiv,http://arxiv.org/abs/2411.18856v2,False,True,False,False,False,2897,Patrick Brady,Minnesota,Active,2023,,"Food insecurity is a significant public health problem in the United States and the are concerns of the impact of the COVID-19 pandemic on household food security. While there are many programs and service available to address food insecurity, including the Supplemental Nutrition Assistance Program (SNAP) and the emergency food system, use of these resources differs between demographic groups. Urban/rural status is a key sociodemographic factor that influences food security and use of food assistance services, but publicly available measures of urban/rural status are imprecise. This study aims to understand relationships between food security, SNAP participation, and receipt of emergency food, and the rural-urban continuum before and during the COVID-19 pandemic. To do this, we will link USDA Rural-Urban Commuting Area (RUCA) codes to 2015-2021 Current Population Survey Food Security Supplement data. RUCA codes provide census-level tract measures of the urban/rural continuum that will allow for a granular examination of the role of urban/rural status plays in our outcomes of interest (food security, SNAP participation, and receipt of emergency food). We will produce weighted prevalences for each outcome and use multivariable logistic regression to examine the impact of time period (before versus during COVID-19 pandemic) and urban/rural status on each. This research is critical to understand rural and urban differences in food security and use of food assistance resources as social determinants of health during the COVID-19 pandemic and influence future policy and programmatic decisions."
"Who Followed the Blueprint? Analyzing the Responses of U.S. Federal
  Agencies to the Blueprint for an AI Bill of Rights","This study examines the extent to which U.S. federal agencies responded to
and implemented the principles outlined in the White House's October 2022
""Blueprint for an AI Bill of Rights."" The Blueprint provided a framework for
the ethical governance of artificial intelligence systems, organized around
five core principles: safety and effectiveness, protection against algorithmic
discrimination, data privacy, notice and explanation about AI systems, and
human alternatives and fallback.
  Through an analysis of publicly available records across 15 federal
departments, the authors found limited evidence that the Blueprint directly
influenced agency actions after its release. Only five departments explicitly
mentioned the Blueprint, while 12 took steps aligned with one or more of its
principles. However, much of this work appeared to have precedents predating
the Blueprint or motivations disconnected from it, such as compliance with
prior executive orders on trustworthy AI. Departments' activities often
emphasized priorities like safety, accountability and transparency that
overlapped with Blueprint principles, but did not necessarily stem from it.
  The authors conclude that the non-binding Blueprint seems to have had minimal
impact on shaping the U.S. government's approach to ethical AI governance in
its first year. Factors like public concerns after high-profile AI releases and
obligations to follow direct executive orders likely carried more influence
over federal agencies. More rigorous study would be needed to definitively
assess the Blueprint's effects within the federal bureaucracy and broader
society.","['Darren Lage', 'Riley Pruitt', 'Jason Ross Arnold']",[],0,arXiv,http://arxiv.org/abs/2404.19076v1,False,True,False,False,False,2907,Regina Werum,Nebraska,Active,2023,,"Our overarching substantive goal is to identify the pathways that lead to increased representation of veterans in STEM fields, particularly among women and racial minorities. Prior research has already established a strong statistical association between military service and STEM-related educational and occupational outcomes. Although there has been a growing amount of research on the gendered dynamics of the veteran-to-STEM pipeline, there is much limited work on the racial, class, and institutional-dynamics that may contribute to this relationship. Moreover, the mechanisms producing this seemingly counterintuitive pattern remain unclear due to the limitations of publicly-available data. As a step toward our long-term goal, our primary substantive goal for this proposal is to identify the links between military service and educational/occupational outcomes that effectively broaden participation in STEM. By combining multiple surveys and administrative data only made possible via the secure RDC environment, our findings will allow us to track longitudinal trajectories related to military service, education, and career - transcending the limitations of extant cross-sectional analyses that only allow an examination of the correlation between military service and STEM outcomes. Using the 2000 Decennial Census (long-form), the 2005-current American Community Survey, the 2013-current National Survey of College Graduates, externally provided IPEDS institutional data, and the 2020 Department of Veteran's Affairs U.S. Veteran's File, we seek to examine to what extent military service/veteran status impact STEM trajectories (educational and occupational), including how this association is moderated by gender, race, class, and institutional characteristics - individually and collectively. We also plan to examine whether our findings vary when using different definitions of ""STEM"" from other federal agencies and may be most pronounced among those in Computer Science and Engineering (CS&E) fields. We plan to use logistic, multinomial logistic, and KHB decomposition analyses to examine our focal aims."
"Using massive health insurance claims data to predict very high-cost
  claimants: a machine learning approach","Due to escalating healthcare costs, accurately predicting which patients will
incur high costs is an important task for payers and providers of healthcare.
High-cost claimants (HiCCs) are patients who have annual costs above
$\$250,000$ and who represent just 0.16% of the insured population but
currently account for 9% of all healthcare costs. In this study, we aimed to
develop a high-performance algorithm to predict HiCCs to inform a novel care
management system. Using health insurance claims from 48 million people and
augmented with census data, we applied machine learning to train binary
classification models to calculate the personal risk of HiCC. To train the
models, we developed a platform starting with 6,006 variables across all
clinical and demographic dimensions and constructed over one hundred candidate
models. The best model achieved an area under the receiver operating
characteristic curve of 91.2%. The model exceeds the highest published
performance (84%) and remains high for patients with no prior history of
high-cost status (89%), who have less than a full year of enrollment (87%), or
lack pharmacy claims data (88%). It attains an area under the precision-recall
curve of 23.1%, and precision of 74% at a threshold of 0.99. A care management
program enrolling 500 people with the highest HiCC risk is expected to treat
199 true HiCCs and generate a net savings of $\$7.3$ million per year. Our
results demonstrate that high-performing predictive models can be constructed
using claims data and publicly available data alone, even for rare high-cost
claimants exceeding $\$250,000$. Our model demonstrates the transformational
power of machine learning and artificial intelligence in care management, which
would allow healthcare payers and providers to introduce the next generation of
care management programs.","['José M. Maisog', 'Wenhong Li', 'Yanchun Xu', 'Brian Hurley', 'Hetal Shah', 'Ryan Lemberg', 'Tina Borden', 'Stephen Bandeian', 'Melissa Schline', 'Roxanna Cross', 'Alan Spiro', 'Russ Michael', 'Alexander Gutfraind']",[],0,arXiv,http://arxiv.org/abs/1912.13032v1,False,True,False,False,False,2909,Franz Fuchs,Colorado,Active,2024,,"In the world of emergency medical services, understanding ""payer mix"" -- i.e., the percentage of patients that are uninsured, covered by public programs, or privately insured -- can help public health departments make informed decisions about how to allocate resources like ambulatory services. However, detailed estimates of the relevant population characteristics at sufficient levels of geographic granularity carry disclosure risks that threaten respondent confidentiality, when produced using standard methodological approaches. This project aims to produce a block group-level map of the payer mix by relevant sociodemographic groups for the State of Wyoming, in coordination with the State's Department of Health, from 10-year pooled American Community Survey data by using Bayesian hierarchical generalized additive models. The resulting estimates will allow the State to proceed with data-informed health policy decisions while upholding disclosure protections and providing a roadmap for how Census might produce similar estimates from geographic areas with small populations in the future."
"Where to Build Food Banks and Pantries: A Two-Level Machine Learning
  Approach","Over 44 million Americans currently suffer from food insecurity, of whom 13
million are children. Across the United States, thousands of food banks and
pantries serve as vital sources of food and other forms of aid for food
insecure families. By optimizing food bank and pantry locations, food would
become more accessible to families who desperately require it. In this work, we
introduce a novel two-level optimization framework, which utilizes the
K-Medoids clustering algorithm in conjunction with the Open-Source Routing
Machine engine, to optimize food bank and pantry locations based on real road
distances to houses and house blocks. Our proposed framework also has the
adaptability to factor in considerations such as median household income using
a pseudo-weighted K-Medoids algorithm. Testing conducted with California and
Indiana household data, as well as comparisons with real food bank and pantry
locations showed that interestingly, our proposed framework yields food pantry
locations superior to those of real existing ones and saves significant
distance for households, while there is a marginal penalty on the first level
food bank to food pantry distance. Overall, we believe that the second-level
benefits of this framework far outweigh any drawbacks and yield a net benefit
result.","['Gavin Ruan', 'Ziqi Guo', 'Guang Lin']",[],0,arXiv,http://arxiv.org/abs/2410.15420v1,False,True,False,False,False,2913,Christopher B Barrett,Cornell,Active,2024,,"Food insecurity is surprisingly common in the US; 10-15% of households experience food insecurity in any given year. But little is known about household food insecurity dynamics, i.e., whether food insecurity is an outcome that households experience only transitorily, for short periods of time, or a more chronic state and for which households food insecurity is chronic versus transitory. We use a new synthetic panel estimation method (Dang et al. 2014), a data-driven approach to estimate dynamics when we cannot directly observe the same households over many time periods (as in panel data).  We adapt the existing method to the rotating panel sampling design in the Current Population Survey, in particular its Food Security Supplements (CPS-FSS). We generate a new food insecurity dynamics measure we developed, the probability of food security (PFS, Lee et al. 2022), by comparing observed household food expenditures to the food expenditures needed to avoid food insecurity, as reflected in the USDA's Cost of the Thrifty Food Plan estimates. Combining the new method and measure permits us to estimate indicators of food insecurity dynamics among US households from 2000-2021. Specifically, we will estimate the prevalence of short-run, transitory food insecurity and of long-run, chronic food insecurity. We will estimate spell lengths for periods of food insecurity and the share of the food insecure who are chronically versus transitorily, food insecure, in both national and state-specific samples. We will establish which household attributes are most strongly associated with persistent versus transitory food insecurity, versus consistent food security."
Using Deep Learning to Find the Next Unicorn: A Practical Synthesis,"Startups often represent newly established business models associated with
disruptive innovation and high scalability. They are commonly regarded as
powerful engines for economic and social development. Meanwhile, startups are
heavily constrained by many factors such as limited financial funding and human
resources. Therefore, the chance for a startup to eventually succeed is as rare
as ""spotting a unicorn in the wild"". Venture Capital (VC) strives to identify
and invest in unicorn startups during their early stages, hoping to gain a high
return. To avoid entirely relying on human domain expertise and intuition,
investors usually employ data-driven approaches to forecast the success
probability of startups. Over the past two decades, the industry has gone
through a paradigm shift moving from conventional statistical approaches
towards becoming machine-learning (ML) based. Notably, the rapid growth of data
volume and variety is quickly ushering in deep learning (DL), a subset of ML,
as a potentially superior approach in terms of capacity and expressivity. In
this work, we carry out a literature review and synthesis on DL-based
approaches, covering the entire DL life cycle. The objective is a) to obtain a
thorough and in-depth understanding of the methodologies for startup evaluation
using DL, and b) to distil valuable and actionable learning for practitioners.
To the best of our knowledge, our work is the first of this kind.","['Lele Cao', 'Vilhelm von Ehrenheim', 'Sebastian Krakowski', 'Xiaoxue Li', 'Alexandra Lutz']",[],0,arXiv,http://arxiv.org/abs/2210.14195v2,False,True,False,False,False,2932,Yerodin Bermiss,Triangle,Active,2023,,"This research will examine how human capital influences startup performance. In particular, the research will study how the prior experience and backgrounds of founders and other employees interact with three different phenomena--startup employee departures, startup hiring distributions, and previous periods of unemployment for startup founders--and how such interactions influence startup performance on metrics such as sales and survival."
"Bridging the Generational Gap: Exploring How Virtual Reality Supports
  Remote Communication Between Grandparents and Grandchildren","When living apart, grandparents and grandchildren often use audio-visual
communication approaches to stay connected. However, these approaches seldom
provide sufficient companionship and intimacy due to a lack of co-presence and
spatial interaction, which can be fulfilled by immersive virtual reality (VR).
To understand how grandparents and grandchildren might leverage VR to
facilitate their remote communication and better inform future design, we
conducted a user-centered participatory design study with twelve pairs of
grandparents and grandchildren. Results show that VR affords casual and equal
communication by reducing the generational gap, and promotes conversation by
offering shared activities as bridges for connection. Participants preferred
resemblant appearances on avatars for conveying well-being but created ideal
selves for gaining playfulness. Based on the results, we contribute eight
design implications that inform future VR-based grandparent-grandchild
communications.","['Xiaoying Wei', 'Yizheng Gu', 'Emily Kuang', 'Xian Wang', 'Beiyan Cao', 'Xiaofu Jin', 'Mingming Fan']",[],0,arXiv,http://arxiv.org/abs/2302.14717v1,False,True,False,False,False,2939,Hongwei Xu,Baruch,Active,2023,,"The number of American grandparents living with grandchildren under age 18 in the same household has increased from 2.3 million in 1980 to 7 million in 2011-2013. Among these grandparents in 2019, 2.3 million provided custodial care of their coresident grandchildren, and about 1.3 million were still in the labor force to support their grandchildren. Previous research has documented the prevalence and trends of grandparents raising their coresident grandchildren. This project adds to the literature by examining the short-term and long-term mortality risks among coresident grandparents. To examine short-term mortality risks, we will identify coresident grandparents (and their caregiving responsibility) in the 2019-2022 American Community Survey (ACS) samples. We will then use the restricted Protected Identification Keys (PIKs) to link them to their death records from the 2022 Numident file. To examine long-term mortality risks, we will identify coresident grandparents (and their caregiving responsibility) in the 2000 Census long-form sample. We will again the restricted PIKs to link these coresident grandparents to their death records from the 2022 Numident file. For both short-term and long-term mortality risks, we will conduct descriptive and regression-based survival analyses to estimate the associations between different levels of coresident grandparenting (ranging from no caregiving to caring for grandchildren for 5 years or longer) and mortality. We will also examine the variations in these associations by race-ethnicity, gender, and immigration status."
"Introductory physics students' recognition of strong peers: Gender and
  racial/ethnic bias differ by course level and context","Researchers have pinpointed recognition from others as one of the most
important dimensions of students' science and engineering identity. Studies,
however, have found gender biases in students' recognition of their peers, with
inconsistent patterns across introductory science and engineering courses.
Toward finding the source of this variation, we examine whether a gender bias
exists in students' nominations of strong peers across three different remote,
introductory physics courses with varying student populations (varying
demographics, majors, and course levels). We also uniquely evaluate possible
racial/ethnic biases and probe the relationship between instructional context
(whether lecture or laboratory) and recognition. Some of our results replicate
previous findings (such as the the association of course grade and small class
section enrollment with nominations), while others offer contradictions.
Comparing across our three courses and the prior work, results suggest that
course level (whether first-year students or beyond-first-year students) might
be more associated with a gender bias in peer recognition than other variables.
Surprisingly, we also find instances of racial/ethnic biases in favor of
students from backgrounds historically underrepresented in science. Finally, we
find that the nomination patterns differ when students nominate individuals
strong in the lecture material versus laboratory material. This work serves as
an important step in determining which courses and contexts exhibit biases in
peer recognition, as well as how students' perceptions of one another form in
remote teaching environments.","['Meagan Sundstrom', 'Ashley B. Heim', 'Barum Park', 'N. G. Holmes']",[],0,arXiv,http://arxiv.org/abs/2212.00871v1,False,True,False,False,False,2951,Jessica H Hardie,Baruch,Active,2023,,"Despite recent upticks in 'deaths of despair,' Non-Hispanic White mortality rates have
remained lower than those of non-Hispanic Black and American Indian or Alaska Native.
Because structural racism funnels marginalized groups into jobs lacking health-promoting
workplace benefits, work amenities and conditions are likely to substantially contribute to racial/
ethnic disparities in health. The proposed study will use data from the Longitudinal Employer-
Household Dynamics (LEHD), Census Numident, Mortality Disparities in American
Communities (MDAC), Decennial Censuses, American Community Survey (ACS), Current
Population Surveys (CPS), Medical Expenditure Panel Survey (MEPS-IC), and O*NET to
examine how cumulative exposure to four sets of workplace context measures (job
amenities and hazards, workplace racial/ethnic composition, workplace area racial
segregation, and employment trajectories) contribute to racial/ethnic health and mortality
disparities."
Achieving an Efficient and Fair Equilibrium Through Taxation,"It is well known that a game equilibrium can be far from efficient or fair,
due to the misalignment between individual and social objectives. The focus of
this paper is to design a new mechanism framework that induces an efficient and
fair equilibrium in a general class of games. To achieve this goal, we propose
a taxation framework, which first imposes a tax on each player based on the
perceived payoff (income), and then redistributes the collected tax to other
players properly. By turning the tax rate, this framework spans the continuum
space between strategic interactions (of selfish players) and altruistic
interactions (of unselfish players), hence provides rich modeling
possibilities. The key challenge in the design of this framework is the proper
taxing rule (i.e., the tax exemption and tax rate) that induces the desired
equilibrium in a wide range of games. First, we propose a flat tax rate (i.e.,
a single tax rate for all players), which is necessary and sufficient for
achieving an efficient equilibrium in any static strategic game with common
knowledge. Then, we provide several tax exemption rules that achieve some
typical fairness criterions (such as the Max-min fairness) at the equilibrium.
We further illustrate the implementation of the framework in the game of
Prisoners' Dilemma.","['Lin Gao', 'Jianwei Huang']",[],0,arXiv,http://arxiv.org/abs/1708.03200v1,False,True,False,False,False,2956,William Hoyt,Kentucky,Active,2024,,"Beginning with Oates (1969) There exists a voluminous literature in economics on the effects of property taxation on housing prices and, to a much less extent, on how property taxes affect locational decisions. That property taxes affect housing prices is, it has been argued by Oates and others, is an indication that households incorporate local public policies, taxes and public expenditures, into their decisions of where to reside as argued by Tiebout (1956). Further, the relationship between how property taxes and levels of public services each affect property values has been used in numerous studies to ascertain whether public services are efficiently provided or not (Oates (1969), Brueckner (1979, 1982), Barrow and Rouse (2004), and Bayer, Blair, and Whalley (2021) among others). What has been ignored in virtually all this literature, except for Banzhaf et al (2021), is the extensive number and variety of property tax exemptions provided by states. For households eligible for these exemptions, their tax payment, the product of the assessed value of their home and the local property tax (millage) rate, is not, in fact, the taxes they pay on their home but something less. That these exemptions vary among states and by taxpayer characteristics, most importantly age, suggest that households might change where they reside (which state or type of residence in which they reside (rental vs. owner-occupied) to take advantage of these exemptions. This project aims to address this deficiency in the literature by estimating various hedonic pricing models to produce real estate value estimates as well as various population estimates including elasticities of migration in response to changes in property tax exemption generosity"
"Measuring the Impact of Taxes and Public Services on Property Values: A
  Double Machine Learning Approach","How do property prices respond to changes in local taxes and local public
services? Attempts to measure this, starting with Oates (1969), have suffered
from a lack of local public service controls. Recent work attempts to overcome
such data limitations through the use of quasi-experimental methods. We revisit
this fundamental problem, but adopt a different empirical strategy that pairs
the double machine learning estimator of Chernozhukov et al. (2018) with a
novel dataset of 947 time-varying local characteristic and public service
controls for all municipalities in Sweden over the 2010-2016 period. We find
that properly controlling for local public service and characteristic controls
more than doubles the estimated impact of local income taxes on house prices.
We also exploit the unique features of our dataset to demonstrate that tax
capitalization is stronger in areas with greater municipal competition,
providing support for a core implication of the Tiebout hypothesis. Finally, we
measure the impact of public services, education, and crime on house prices and
the effect of local taxes on migration.","['Isaiah Hull', 'Anna Grodecka-Messi']",[],0,arXiv,http://arxiv.org/abs/2203.14751v1,False,True,False,False,False,2956,William Hoyt,Kentucky,Active,2024,,"Beginning with Oates (1969) There exists a voluminous literature in economics on the effects of property taxation on housing prices and, to a much less extent, on how property taxes affect locational decisions. That property taxes affect housing prices is, it has been argued by Oates and others, is an indication that households incorporate local public policies, taxes and public expenditures, into their decisions of where to reside as argued by Tiebout (1956). Further, the relationship between how property taxes and levels of public services each affect property values has been used in numerous studies to ascertain whether public services are efficiently provided or not (Oates (1969), Brueckner (1979, 1982), Barrow and Rouse (2004), and Bayer, Blair, and Whalley (2021) among others). What has been ignored in virtually all this literature, except for Banzhaf et al (2021), is the extensive number and variety of property tax exemptions provided by states. For households eligible for these exemptions, their tax payment, the product of the assessed value of their home and the local property tax (millage) rate, is not, in fact, the taxes they pay on their home but something less. That these exemptions vary among states and by taxpayer characteristics, most importantly age, suggest that households might change where they reside (which state or type of residence in which they reside (rental vs. owner-occupied) to take advantage of these exemptions. This project aims to address this deficiency in the literature by estimating various hedonic pricing models to produce real estate value estimates as well as various population estimates including elasticities of migration in response to changes in property tax exemption generosity"
"Association Between Neighborhood Factors and Adult Obesity in Shelby
  County, Tennessee: Geospatial Machine Learning Approach","Obesity is a global epidemic causing at least 2.8 million deaths per year.
This complex disease is associated with significant socioeconomic burden,
reduced work productivity, unemployment, and other social determinants of
Health (SDoH) disparities. Objective: The objective of this study was to
investigate the effects of SDoH on obesity prevalence among adults in Shelby
County, Tennessee, USA using a geospatial machine-learning approach. Obesity
prevalence was obtained from publicly available CDC 500 cities database while
SDoH indicators were extracted from the U.S. Census and USDA. We examined the
geographic distributions of obesity prevalence patterns using Getis-Ord Gi*
statistics and calibrated multiple models to study the association between SDoH
and adult obesity. Also, unsupervised machine learning was used to conduct
grouping analysis to investigate the distribution of obesity prevalence and
associated SDoH indicators. Results depicted a high percentage of neighborhoods
experiencing high adult obesity prevalence within Shelby County. In the census
tract, median household income, as well as the percentage of individuals who
were black, home renters, living below the poverty level, fifty-five years or
older, unmarried, and uninsured, had a significant association with adult
obesity prevalence. The grouping analysis revealed disparities in obesity
prevalence amongst disadvantaged neighborhoods. More research is needed that
examines linkages between geographical location, SDoH, and chronic diseases.
These findings, which depict a significantly higher prevalence of obesity
within disadvantaged neighborhoods, and other geospatial information can be
leveraged to offer valuable insights informing health decision-making and
interventions that mitigate risk factors for increasing obesity prevalence.","['Whitney S Brakefield', 'Olufunto A Olusanya', 'Arash Shaban-Nejad']",[],0,arXiv,http://arxiv.org/abs/2208.05335v1,False,True,False,False,False,2970,Latetia V Moore Freeman,Atlanta,Active,2023,,"Obesity is common, serious, and costly. Local data is needed to inform program interventions but traditional survey methods for obtaining local estimates of obesity nationwide are too costly. We propose to estimate the prevalence of childhood obesity prevalence for every U.S. county using previously developed methods and the 2020-2021 National Survey of Children's Health (NSCH). We will first construct a multilevel logistic regression model to evaluate the influence of child demographic characteristics and area level characteristics (block group, county, and state) on childhood obesity. We then estimate the obesity risk for a child in each census block group based on this multilevel model and obtain county level obesity estimates using a post stratification approach. We will use population level statistics generated to identify areas of high need and where grantees from CDC's High Obesity Program may want to collaborate with local and state practitioners to implement programs to reduce obesity."
"Semi-Parametric Empirical Best Prediction for small area estimation of
  unemployment indicators","The Italian National Institute for Statistics regularly provides estimates of
unemployment indicators using data from the Labor Force Survey. However, direct
estimates of unemployment incidence cannot be released for Local Labor Market
Areas. These are unplanned domains defined as clusters of municipalities; many
are out-of-sample areas and the majority is characterized by a small sample
size, which render direct estimates inadequate. The Empirical Best Predictor
represents an appropriate, model-based, alternative. However, for non-Gaussian
responses, its computation and the computation of the analytic approximation to
its Mean Squared Error require the solution of (possibly) multiple integrals
that, generally, have not a closed form. To solve the issue, Monte Carlo
methods and parametric bootstrap are common choices, even though the
computational burden is a non trivial task. In this paper, we propose a
Semi-Parametric Empirical Best Predictor for a (possibly) non-linear mixed
effect model by leaving the distribution of the area-specific random effects
unspecified and estimating it from the observed data. This approach is known to
lead to a discrete mixing distribution which helps avoid unverifiable
parametric assumptions and heavy integral approximations. We also derive a
second-order, bias-corrected, analytic approximation to the corresponding Mean
Squared Error. Finite sample properties of the proposed approach are tested via
a large scale simulation study. Furthermore, the proposal is applied to
unit-level data from the 2012 Italian Labor Force Survey to estimate
unemployment incidence for 611 Local Labor Market Areas using auxiliary
information from administrative registers and the 2011 Census.","['Maria Francesca Marino', 'Maria Giovanna Ranalli', 'Nicola Salvati', ""Marco Alfo'""]",[],0,arXiv,http://arxiv.org/abs/1704.02220v2,False,True,False,False,False,2985,Maria Zhu,Cornell,Active,2023,,"This project assesses the labor market returns to double majoring in college compared to single majoring. This is an especially important question as double majoring has become an increasingly common phenomenon in the US over the past few decades. To do so, we use data from the National Survey of College Graduates to compare the earnings of students who double major with earnings of peers who graduated from the same institution and who are observationally similar but single major instead. To address concerns that students sort into major type (double versus single) along unobserved characteristics that may affect labor market outcomes, we use a partial identification method to obtain bounded estimates for the causal effect of double majoring, which accounts for the role of selection on unobserved characteristics. We hypothesize that double majoring will lead to slightly higher earnings by providing students with a broader skillset that provides them with more options in the labor market."
Going Green: A Holistic Approach to Transform Business,"In recent years environmental and energy conservation issues have taken the
central theme in the global business arena. The reality of rising energy cost
and their impact on international affairs coupled with the different kinds of
environmental issues has shifted the social and economic consciousness of the
business community. Hence, the business community is now in search of an
eco-friendly business model. This paper highlights the concept of green
business and their needs in the current global scenario.","['Sajal Kabiraj', 'Vinay Topkar', 'R. C. Walke']",[],0,arXiv,http://arxiv.org/abs/1009.0844v1,False,True,False,False,False,2998,Earnest Curtis,Triangle,Active,2023,,"This research will use internal Census microdata on firms and workers to produce population estimates that investigate two broad sets of questions: (1) How do plants adjust output and employment in response to changes in the business environment, changes in environmental and energy prices and other shocks; Which types of workers are impacted by these adjustments and do firms shift employment and output from plants located in a region that receives a negative environmental or input cost shock to plants located in regions that did not receive the shock? (2) What firm, plant and manager characteristics correlate with plant and worker outcomes such as energy intensity, pollution intensity, productivity and earnings?"
"Creating an institutional ecosystem for cash transfer programming:
  Lessons from post-disaster governance in Indonesia","Humanitarian and disaster management actors have increasingly adopted cash
transfer to reduce the sufferings and vulnerability of the survivors. Case
transfers have also been used as a critical instrument in the current COVID-19
pandemic. Unfortunately, academic work on humanitarian and disaster-cash
transfer related issues remains limited. This article explores how NGOs and
governments implement humanitarian cash transfer in a post-disaster setting
using an exploratory research strategy. It asks What are institutional
constraints and opportunities faced by humanitarian emergency responders in
ensuring an effective humanitarian cash transfer and how humanitarian actors
address such institutional conditions. We introduced a new conceptual
framework, namely humanitarian and disaster management ecosystem for cash
transfer. This framework allows non-governmental actors to restore complex
relations between the state, disaster survivors or citizen, local market
economy and civil society. Mixed methods and multistage research strategy were
used to collect and analyze primary and secondary data. The findings suggest
that implementing cash transfers in the context of post tsunamigenic
earthquakes and liquefaction hazards, NGOs must co-create an ecosystem of
response that not only aimed at restoring peoples access to cash and basic
needs but first they must restore relations between the states and their
citizen while linking the at-risk communities with the private sectors to
jump-starting local livelihoods and market economy.","['Jonatan A. Lassa', 'Gisela Emanuela Nappoe', 'Susilo Budhi Sulistyo']",[],0,arXiv,http://arxiv.org/abs/2202.04811v1,False,True,False,False,False,3011,Sarah M Miller,Michigan,Active,2024,,"Policymakers have increasingly turned to cash transfers to alleviate poverty, improve outcomes among the disadvantaged, and reduce inequality. However, little is known about how such transfers may affect labor market outcomes, health, childbearing, or use of other public programs. While several studies have investigated these questions in the context of developing (e.g., Haushofer and Shapiro 2016) or European (e.g., Cesarini et al. 2016) countries, less is known about the impact of these transfers in the United States. This study will provide new, timely evidence on this topic by analyzing the impact of a large-scale randomized controlled trial using data that links information on study participants to Census-held administrative records on mortality, use of public programs, labor market participation, and childbearing."
The Policy Paradox: Government Debt Servicing and Local Bank Risk Growth,"The issue of local government debt is widely recognized as one of the ""gray
rhinos"" affecting the stable development of China's economy. Government debt
can transmit risks to local banks, which are among the primary holders of local
debt, thereby triggering systemic financial risks. Consequently, exploring debt
resolution pathways and evaluating the systematic effects of debt servicing
policies has become critically important. This study employs panel data from
348 local commercial banks across 29 provincial-level administrative regions in
China from 2010 to 2023, and constructs a difference-in-differences (DID) model
to investigate the impact of the State Council's special supervision of debt
servicing on local bank risks. The findings indicate that the government's debt
servicing policy essentially represents a shift of government debt from
explicit to implicit forms, significantly increasing the risks faced by local
banks and producing outcomes contrary to the policy's original intent. This
effect is particularly pronounced for rural commercial banks and banks with
high customer concentration and fewer branches. Mechanism analysis reveals two
key insights. First, local banks are heavily influenced by local government
control; the government's debt servicing requires banks to support the
government by purchasing government bonds and other financial instruments,
which leads to a deterioration in asset quality and an expansion of risk
exposure. Second, government debt crowds out private credit from local banks,
weakening the region's repayment capacity and ultimately increasing bank risk.
Our research uncovers the counterintuitive effects of government debt servicing
and offers corresponding policy recommendations.",['Yan Li'],[],0,arXiv,http://arxiv.org/abs/2502.13423v1,False,True,False,False,False,3020,Vyacheslav Mikhed,Philadelphia,Active,2024,,"This research will benefit the U.S. Census Bureau by linking external credit bureau data to Census data and will help the Census Bureau to understand and improve the quality of data produced through a Title 13, Chapter 5. It will also prepare estimates of population and characteristics of population as authorized under Title 13, Chapter 5. In particular, the project will verify information contained in the Longitudinal Employer-Household Dynamics (LEHD) data set and American Community Survey (ACS) data set. It will also generate new estimates of characteristics of population as described below. The researchers will use individual address information (available down to individual's census block, tract, and ZIP code) contained in the credit bureau data to verify employee address information in the LEHD. In addition, the researchers will use information on identity theft and fraud provided in the credit bureau data to verify the LEHD data at the time of such incidents. Finally, the credit data include mortgage information for each individual (including payments, balance, delinquency), which will be used to verify ACS mortgage data (which is allocated at higher rates) and provide the Census with methods to improve the quality of these data.

This research project seeks to examine the effects of debt overhang on individual labor outcomes, as well as new business creation and growth. As a result of these analyses, we will provide the Census Bureau with new estimates of population characteristics not available and examined in the existing literature. We define debt overhang in a few different ways. First, we would like to examine how a high and growing student debt burden affects earnings, job search, and entrepreneurial activity of individual borrowers. Second, we would like to examine how changes to mortgage payment schedules (because of loan modifications, forbearance, or interest rate changes) can affect homeowners' mobility decisions, labor supply, earnings and entrepreneurial outcomes. Third, we would like to use credit bureau data on third-party collections, tax liens, and wage garnishments to examine how creditors' adverse legal actions and collection practices, which effectively reduce take-home pay for affected individuals, change individual labor supply, overtime work, and attachment to the labor market.

This research project will use the following restricted Census datasets: Integrated Longitudinal Business Database (ILBD), Longitudinal Business Database (LBD), LEHD Employer Characteristics File (ECF), LEHD Individual Characteristics File (ICF), Longitudinal Business Database - With EIN and Revenue (LBDREV), American Community Survey (ACS), LEHD Employment History File (EHF), LEHD Individual Characteristics File (ICF), Decennial Census, CoreLogic and Black Knight data (if / when available). All LEHD, LBD, and ILBD files are requested for the years 1976-2022 (if / when available). ACS data are requested for 2005-2020, 2021-2025. Decennial Census data are needed for the years 2000 and 2010."
"Aligning Technical Debt Prioritization with Business Objectives: A
  Multiple-Case Study","Technical debt (TD) is a metaphor to describe the trade-off between
short-term workarounds and long-term goals in software development. Despite
being widely used to explain technical issues in business terms, industry and
academia still lack a proper way to manage technical debt while explicitly
considering business priorities. In this paper, we report on a multiple-case
study of how two big software development companies handle technical debt
items, and we show how taking the business perspective into account can improve
the decision making for the prioritization of technical debt. We also propose a
first step toward an approach that uses business process management (BPM) to
manage technical debt. We interviewed a set of IT business stakeholders, and we
collected and analyzed different sets of technical debt items, comparing how
these items would be prioritized using a purely technical versus a
business-oriented approach. We found that the use of business process
management to support technical debt management makes the technical debt
prioritization decision process more aligned with business expectations. We
also found evidence that the business process management approach can help
technical debt management achieve business objectives.","['Rodrigo Rebouças de Almeida', 'Uirá Kulesza', 'Christoph Treude', ""D'angellys Cavalcanti Feitosa"", 'Aliandro Higino Guedes Lima']",[],0,arXiv,http://arxiv.org/abs/1807.05582v1,False,True,False,False,False,3020,Vyacheslav Mikhed,Philadelphia,Active,2024,,"This research will benefit the U.S. Census Bureau by linking external credit bureau data to Census data and will help the Census Bureau to understand and improve the quality of data produced through a Title 13, Chapter 5. It will also prepare estimates of population and characteristics of population as authorized under Title 13, Chapter 5. In particular, the project will verify information contained in the Longitudinal Employer-Household Dynamics (LEHD) data set and American Community Survey (ACS) data set. It will also generate new estimates of characteristics of population as described below. The researchers will use individual address information (available down to individual's census block, tract, and ZIP code) contained in the credit bureau data to verify employee address information in the LEHD. In addition, the researchers will use information on identity theft and fraud provided in the credit bureau data to verify the LEHD data at the time of such incidents. Finally, the credit data include mortgage information for each individual (including payments, balance, delinquency), which will be used to verify ACS mortgage data (which is allocated at higher rates) and provide the Census with methods to improve the quality of these data.

This research project seeks to examine the effects of debt overhang on individual labor outcomes, as well as new business creation and growth. As a result of these analyses, we will provide the Census Bureau with new estimates of population characteristics not available and examined in the existing literature. We define debt overhang in a few different ways. First, we would like to examine how a high and growing student debt burden affects earnings, job search, and entrepreneurial activity of individual borrowers. Second, we would like to examine how changes to mortgage payment schedules (because of loan modifications, forbearance, or interest rate changes) can affect homeowners' mobility decisions, labor supply, earnings and entrepreneurial outcomes. Third, we would like to use credit bureau data on third-party collections, tax liens, and wage garnishments to examine how creditors' adverse legal actions and collection practices, which effectively reduce take-home pay for affected individuals, change individual labor supply, overtime work, and attachment to the labor market.

This research project will use the following restricted Census datasets: Integrated Longitudinal Business Database (ILBD), Longitudinal Business Database (LBD), LEHD Employer Characteristics File (ECF), LEHD Individual Characteristics File (ICF), Longitudinal Business Database - With EIN and Revenue (LBDREV), American Community Survey (ACS), LEHD Employment History File (EHF), LEHD Individual Characteristics File (ICF), Decennial Census, CoreLogic and Black Knight data (if / when available). All LEHD, LBD, and ILBD files are requested for the years 1976-2022 (if / when available). ACS data are requested for 2005-2020, 2021-2025. Decennial Census data are needed for the years 2000 and 2010."
"Exploring the Effects of Population and Employment Characteristics on
  Truck Flows: An Analysis of NextGen NHTS Origin-Destination Data","Truck transportation remains the dominant mode of US freight transportation
because of its advantages, such as the flexibility of accessing pickup and
drop-off points and faster delivery. Because of the massive freight volume
transported by trucks, understanding the effects of population and employment
characteristics on truck flows is critical for better transportation planning
and investment decisions. The US Federal Highway Administration published a
truck travel origin-destination data set as part of the Next Generation
National Household Travel Survey program. This data set contains the total
number of truck trips in 2020 within and between 583 predefined zones
encompassing metropolitan and nonmetropolitan statistical areas within each
state and Washington, DC. In this study, origin-destination-level truck trip
flow data was augmented to include zone-level population and employment
characteristics from the US Census Bureau. Census population and County
Business Patterns data were included. The final data set was used to train a
machine learning algorithm-based model, Extreme Gradient Boosting (XGBoost),
where the target variable is the number of total truck trips. Shapley Additive
ExPlanation (SHAP) was adopted to explain the model results. Results showed
that the distance between the zones was the most important variable and had a
nonlinear relationship with truck flows.","['Majbah Uddin', 'Yuandong Liu', 'Hyeonsup Lim']",[],0,arXiv,http://arxiv.org/abs/2402.04019v1,True,True,False,False,False,3031,Darryl Cooney,Triangle,Active,2024,,"This research will benefit the U.S. Census Bureau as it generates population estimates during the  process of: (1) improving the linkage methodology for two types of Census Bureau-collected surveys; (2) linking currently unlinked data from the National Survey of College Graduates (NSCG) to the Census Bureau's internal business data, providing new information on foreign-born scientists and engineers; and (3) comparing two parallel linking processes for matching the NSCG data to the Census Bureau's business data and Longitudinal Employer-Household Dynamics (LEHD) program data.
The researchers will use both direct and probabilistic entity resolution (i.e., record linkage) to match employer information in the NSCG to U.S. Census Bureau employer records, using data from the LEHD and County Business Patterns Business Register (CBPBR) for employer matching. The Longitudinal Business Database (LBD) will be used to assist with the matching effort, especially in terms of tracking establishments and firms over time. Links between the NSCG and LBD/CBPBR will be performed probabilistically based on employer information reported in the NSCG. Links between the NSCG and LEHD data will be made directly at the employee-level using Protected Identification Key (PIK) information available in both data sources. For cases where a PIK match can be made,  the accuracy of these ""employee"" matches can be straightforwardly compared at employer-level since both linking processes can be employed side-by-side. Correspondingly, one Census benefit of the project will be the comparison of the accuracy for the two methods as well as verification of one process (when possible) through use of the other. The end result will ultimately enable data-driven analysis of foreign-born scientist and engineer (FBSE) employment in the context of immigration status, degree fields, and industry sectors, improving the analytic utility of the NSCG data for future users. Moreover, the Census Bureau's LEHD and LBD data, as well as other business data that can be linked to the LBD, will be enhanced going forward through the ability to link those data sources to the NSCG. The researchers will provide the Bureau with a technical memorandum summarizing the results of the linking process, and comparisons made during that process, to summarize the benefits provided.
The effort will link databases utilizing the approach of d-blink, which proposes a scalable unsupervised joint approach for blocking and entity resolution. This approach will handle missing information and will be more accurate in situations where firm ownership, names or addresses change over time. The d-blink approach can also simultaneously merge more than two databases, is based on Bayesian methods and is less sensitive to the choice of matching threshold and will provide uncertainty quantification from the entity resolution process. In contrast to existing methods for canonicalization, our proposed approach does not rely on training data and can handle categorical, ordinal, and numerical attributes. By performing each stage--entity resolution, canonicalization, and downstream task--in a Bayesian framework, uncertainty is propagated throughout and properly accounted for when reaching final conclusions. This statistical solution will enable data-driven analysis of foreign-born scientist and engineer (FBSE) employment in the context of their immigration status, degree fields, and the industry sectors in which they are employed. The research will ultimately improve the analytic utility of the NSCG data for Census Bureau stakeholders, both internal and external."
Building Consistent Regression Trees From Complex Sample Data,"In the past several years a wide range of methods for the construction of
regression trees and other estimators based on the recursive partitioning of
samples have appeared in the statistics literature. Many applications involve
data collected through a complex sample design. At present, however, relatively
little is known regarding the properties of these methods under complex
designs. This article proposes a method for incorporating information about the
complex sample design when building a regression tree using a recursive
partitioning algorithm. Sufficient conditions are established for asymptotic
design L 2 consistency of these regression trees as estimators for an arbitrary
regression function. The proposed method is illustrated with Occupational
Employment Statistics establishment survey data linked to Quarterly Census of
Employment and Wage payroll data of the Bureau of Labor Statistics. Performance
of the nonparametric estimator is investigated through a simulation study based
on this example.","['Daniell Toth', 'John Eltinge']",[],0,arXiv,http://arxiv.org/abs/1407.1079v1,False,True,False,False,False,3031,Darryl Cooney,Triangle,Active,2024,,"This research will benefit the U.S. Census Bureau as it generates population estimates during the  process of: (1) improving the linkage methodology for two types of Census Bureau-collected surveys; (2) linking currently unlinked data from the National Survey of College Graduates (NSCG) to the Census Bureau's internal business data, providing new information on foreign-born scientists and engineers; and (3) comparing two parallel linking processes for matching the NSCG data to the Census Bureau's business data and Longitudinal Employer-Household Dynamics (LEHD) program data.
The researchers will use both direct and probabilistic entity resolution (i.e., record linkage) to match employer information in the NSCG to U.S. Census Bureau employer records, using data from the LEHD and County Business Patterns Business Register (CBPBR) for employer matching. The Longitudinal Business Database (LBD) will be used to assist with the matching effort, especially in terms of tracking establishments and firms over time. Links between the NSCG and LBD/CBPBR will be performed probabilistically based on employer information reported in the NSCG. Links between the NSCG and LEHD data will be made directly at the employee-level using Protected Identification Key (PIK) information available in both data sources. For cases where a PIK match can be made,  the accuracy of these ""employee"" matches can be straightforwardly compared at employer-level since both linking processes can be employed side-by-side. Correspondingly, one Census benefit of the project will be the comparison of the accuracy for the two methods as well as verification of one process (when possible) through use of the other. The end result will ultimately enable data-driven analysis of foreign-born scientist and engineer (FBSE) employment in the context of immigration status, degree fields, and industry sectors, improving the analytic utility of the NSCG data for future users. Moreover, the Census Bureau's LEHD and LBD data, as well as other business data that can be linked to the LBD, will be enhanced going forward through the ability to link those data sources to the NSCG. The researchers will provide the Bureau with a technical memorandum summarizing the results of the linking process, and comparisons made during that process, to summarize the benefits provided.
The effort will link databases utilizing the approach of d-blink, which proposes a scalable unsupervised joint approach for blocking and entity resolution. This approach will handle missing information and will be more accurate in situations where firm ownership, names or addresses change over time. The d-blink approach can also simultaneously merge more than two databases, is based on Bayesian methods and is less sensitive to the choice of matching threshold and will provide uncertainty quantification from the entity resolution process. In contrast to existing methods for canonicalization, our proposed approach does not rely on training data and can handle categorical, ordinal, and numerical attributes. By performing each stage--entity resolution, canonicalization, and downstream task--in a Bayesian framework, uncertainty is propagated throughout and properly accounted for when reaching final conclusions. This statistical solution will enable data-driven analysis of foreign-born scientist and engineer (FBSE) employment in the context of their immigration status, degree fields, and the industry sectors in which they are employed. The research will ultimately improve the analytic utility of the NSCG data for Census Bureau stakeholders, both internal and external."
Strain Relaxation via Phase Transformation in SrSnO3,"SrSnO3 (SSO) is an emerging ultra-wide bandgap (UWBG) semiconductor with
potential for highpower applications. In-plane compressive strain was recently
shown to stabilize the high temperature tetragonal phase of SSO at room
temperature (RT) which exists at T > 1062 K in bulk. Here, we report on the
study of strain relaxation in epitaxial, tetragonal phase of Nd-doped SSO films
grown on GdScO3 (110) (GSO) substrates using radical-based hybrid molecular
beam epitaxy. The thinnest SSO film (thickness, t = 12 nm) yielded a fully
coherent tetragonal phase at RT. At 12 nm < t < 110 nm, the tetragonal phase
first transformed into orthorhombic phase and then at t > 110 nm, the
orthorhombic phase began to relax by forming misfit dislocations. Remarkably,
the tetragonal phase remained fully coherent until it completely transformed
into the orthorhombic phase. Using thickness- and temperature-dependent
electronic transport measurements, we discuss the important roles of the
surface, phase coexistence, and misfit dislocations on carrier density and
mobility in Nd-doped SSO. This study provides unprecedented insights into the
strain relaxation behavior and its consequences for electronic transport in
doped SSO with implications in the development of high-power electronic
devices.","['Tristan K Truttmann', 'Fengdeng Liu', 'Javier Garcia Barriocanal', 'Richard D. James', 'Bharat Jalan']",[],0,arXiv,http://arxiv.org/abs/2010.09817v1,False,True,False,False,False,3042,Bharat K Chandar,Stanford,Active,2024,,"I develop a dynamic economic geography model of spatially distinct labor markets. The model fea- tures mobile workers between regions and industries, input-output relationships between sectors, and producitivity spillovers, three forces that determine patterns of agglomeration and coagglomer- ation in the economy. Firms are multiestablishment and endogenously choose where to hire workers for R&D development and for production given their productivity and trade costs. I estimate the frictions in the model using panel microdata to track flows of workers and firms between regions following local labor supply shocks. I study how regional industrial policies enacted by the U.S. government impact the distribution of economic activity across space. The effects of these policies depend on the labor, input-output, and productivity spillovers across industries and regions esti- mated in my model. Further, the policies affect where firms open R&D branches and where they open production branches, which impact regional price and wage inequality."
"Coupling Coordinated Development among Digital Economy, Regional
  Innovation and Talent Employment A case study of Hangzhou Metropolitan
  Circle, China","Coordination development across various subsystems, particularly economic,
social, cultural, and human resources subsystems, is a key aspect of urban
sustainability that has a direct impact on the quality of urbanization.
Hangzhou Metropolitan Circle composing Hangzhou, Huzhou, Jiaxing, Shaoxing, was
the first metropolitan circle approved by National Development and Reform
Commission (NDRC) as a demonstration of economic transformation. To evaluate
the coupling degree of the four cities and to analyze the coordinative
development in the three systems (Digital Economy System, Regional Innovation
System, and Talent Employment System), panel data of these four cities during
the period 2015-2022 were collected. The development level of these three
systems were evaluated using standard deviation and comprehensive development
index evaluation. The results are as follows: (1) the coupling coordination
degree of the four cities in Hangzhou Metropolitan Circle has significant
regional differences, with Hangzhou being a leader while Huzhou, Jiaxing,
Shaoxing have shown steady but slow progress in the coupling development of the
three systems; and (2) the development of digital economy and talent employment
are the breakthrough points for construction in Huzhou, Jiaxing, Shaoxing.
Related suggestions are made based on the coupling coordination results of the
Hangzhou Metropolitan Circle.",['Luyi Qiu'],[],0,arXiv,http://arxiv.org/abs/2310.19008v1,False,True,False,False,False,3044,Luyi Han,Penn State,Active,2023,,"Regional economic development crucially depends on human resources and natural resources.  In particular, highly-educated professionals and entrepreneurs are expected to have important effects creating local jobs and increasing incomes and well-being for themselves and others.  Primary, secondary, and higher education institutions help build, attract, and retain local human capital, but access to high-quality education institutions varies considerably across geographic areas and lack of educational access is expected to especially hinder non-metropolitan areas and smaller metropolitan areas.  Furthermore, natural resources play an important role in job creation for less densely populated areas.  However, some natural resources may enhance human resources, while other natural resources may hinder the creation and retention of human capital.  For example, mountains, lakes, and parks may be especially attractive natural amenities that attract and retain skilled workers and entrepreneurs, while oil and gas fracking activity creates jobs but also environmental disamenities that may push away skilled workers and entrepreneurs that are vital for long-run economic development.  We propose to investigate these and related issues to better understand how human resources and natural resources shape regional economic activity using restricted access microdata from the American Community Survey (ACS) and the 2000 decennial census long-form surveys.  Restricted access data are critical for this analysis because publicly available microdata do not identify counties with population less than 100,000 and do not identify the specific county where individuals were born.  Our project will deliver benefit to the Census Bureau by analyzing demographic, social, and economic processes that affect Census bureau programs (Criterion 2) and creating new estimates of population characteristics as authorized under Title 13, Chapter 5 (Criterion 11)."
"Solving Word-Sense Disambiguation and Word-Sense Induction with
  Dictionary Examples","Many less-resourced languages struggle with a lack of large, task-specific
datasets that are required for solving relevant tasks with modern
transformer-based large language models (LLMs). On the other hand, many
linguistic resources, such as dictionaries, are rarely used in this context
despite their large information contents. We show how LLMs can be used to
extend existing language resources in less-resourced languages for two
important tasks: word-sense disambiguation (WSD) and word-sense induction
(WSI). We approach the two tasks through the related but much more accessible
word-in-context (WiC) task where, given a pair of sentences and a target word,
a classification model is tasked with predicting whether the sense of a given
word differs between sentences. We demonstrate that a well-trained model for
this task can distinguish between different word senses and can be adapted to
solve the WSD and WSI tasks. The advantage of using the WiC task, instead of
directly predicting senses, is that the WiC task does not need pre-constructed
sense inventories with a sufficient number of examples for each sense, which
are rarely available in less-resourced languages. We show that sentence pairs
for the WiC task can be successfully generated from dictionary examples using
LLMs. The resulting prediction models outperform existing models on WiC, WSD,
and WSI tasks. We demonstrate our methodology on the Slovene language, where a
monolingual dictionary is available, but word-sense resources are tiny.","['Tadej Škvorc', 'Marko Robnik-Šikonja']",[],0,arXiv,http://arxiv.org/abs/2503.04328v1,False,True,False,False,False,3059,Colleen Heflin,Cornell,Active,2024,,"A substantial body of research suggests reducing administrative burdens in safety net programs can improve uptake. But some scholars have emphasized that certain administrative procedures support, rather than burden, program participants. We investigate this potential tradeoff in the context of physical presence requirements in the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC). Opponents of physical presence requirements in WIC emphasize their disenrollment effects, while proponents argue they provide WIC participants with important information and referrals to other resources. Using restricted SIPP and WIC administrative records, merged with restricted SNAP, TANF and CPS FSS files, we exploit geographic and time variation in the use of physical presence requirement waivers among local WIC agencies during COVID-19 to examine their effects on program participation outcomes, emergency nutrition assistance and healthcare use. To investigate whether physical presence requirements preclude program uptake, we first estimate whether WIC enrollment increased following the availability of in-person requirement waivers for waiver counties relative to nonwaiver counties. Next, we examine whether WIC enrollees in waiver counties were less likely to redeem WIC benefits, participate in SNAP or TANF, receive emergency food assistance, or utilize healthcare. If in-person appointments support connecting WIC enrollees to other resources, we should find that waivers reduced WIC benefit redemption, multiple program participation, and emergency food assistance and healthcare use."
Experimental realization of Counterfactual Quantum Cryptography,"In counterfactual QKD information is transfered, in a secure way, between
Alice and Bob even when no particle carrying the information is in fact
transmitted between them. In this letter we fully implement the scheme for
counterfactual QKD proposed in [T. Noh, \PRL \textbf{103}, 230501 (2009)],
demonstrating for the first time that information can be transmitted between
two parties without the transmission of a carrier.","['Giorgio Brida', 'Andrea Cavanna', 'Ivo Pietro Degiovanni', 'Marco Genovese', 'Paolo Traina']",[],0,arXiv,http://arxiv.org/abs/1107.5467v1,False,True,False,False,False,3069,James Traina,Berkeley,Active,2024,,"This project delves into the intricate dynamics of distortions and their impact on productivity measurement across diverse sectors, including manufacturing, trade, and services. Leveraging the granularity of restricted-use microdata, this study employs a novel micro-econometric approach to uncover productions functions, productivity estimates, and distortions. The central hypothesis of the research is that distortions significantly influence productivity measurements, with these influences varying across sectors due to the specific types of distortions prevalent in each market. Additionally, this study posits several secondary hypotheses for testing: distortions may be larger in service sectors compared to manufacturing and trade sectors; less capital-intensive industries may exhibit different manifestations and sensitivities to distortions; and the impact of distortions on productivity may be moderated by firm-specific factors such as size, age, and management practices. The final analysis will provide a nuanced understanding of the interplay between distortions and productivity. The findings are expected to contribute to the broader discourse on productivity measurement and distortions, improving Census estimates of sectoral productivity."
